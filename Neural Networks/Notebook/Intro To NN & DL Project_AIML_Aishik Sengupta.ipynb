{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_KYX8TDDCsO"
   },
   "source": [
    "## The Problem Description:\n",
    "\n",
    "In this hands-on project, the goal is to build a Face Mask Segmentation model which includes building a face detector to locate the position of a face in an image.\n",
    "\n",
    "\n",
    "## Data Description:\n",
    "\n",
    "### ***WIDER Face Dataset*** \n",
    "\n",
    "WIDER FACE dataset is a Face Mask Segmentation benchmark dataset, of which images are selected from the publicly available WIDER dataset. This data have 32,203 images and 393,703 faces are labeled with a high degree of variability in scale, pose and occlusion as depicted in the sample images.\n",
    "In this project, we are using 409 images and around 1000 faces for ease of computation.\n",
    "We will be using transfer learning on an already trained model to build our segmenter. We will perform transfer learning on the MobileNet model which is already trained to perform image segmentation. We will need to train the last 6-7 layers and freeze the remaining layers to train the model for face mask segmentation. To be able to train the MobileNet model for face mask segmentation, we will be using the WIDER FACE dataset for various images with a single face and multiple faces. The output of the model is the face mask segmented data which masks the face in an image. We learn to build a face mask segmentation model using Keras supported by Tensorflow.\n",
    "\n",
    "**Link to the dataset:** \n",
    "https://drive.google.com/file/d/1L2-WXzguhUsCArrFUc8EEkXcj33pahoS/view?usp=sharing\n",
    "\n",
    "\n",
    "**Acknowledgement for the datasets.** \n",
    "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011. PDF http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary\n",
    "\n",
    "\n",
    "## Objective:\n",
    "\n",
    "In this problem, we use \"Transfer Learning\" of an Image Segmentation model to detect any object according to the problem in hand.\n",
    "Here, we are particularly interested in segmenting faces in a given image:\n",
    "Steps and tasks\n",
    "1. Load the dataset given in form .npy format.\n",
    "2. Create Features(images) and labels(mask) using that data.\n",
    "3. Load the pre-trained model and weights.\n",
    "4. Create a model using the above model.\n",
    "5. Define the Dice Coefficient and Loss function.\n",
    "6. Compile and fit the model.\n",
    "7. Evaluate the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2188,
     "status": "ok",
     "timestamp": 1594530326658,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "zpWXBLFpDE-Q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31163,
     "status": "ok",
     "timestamp": 1594530360142,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "5gv1u4ySDrr_",
    "outputId": "89de9af6-cf43-49b0-a8ab-5dc007a6cd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/gdrive') # mounting for storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxFBaqEba8hF"
   },
   "source": [
    "Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3199,
     "status": "ok",
     "timestamp": 1594530468796,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "-UlX-KTsE1tE",
    "outputId": "8d435d0e-9ff9-4afe-f489-269cd776b05c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report,multilabel_confusion_matrix,confusion_matrix,accuracy_score\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Activation,Dense,BatchNormalization,Dropout,LeakyReLU\n",
    "from tensorflow.keras import regularizers,optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9czLd97EbCgc"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9362,
     "status": "ok",
     "timestamp": 1594530491898,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "FSHCEFzzGbDl"
   },
   "outputs": [],
   "source": [
    "# To load train set, validation set and test set\n",
    "\n",
    "h5f = h5py.File('/content/gdrive/My Drive/AIML_Colab/NN Assignment 1/SVHN_single_grey1.h5','r')\n",
    "\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "X_val = h5f['X_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "# saving a copy\n",
    "Xtrain = X_train\n",
    "ytrain = y_train\n",
    "Xtest = X_test\n",
    "ytest = y_test\n",
    "Xval = X_val\n",
    "yval = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1594530509575,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "qO3ioS8uGxVn",
    "outputId": "f1d71fda-0dfa-4ac3-c83c-9c571b9d74f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Train Set are (42000, 32, 32)\n",
      "Dimensions of Validation Set are (60000, 32, 32)\n",
      "Dimensions of Test Set are (18000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of Train Set are {}'.format(X_train.shape))\n",
    "print('Dimensions of Validation Set are {}'.format(X_val.shape))\n",
    "print('Dimensions of Test Set are {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1594530533837,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "tWCPQWZlJ3fQ",
    "outputId": "4f026414-3800-4d8d-b9c0-07a75a9a1b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Checking the unique values of the labels\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_val))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yLSsqNUbUt5"
   },
   "source": [
    "### To check the True labels and Predicted labels after model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1594530580695,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "i5eqssAIKN_a"
   },
   "outputs": [],
   "source": [
    "# To check training and predicted data before and after model prediction\n",
    "def plot_images(img,labels,nrows,ncols,pred_labels=None):\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    axes = fig.subplots(nrows,ncols)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(img[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        if pred_labels is None:\n",
    "            ax.set_title(f'True: {labels[i]}')\n",
    "        else:\n",
    "            ax.set_title(f'True: {labels[i]}, Pred: {np.argmax(pred_labels[i], axis = -1)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2888,
     "status": "ok",
     "timestamp": 1594530599490,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "H0CSdBS4KQbC",
    "outputId": "b54ec35c-7688-40f2-ac95-ace799ced7af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking on a few training images\n",
    "plot_images(X_train, y_train, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3555,
     "status": "ok",
     "timestamp": 1594530634864,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "sffgIkXgKSHj",
    "outputId": "9a025b2e-a45d-4e68-d1d7-e070b356ff6a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking at gray scale version of images\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    #plt.axis('off')\n",
    "    plt.xlabel(f'Label - {y_train[i]}', fontsize = 15)\n",
    "    #plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcaKdxp4b1Bt"
   },
   "source": [
    "### Reshaping Features for input data to NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1594530702427,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "QAWn-joMKUxs",
    "outputId": "8811b312-5c40-4322-fda2-65dfe10bb7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Train Set after re-shaping are (42000, 1024)\n",
      "Dimensions of Validation Set after re-shaping are (60000, 1024)\n",
      "Dimensions of Test Set after re-shaping are (18000, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1] * X_val.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "print('Dimensions of Train Set after re-shaping are {}'.format(X_train.shape))\n",
    "print('Dimensions of Validation Set after re-shaping are {}'.format(X_val.shape))\n",
    "print('Dimensions of Test Set after re-shaping are {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtUePLTUb9JK"
   },
   "source": [
    "### Normalizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2739,
     "status": "ok",
     "timestamp": 1594530754781,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "WfJy8UVWKZIH",
    "outputId": "2a86fb6c-1e36-4822-b8e6-42ecebf90202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "254.9745\n",
      "0.0\n",
      "After\n",
      "0.9999\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print('Before')\n",
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print('After')\n",
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZZRMJ4QcI9r"
   },
   "source": [
    "### Converting the labels to one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5536,
     "status": "ok",
     "timestamp": 1594530786576,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "m6jyOMjUKbKf",
    "outputId": "ddeece68-017d-4c7e-b412-c99eecbb92d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Before and After sanity check\n",
    "print(y_train[10])\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_val = to_categorical(y_val, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8DtkURlKc1o"
   },
   "outputs": [],
   "source": [
    "#sgd1 = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "#sgd2 = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
    "#adam1 = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "#adam2 = optimizers.Adam(lr=0.001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RS5VhB8cTeq"
   },
   "source": [
    "### First Basic Neural Networks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1253,
     "status": "ok",
     "timestamp": 1594531075933,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "EgKe7XAsKezp"
   },
   "outputs": [],
   "source": [
    "# We use adam optimizer throught out the notebook, initializing the kernel with he initializer at each hidden network and L2 regularization at output.\n",
    "# Learning rate and Lambda are the hyperparameters here that we can tweak \n",
    "\n",
    "def basicFCNN(lr, Lambda, activation, k_initial, verb=True):\n",
    "    ## hyperparameters\n",
    "    #epochs = iterations\n",
    "    learning_rate = lr\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape = (X_train.shape[1], ), kernel_initializer=k_initial, name='Input'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(256, kernel_initializer=k_initial, name='Hidden_Layer_1'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(128, kernel_initializer=k_initial, name='Hidden_Layer_2'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(64, kernel_initializer=k_initial, name='Hidden_Layer_3'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(32, kernel_initializer=k_initial, name='Hidden_Layer_4'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(10, kernel_regularizer=regularizers.l2(Lambda), name='Output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #opt = adam1\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate, decay=1e-6), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #model.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]), \n",
    "    #                epochs=100, batch_size=1000, verbose= 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hu21IuEdfUe"
   },
   "source": [
    "Now we are gonna build the model slowly, performing sanity checks along the way by optimizing the hyperparameters as much as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8411,
     "status": "ok",
     "timestamp": 1594531210485,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "fcMP9pqEQOf5",
    "outputId": "901766c8-0afc-4942-d810-e7ff8bf6b1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR - 0.00001, Lambda - 0\n",
    "model1 = basicFCNN(0.00001, 0, 'relu', 'he_normal', verb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4003,
     "status": "ok",
     "timestamp": 1594531252463,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "jA0lomP1QtnQ",
    "outputId": "9af8094f-ca25-48f7-9550-76bb71ce84fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3231 - accuracy: 0.1077 - val_loss: 2.2704 - val_accuracy: 0.1648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7aa0f84550>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]), \n",
    "                    epochs=1, batch_size=500, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgyekP4peBOB"
   },
   "source": [
    "There are 10 output classes and the model is correctly predicting 1 up on 10 times (1/10 = 0.1% approx) as it is untrained and having 0.00001 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1594452307446,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "ukjVtr2kSKkL",
    "outputId": "4d26816c-4284-4f4b-e6a0-123e41595f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR - 0.00001, Lambda - 1e3\n",
    "model2 = basicFCNN(0.00001, 1e3, 'relu', 'he_normal', verb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1686,
     "status": "ok",
     "timestamp": 1594452313978,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "mPEmYMw6SbxM",
    "outputId": "f9a22432-4e68-4380-ca24-57edc33294b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 6ms/step - loss: 13189.6396 - accuracy: 0.1109 - val_loss: 13142.5840 - val_accuracy: 0.1258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f09d53a0a58>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]), \n",
    "                    epochs=1, batch_size=500, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dRA9lJseuqg"
   },
   "source": [
    "Loss shot up ! That is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1594531517205,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "xoP0MhUJSfVl",
    "outputId": "505e7195-91bf-45f7-a165-971bbac09d3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR - 0.001, Lambda - 0\n",
    "model3 = basicFCNN(0.001, 0, 'relu', 'he_normal', verb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TxZhh1SfDfR"
   },
   "source": [
    "Here we are going to overfit the model by taking a subset of the dataset to ensure that our model architecture is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27040,
     "status": "ok",
     "timestamp": 1594531591309,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "Pq_VcS5XTK2z",
    "outputId": "d529123f-1245-4742-b31b-e6ecf9a70fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 2.6287 - accuracy: 0.0000e+00 - val_loss: 2.5087 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3465 - accuracy: 0.1500 - val_loss: 2.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0588 - accuracy: 0.1500 - val_loss: 1.8243 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.9498 - accuracy: 0.1500 - val_loss: 1.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8697 - accuracy: 0.1500 - val_loss: 1.4419 - val_accuracy: 0.7143\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.8433 - accuracy: 0.3500 - val_loss: 1.4865 - val_accuracy: 0.1429\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8252 - accuracy: 0.2500 - val_loss: 1.4865 - val_accuracy: 0.8571\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7713 - accuracy: 0.5000 - val_loss: 1.3883 - val_accuracy: 0.5714\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.7217 - accuracy: 0.4000 - val_loss: 1.4793 - val_accuracy: 0.2857\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6894 - accuracy: 0.3000 - val_loss: 1.4863 - val_accuracy: 0.4286\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6367 - accuracy: 0.6000 - val_loss: 1.4468 - val_accuracy: 0.4286\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5572 - accuracy: 0.6500 - val_loss: 1.3749 - val_accuracy: 0.2857\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5139 - accuracy: 0.6000 - val_loss: 1.3892 - val_accuracy: 0.2857\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4764 - accuracy: 0.5000 - val_loss: 1.4299 - val_accuracy: 0.2857\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3872 - accuracy: 0.6500 - val_loss: 1.2456 - val_accuracy: 0.4286\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3116 - accuracy: 0.5500 - val_loss: 1.5150 - val_accuracy: 0.2857\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2575 - accuracy: 0.6500 - val_loss: 1.2416 - val_accuracy: 0.4286\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1929 - accuracy: 0.7000 - val_loss: 1.3933 - val_accuracy: 0.4286\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1151 - accuracy: 0.6500 - val_loss: 1.7131 - val_accuracy: 0.2857\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1111 - accuracy: 0.5500 - val_loss: 1.3262 - val_accuracy: 0.2857\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1740 - accuracy: 0.5500 - val_loss: 1.3480 - val_accuracy: 0.2857\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0319 - accuracy: 0.6500 - val_loss: 1.5388 - val_accuracy: 0.2857\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.9904 - accuracy: 0.7500 - val_loss: 1.2998 - val_accuracy: 0.4286\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9696 - accuracy: 0.8000 - val_loss: 1.5544 - val_accuracy: 0.4286\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8986 - accuracy: 0.8000 - val_loss: 1.8437 - val_accuracy: 0.2857\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8716 - accuracy: 0.8000 - val_loss: 1.3151 - val_accuracy: 0.4286\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7851 - accuracy: 0.8000 - val_loss: 1.2338 - val_accuracy: 0.5714\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7654 - accuracy: 0.7000 - val_loss: 1.7845 - val_accuracy: 0.1429\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7029 - accuracy: 0.9500 - val_loss: 1.9198 - val_accuracy: 0.2857\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6583 - accuracy: 0.9500 - val_loss: 1.4997 - val_accuracy: 0.4286\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6179 - accuracy: 0.8500 - val_loss: 1.4158 - val_accuracy: 0.4286\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5872 - accuracy: 0.8500 - val_loss: 1.8177 - val_accuracy: 0.2857\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5416 - accuracy: 0.9500 - val_loss: 1.8789 - val_accuracy: 0.1429\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5142 - accuracy: 0.9500 - val_loss: 1.6596 - val_accuracy: 0.2857\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4843 - accuracy: 0.9500 - val_loss: 1.7165 - val_accuracy: 0.2857\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4371 - accuracy: 0.9500 - val_loss: 1.8951 - val_accuracy: 0.2857\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4186 - accuracy: 0.9500 - val_loss: 1.8689 - val_accuracy: 0.2857\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3795 - accuracy: 0.9500 - val_loss: 1.9654 - val_accuracy: 0.2857\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3543 - accuracy: 0.9500 - val_loss: 2.0934 - val_accuracy: 0.2857\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3196 - accuracy: 0.9500 - val_loss: 2.0225 - val_accuracy: 0.2857\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3004 - accuracy: 0.9500 - val_loss: 1.9274 - val_accuracy: 0.2857\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2810 - accuracy: 0.9500 - val_loss: 2.2561 - val_accuracy: 0.2857\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2543 - accuracy: 0.9500 - val_loss: 2.4079 - val_accuracy: 0.2857\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 2.1537 - val_accuracy: 0.2857\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2081 - accuracy: 1.0000 - val_loss: 2.1235 - val_accuracy: 0.2857\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1923 - accuracy: 1.0000 - val_loss: 2.5428 - val_accuracy: 0.2857\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1696 - accuracy: 1.0000 - val_loss: 2.7638 - val_accuracy: 0.2857\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1569 - accuracy: 1.0000 - val_loss: 2.5165 - val_accuracy: 0.2857\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1393 - accuracy: 1.0000 - val_loss: 2.3636 - val_accuracy: 0.2857\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1303 - accuracy: 1.0000 - val_loss: 2.5992 - val_accuracy: 0.2857\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1169 - accuracy: 1.0000 - val_loss: 2.9784 - val_accuracy: 0.2857\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 3.0591 - val_accuracy: 0.2857\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 2.8690 - val_accuracy: 0.2857\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 2.7854 - val_accuracy: 0.2857\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 2.9857 - val_accuracy: 0.2857\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0722 - accuracy: 1.0000 - val_loss: 3.2487 - val_accuracy: 0.2857\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 3.3512 - val_accuracy: 0.2857\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 3.2622 - val_accuracy: 0.2857\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 3.1356 - val_accuracy: 0.2857\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 3.1157 - val_accuracy: 0.2857\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 3.2354 - val_accuracy: 0.2857\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 3.4015 - val_accuracy: 0.1429\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 3.5271 - val_accuracy: 0.1429\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 3.5641 - val_accuracy: 0.1429\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 3.5338 - val_accuracy: 0.1429\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 3.4904 - val_accuracy: 0.1429\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 3.4870 - val_accuracy: 0.2857\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 3.5399 - val_accuracy: 0.1429\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 3.6428 - val_accuracy: 0.1429\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.7598 - val_accuracy: 0.1429\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 3.8290 - val_accuracy: 0.1429\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 3.8393 - val_accuracy: 0.1429\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 3.8041 - val_accuracy: 0.1429\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 3.7587 - val_accuracy: 0.1429\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 3.7382 - val_accuracy: 0.1429\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.7621 - val_accuracy: 0.1429\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 3.8244 - val_accuracy: 0.1429\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 3.8982 - val_accuracy: 0.1429\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.9662 - val_accuracy: 0.1429\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 4.0135 - val_accuracy: 0.1429\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 4.0303 - val_accuracy: 0.1429\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 4.0197 - val_accuracy: 0.1429\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.9919 - val_accuracy: 0.1429\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.9605 - val_accuracy: 0.1429\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.9438 - val_accuracy: 0.1429\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.9492 - val_accuracy: 0.1429\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.9752 - val_accuracy: 0.1429\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 4.0182 - val_accuracy: 0.1429\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 4.0681 - val_accuracy: 0.1429\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 4.1158 - val_accuracy: 0.1429\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 4.1478 - val_accuracy: 0.1429\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 4.1674 - val_accuracy: 0.1429\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 4.1720 - val_accuracy: 0.1429\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 4.1704 - val_accuracy: 0.1429\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 4.1680 - val_accuracy: 0.1429\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 4.1691 - val_accuracy: 0.1429\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 4.1765 - val_accuracy: 0.1429\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 4.1866 - val_accuracy: 0.1429\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 4.1948 - val_accuracy: 0.1429\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 4.2030 - val_accuracy: 0.1429\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 4.2096 - val_accuracy: 0.1429\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 4.2158 - val_accuracy: 0.1429\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 4.2222 - val_accuracy: 0.1429\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.2292 - val_accuracy: 0.1429\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 4.2360 - val_accuracy: 0.1429\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 4.2448 - val_accuracy: 0.1429\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 4.2577 - val_accuracy: 0.1429\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 4.2709 - val_accuracy: 0.1429\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 4.2874 - val_accuracy: 0.1429\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 4.3045 - val_accuracy: 0.1429\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 4.3154 - val_accuracy: 0.1429\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 4.3155 - val_accuracy: 0.1429\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.3068 - val_accuracy: 0.1429\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.3003 - val_accuracy: 0.1429\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 4.3010 - val_accuracy: 0.1429\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 4.3086 - val_accuracy: 0.1429\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.3209 - val_accuracy: 0.1429\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.3346 - val_accuracy: 0.1429\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.3472 - val_accuracy: 0.1429\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.3577 - val_accuracy: 0.1429\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 4.3635 - val_accuracy: 0.1429\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.3634 - val_accuracy: 0.1429\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.3590 - val_accuracy: 0.1429\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.3535 - val_accuracy: 0.1429\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.3516 - val_accuracy: 0.1429\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.3576 - val_accuracy: 0.1429\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.3684 - val_accuracy: 0.1429\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.3793 - val_accuracy: 0.1429\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.3872 - val_accuracy: 0.1429\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.3892 - val_accuracy: 0.1429\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.3867 - val_accuracy: 0.1429\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.3890 - val_accuracy: 0.1429\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.3953 - val_accuracy: 0.1429\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.4038 - val_accuracy: 0.1429\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.4083 - val_accuracy: 0.1429\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.4121 - val_accuracy: 0.1429\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.4185 - val_accuracy: 0.1429\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.4234 - val_accuracy: 0.1429\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.4286 - val_accuracy: 0.1429\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.4309 - val_accuracy: 0.1429\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4356 - val_accuracy: 0.1429\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4429 - val_accuracy: 0.1429\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.4557 - val_accuracy: 0.1429\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.4603 - val_accuracy: 0.1429\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.4596 - val_accuracy: 0.1429\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.4606 - val_accuracy: 0.1429\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.4628 - val_accuracy: 0.1429\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.4707 - val_accuracy: 0.1429\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4831 - val_accuracy: 0.1429\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4900 - val_accuracy: 0.1429\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4978 - val_accuracy: 0.1429\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.5035 - val_accuracy: 0.1429\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.5084 - val_accuracy: 0.1429\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.5137 - val_accuracy: 0.1429\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.5183 - val_accuracy: 0.1429\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.5268 - val_accuracy: 0.1429\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.5324 - val_accuracy: 0.1429\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.5364 - val_accuracy: 0.1429\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.5437 - val_accuracy: 0.1429\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.5492 - val_accuracy: 0.1429\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.5529 - val_accuracy: 0.1429\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.5592 - val_accuracy: 0.1429\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5662 - val_accuracy: 0.1429\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5719 - val_accuracy: 0.1429\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5768 - val_accuracy: 0.1429\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5836 - val_accuracy: 0.1429\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.5906 - val_accuracy: 0.1429\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.6003 - val_accuracy: 0.1429\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.6309 - val_accuracy: 0.1429\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.6709 - val_accuracy: 0.1429\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.7175 - val_accuracy: 0.1429\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.7528 - val_accuracy: 0.1429\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.7428 - val_accuracy: 0.1429\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.6969 - val_accuracy: 0.1429\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6560 - val_accuracy: 0.1429\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6385 - val_accuracy: 0.1429\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6409 - val_accuracy: 0.1429\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6544 - val_accuracy: 0.1429\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6516 - val_accuracy: 0.1429\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6606 - val_accuracy: 0.1429\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6793 - val_accuracy: 0.1429\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.7049 - val_accuracy: 0.1429\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7347 - val_accuracy: 0.1429\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7625 - val_accuracy: 0.1429\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7811 - val_accuracy: 0.1429\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7902 - val_accuracy: 0.1429\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7936 - val_accuracy: 0.1429\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7958 - val_accuracy: 0.1429\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7973 - val_accuracy: 0.1429\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.8009 - val_accuracy: 0.1429\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.8081 - val_accuracy: 0.1429\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.8043 - val_accuracy: 0.1429\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8012 - val_accuracy: 0.1429\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8026 - val_accuracy: 0.1429\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8081 - val_accuracy: 0.1429\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8161 - val_accuracy: 0.1429\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8210 - val_accuracy: 0.1429\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8186 - val_accuracy: 0.1429\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8121 - val_accuracy: 0.1429\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8063 - val_accuracy: 0.1429\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8044 - val_accuracy: 0.1429\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8084 - val_accuracy: 0.1429\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.9262e-04 - accuracy: 1.0000 - val_loss: 4.8173 - val_accuracy: 0.1429\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.7883e-04 - accuracy: 1.0000 - val_loss: 4.8288 - val_accuracy: 0.1429\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.6597e-04 - accuracy: 1.0000 - val_loss: 4.8393 - val_accuracy: 0.1429\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.5349e-04 - accuracy: 1.0000 - val_loss: 4.8460 - val_accuracy: 0.1429\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.4040e-04 - accuracy: 1.0000 - val_loss: 4.8507 - val_accuracy: 0.1429\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.2814e-04 - accuracy: 1.0000 - val_loss: 4.8568 - val_accuracy: 0.1429\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.1652e-04 - accuracy: 1.0000 - val_loss: 4.8648 - val_accuracy: 0.1429\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.0496e-04 - accuracy: 1.0000 - val_loss: 4.8743 - val_accuracy: 0.1429\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.9334e-04 - accuracy: 1.0000 - val_loss: 4.8836 - val_accuracy: 0.1429\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.8236e-04 - accuracy: 1.0000 - val_loss: 4.8895 - val_accuracy: 0.1429\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.7149e-04 - accuracy: 1.0000 - val_loss: 4.8925 - val_accuracy: 0.1429\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.6059e-04 - accuracy: 1.0000 - val_loss: 4.8953 - val_accuracy: 0.1429\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.5006e-04 - accuracy: 1.0000 - val_loss: 4.9004 - val_accuracy: 0.1429\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.4005e-04 - accuracy: 1.0000 - val_loss: 4.9086 - val_accuracy: 0.1429\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.3004e-04 - accuracy: 1.0000 - val_loss: 4.9194 - val_accuracy: 0.1429\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2018e-04 - accuracy: 1.0000 - val_loss: 4.9309 - val_accuracy: 0.1429\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.1050e-04 - accuracy: 1.0000 - val_loss: 4.9390 - val_accuracy: 0.1429\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.0132e-04 - accuracy: 1.0000 - val_loss: 4.9416 - val_accuracy: 0.1429\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.9203e-04 - accuracy: 1.0000 - val_loss: 4.9403 - val_accuracy: 0.1429\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8285e-04 - accuracy: 1.0000 - val_loss: 4.9397 - val_accuracy: 0.1429\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.7399e-04 - accuracy: 1.0000 - val_loss: 4.9408 - val_accuracy: 0.1429\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6528e-04 - accuracy: 1.0000 - val_loss: 4.9432 - val_accuracy: 0.1429\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.5672e-04 - accuracy: 1.0000 - val_loss: 4.9477 - val_accuracy: 0.1429\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.4837e-04 - accuracy: 1.0000 - val_loss: 4.9520 - val_accuracy: 0.1429\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.4019e-04 - accuracy: 1.0000 - val_loss: 4.9574 - val_accuracy: 0.1429\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.3207e-04 - accuracy: 1.0000 - val_loss: 4.9633 - val_accuracy: 0.1429\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.2408e-04 - accuracy: 1.0000 - val_loss: 4.9676 - val_accuracy: 0.1429\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.1653e-04 - accuracy: 1.0000 - val_loss: 4.9749 - val_accuracy: 0.1429\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.0860e-04 - accuracy: 1.0000 - val_loss: 4.9844 - val_accuracy: 0.1429\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.0143e-04 - accuracy: 1.0000 - val_loss: 4.9897 - val_accuracy: 0.1429\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.9423e-04 - accuracy: 1.0000 - val_loss: 4.9918 - val_accuracy: 0.1429\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.8670e-04 - accuracy: 1.0000 - val_loss: 4.9927 - val_accuracy: 0.1429\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.7928e-04 - accuracy: 1.0000 - val_loss: 4.9943 - val_accuracy: 0.1429\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.7279e-04 - accuracy: 1.0000 - val_loss: 5.0007 - val_accuracy: 0.1429\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.6558e-04 - accuracy: 1.0000 - val_loss: 5.0085 - val_accuracy: 0.1429\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.5875e-04 - accuracy: 1.0000 - val_loss: 5.0116 - val_accuracy: 0.1429\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.5232e-04 - accuracy: 1.0000 - val_loss: 5.0113 - val_accuracy: 0.1429\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.4564e-04 - accuracy: 1.0000 - val_loss: 5.0098 - val_accuracy: 0.1429\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.3890e-04 - accuracy: 1.0000 - val_loss: 5.0089 - val_accuracy: 0.1429\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.3295e-04 - accuracy: 1.0000 - val_loss: 5.0139 - val_accuracy: 0.1429\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.2641e-04 - accuracy: 1.0000 - val_loss: 5.0240 - val_accuracy: 0.1429\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.2040e-04 - accuracy: 1.0000 - val_loss: 5.0303 - val_accuracy: 0.1429\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.1475e-04 - accuracy: 1.0000 - val_loss: 5.0305 - val_accuracy: 0.1429\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.0850e-04 - accuracy: 1.0000 - val_loss: 5.0284 - val_accuracy: 0.1429\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.0245e-04 - accuracy: 1.0000 - val_loss: 5.0273 - val_accuracy: 0.1429\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.9678e-04 - accuracy: 1.0000 - val_loss: 5.0316 - val_accuracy: 0.1429\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.9108e-04 - accuracy: 1.0000 - val_loss: 5.0399 - val_accuracy: 0.1429\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.8550e-04 - accuracy: 1.0000 - val_loss: 5.0461 - val_accuracy: 0.1429\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.8010e-04 - accuracy: 1.0000 - val_loss: 5.0483 - val_accuracy: 0.1429\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.7462e-04 - accuracy: 1.0000 - val_loss: 5.0497 - val_accuracy: 0.1429\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.6954e-04 - accuracy: 1.0000 - val_loss: 5.0540 - val_accuracy: 0.1429\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.6440e-04 - accuracy: 1.0000 - val_loss: 5.0603 - val_accuracy: 0.1429\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.5929e-04 - accuracy: 1.0000 - val_loss: 5.0664 - val_accuracy: 0.1429\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.5423e-04 - accuracy: 1.0000 - val_loss: 5.0713 - val_accuracy: 0.1429\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.4932e-04 - accuracy: 1.0000 - val_loss: 5.0718 - val_accuracy: 0.1429\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.4431e-04 - accuracy: 1.0000 - val_loss: 5.0708 - val_accuracy: 0.1429\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.3951e-04 - accuracy: 1.0000 - val_loss: 5.0716 - val_accuracy: 0.1429\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.3486e-04 - accuracy: 1.0000 - val_loss: 5.0788 - val_accuracy: 0.1429\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.3003e-04 - accuracy: 1.0000 - val_loss: 5.0862 - val_accuracy: 0.1429\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.2558e-04 - accuracy: 1.0000 - val_loss: 5.0886 - val_accuracy: 0.1429\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.2088e-04 - accuracy: 1.0000 - val_loss: 5.0905 - val_accuracy: 0.1429\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.1654e-04 - accuracy: 1.0000 - val_loss: 5.0982 - val_accuracy: 0.1429\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.1187e-04 - accuracy: 1.0000 - val_loss: 5.1054 - val_accuracy: 0.1429\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.0776e-04 - accuracy: 1.0000 - val_loss: 5.1076 - val_accuracy: 0.1429\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.0325e-04 - accuracy: 1.0000 - val_loss: 5.1100 - val_accuracy: 0.1429\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.9912e-04 - accuracy: 1.0000 - val_loss: 5.1165 - val_accuracy: 0.1429\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.9497e-04 - accuracy: 1.0000 - val_loss: 5.1195 - val_accuracy: 0.1429\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.9081e-04 - accuracy: 1.0000 - val_loss: 5.1208 - val_accuracy: 0.1429\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.8692e-04 - accuracy: 1.0000 - val_loss: 5.1291 - val_accuracy: 0.1429\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.8266e-04 - accuracy: 1.0000 - val_loss: 5.1398 - val_accuracy: 0.1429\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.7912e-04 - accuracy: 1.0000 - val_loss: 5.1416 - val_accuracy: 0.1429\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.7499e-04 - accuracy: 1.0000 - val_loss: 5.1359 - val_accuracy: 0.1429\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.7108e-04 - accuracy: 1.0000 - val_loss: 5.1356 - val_accuracy: 0.1429\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.6736e-04 - accuracy: 1.0000 - val_loss: 5.1419 - val_accuracy: 0.1429\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.6326e-04 - accuracy: 1.0000 - val_loss: 5.1492 - val_accuracy: 0.1429\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.5980e-04 - accuracy: 1.0000 - val_loss: 5.1498 - val_accuracy: 0.1429\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.5590e-04 - accuracy: 1.0000 - val_loss: 5.1494 - val_accuracy: 0.1429\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.5232e-04 - accuracy: 1.0000 - val_loss: 5.1527 - val_accuracy: 0.1429\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.4867e-04 - accuracy: 1.0000 - val_loss: 5.1566 - val_accuracy: 0.1429\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.4516e-04 - accuracy: 1.0000 - val_loss: 5.1604 - val_accuracy: 0.1429\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.4169e-04 - accuracy: 1.0000 - val_loss: 5.1634 - val_accuracy: 0.1429\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.3815e-04 - accuracy: 1.0000 - val_loss: 5.1646 - val_accuracy: 0.1429\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3477e-04 - accuracy: 1.0000 - val_loss: 5.1697 - val_accuracy: 0.1429\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.3137e-04 - accuracy: 1.0000 - val_loss: 5.1729 - val_accuracy: 0.1429\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.2808e-04 - accuracy: 1.0000 - val_loss: 5.1726 - val_accuracy: 0.1429\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.2496e-04 - accuracy: 1.0000 - val_loss: 5.1789 - val_accuracy: 0.1429\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.2161e-04 - accuracy: 1.0000 - val_loss: 5.1881 - val_accuracy: 0.1429\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.1865e-04 - accuracy: 1.0000 - val_loss: 5.1892 - val_accuracy: 0.1429\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.1541e-04 - accuracy: 1.0000 - val_loss: 5.1850 - val_accuracy: 0.1429\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.1231e-04 - accuracy: 1.0000 - val_loss: 5.1883 - val_accuracy: 0.1429\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.0924e-04 - accuracy: 1.0000 - val_loss: 5.1986 - val_accuracy: 0.1429\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.0614e-04 - accuracy: 1.0000 - val_loss: 5.2030 - val_accuracy: 0.1429\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.0310e-04 - accuracy: 1.0000 - val_loss: 5.2009 - val_accuracy: 0.1429\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.0021e-04 - accuracy: 1.0000 - val_loss: 5.2043 - val_accuracy: 0.1429\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.9725e-04 - accuracy: 1.0000 - val_loss: 5.2133 - val_accuracy: 0.1429\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.9420e-04 - accuracy: 1.0000 - val_loss: 5.2179 - val_accuracy: 0.1429\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.9146e-04 - accuracy: 1.0000 - val_loss: 5.2178 - val_accuracy: 0.1429\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.8843e-04 - accuracy: 1.0000 - val_loss: 5.2170 - val_accuracy: 0.1429\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.8566e-04 - accuracy: 1.0000 - val_loss: 5.2228 - val_accuracy: 0.1429\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.8267e-04 - accuracy: 1.0000 - val_loss: 5.2321 - val_accuracy: 0.1429\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.7997e-04 - accuracy: 1.0000 - val_loss: 5.2367 - val_accuracy: 0.1429\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.7739e-04 - accuracy: 1.0000 - val_loss: 5.2335 - val_accuracy: 0.1429\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.7433e-04 - accuracy: 1.0000 - val_loss: 5.2294 - val_accuracy: 0.1429\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.7173e-04 - accuracy: 1.0000 - val_loss: 5.2327 - val_accuracy: 0.1429\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.6894e-04 - accuracy: 1.0000 - val_loss: 5.2422 - val_accuracy: 0.1429\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6597e-04 - accuracy: 1.0000 - val_loss: 5.2491 - val_accuracy: 0.1429\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.6329e-04 - accuracy: 1.0000 - val_loss: 5.2493 - val_accuracy: 0.1429\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.6051e-04 - accuracy: 1.0000 - val_loss: 5.2467 - val_accuracy: 0.1429\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5793e-04 - accuracy: 1.0000 - val_loss: 5.2494 - val_accuracy: 0.1429\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.5521e-04 - accuracy: 1.0000 - val_loss: 5.2563 - val_accuracy: 0.1429\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5245e-04 - accuracy: 1.0000 - val_loss: 5.2591 - val_accuracy: 0.1429\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4980e-04 - accuracy: 1.0000 - val_loss: 5.2594 - val_accuracy: 0.1429\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.4713e-04 - accuracy: 1.0000 - val_loss: 5.2589 - val_accuracy: 0.1429\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4437e-04 - accuracy: 1.0000 - val_loss: 5.2624 - val_accuracy: 0.1429\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4166e-04 - accuracy: 1.0000 - val_loss: 5.2682 - val_accuracy: 0.1429\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.3904e-04 - accuracy: 1.0000 - val_loss: 5.2703 - val_accuracy: 0.1429\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3654e-04 - accuracy: 1.0000 - val_loss: 5.2678 - val_accuracy: 0.1429\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.3399e-04 - accuracy: 1.0000 - val_loss: 5.2705 - val_accuracy: 0.1429\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3156e-04 - accuracy: 1.0000 - val_loss: 5.2761 - val_accuracy: 0.1429\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 3.2929e-04 - accuracy: 1.0000 - val_loss: 5.2781 - val_accuracy: 0.1429\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2714e-04 - accuracy: 1.0000 - val_loss: 5.2770 - val_accuracy: 0.1429\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2493e-04 - accuracy: 1.0000 - val_loss: 5.2744 - val_accuracy: 0.1429\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.2271e-04 - accuracy: 1.0000 - val_loss: 5.2723 - val_accuracy: 0.1429\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2060e-04 - accuracy: 1.0000 - val_loss: 5.2765 - val_accuracy: 0.1429\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1836e-04 - accuracy: 1.0000 - val_loss: 5.2790 - val_accuracy: 0.1429\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1626e-04 - accuracy: 1.0000 - val_loss: 5.2797 - val_accuracy: 0.1429\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1410e-04 - accuracy: 1.0000 - val_loss: 5.2798 - val_accuracy: 0.1429\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1202e-04 - accuracy: 1.0000 - val_loss: 5.2849 - val_accuracy: 0.1429\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0989e-04 - accuracy: 1.0000 - val_loss: 5.2894 - val_accuracy: 0.1429\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0782e-04 - accuracy: 1.0000 - val_loss: 5.2918 - val_accuracy: 0.1429\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0575e-04 - accuracy: 1.0000 - val_loss: 5.2931 - val_accuracy: 0.1429\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0372e-04 - accuracy: 1.0000 - val_loss: 5.2980 - val_accuracy: 0.1429\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0174e-04 - accuracy: 1.0000 - val_loss: 5.2981 - val_accuracy: 0.1429\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9984e-04 - accuracy: 1.0000 - val_loss: 5.3038 - val_accuracy: 0.1429\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9785e-04 - accuracy: 1.0000 - val_loss: 5.3126 - val_accuracy: 0.1429\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9600e-04 - accuracy: 1.0000 - val_loss: 5.3154 - val_accuracy: 0.1429\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.9404e-04 - accuracy: 1.0000 - val_loss: 5.3144 - val_accuracy: 0.1429\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9233e-04 - accuracy: 1.0000 - val_loss: 5.3196 - val_accuracy: 0.1429\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9051e-04 - accuracy: 1.0000 - val_loss: 5.3288 - val_accuracy: 0.1429\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8856e-04 - accuracy: 1.0000 - val_loss: 5.3381 - val_accuracy: 0.1429\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8680e-04 - accuracy: 1.0000 - val_loss: 5.3414 - val_accuracy: 0.1429\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8508e-04 - accuracy: 1.0000 - val_loss: 5.3364 - val_accuracy: 0.1429\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8321e-04 - accuracy: 1.0000 - val_loss: 5.3307 - val_accuracy: 0.1429\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8157e-04 - accuracy: 1.0000 - val_loss: 5.3321 - val_accuracy: 0.1429\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7980e-04 - accuracy: 1.0000 - val_loss: 5.3404 - val_accuracy: 0.1429\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7795e-04 - accuracy: 1.0000 - val_loss: 5.3478 - val_accuracy: 0.1429\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7630e-04 - accuracy: 1.0000 - val_loss: 5.3508 - val_accuracy: 0.1429\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7468e-04 - accuracy: 1.0000 - val_loss: 5.3497 - val_accuracy: 0.1429\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7293e-04 - accuracy: 1.0000 - val_loss: 5.3469 - val_accuracy: 0.1429\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7125e-04 - accuracy: 1.0000 - val_loss: 5.3478 - val_accuracy: 0.1429\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6964e-04 - accuracy: 1.0000 - val_loss: 5.3520 - val_accuracy: 0.1429\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6805e-04 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.1429\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6647e-04 - accuracy: 1.0000 - val_loss: 5.3556 - val_accuracy: 0.1429\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6484e-04 - accuracy: 1.0000 - val_loss: 5.3541 - val_accuracy: 0.1429\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6329e-04 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.1429\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6171e-04 - accuracy: 1.0000 - val_loss: 5.3581 - val_accuracy: 0.1429\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6014e-04 - accuracy: 1.0000 - val_loss: 5.3607 - val_accuracy: 0.1429\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5864e-04 - accuracy: 1.0000 - val_loss: 5.3622 - val_accuracy: 0.1429\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5711e-04 - accuracy: 1.0000 - val_loss: 5.3621 - val_accuracy: 0.1429\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5565e-04 - accuracy: 1.0000 - val_loss: 5.3637 - val_accuracy: 0.1429\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5415e-04 - accuracy: 1.0000 - val_loss: 5.3680 - val_accuracy: 0.1429\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5269e-04 - accuracy: 1.0000 - val_loss: 5.3721 - val_accuracy: 0.1429\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5123e-04 - accuracy: 1.0000 - val_loss: 5.3735 - val_accuracy: 0.1429\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4976e-04 - accuracy: 1.0000 - val_loss: 5.3719 - val_accuracy: 0.1429\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.4837e-04 - accuracy: 1.0000 - val_loss: 5.3739 - val_accuracy: 0.1429\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4696e-04 - accuracy: 1.0000 - val_loss: 5.3804 - val_accuracy: 0.1429\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4551e-04 - accuracy: 1.0000 - val_loss: 5.3842 - val_accuracy: 0.1429\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.4414e-04 - accuracy: 1.0000 - val_loss: 5.3849 - val_accuracy: 0.1429\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4277e-04 - accuracy: 1.0000 - val_loss: 5.3864 - val_accuracy: 0.1429\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4137e-04 - accuracy: 1.0000 - val_loss: 5.3898 - val_accuracy: 0.1429\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3997e-04 - accuracy: 1.0000 - val_loss: 5.3916 - val_accuracy: 0.1429\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3866e-04 - accuracy: 1.0000 - val_loss: 5.3963 - val_accuracy: 0.1429\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3733e-04 - accuracy: 1.0000 - val_loss: 5.3976 - val_accuracy: 0.1429\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.3602e-04 - accuracy: 1.0000 - val_loss: 5.3973 - val_accuracy: 0.1429\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3475e-04 - accuracy: 1.0000 - val_loss: 5.4011 - val_accuracy: 0.1429\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.3341e-04 - accuracy: 1.0000 - val_loss: 5.4084 - val_accuracy: 0.1429\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3210e-04 - accuracy: 1.0000 - val_loss: 5.4141 - val_accuracy: 0.1429\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3100e-04 - accuracy: 1.0000 - val_loss: 5.4123 - val_accuracy: 0.1429\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2955e-04 - accuracy: 1.0000 - val_loss: 5.4058 - val_accuracy: 0.1429\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2848e-04 - accuracy: 1.0000 - val_loss: 5.4059 - val_accuracy: 0.1429\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2731e-04 - accuracy: 1.0000 - val_loss: 5.4118 - val_accuracy: 0.1429\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2599e-04 - accuracy: 1.0000 - val_loss: 5.4210 - val_accuracy: 0.1429\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2465e-04 - accuracy: 1.0000 - val_loss: 5.4305 - val_accuracy: 0.1429\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2360e-04 - accuracy: 1.0000 - val_loss: 5.4290 - val_accuracy: 0.1429\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2227e-04 - accuracy: 1.0000 - val_loss: 5.4199 - val_accuracy: 0.1429\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.2108e-04 - accuracy: 1.0000 - val_loss: 5.4168 - val_accuracy: 0.1429\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.2000e-04 - accuracy: 1.0000 - val_loss: 5.4210 - val_accuracy: 0.1429\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1881e-04 - accuracy: 1.0000 - val_loss: 5.4313 - val_accuracy: 0.1429\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1752e-04 - accuracy: 1.0000 - val_loss: 5.4438 - val_accuracy: 0.1429\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1642e-04 - accuracy: 1.0000 - val_loss: 5.4496 - val_accuracy: 0.1429\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1536e-04 - accuracy: 1.0000 - val_loss: 5.4478 - val_accuracy: 0.1429\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1412e-04 - accuracy: 1.0000 - val_loss: 5.4411 - val_accuracy: 0.1429\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.1295e-04 - accuracy: 1.0000 - val_loss: 5.4382 - val_accuracy: 0.1429\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.1192e-04 - accuracy: 1.0000 - val_loss: 5.4412 - val_accuracy: 0.1429\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1080e-04 - accuracy: 1.0000 - val_loss: 5.4492 - val_accuracy: 0.1429\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.0955e-04 - accuracy: 1.0000 - val_loss: 5.4538 - val_accuracy: 0.1429\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.0852e-04 - accuracy: 1.0000 - val_loss: 5.4537 - val_accuracy: 0.1429\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0739e-04 - accuracy: 1.0000 - val_loss: 5.4506 - val_accuracy: 0.1429\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0635e-04 - accuracy: 1.0000 - val_loss: 5.4527 - val_accuracy: 0.1429\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0529e-04 - accuracy: 1.0000 - val_loss: 5.4585 - val_accuracy: 0.1429\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0420e-04 - accuracy: 1.0000 - val_loss: 5.4620 - val_accuracy: 0.1429\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0317e-04 - accuracy: 1.0000 - val_loss: 5.4634 - val_accuracy: 0.1429\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.0209e-04 - accuracy: 1.0000 - val_loss: 5.4670 - val_accuracy: 0.1429\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0106e-04 - accuracy: 1.0000 - val_loss: 5.4697 - val_accuracy: 0.1429\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.0007e-04 - accuracy: 1.0000 - val_loss: 5.4685 - val_accuracy: 0.1429\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.9904e-04 - accuracy: 1.0000 - val_loss: 5.4696 - val_accuracy: 0.1429\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9800e-04 - accuracy: 1.0000 - val_loss: 5.4699 - val_accuracy: 0.1429\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9703e-04 - accuracy: 1.0000 - val_loss: 5.4727 - val_accuracy: 0.1429\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9602e-04 - accuracy: 1.0000 - val_loss: 5.4761 - val_accuracy: 0.1429\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9509e-04 - accuracy: 1.0000 - val_loss: 5.4754 - val_accuracy: 0.1429\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9405e-04 - accuracy: 1.0000 - val_loss: 5.4732 - val_accuracy: 0.1429\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9312e-04 - accuracy: 1.0000 - val_loss: 5.4759 - val_accuracy: 0.1429\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9217e-04 - accuracy: 1.0000 - val_loss: 5.4825 - val_accuracy: 0.1429\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9117e-04 - accuracy: 1.0000 - val_loss: 5.4888 - val_accuracy: 0.1429\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9032e-04 - accuracy: 1.0000 - val_loss: 5.4902 - val_accuracy: 0.1429\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.8938e-04 - accuracy: 1.0000 - val_loss: 5.4878 - val_accuracy: 0.1429\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.8838e-04 - accuracy: 1.0000 - val_loss: 5.4846 - val_accuracy: 0.1429\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.8750e-04 - accuracy: 1.0000 - val_loss: 5.4866 - val_accuracy: 0.1429\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8661e-04 - accuracy: 1.0000 - val_loss: 5.4928 - val_accuracy: 0.1429\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8567e-04 - accuracy: 1.0000 - val_loss: 5.4994 - val_accuracy: 0.1429\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8474e-04 - accuracy: 1.0000 - val_loss: 5.5005 - val_accuracy: 0.1429\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8383e-04 - accuracy: 1.0000 - val_loss: 5.4985 - val_accuracy: 0.1429\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8295e-04 - accuracy: 1.0000 - val_loss: 5.5005 - val_accuracy: 0.1429\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8207e-04 - accuracy: 1.0000 - val_loss: 5.5068 - val_accuracy: 0.1429\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.8115e-04 - accuracy: 1.0000 - val_loss: 5.5083 - val_accuracy: 0.1429\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8031e-04 - accuracy: 1.0000 - val_loss: 5.5102 - val_accuracy: 0.1429\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7943e-04 - accuracy: 1.0000 - val_loss: 5.5135 - val_accuracy: 0.1429\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7859e-04 - accuracy: 1.0000 - val_loss: 5.5155 - val_accuracy: 0.1429\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7772e-04 - accuracy: 1.0000 - val_loss: 5.5199 - val_accuracy: 0.1429\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.7687e-04 - accuracy: 1.0000 - val_loss: 5.5234 - val_accuracy: 0.1429\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7604e-04 - accuracy: 1.0000 - val_loss: 5.5227 - val_accuracy: 0.1429\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7520e-04 - accuracy: 1.0000 - val_loss: 5.5227 - val_accuracy: 0.1429\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7440e-04 - accuracy: 1.0000 - val_loss: 5.5270 - val_accuracy: 0.1429\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7353e-04 - accuracy: 1.0000 - val_loss: 5.5288 - val_accuracy: 0.1429\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.7274e-04 - accuracy: 1.0000 - val_loss: 5.5275 - val_accuracy: 0.1429\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.7195e-04 - accuracy: 1.0000 - val_loss: 5.5285 - val_accuracy: 0.1429\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7115e-04 - accuracy: 1.0000 - val_loss: 5.5329 - val_accuracy: 0.1429\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7032e-04 - accuracy: 1.0000 - val_loss: 5.5403 - val_accuracy: 0.1429\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6958e-04 - accuracy: 1.0000 - val_loss: 5.5373 - val_accuracy: 0.1429\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6873e-04 - accuracy: 1.0000 - val_loss: 5.5377 - val_accuracy: 0.1429\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6798e-04 - accuracy: 1.0000 - val_loss: 5.5423 - val_accuracy: 0.1429\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6718e-04 - accuracy: 1.0000 - val_loss: 5.5479 - val_accuracy: 0.1429\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6640e-04 - accuracy: 1.0000 - val_loss: 5.5481 - val_accuracy: 0.1429\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6560e-04 - accuracy: 1.0000 - val_loss: 5.5438 - val_accuracy: 0.1429\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.6486e-04 - accuracy: 1.0000 - val_loss: 5.5428 - val_accuracy: 0.1429\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.6414e-04 - accuracy: 1.0000 - val_loss: 5.5463 - val_accuracy: 0.1429\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6338e-04 - accuracy: 1.0000 - val_loss: 5.5532 - val_accuracy: 0.1429\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6260e-04 - accuracy: 1.0000 - val_loss: 5.5561 - val_accuracy: 0.1429\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6184e-04 - accuracy: 1.0000 - val_loss: 5.5542 - val_accuracy: 0.1429\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6110e-04 - accuracy: 1.0000 - val_loss: 5.5571 - val_accuracy: 0.1429\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6038e-04 - accuracy: 1.0000 - val_loss: 5.5631 - val_accuracy: 0.1429\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.5964e-04 - accuracy: 1.0000 - val_loss: 5.5657 - val_accuracy: 0.1429\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5894e-04 - accuracy: 1.0000 - val_loss: 5.5641 - val_accuracy: 0.1429\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.5822e-04 - accuracy: 1.0000 - val_loss: 5.5643 - val_accuracy: 0.1429\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.5750e-04 - accuracy: 1.0000 - val_loss: 5.5682 - val_accuracy: 0.1429\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.5677e-04 - accuracy: 1.0000 - val_loss: 5.5720 - val_accuracy: 0.1429\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5606e-04 - accuracy: 1.0000 - val_loss: 5.5733 - val_accuracy: 0.1429\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5537e-04 - accuracy: 1.0000 - val_loss: 5.5746 - val_accuracy: 0.1429\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5466e-04 - accuracy: 1.0000 - val_loss: 5.5773 - val_accuracy: 0.1429\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5397e-04 - accuracy: 1.0000 - val_loss: 5.5787 - val_accuracy: 0.1429\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5328e-04 - accuracy: 1.0000 - val_loss: 5.5801 - val_accuracy: 0.1429\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5260e-04 - accuracy: 1.0000 - val_loss: 5.5832 - val_accuracy: 0.1429\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5192e-04 - accuracy: 1.0000 - val_loss: 5.5867 - val_accuracy: 0.1429\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.5127e-04 - accuracy: 1.0000 - val_loss: 5.5865 - val_accuracy: 0.1429\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5056e-04 - accuracy: 1.0000 - val_loss: 5.5861 - val_accuracy: 0.1429\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.4994e-04 - accuracy: 1.0000 - val_loss: 5.5897 - val_accuracy: 0.1429\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4923e-04 - accuracy: 1.0000 - val_loss: 5.5945 - val_accuracy: 0.1429\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.4860e-04 - accuracy: 1.0000 - val_loss: 5.5959 - val_accuracy: 0.1429\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4792e-04 - accuracy: 1.0000 - val_loss: 5.5935 - val_accuracy: 0.1429\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.4728e-04 - accuracy: 1.0000 - val_loss: 5.5956 - val_accuracy: 0.1429\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4662e-04 - accuracy: 1.0000 - val_loss: 5.6019 - val_accuracy: 0.1429\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.4602e-04 - accuracy: 1.0000 - val_loss: 5.5986 - val_accuracy: 0.1429\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4536e-04 - accuracy: 1.0000 - val_loss: 5.5975 - val_accuracy: 0.1429\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4473e-04 - accuracy: 1.0000 - val_loss: 5.5998 - val_accuracy: 0.1429\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.4408e-04 - accuracy: 1.0000 - val_loss: 5.6067 - val_accuracy: 0.1429\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.4340e-04 - accuracy: 1.0000 - val_loss: 5.6136 - val_accuracy: 0.1429\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4285e-04 - accuracy: 1.0000 - val_loss: 5.6099 - val_accuracy: 0.1429\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4217e-04 - accuracy: 1.0000 - val_loss: 5.6065 - val_accuracy: 0.1429\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4160e-04 - accuracy: 1.0000 - val_loss: 5.6080 - val_accuracy: 0.1429\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4099e-04 - accuracy: 1.0000 - val_loss: 5.6121 - val_accuracy: 0.1429\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4035e-04 - accuracy: 1.0000 - val_loss: 5.6172 - val_accuracy: 0.1429\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3975e-04 - accuracy: 1.0000 - val_loss: 5.6210 - val_accuracy: 0.1429\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.3916e-04 - accuracy: 1.0000 - val_loss: 5.6221 - val_accuracy: 0.1429\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3855e-04 - accuracy: 1.0000 - val_loss: 5.6207 - val_accuracy: 0.1429\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3795e-04 - accuracy: 1.0000 - val_loss: 5.6213 - val_accuracy: 0.1429\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3735e-04 - accuracy: 1.0000 - val_loss: 5.6245 - val_accuracy: 0.1429\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3678e-04 - accuracy: 1.0000 - val_loss: 5.6267 - val_accuracy: 0.1429\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3619e-04 - accuracy: 1.0000 - val_loss: 5.6281 - val_accuracy: 0.1429\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3561e-04 - accuracy: 1.0000 - val_loss: 5.6278 - val_accuracy: 0.1429\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3501e-04 - accuracy: 1.0000 - val_loss: 5.6304 - val_accuracy: 0.1429\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.3447e-04 - accuracy: 1.0000 - val_loss: 5.6339 - val_accuracy: 0.1429\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3389e-04 - accuracy: 1.0000 - val_loss: 5.6361 - val_accuracy: 0.1429\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3332e-04 - accuracy: 1.0000 - val_loss: 5.6356 - val_accuracy: 0.1429\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3276e-04 - accuracy: 1.0000 - val_loss: 5.6376 - val_accuracy: 0.1429\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3219e-04 - accuracy: 1.0000 - val_loss: 5.6413 - val_accuracy: 0.1429\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3165e-04 - accuracy: 1.0000 - val_loss: 5.6420 - val_accuracy: 0.1429\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3110e-04 - accuracy: 1.0000 - val_loss: 5.6427 - val_accuracy: 0.1429\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3057e-04 - accuracy: 1.0000 - val_loss: 5.6459 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7aa1248710>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train[:20], y_train[:20], validation_data=(X_val[:7], y_val[:7]), \n",
    "                    epochs=500, batch_size=500, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xReIW7mfX6I"
   },
   "source": [
    "### Sanity Checks Passed. Time to fine tune the Hyperparameters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1594531658676,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "Mb2fQEuJTM18",
    "outputId": "c6841ece-0e68-474f-a220-2ced711ed9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR - 1e-7, Lambda - 1e-7\n",
    "model4 = basicFCNN(1e-7, 1e-7, 'relu', 'he_normal', verb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15769,
     "status": "ok",
     "timestamp": 1594531685637,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "i_kcWboJTkh3",
    "outputId": "69f31643-3268-4b9a-910c-3e7a02b5f19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3466 - accuracy: 0.1025 - val_loss: 2.3893 - val_accuracy: 0.0190\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3434 - accuracy: 0.1012 - val_loss: 2.3867 - val_accuracy: 0.0150\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3404 - accuracy: 0.1009 - val_loss: 2.3843 - val_accuracy: 0.0129\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3377 - accuracy: 0.1005 - val_loss: 2.3820 - val_accuracy: 0.0109\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3351 - accuracy: 0.0995 - val_loss: 2.3800 - val_accuracy: 0.0090\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3328 - accuracy: 0.0988 - val_loss: 2.3778 - val_accuracy: 0.0083\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3307 - accuracy: 0.0985 - val_loss: 2.3759 - val_accuracy: 0.0083\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3287 - accuracy: 0.0980 - val_loss: 2.3740 - val_accuracy: 0.0081\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3269 - accuracy: 0.0978 - val_loss: 2.3721 - val_accuracy: 0.0079\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3253 - accuracy: 0.0973 - val_loss: 2.3703 - val_accuracy: 0.0078\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3238 - accuracy: 0.0970 - val_loss: 2.3686 - val_accuracy: 0.0079\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3224 - accuracy: 0.0972 - val_loss: 2.3667 - val_accuracy: 0.0081\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3211 - accuracy: 0.0973 - val_loss: 2.3651 - val_accuracy: 0.0083\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3199 - accuracy: 0.0974 - val_loss: 2.3635 - val_accuracy: 0.0084\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3188 - accuracy: 0.0967 - val_loss: 2.3618 - val_accuracy: 0.0091\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3178 - accuracy: 0.0971 - val_loss: 2.3603 - val_accuracy: 0.0096\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3169 - accuracy: 0.0979 - val_loss: 2.3588 - val_accuracy: 0.0099\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3160 - accuracy: 0.0984 - val_loss: 2.3575 - val_accuracy: 0.0109\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3152 - accuracy: 0.0992 - val_loss: 2.3560 - val_accuracy: 0.0116\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3144 - accuracy: 0.0992 - val_loss: 2.3546 - val_accuracy: 0.0129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7aa318b748>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]), \n",
    "                    epochs=20, batch_size=500, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsVyHuD8fqX9"
   },
   "source": [
    "Lets increase the learning rate as it is too low.\n",
    "Lets increase LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1594531792356,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "TTFKXJBXTtkD",
    "outputId": "5adae012-48d2-4d1d-f181-d5c79c95f44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR - 1e4, Lambda - 1e-7\n",
    "model5 = basicFCNN(1e4, 1e-7, 'relu', 'he_normal', verb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15651,
     "status": "ok",
     "timestamp": 1594531811524,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "foj23IEST5U5",
    "outputId": "cb3a3999-9753-445a-cc6b-9d3509c96f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "84/84 [==============================] - 1s 10ms/step - loss: nan - accuracy: 0.0995 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7a906d5710>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]), \n",
    "                    epochs=20, batch_size=500, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlCa2mHkgUb4"
   },
   "source": [
    "### Loss exploding. Learning rate is too high.\n",
    "### Cost is very high. Always means high learning rate.\n",
    "\n",
    "### Now lets do cross validation to optimize hyperparameters by taking a rough range of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clILkKOWgmro"
   },
   "source": [
    "We use the first 14000 images of validation set to validate and full test set to evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1206,
     "status": "ok",
     "timestamp": 1594532085917,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "nMdQY9gQVJZX"
   },
   "outputs": [],
   "source": [
    "def basicHPCheckFCNN(iterations ,lr, Lambda, activation, k_initial, verb=True):\n",
    "    ## hyperparameters\n",
    "    epochs = iterations\n",
    "    learning_rate = lr\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape = (X_train.shape[1], ), kernel_initializer=k_initial, name='Input'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(256, kernel_initializer=k_initial, name='Hidden_Layer_1'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(128, kernel_initializer=k_initial, name='Hidden_Layer_2'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(64, kernel_initializer=k_initial, name='Hidden_Layer_3'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(32, kernel_initializer=k_initial, name='Hidden_Layer_4'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(10, kernel_initializer=k_initial, kernel_regularizer=regularizers.l2(Lambda), name='Output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #opt = adam1\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]),\n",
    "              epochs=iterations, batch_size=500, verbose= 1)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 646524,
     "status": "ok",
     "timestamp": 1594532758256,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "N3zLvawUVaDP",
    "outputId": "ba051eac-1056-4dc3-d742-39babb42062e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3589 - accuracy: 0.1112 - val_loss: 2.3043 - val_accuracy: 0.0979\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1565 - val_loss: 2.3007 - val_accuracy: 0.1147\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2810 - accuracy: 0.1940 - val_loss: 2.2696 - val_accuracy: 0.1604\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2545 - accuracy: 0.2228 - val_loss: 2.2456 - val_accuracy: 0.1524\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2198 - accuracy: 0.2417 - val_loss: 2.2410 - val_accuracy: 0.1344\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1750 - accuracy: 0.2635 - val_loss: 2.2025 - val_accuracy: 0.1578\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1206 - accuracy: 0.2845 - val_loss: 2.1573 - val_accuracy: 0.1810\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0612 - accuracy: 0.3061 - val_loss: 2.0844 - val_accuracy: 0.2138\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9980 - accuracy: 0.3378 - val_loss: 1.9946 - val_accuracy: 0.3014\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9337 - accuracy: 0.3766 - val_loss: 1.8988 - val_accuracy: 0.3688\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8717 - accuracy: 0.4099 - val_loss: 1.8585 - val_accuracy: 0.4111\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8122 - accuracy: 0.4405 - val_loss: 1.7964 - val_accuracy: 0.4437\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7570 - accuracy: 0.4638 - val_loss: 1.7200 - val_accuracy: 0.5186\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7035 - accuracy: 0.4866 - val_loss: 1.6790 - val_accuracy: 0.4976\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6530 - accuracy: 0.5088 - val_loss: 1.6181 - val_accuracy: 0.5580\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6062 - accuracy: 0.5252 - val_loss: 1.5876 - val_accuracy: 0.5509\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5604 - accuracy: 0.5408 - val_loss: 1.5194 - val_accuracy: 0.5894\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5199 - accuracy: 0.5543 - val_loss: 1.5283 - val_accuracy: 0.5595\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4837 - accuracy: 0.5644 - val_loss: 1.4655 - val_accuracy: 0.5916\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4476 - accuracy: 0.5784 - val_loss: 1.4131 - val_accuracy: 0.6091\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4143 - accuracy: 0.5880 - val_loss: 1.3783 - val_accuracy: 0.6286\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3875 - accuracy: 0.5974 - val_loss: 1.3710 - val_accuracy: 0.6218\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3591 - accuracy: 0.6050 - val_loss: 1.3609 - val_accuracy: 0.6152\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3352 - accuracy: 0.6141 - val_loss: 1.3144 - val_accuracy: 0.6351\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3110 - accuracy: 0.6198 - val_loss: 1.3351 - val_accuracy: 0.6204\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2919 - accuracy: 0.6268 - val_loss: 1.3261 - val_accuracy: 0.6169\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2694 - accuracy: 0.6331 - val_loss: 1.2662 - val_accuracy: 0.6421\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2506 - accuracy: 0.6380 - val_loss: 1.2502 - val_accuracy: 0.6487\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2328 - accuracy: 0.6428 - val_loss: 1.2310 - val_accuracy: 0.6581\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2152 - accuracy: 0.6496 - val_loss: 1.2183 - val_accuracy: 0.6624\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2004 - accuracy: 0.6530 - val_loss: 1.2037 - val_accuracy: 0.6591\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1869 - accuracy: 0.6570 - val_loss: 1.1780 - val_accuracy: 0.6751\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1725 - accuracy: 0.6608 - val_loss: 1.1791 - val_accuracy: 0.6660\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1579 - accuracy: 0.6665 - val_loss: 1.1942 - val_accuracy: 0.6599\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1456 - accuracy: 0.6700 - val_loss: 1.1389 - val_accuracy: 0.6866\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1323 - accuracy: 0.6744 - val_loss: 1.1782 - val_accuracy: 0.6606\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1218 - accuracy: 0.6776 - val_loss: 1.1445 - val_accuracy: 0.6736\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1111 - accuracy: 0.6820 - val_loss: 1.1415 - val_accuracy: 0.6797\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1001 - accuracy: 0.6839 - val_loss: 1.1152 - val_accuracy: 0.6886\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0898 - accuracy: 0.6852 - val_loss: 1.0915 - val_accuracy: 0.6955\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0811 - accuracy: 0.6893 - val_loss: 1.1121 - val_accuracy: 0.6859\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0714 - accuracy: 0.6923 - val_loss: 1.0826 - val_accuracy: 0.6996\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0628 - accuracy: 0.6953 - val_loss: 1.0765 - val_accuracy: 0.6960\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0539 - accuracy: 0.6977 - val_loss: 1.0650 - val_accuracy: 0.7006\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0460 - accuracy: 0.6982 - val_loss: 1.0747 - val_accuracy: 0.6989\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0391 - accuracy: 0.7002 - val_loss: 1.0651 - val_accuracy: 0.7014\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0309 - accuracy: 0.7033 - val_loss: 1.0596 - val_accuracy: 0.7014\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0209 - accuracy: 0.7090 - val_loss: 1.0557 - val_accuracy: 0.7034\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0154 - accuracy: 0.7094 - val_loss: 1.0262 - val_accuracy: 0.7193\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0091 - accuracy: 0.7108 - val_loss: 1.0355 - val_accuracy: 0.7124\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0009 - accuracy: 0.7130 - val_loss: 1.0513 - val_accuracy: 0.7003\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9940 - accuracy: 0.7167 - val_loss: 1.0567 - val_accuracy: 0.7041\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9900 - accuracy: 0.7159 - val_loss: 1.0321 - val_accuracy: 0.7099\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9812 - accuracy: 0.7189 - val_loss: 0.9931 - val_accuracy: 0.7257\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9762 - accuracy: 0.7213 - val_loss: 1.0386 - val_accuracy: 0.7051\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9689 - accuracy: 0.7238 - val_loss: 1.0285 - val_accuracy: 0.7050\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9630 - accuracy: 0.7246 - val_loss: 0.9747 - val_accuracy: 0.7348\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9589 - accuracy: 0.7255 - val_loss: 0.9552 - val_accuracy: 0.7391\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9515 - accuracy: 0.7288 - val_loss: 0.9679 - val_accuracy: 0.7284\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9478 - accuracy: 0.7303 - val_loss: 0.9594 - val_accuracy: 0.7376\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9403 - accuracy: 0.7319 - val_loss: 0.9611 - val_accuracy: 0.7301\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9360 - accuracy: 0.7330 - val_loss: 0.9373 - val_accuracy: 0.7431\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9322 - accuracy: 0.7337 - val_loss: 0.9722 - val_accuracy: 0.7291\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9259 - accuracy: 0.7351 - val_loss: 0.9370 - val_accuracy: 0.7418\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9215 - accuracy: 0.7360 - val_loss: 0.9182 - val_accuracy: 0.7478\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9173 - accuracy: 0.7360 - val_loss: 0.9401 - val_accuracy: 0.7403\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9106 - accuracy: 0.7397 - val_loss: 0.9287 - val_accuracy: 0.7401\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9059 - accuracy: 0.7402 - val_loss: 0.9805 - val_accuracy: 0.7157\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9022 - accuracy: 0.7425 - val_loss: 0.9681 - val_accuracy: 0.7236\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8980 - accuracy: 0.7424 - val_loss: 0.9207 - val_accuracy: 0.7458\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8934 - accuracy: 0.7444 - val_loss: 0.9145 - val_accuracy: 0.7468\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8880 - accuracy: 0.7459 - val_loss: 0.9344 - val_accuracy: 0.7343\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8856 - accuracy: 0.7463 - val_loss: 0.8896 - val_accuracy: 0.7542\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8821 - accuracy: 0.7473 - val_loss: 0.9187 - val_accuracy: 0.7433\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8766 - accuracy: 0.7494 - val_loss: 0.8971 - val_accuracy: 0.7481\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8722 - accuracy: 0.7481 - val_loss: 0.8816 - val_accuracy: 0.7554\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8697 - accuracy: 0.7500 - val_loss: 0.8819 - val_accuracy: 0.7503\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8650 - accuracy: 0.7515 - val_loss: 0.9011 - val_accuracy: 0.7474\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8620 - accuracy: 0.7530 - val_loss: 0.8789 - val_accuracy: 0.7501\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8561 - accuracy: 0.7552 - val_loss: 0.9169 - val_accuracy: 0.7379\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8535 - accuracy: 0.7554 - val_loss: 0.8674 - val_accuracy: 0.7623\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8509 - accuracy: 0.7560 - val_loss: 0.8587 - val_accuracy: 0.7639\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8478 - accuracy: 0.7556 - val_loss: 0.8620 - val_accuracy: 0.7630\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8423 - accuracy: 0.7585 - val_loss: 0.8721 - val_accuracy: 0.7530\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8393 - accuracy: 0.7595 - val_loss: 0.8578 - val_accuracy: 0.7656\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8370 - accuracy: 0.7604 - val_loss: 0.8585 - val_accuracy: 0.7631\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8329 - accuracy: 0.7602 - val_loss: 0.8898 - val_accuracy: 0.7482\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8306 - accuracy: 0.7612 - val_loss: 0.8947 - val_accuracy: 0.7442\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8260 - accuracy: 0.7630 - val_loss: 0.8873 - val_accuracy: 0.7537\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8253 - accuracy: 0.7629 - val_loss: 0.8480 - val_accuracy: 0.7657\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8174 - accuracy: 0.7661 - val_loss: 0.8573 - val_accuracy: 0.7589\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8155 - accuracy: 0.7656 - val_loss: 0.8270 - val_accuracy: 0.7726\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8120 - accuracy: 0.7664 - val_loss: 0.8367 - val_accuracy: 0.7703\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8117 - accuracy: 0.7673 - val_loss: 0.8237 - val_accuracy: 0.7711\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8080 - accuracy: 0.7690 - val_loss: 0.8403 - val_accuracy: 0.7699\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8046 - accuracy: 0.7704 - val_loss: 0.8292 - val_accuracy: 0.7674\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8009 - accuracy: 0.7703 - val_loss: 0.8197 - val_accuracy: 0.7693\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7987 - accuracy: 0.7706 - val_loss: 0.8325 - val_accuracy: 0.7690\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7964 - accuracy: 0.7718 - val_loss: 0.8364 - val_accuracy: 0.7678\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7931 - accuracy: 0.7737 - val_loss: 0.8407 - val_accuracy: 0.7647\n",
      "Try 1/100: Best_val_acc: [0.8682093620300293, 0.7500555515289307], lr: 1.911243704474744e-05, Lambda: 0.0010160386298528402\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3836 - accuracy: 0.0986 - val_loss: 2.3260 - val_accuracy: 0.0016\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3659 - accuracy: 0.0986 - val_loss: 2.3216 - val_accuracy: 0.0021\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3540 - accuracy: 0.0986 - val_loss: 2.3222 - val_accuracy: 0.0024\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3451 - accuracy: 0.0986 - val_loss: 2.3243 - val_accuracy: 0.0027\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3382 - accuracy: 0.0985 - val_loss: 2.3266 - val_accuracy: 0.0034\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3327 - accuracy: 0.0983 - val_loss: 2.3293 - val_accuracy: 0.0044\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3284 - accuracy: 0.0984 - val_loss: 2.3323 - val_accuracy: 0.0053\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3248 - accuracy: 0.0982 - val_loss: 2.3347 - val_accuracy: 0.0067\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3219 - accuracy: 0.0987 - val_loss: 2.3368 - val_accuracy: 0.0087\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3194 - accuracy: 0.0994 - val_loss: 2.3386 - val_accuracy: 0.0107\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3173 - accuracy: 0.1000 - val_loss: 2.3397 - val_accuracy: 0.0123\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3155 - accuracy: 0.1005 - val_loss: 2.3400 - val_accuracy: 0.0148\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3139 - accuracy: 0.1008 - val_loss: 2.3393 - val_accuracy: 0.0165\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3124 - accuracy: 0.1020 - val_loss: 2.3393 - val_accuracy: 0.0184\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3111 - accuracy: 0.1024 - val_loss: 2.3378 - val_accuracy: 0.0198\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3098 - accuracy: 0.1033 - val_loss: 2.3371 - val_accuracy: 0.0216\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3087 - accuracy: 0.1038 - val_loss: 2.3354 - val_accuracy: 0.0239\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3076 - accuracy: 0.1045 - val_loss: 2.3339 - val_accuracy: 0.0256\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3066 - accuracy: 0.1052 - val_loss: 2.3315 - val_accuracy: 0.0284\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3056 - accuracy: 0.1058 - val_loss: 2.3301 - val_accuracy: 0.0296\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3047 - accuracy: 0.1065 - val_loss: 2.3281 - val_accuracy: 0.0314\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3038 - accuracy: 0.1069 - val_loss: 2.3259 - val_accuracy: 0.0339\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3029 - accuracy: 0.1072 - val_loss: 2.3246 - val_accuracy: 0.0363\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3021 - accuracy: 0.1079 - val_loss: 2.3230 - val_accuracy: 0.0380\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3012 - accuracy: 0.1086 - val_loss: 2.3212 - val_accuracy: 0.0417\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3003 - accuracy: 0.1089 - val_loss: 2.3190 - val_accuracy: 0.0436\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2994 - accuracy: 0.1097 - val_loss: 2.3174 - val_accuracy: 0.0459\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2986 - accuracy: 0.1097 - val_loss: 2.3160 - val_accuracy: 0.0481\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2978 - accuracy: 0.1110 - val_loss: 2.3142 - val_accuracy: 0.0519\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2970 - accuracy: 0.1116 - val_loss: 2.3132 - val_accuracy: 0.0535\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2962 - accuracy: 0.1123 - val_loss: 2.3119 - val_accuracy: 0.0554\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2954 - accuracy: 0.1130 - val_loss: 2.3103 - val_accuracy: 0.0589\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2946 - accuracy: 0.1135 - val_loss: 2.3088 - val_accuracy: 0.0615\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2939 - accuracy: 0.1148 - val_loss: 2.3081 - val_accuracy: 0.0627\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2932 - accuracy: 0.1154 - val_loss: 2.3068 - val_accuracy: 0.0663\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2925 - accuracy: 0.1163 - val_loss: 2.3054 - val_accuracy: 0.0699\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2918 - accuracy: 0.1175 - val_loss: 2.3052 - val_accuracy: 0.0708\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2912 - accuracy: 0.1187 - val_loss: 2.3037 - val_accuracy: 0.0737\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2905 - accuracy: 0.1198 - val_loss: 2.3033 - val_accuracy: 0.0764\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2898 - accuracy: 0.1208 - val_loss: 2.3026 - val_accuracy: 0.0777\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2892 - accuracy: 0.1213 - val_loss: 2.3024 - val_accuracy: 0.0781\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2886 - accuracy: 0.1218 - val_loss: 2.3015 - val_accuracy: 0.0809\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2879 - accuracy: 0.1228 - val_loss: 2.3006 - val_accuracy: 0.0842\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2873 - accuracy: 0.1243 - val_loss: 2.3002 - val_accuracy: 0.0847\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2867 - accuracy: 0.1252 - val_loss: 2.3002 - val_accuracy: 0.0821\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2861 - accuracy: 0.1268 - val_loss: 2.2986 - val_accuracy: 0.0874\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2855 - accuracy: 0.1273 - val_loss: 2.2980 - val_accuracy: 0.0895\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2850 - accuracy: 0.1282 - val_loss: 2.2976 - val_accuracy: 0.0894\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2844 - accuracy: 0.1285 - val_loss: 2.2969 - val_accuracy: 0.0899\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2838 - accuracy: 0.1296 - val_loss: 2.2968 - val_accuracy: 0.0901\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2832 - accuracy: 0.1303 - val_loss: 2.2957 - val_accuracy: 0.0916\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2826 - accuracy: 0.1313 - val_loss: 2.2955 - val_accuracy: 0.0901\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2820 - accuracy: 0.1315 - val_loss: 2.2944 - val_accuracy: 0.0939\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2815 - accuracy: 0.1330 - val_loss: 2.2934 - val_accuracy: 0.0956\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2809 - accuracy: 0.1338 - val_loss: 2.2931 - val_accuracy: 0.0963\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2803 - accuracy: 0.1338 - val_loss: 2.2922 - val_accuracy: 0.0962\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2797 - accuracy: 0.1355 - val_loss: 2.2917 - val_accuracy: 0.0973\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2792 - accuracy: 0.1352 - val_loss: 2.2916 - val_accuracy: 0.0969\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2786 - accuracy: 0.1360 - val_loss: 2.2906 - val_accuracy: 0.0994\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2780 - accuracy: 0.1369 - val_loss: 2.2901 - val_accuracy: 0.1001\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2774 - accuracy: 0.1387 - val_loss: 2.2896 - val_accuracy: 0.1018\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2769 - accuracy: 0.1390 - val_loss: 2.2895 - val_accuracy: 0.1011\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2763 - accuracy: 0.1403 - val_loss: 2.2889 - val_accuracy: 0.1030\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2758 - accuracy: 0.1414 - val_loss: 2.2883 - val_accuracy: 0.1025\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2752 - accuracy: 0.1409 - val_loss: 2.2874 - val_accuracy: 0.1056\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2746 - accuracy: 0.1423 - val_loss: 2.2868 - val_accuracy: 0.1066\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2741 - accuracy: 0.1431 - val_loss: 2.2859 - val_accuracy: 0.1100\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2735 - accuracy: 0.1445 - val_loss: 2.2861 - val_accuracy: 0.1079\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2729 - accuracy: 0.1451 - val_loss: 2.2847 - val_accuracy: 0.1113\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2724 - accuracy: 0.1458 - val_loss: 2.2848 - val_accuracy: 0.1102\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2718 - accuracy: 0.1449 - val_loss: 2.2840 - val_accuracy: 0.1127\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2713 - accuracy: 0.1467 - val_loss: 2.2839 - val_accuracy: 0.1115\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2707 - accuracy: 0.1472 - val_loss: 2.2828 - val_accuracy: 0.1130\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2702 - accuracy: 0.1476 - val_loss: 2.2817 - val_accuracy: 0.1168\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2696 - accuracy: 0.1488 - val_loss: 2.2816 - val_accuracy: 0.1163\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2690 - accuracy: 0.1487 - val_loss: 2.2803 - val_accuracy: 0.1206\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2685 - accuracy: 0.1506 - val_loss: 2.2796 - val_accuracy: 0.1229\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2679 - accuracy: 0.1516 - val_loss: 2.2790 - val_accuracy: 0.1224\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2673 - accuracy: 0.1512 - val_loss: 2.2778 - val_accuracy: 0.1261\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2667 - accuracy: 0.1528 - val_loss: 2.2773 - val_accuracy: 0.1265\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2661 - accuracy: 0.1528 - val_loss: 2.2762 - val_accuracy: 0.1282\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2655 - accuracy: 0.1533 - val_loss: 2.2753 - val_accuracy: 0.1309\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2649 - accuracy: 0.1546 - val_loss: 2.2747 - val_accuracy: 0.1332\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2642 - accuracy: 0.1557 - val_loss: 2.2744 - val_accuracy: 0.1329\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2636 - accuracy: 0.1566 - val_loss: 2.2732 - val_accuracy: 0.1343\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2630 - accuracy: 0.1559 - val_loss: 2.2732 - val_accuracy: 0.1316\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2623 - accuracy: 0.1560 - val_loss: 2.2713 - val_accuracy: 0.1371\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2616 - accuracy: 0.1579 - val_loss: 2.2710 - val_accuracy: 0.1354\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2610 - accuracy: 0.1577 - val_loss: 2.2696 - val_accuracy: 0.1384\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2603 - accuracy: 0.1582 - val_loss: 2.2695 - val_accuracy: 0.1406\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2596 - accuracy: 0.1589 - val_loss: 2.2677 - val_accuracy: 0.1451\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2589 - accuracy: 0.1597 - val_loss: 2.2679 - val_accuracy: 0.1419\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2583 - accuracy: 0.1592 - val_loss: 2.2666 - val_accuracy: 0.1460\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2576 - accuracy: 0.1607 - val_loss: 2.2661 - val_accuracy: 0.1476\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2569 - accuracy: 0.1618 - val_loss: 2.2661 - val_accuracy: 0.1461\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2562 - accuracy: 0.1617 - val_loss: 2.2655 - val_accuracy: 0.1483\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2555 - accuracy: 0.1625 - val_loss: 2.2646 - val_accuracy: 0.1482\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2549 - accuracy: 0.1628 - val_loss: 2.2645 - val_accuracy: 0.1499\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2542 - accuracy: 0.1634 - val_loss: 2.2640 - val_accuracy: 0.1524\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2535 - accuracy: 0.1636 - val_loss: 2.2629 - val_accuracy: 0.1546\n",
      "Try 2/100: Best_val_acc: [2.253401517868042, 0.16699999570846558], lr: 3.49510420365358e-07, Lambda: 5.855088542078346e-06\n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 14084.0869 - accuracy: 0.0992 - val_loss: 3.7858 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7857 - accuracy: 0.0970 - val_loss: 3.7666 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7855 - accuracy: 0.0986 - val_loss: 3.8106 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7851 - accuracy: 0.1005 - val_loss: 3.8016 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7840 - accuracy: 0.1043 - val_loss: 3.7490 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7836 - accuracy: 0.0973 - val_loss: 3.7849 - val_accuracy: 0.2857\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7812 - accuracy: 0.0992 - val_loss: 3.7993 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7784 - accuracy: 0.1014 - val_loss: 3.7799 - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7764 - accuracy: 0.1024 - val_loss: 3.7913 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7774 - accuracy: 0.0984 - val_loss: 3.7478 - val_accuracy: 0.2857\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7726 - accuracy: 0.1000 - val_loss: 3.7871 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7724 - accuracy: 0.1006 - val_loss: 3.7278 - val_accuracy: 0.2857\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7682 - accuracy: 0.0988 - val_loss: 3.7551 - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7652 - accuracy: 0.1023 - val_loss: 3.6991 - val_accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7639 - accuracy: 0.1007 - val_loss: 3.8063 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7593 - accuracy: 0.1002 - val_loss: 3.7196 - val_accuracy: 0.1429\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7583 - accuracy: 0.0969 - val_loss: 3.8392 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7547 - accuracy: 0.1031 - val_loss: 3.7289 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7481 - accuracy: 0.0993 - val_loss: 3.6992 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7460 - accuracy: 0.0976 - val_loss: 3.7015 - val_accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7427 - accuracy: 0.1014 - val_loss: 3.8125 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7372 - accuracy: 0.1015 - val_loss: 3.6888 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7330 - accuracy: 0.1005 - val_loss: 3.7512 - val_accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7290 - accuracy: 0.1008 - val_loss: 3.7154 - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7257 - accuracy: 0.0978 - val_loss: 3.7182 - val_accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7198 - accuracy: 0.0996 - val_loss: 3.6701 - val_accuracy: 0.1429\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7170 - accuracy: 0.0991 - val_loss: 3.7216 - val_accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7100 - accuracy: 0.0983 - val_loss: 3.7710 - val_accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.7038 - accuracy: 0.1000 - val_loss: 3.6606 - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6984 - accuracy: 0.1012 - val_loss: 3.6724 - val_accuracy: 0.1429\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6940 - accuracy: 0.1005 - val_loss: 3.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6878 - accuracy: 0.0972 - val_loss: 3.6938 - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6835 - accuracy: 0.0995 - val_loss: 3.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6754 - accuracy: 0.0996 - val_loss: 3.6370 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6695 - accuracy: 0.0978 - val_loss: 3.7645 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6629 - accuracy: 0.0992 - val_loss: 3.6980 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6558 - accuracy: 0.0987 - val_loss: 3.6840 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6489 - accuracy: 0.0989 - val_loss: 3.5919 - val_accuracy: 0.2857\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6417 - accuracy: 0.1007 - val_loss: 3.6117 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6330 - accuracy: 0.1010 - val_loss: 3.6107 - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6265 - accuracy: 0.1002 - val_loss: 3.6744 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6187 - accuracy: 0.0998 - val_loss: 3.6112 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6116 - accuracy: 0.1012 - val_loss: 3.5547 - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.6033 - accuracy: 0.1000 - val_loss: 3.6464 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5928 - accuracy: 0.0998 - val_loss: 3.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5857 - accuracy: 0.0986 - val_loss: 3.7161 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5783 - accuracy: 0.0991 - val_loss: 3.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5702 - accuracy: 0.0969 - val_loss: 3.4552 - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5599 - accuracy: 0.0989 - val_loss: 3.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5511 - accuracy: 0.1021 - val_loss: 3.5233 - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5443 - accuracy: 0.0977 - val_loss: 3.4935 - val_accuracy: 0.2857\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5325 - accuracy: 0.0998 - val_loss: 3.4987 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5223 - accuracy: 0.1019 - val_loss: 3.4907 - val_accuracy: 0.1429\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5150 - accuracy: 0.1017 - val_loss: 3.4071 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.5053 - accuracy: 0.0973 - val_loss: 3.5206 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4963 - accuracy: 0.1010 - val_loss: 3.5214 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4858 - accuracy: 0.0963 - val_loss: 3.5621 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4744 - accuracy: 0.1013 - val_loss: 3.6009 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4668 - accuracy: 0.0995 - val_loss: 3.5173 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4543 - accuracy: 0.1022 - val_loss: 3.4462 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4432 - accuracy: 0.1005 - val_loss: 3.4645 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4339 - accuracy: 0.1022 - val_loss: 3.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4245 - accuracy: 0.0997 - val_loss: 3.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4146 - accuracy: 0.0999 - val_loss: 3.3980 - val_accuracy: 0.1429\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.4031 - accuracy: 0.1002 - val_loss: 3.3709 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3941 - accuracy: 0.1001 - val_loss: 3.3562 - val_accuracy: 0.2857\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3828 - accuracy: 0.1017 - val_loss: 3.3934 - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3730 - accuracy: 0.0980 - val_loss: 3.3734 - val_accuracy: 0.1429\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3619 - accuracy: 0.1007 - val_loss: 3.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3507 - accuracy: 0.1024 - val_loss: 3.3947 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3401 - accuracy: 0.1019 - val_loss: 3.3509 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3303 - accuracy: 0.1012 - val_loss: 3.3100 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3191 - accuracy: 0.1007 - val_loss: 3.2724 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3105 - accuracy: 0.1016 - val_loss: 3.4036 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.3004 - accuracy: 0.0998 - val_loss: 3.2883 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2897 - accuracy: 0.1001 - val_loss: 3.3500 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2779 - accuracy: 0.0980 - val_loss: 3.2909 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2669 - accuracy: 0.0976 - val_loss: 3.2479 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2565 - accuracy: 0.0986 - val_loss: 3.2140 - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2458 - accuracy: 0.0983 - val_loss: 3.1847 - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2355 - accuracy: 0.0992 - val_loss: 3.3045 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2252 - accuracy: 0.1018 - val_loss: 3.1979 - val_accuracy: 0.2857\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2148 - accuracy: 0.0975 - val_loss: 3.3066 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.2044 - accuracy: 0.0981 - val_loss: 3.1796 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1922 - accuracy: 0.1000 - val_loss: 3.1741 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1808 - accuracy: 0.1001 - val_loss: 3.1920 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1724 - accuracy: 0.0982 - val_loss: 3.1534 - val_accuracy: 0.1429\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1620 - accuracy: 0.0992 - val_loss: 3.1772 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1509 - accuracy: 0.1000 - val_loss: 3.1784 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1419 - accuracy: 0.0982 - val_loss: 3.0544 - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1297 - accuracy: 0.0991 - val_loss: 3.0971 - val_accuracy: 0.1429\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1192 - accuracy: 0.1016 - val_loss: 3.1051 - val_accuracy: 0.2857\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.1090 - accuracy: 0.0985 - val_loss: 3.1239 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.0981 - accuracy: 0.1000 - val_loss: 3.2139 - val_accuracy: 0.1429\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.0902 - accuracy: 0.1015 - val_loss: 3.1695 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 3.0770 - accuracy: 0.1000 - val_loss: 3.1545 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 3.0675 - accuracy: 0.0992 - val_loss: 3.0204 - val_accuracy: 0.2857\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 3.0552 - accuracy: 0.0959 - val_loss: 3.0338 - val_accuracy: 0.2857\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 3.0444 - accuracy: 0.0980 - val_loss: 2.9425 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 3.0319 - accuracy: 0.1002 - val_loss: 3.0704 - val_accuracy: 0.0000e+00\n",
      "Try 3/100: Best_val_acc: [3.0305421352386475, 0.10066666454076767], lr: 0.2583069651155033, Lambda: 0.006767523432454671\n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: nan - accuracy: 0.0993 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Try 4/100: Best_val_acc: [nan, 0.1007777750492096], lr: 132.39660512566152, Lambda: 2.246670536741038e-06\n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.6046 - accuracy: 0.0998 - val_loss: 2.3774 - val_accuracy: 0.0237\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.5648 - accuracy: 0.1004 - val_loss: 2.3356 - val_accuracy: 0.0386\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.5318 - accuracy: 0.1016 - val_loss: 2.3029 - val_accuracy: 0.0620\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.5042 - accuracy: 0.1035 - val_loss: 2.2774 - val_accuracy: 0.0890\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4807 - accuracy: 0.1058 - val_loss: 2.2574 - val_accuracy: 0.1168\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.4604 - accuracy: 0.1062 - val_loss: 2.2421 - val_accuracy: 0.1390\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.4427 - accuracy: 0.1049 - val_loss: 2.2297 - val_accuracy: 0.1558\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4269 - accuracy: 0.1040 - val_loss: 2.2204 - val_accuracy: 0.1671\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4128 - accuracy: 0.1038 - val_loss: 2.2129 - val_accuracy: 0.1779\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4001 - accuracy: 0.1037 - val_loss: 2.2076 - val_accuracy: 0.1820\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3887 - accuracy: 0.1033 - val_loss: 2.2032 - val_accuracy: 0.1857\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3783 - accuracy: 0.1030 - val_loss: 2.1999 - val_accuracy: 0.1873\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 2.3691 - accuracy: 0.1028 - val_loss: 2.1974 - val_accuracy: 0.1876\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3608 - accuracy: 0.1026 - val_loss: 2.1960 - val_accuracy: 0.1876\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3535 - accuracy: 0.1035 - val_loss: 2.1951 - val_accuracy: 0.1870\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3469 - accuracy: 0.1043 - val_loss: 2.1941 - val_accuracy: 0.1854\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3410 - accuracy: 0.1045 - val_loss: 2.1937 - val_accuracy: 0.1833\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3359 - accuracy: 0.1059 - val_loss: 2.1940 - val_accuracy: 0.1814\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3314 - accuracy: 0.1064 - val_loss: 2.1936 - val_accuracy: 0.1802\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3275 - accuracy: 0.1077 - val_loss: 2.1934 - val_accuracy: 0.1804\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3242 - accuracy: 0.1096 - val_loss: 2.1936 - val_accuracy: 0.1822\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3214 - accuracy: 0.1110 - val_loss: 2.1938 - val_accuracy: 0.1854\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3189 - accuracy: 0.1125 - val_loss: 2.1940 - val_accuracy: 0.1890\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3169 - accuracy: 0.1145 - val_loss: 2.1941 - val_accuracy: 0.1959\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3150 - accuracy: 0.1166 - val_loss: 2.1944 - val_accuracy: 0.2034\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3133 - accuracy: 0.1183 - val_loss: 2.1947 - val_accuracy: 0.2111\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3119 - accuracy: 0.1199 - val_loss: 2.1950 - val_accuracy: 0.2182\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3106 - accuracy: 0.1205 - val_loss: 2.1959 - val_accuracy: 0.2233\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3094 - accuracy: 0.1214 - val_loss: 2.1967 - val_accuracy: 0.2267\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3083 - accuracy: 0.1218 - val_loss: 2.1971 - val_accuracy: 0.2309\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3073 - accuracy: 0.1226 - val_loss: 2.1975 - val_accuracy: 0.2356\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3063 - accuracy: 0.1242 - val_loss: 2.1981 - val_accuracy: 0.2387\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3055 - accuracy: 0.1245 - val_loss: 2.1988 - val_accuracy: 0.2417\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3046 - accuracy: 0.1253 - val_loss: 2.1989 - val_accuracy: 0.2440\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3038 - accuracy: 0.1258 - val_loss: 2.1994 - val_accuracy: 0.2464\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.1262 - val_loss: 2.2000 - val_accuracy: 0.2488\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3023 - accuracy: 0.1266 - val_loss: 2.2002 - val_accuracy: 0.2501\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3015 - accuracy: 0.1280 - val_loss: 2.2011 - val_accuracy: 0.2513\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3008 - accuracy: 0.1285 - val_loss: 2.2017 - val_accuracy: 0.2535\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3001 - accuracy: 0.1288 - val_loss: 2.2021 - val_accuracy: 0.2548\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2994 - accuracy: 0.1296 - val_loss: 2.2020 - val_accuracy: 0.2566\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2987 - accuracy: 0.1305 - val_loss: 2.2027 - val_accuracy: 0.2572\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2980 - accuracy: 0.1316 - val_loss: 2.2027 - val_accuracy: 0.2596\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2974 - accuracy: 0.1324 - val_loss: 2.2026 - val_accuracy: 0.2611\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2967 - accuracy: 0.1333 - val_loss: 2.2030 - val_accuracy: 0.2622\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2960 - accuracy: 0.1340 - val_loss: 2.2035 - val_accuracy: 0.2619\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2954 - accuracy: 0.1350 - val_loss: 2.2040 - val_accuracy: 0.2623\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.2947 - accuracy: 0.1359 - val_loss: 2.2042 - val_accuracy: 0.2634\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2940 - accuracy: 0.1369 - val_loss: 2.2039 - val_accuracy: 0.2654\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2934 - accuracy: 0.1377 - val_loss: 2.2040 - val_accuracy: 0.2653\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2927 - accuracy: 0.1389 - val_loss: 2.2045 - val_accuracy: 0.2648\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2921 - accuracy: 0.1396 - val_loss: 2.2040 - val_accuracy: 0.2681\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2914 - accuracy: 0.1409 - val_loss: 2.2042 - val_accuracy: 0.2691\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2908 - accuracy: 0.1423 - val_loss: 2.2049 - val_accuracy: 0.2679\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2901 - accuracy: 0.1429 - val_loss: 2.2047 - val_accuracy: 0.2690\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2895 - accuracy: 0.1436 - val_loss: 2.2049 - val_accuracy: 0.2704\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2888 - accuracy: 0.1450 - val_loss: 2.2046 - val_accuracy: 0.2719\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2882 - accuracy: 0.1454 - val_loss: 2.2047 - val_accuracy: 0.2739\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2876 - accuracy: 0.1466 - val_loss: 2.2050 - val_accuracy: 0.2730\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2869 - accuracy: 0.1474 - val_loss: 2.2049 - val_accuracy: 0.2751\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2863 - accuracy: 0.1479 - val_loss: 2.2047 - val_accuracy: 0.2762\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2857 - accuracy: 0.1490 - val_loss: 2.2054 - val_accuracy: 0.2766\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2851 - accuracy: 0.1500 - val_loss: 2.2048 - val_accuracy: 0.2769\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2844 - accuracy: 0.1506 - val_loss: 2.2053 - val_accuracy: 0.2764\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2838 - accuracy: 0.1511 - val_loss: 2.2052 - val_accuracy: 0.2769\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2833 - accuracy: 0.1519 - val_loss: 2.2049 - val_accuracy: 0.2782\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2827 - accuracy: 0.1528 - val_loss: 2.2050 - val_accuracy: 0.2783\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2821 - accuracy: 0.1537 - val_loss: 2.2047 - val_accuracy: 0.2783\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2815 - accuracy: 0.1542 - val_loss: 2.2049 - val_accuracy: 0.2784\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2810 - accuracy: 0.1547 - val_loss: 2.2047 - val_accuracy: 0.2784\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2804 - accuracy: 0.1552 - val_loss: 2.2047 - val_accuracy: 0.2790\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2799 - accuracy: 0.1559 - val_loss: 2.2041 - val_accuracy: 0.2805\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2793 - accuracy: 0.1559 - val_loss: 2.2044 - val_accuracy: 0.2796\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2788 - accuracy: 0.1569 - val_loss: 2.2045 - val_accuracy: 0.2790\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2782 - accuracy: 0.1576 - val_loss: 2.2043 - val_accuracy: 0.2804\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2777 - accuracy: 0.1584 - val_loss: 2.2042 - val_accuracy: 0.2814\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2772 - accuracy: 0.1584 - val_loss: 2.2035 - val_accuracy: 0.2830\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2766 - accuracy: 0.1598 - val_loss: 2.2040 - val_accuracy: 0.2821\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2761 - accuracy: 0.1601 - val_loss: 2.2032 - val_accuracy: 0.2844\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2756 - accuracy: 0.1615 - val_loss: 2.2035 - val_accuracy: 0.2846\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2751 - accuracy: 0.1623 - val_loss: 2.2034 - val_accuracy: 0.2838\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2745 - accuracy: 0.1627 - val_loss: 2.2025 - val_accuracy: 0.2862\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2740 - accuracy: 0.1635 - val_loss: 2.2022 - val_accuracy: 0.2866\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2735 - accuracy: 0.1645 - val_loss: 2.2023 - val_accuracy: 0.2854\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2730 - accuracy: 0.1651 - val_loss: 2.2023 - val_accuracy: 0.2859\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2724 - accuracy: 0.1660 - val_loss: 2.2019 - val_accuracy: 0.2867\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2719 - accuracy: 0.1670 - val_loss: 2.2014 - val_accuracy: 0.2876\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2714 - accuracy: 0.1673 - val_loss: 2.2006 - val_accuracy: 0.2902\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2709 - accuracy: 0.1681 - val_loss: 2.2010 - val_accuracy: 0.2882\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2704 - accuracy: 0.1690 - val_loss: 2.2000 - val_accuracy: 0.2911\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2699 - accuracy: 0.1699 - val_loss: 2.2004 - val_accuracy: 0.2888\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2693 - accuracy: 0.1706 - val_loss: 2.1994 - val_accuracy: 0.2921\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2688 - accuracy: 0.1713 - val_loss: 2.1993 - val_accuracy: 0.2921\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2683 - accuracy: 0.1716 - val_loss: 2.1986 - val_accuracy: 0.2938\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2678 - accuracy: 0.1728 - val_loss: 2.1988 - val_accuracy: 0.2933\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2673 - accuracy: 0.1737 - val_loss: 2.1983 - val_accuracy: 0.2933\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2669 - accuracy: 0.1739 - val_loss: 2.1977 - val_accuracy: 0.2938\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2664 - accuracy: 0.1745 - val_loss: 2.1976 - val_accuracy: 0.2942\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2659 - accuracy: 0.1752 - val_loss: 2.1974 - val_accuracy: 0.2944\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2654 - accuracy: 0.1757 - val_loss: 2.1974 - val_accuracy: 0.2941\n",
      "Try 5/100: Best_val_acc: [2.266181230545044, 0.17644444108009338], lr: 2.1298849735211875e-07, Lambda: 6.019683990508908e-05\n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.5763 - accuracy: 0.0997 - val_loss: 2.4098 - val_accuracy: 7.1429e-05\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.5064 - accuracy: 0.0997 - val_loss: 2.3899 - val_accuracy: 7.1429e-05\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4585 - accuracy: 0.0997 - val_loss: 2.3783 - val_accuracy: 1.4286e-04\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4248 - accuracy: 0.0997 - val_loss: 2.3704 - val_accuracy: 2.8571e-04\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3998 - accuracy: 0.0997 - val_loss: 2.3630 - val_accuracy: 3.5714e-04\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3806 - accuracy: 0.0997 - val_loss: 2.3560 - val_accuracy: 5.0000e-04\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3654 - accuracy: 0.0997 - val_loss: 2.3486 - val_accuracy: 7.1429e-04\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3532 - accuracy: 0.0998 - val_loss: 2.3416 - val_accuracy: 0.0010\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3432 - accuracy: 0.0998 - val_loss: 2.3364 - val_accuracy: 0.0015\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3350 - accuracy: 0.1000 - val_loss: 2.3318 - val_accuracy: 0.0024\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3282 - accuracy: 0.1000 - val_loss: 2.3279 - val_accuracy: 0.0034\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3227 - accuracy: 0.1000 - val_loss: 2.3244 - val_accuracy: 0.0049\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3182 - accuracy: 0.1000 - val_loss: 2.3222 - val_accuracy: 0.0081\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3144 - accuracy: 0.1005 - val_loss: 2.3207 - val_accuracy: 0.0113\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3113 - accuracy: 0.1004 - val_loss: 2.3196 - val_accuracy: 0.0144\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3085 - accuracy: 0.1009 - val_loss: 2.3174 - val_accuracy: 0.0184\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3061 - accuracy: 0.1017 - val_loss: 2.3159 - val_accuracy: 0.0228\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3039 - accuracy: 0.1020 - val_loss: 2.3146 - val_accuracy: 0.0285\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3019 - accuracy: 0.1025 - val_loss: 2.3137 - val_accuracy: 0.0344\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3000 - accuracy: 0.1043 - val_loss: 2.3126 - val_accuracy: 0.0417\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2982 - accuracy: 0.1056 - val_loss: 2.3107 - val_accuracy: 0.0498\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2964 - accuracy: 0.1071 - val_loss: 2.3090 - val_accuracy: 0.0573\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2948 - accuracy: 0.1094 - val_loss: 2.3068 - val_accuracy: 0.0637\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2933 - accuracy: 0.1112 - val_loss: 2.3052 - val_accuracy: 0.0779\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2918 - accuracy: 0.1137 - val_loss: 2.3017 - val_accuracy: 0.0845\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2905 - accuracy: 0.1158 - val_loss: 2.3001 - val_accuracy: 0.0907\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2891 - accuracy: 0.1181 - val_loss: 2.2974 - val_accuracy: 0.0991\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2878 - accuracy: 0.1204 - val_loss: 2.2955 - val_accuracy: 0.1048\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2866 - accuracy: 0.1225 - val_loss: 2.2928 - val_accuracy: 0.1124\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2854 - accuracy: 0.1247 - val_loss: 2.2898 - val_accuracy: 0.1206\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2841 - accuracy: 0.1272 - val_loss: 2.2861 - val_accuracy: 0.1271\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2828 - accuracy: 0.1294 - val_loss: 2.2839 - val_accuracy: 0.1341\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2815 - accuracy: 0.1319 - val_loss: 2.2813 - val_accuracy: 0.1481\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2801 - accuracy: 0.1345 - val_loss: 2.2779 - val_accuracy: 0.1573\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2787 - accuracy: 0.1383 - val_loss: 2.2757 - val_accuracy: 0.1675\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2772 - accuracy: 0.1412 - val_loss: 2.2729 - val_accuracy: 0.1756\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2757 - accuracy: 0.1449 - val_loss: 2.2702 - val_accuracy: 0.1848\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2743 - accuracy: 0.1481 - val_loss: 2.2672 - val_accuracy: 0.1916\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2729 - accuracy: 0.1494 - val_loss: 2.2648 - val_accuracy: 0.1976\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2715 - accuracy: 0.1523 - val_loss: 2.2625 - val_accuracy: 0.2014\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2701 - accuracy: 0.1547 - val_loss: 2.2609 - val_accuracy: 0.2056\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2688 - accuracy: 0.1561 - val_loss: 2.2596 - val_accuracy: 0.2103\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2675 - accuracy: 0.1577 - val_loss: 2.2576 - val_accuracy: 0.2161\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2663 - accuracy: 0.1605 - val_loss: 2.2567 - val_accuracy: 0.2175\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2650 - accuracy: 0.1605 - val_loss: 2.2544 - val_accuracy: 0.2240\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2637 - accuracy: 0.1637 - val_loss: 2.2529 - val_accuracy: 0.2256\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2625 - accuracy: 0.1664 - val_loss: 2.2522 - val_accuracy: 0.2271\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2612 - accuracy: 0.1664 - val_loss: 2.2504 - val_accuracy: 0.2290\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2598 - accuracy: 0.1687 - val_loss: 2.2488 - val_accuracy: 0.2314\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2585 - accuracy: 0.1707 - val_loss: 2.2463 - val_accuracy: 0.2371\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2572 - accuracy: 0.1729 - val_loss: 2.2463 - val_accuracy: 0.2374\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2559 - accuracy: 0.1745 - val_loss: 2.2451 - val_accuracy: 0.2395\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2545 - accuracy: 0.1773 - val_loss: 2.2438 - val_accuracy: 0.2391\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2532 - accuracy: 0.1787 - val_loss: 2.2413 - val_accuracy: 0.2449\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2519 - accuracy: 0.1807 - val_loss: 2.2408 - val_accuracy: 0.2434\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2505 - accuracy: 0.1829 - val_loss: 2.2388 - val_accuracy: 0.2462\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2491 - accuracy: 0.1846 - val_loss: 2.2384 - val_accuracy: 0.2471\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2478 - accuracy: 0.1866 - val_loss: 2.2371 - val_accuracy: 0.2480\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2465 - accuracy: 0.1883 - val_loss: 2.2352 - val_accuracy: 0.2505\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2451 - accuracy: 0.1906 - val_loss: 2.2347 - val_accuracy: 0.2521\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2438 - accuracy: 0.1916 - val_loss: 2.2340 - val_accuracy: 0.2549\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2424 - accuracy: 0.1921 - val_loss: 2.2322 - val_accuracy: 0.2571\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2410 - accuracy: 0.1947 - val_loss: 2.2306 - val_accuracy: 0.2608\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2396 - accuracy: 0.1971 - val_loss: 2.2290 - val_accuracy: 0.2599\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2382 - accuracy: 0.1971 - val_loss: 2.2278 - val_accuracy: 0.2650\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2368 - accuracy: 0.1984 - val_loss: 2.2260 - val_accuracy: 0.2611\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2354 - accuracy: 0.2003 - val_loss: 2.2255 - val_accuracy: 0.2617\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2340 - accuracy: 0.2017 - val_loss: 2.2250 - val_accuracy: 0.2642\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2325 - accuracy: 0.2035 - val_loss: 2.2228 - val_accuracy: 0.2665\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2311 - accuracy: 0.2060 - val_loss: 2.2217 - val_accuracy: 0.2684\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2296 - accuracy: 0.2071 - val_loss: 2.2197 - val_accuracy: 0.2707\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2280 - accuracy: 0.2087 - val_loss: 2.2183 - val_accuracy: 0.2744\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2265 - accuracy: 0.2103 - val_loss: 2.2149 - val_accuracy: 0.2768\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2250 - accuracy: 0.2107 - val_loss: 2.2144 - val_accuracy: 0.2785\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2234 - accuracy: 0.2133 - val_loss: 2.2143 - val_accuracy: 0.2781\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2218 - accuracy: 0.2137 - val_loss: 2.2124 - val_accuracy: 0.2801\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2201 - accuracy: 0.2156 - val_loss: 2.2109 - val_accuracy: 0.2811\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2185 - accuracy: 0.2170 - val_loss: 2.2093 - val_accuracy: 0.2831\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2168 - accuracy: 0.2185 - val_loss: 2.2068 - val_accuracy: 0.2852\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2152 - accuracy: 0.2209 - val_loss: 2.2049 - val_accuracy: 0.2831\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2135 - accuracy: 0.2217 - val_loss: 2.2013 - val_accuracy: 0.2896\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2119 - accuracy: 0.2235 - val_loss: 2.1994 - val_accuracy: 0.2916\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2102 - accuracy: 0.2246 - val_loss: 2.1989 - val_accuracy: 0.2920\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2086 - accuracy: 0.2269 - val_loss: 2.1972 - val_accuracy: 0.2914\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2070 - accuracy: 0.2270 - val_loss: 2.1938 - val_accuracy: 0.2975\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2053 - accuracy: 0.2295 - val_loss: 2.1918 - val_accuracy: 0.2956\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2036 - accuracy: 0.2300 - val_loss: 2.1914 - val_accuracy: 0.2959\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2020 - accuracy: 0.2314 - val_loss: 2.1891 - val_accuracy: 0.2971\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2003 - accuracy: 0.2328 - val_loss: 2.1874 - val_accuracy: 0.3000\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1987 - accuracy: 0.2342 - val_loss: 2.1849 - val_accuracy: 0.3025\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1970 - accuracy: 0.2373 - val_loss: 2.1820 - val_accuracy: 0.3063\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1953 - accuracy: 0.2377 - val_loss: 2.1811 - val_accuracy: 0.3037\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1936 - accuracy: 0.2386 - val_loss: 2.1785 - val_accuracy: 0.3053\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1919 - accuracy: 0.2411 - val_loss: 2.1759 - val_accuracy: 0.3056\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1902 - accuracy: 0.2418 - val_loss: 2.1742 - val_accuracy: 0.3119\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1885 - accuracy: 0.2431 - val_loss: 2.1730 - val_accuracy: 0.3111\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1868 - accuracy: 0.2441 - val_loss: 2.1715 - val_accuracy: 0.3105\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1851 - accuracy: 0.2462 - val_loss: 2.1695 - val_accuracy: 0.3076\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1835 - accuracy: 0.2463 - val_loss: 2.1674 - val_accuracy: 0.3150\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1817 - accuracy: 0.2494 - val_loss: 2.1657 - val_accuracy: 0.3109\n",
      "Try 6/100: Best_val_acc: [2.182333469390869, 0.25], lr: 5.910812269175405e-07, Lambda: 1.1130480363768867e-05\n",
      "\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: nan - accuracy: 0.0998 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.2857\n",
      "Try 7/100: Best_val_acc: [nan, 0.1007777750492096], lr: 478.3558678201535, Lambda: 1.4992046673471612e-06\n",
      "\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2866930778112.0000 - accuracy: 0.0989 - val_loss: 6.1514 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7943 - accuracy: 0.0998 - val_loss: 5.7635 - val_accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7535 - accuracy: 0.0987 - val_loss: 5.4871 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7597 - accuracy: 0.1025 - val_loss: 5.6838 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7663 - accuracy: 0.0993 - val_loss: 5.7825 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7568 - accuracy: 0.0996 - val_loss: 5.7748 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7705 - accuracy: 0.1003 - val_loss: 5.7251 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7802 - accuracy: 0.1011 - val_loss: 6.0031 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7939 - accuracy: 0.1000 - val_loss: 5.7032 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8077 - accuracy: 0.0990 - val_loss: 5.9409 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7736 - accuracy: 0.1031 - val_loss: 5.7820 - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7843 - accuracy: 0.0997 - val_loss: 5.7081 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7865 - accuracy: 0.1006 - val_loss: 6.0242 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7990 - accuracy: 0.0985 - val_loss: 5.9895 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8047 - accuracy: 0.1020 - val_loss: 5.9335 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8024 - accuracy: 0.1001 - val_loss: 6.1201 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7883 - accuracy: 0.1022 - val_loss: 5.7168 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8101 - accuracy: 0.0972 - val_loss: 5.7176 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8371 - accuracy: 0.1000 - val_loss: 5.7504 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8204 - accuracy: 0.1012 - val_loss: 6.0479 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8167 - accuracy: 0.0990 - val_loss: 5.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8017 - accuracy: 0.1004 - val_loss: 5.7732 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8059 - accuracy: 0.1008 - val_loss: 6.0509 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8022 - accuracy: 0.0978 - val_loss: 5.9926 - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8279 - accuracy: 0.1025 - val_loss: 5.5910 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8122 - accuracy: 0.0985 - val_loss: 5.5443 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8521 - accuracy: 0.1000 - val_loss: 5.9247 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8488 - accuracy: 0.0994 - val_loss: 6.3713 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8032 - accuracy: 0.0981 - val_loss: 5.4818 - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8344 - accuracy: 0.0990 - val_loss: 5.7719 - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8288 - accuracy: 0.1006 - val_loss: 6.1605 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8466 - accuracy: 0.0984 - val_loss: 6.1023 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8166 - accuracy: 0.1011 - val_loss: 5.6143 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8071 - accuracy: 0.0999 - val_loss: 5.7882 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7995 - accuracy: 0.1035 - val_loss: 5.8236 - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8130 - accuracy: 0.1019 - val_loss: 5.6586 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8276 - accuracy: 0.1000 - val_loss: 6.0237 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8433 - accuracy: 0.1000 - val_loss: 5.8354 - val_accuracy: 0.1429\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8493 - accuracy: 0.1007 - val_loss: 5.6831 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8277 - accuracy: 0.0984 - val_loss: 5.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 5.8192 - accuracy: 0.0991 - val_loss: 5.7339 - val_accuracy: 0.1429\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8203 - accuracy: 0.0990 - val_loss: 6.1389 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8565 - accuracy: 0.0984 - val_loss: 5.7491 - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8286 - accuracy: 0.1004 - val_loss: 5.6725 - val_accuracy: 0.2857\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8740 - accuracy: 0.1007 - val_loss: 5.8570 - val_accuracy: 0.2857\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8358 - accuracy: 0.1006 - val_loss: 5.7165 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8273 - accuracy: 0.1008 - val_loss: 6.0491 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8283 - accuracy: 0.0984 - val_loss: 5.6733 - val_accuracy: 0.1429\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8158 - accuracy: 0.0986 - val_loss: 6.1995 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8059 - accuracy: 0.1018 - val_loss: 5.8665 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8108 - accuracy: 0.1002 - val_loss: 5.7556 - val_accuracy: 0.2857\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8088 - accuracy: 0.1000 - val_loss: 5.5949 - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8227 - accuracy: 0.1010 - val_loss: 5.5691 - val_accuracy: 0.1429\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8083 - accuracy: 0.0991 - val_loss: 5.7873 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8112 - accuracy: 0.0999 - val_loss: 5.8786 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8116 - accuracy: 0.1014 - val_loss: 5.7145 - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7995 - accuracy: 0.0990 - val_loss: 5.7118 - val_accuracy: 0.1429\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8147 - accuracy: 0.0992 - val_loss: 5.7325 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8276 - accuracy: 0.1002 - val_loss: 5.8203 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8048 - accuracy: 0.1017 - val_loss: 5.7895 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8267 - accuracy: 0.0970 - val_loss: 5.8961 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8235 - accuracy: 0.0986 - val_loss: 6.3111 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8295 - accuracy: 0.1008 - val_loss: 5.7740 - val_accuracy: 0.2857\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8293 - accuracy: 0.0999 - val_loss: 5.9682 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8222 - accuracy: 0.0976 - val_loss: 5.8286 - val_accuracy: 0.1429\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8885 - accuracy: 0.1007 - val_loss: 6.0892 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7921 - accuracy: 0.1008 - val_loss: 6.1206 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8262 - accuracy: 0.0960 - val_loss: 5.9821 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8029 - accuracy: 0.0968 - val_loss: 6.1367 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8138 - accuracy: 0.1011 - val_loss: 5.9391 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8433 - accuracy: 0.1022 - val_loss: 6.3035 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8053 - accuracy: 0.0987 - val_loss: 5.8813 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8214 - accuracy: 0.1023 - val_loss: 6.0599 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8564 - accuracy: 0.0985 - val_loss: 5.7938 - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8055 - accuracy: 0.1015 - val_loss: 5.6669 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8266 - accuracy: 0.0997 - val_loss: 5.6688 - val_accuracy: 0.1429\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8289 - accuracy: 0.1004 - val_loss: 5.9680 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8280 - accuracy: 0.0988 - val_loss: 5.6537 - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.7967 - accuracy: 0.0989 - val_loss: 5.6189 - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8207 - accuracy: 0.1013 - val_loss: 5.7014 - val_accuracy: 0.1429\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8215 - accuracy: 0.1006 - val_loss: 5.9447 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8146 - accuracy: 0.1002 - val_loss: 6.0975 - val_accuracy: 0.1429\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8221 - accuracy: 0.0976 - val_loss: 5.8413 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8249 - accuracy: 0.0990 - val_loss: 6.1802 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8164 - accuracy: 0.0976 - val_loss: 5.5922 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8197 - accuracy: 0.0984 - val_loss: 5.7063 - val_accuracy: 0.1429\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8339 - accuracy: 0.0995 - val_loss: 5.7572 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8417 - accuracy: 0.1020 - val_loss: 6.0120 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8222 - accuracy: 0.0998 - val_loss: 5.6567 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8045 - accuracy: 0.1014 - val_loss: 5.5162 - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8088 - accuracy: 0.0989 - val_loss: 5.7769 - val_accuracy: 0.1429\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8187 - accuracy: 0.1007 - val_loss: 5.7445 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8267 - accuracy: 0.1004 - val_loss: 5.5762 - val_accuracy: 0.2857\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8403 - accuracy: 0.0999 - val_loss: 5.8449 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8299 - accuracy: 0.0995 - val_loss: 6.1825 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8204 - accuracy: 0.0985 - val_loss: 5.8177 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8076 - accuracy: 0.1004 - val_loss: 5.8223 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8074 - accuracy: 0.0999 - val_loss: 5.6336 - val_accuracy: 0.2857\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8251 - accuracy: 0.1017 - val_loss: 5.7999 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 5.8093 - accuracy: 0.0993 - val_loss: 5.4818 - val_accuracy: 0.2857\n",
      "Try 8/100: Best_val_acc: [5.846991062164307, 0.10155555605888367], lr: 5.846661967302075, Lambda: 8.36261523828998e-06\n",
      "\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 3.0806 - accuracy: 0.1010 - val_loss: 2.3019 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1007 - val_loss: 2.3016 - val_accuracy: 0.1429\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1008 - val_loss: 2.3006 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1014 - val_loss: 2.3057 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1000 - val_loss: 2.3057 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0998 - val_loss: 2.3015 - val_accuracy: 0.1429\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0986 - val_loss: 2.3078 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0991 - val_loss: 2.3039 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0982 - val_loss: 2.3048 - val_accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0984 - val_loss: 2.2957 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0987 - val_loss: 2.2947 - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0990 - val_loss: 2.3084 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1002 - val_loss: 2.2806 - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1006 - val_loss: 2.2809 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0995 - val_loss: 2.3058 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1021 - val_loss: 2.3034 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1010 - val_loss: 2.2940 - val_accuracy: 0.2857\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1008 - val_loss: 2.2986 - val_accuracy: 0.2857\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0980 - val_loss: 2.3093 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0963 - val_loss: 2.3032 - val_accuracy: 0.1429\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1005 - val_loss: 2.2975 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1005 - val_loss: 2.3125 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0997 - val_loss: 2.2980 - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0972 - val_loss: 2.2985 - val_accuracy: 0.1429\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0981 - val_loss: 2.2902 - val_accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0964 - val_loss: 2.3065 - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1020 - val_loss: 2.2973 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3034 - accuracy: 0.0986 - val_loss: 2.3244 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3033 - accuracy: 0.0997 - val_loss: 2.2810 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3035 - accuracy: 0.0983 - val_loss: 2.2974 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3033 - accuracy: 0.0987 - val_loss: 2.3131 - val_accuracy: 0.1429\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3033 - accuracy: 0.0989 - val_loss: 2.3041 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3034 - accuracy: 0.0990 - val_loss: 2.2902 - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.3033 - accuracy: 0.0977 - val_loss: 2.3043 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0988 - val_loss: 2.3220 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0992 - val_loss: 2.3081 - val_accuracy: 0.1429\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1000 - val_loss: 2.3002 - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.1001 - val_loss: 2.3066 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0991 - val_loss: 2.3043 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0976 - val_loss: 2.3025 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0979 - val_loss: 2.2943 - val_accuracy: 0.1429\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0984 - val_loss: 2.2853 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0986 - val_loss: 2.2933 - val_accuracy: 0.1429\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0981 - val_loss: 2.3028 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3035 - accuracy: 0.0992 - val_loss: 2.3075 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1007 - val_loss: 2.3005 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1000 - val_loss: 2.3091 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0991 - val_loss: 2.2981 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1000 - val_loss: 2.3009 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0995 - val_loss: 2.3125 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0983 - val_loss: 2.3109 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0995 - val_loss: 2.3063 - val_accuracy: 0.1429\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0983 - val_loss: 2.3056 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0981 - val_loss: 2.3160 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0973 - val_loss: 2.3000 - val_accuracy: 0.2857\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0970 - val_loss: 2.2879 - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0969 - val_loss: 2.2828 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1000 - val_loss: 2.2788 - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.1001 - val_loss: 2.2998 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1003 - val_loss: 2.2748 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0972 - val_loss: 2.2975 - val_accuracy: 0.2857\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1026 - val_loss: 2.3262 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0995 - val_loss: 2.3107 - val_accuracy: 0.2857\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1009 - val_loss: 2.3090 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1012 - val_loss: 2.3222 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0968 - val_loss: 2.3033 - val_accuracy: 0.1429\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0988 - val_loss: 2.2939 - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0969 - val_loss: 2.3060 - val_accuracy: 0.1429\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0990 - val_loss: 2.3005 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0988 - val_loss: 2.2954 - val_accuracy: 0.2857\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0972 - val_loss: 2.3060 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0995 - val_loss: 2.2965 - val_accuracy: 0.1429\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3034 - accuracy: 0.0986 - val_loss: 2.3032 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0982 - val_loss: 2.3028 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0994 - val_loss: 2.3085 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0980 - val_loss: 2.2914 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.1011 - val_loss: 2.3068 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0987 - val_loss: 2.3140 - val_accuracy: 0.1429\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.0988 - val_loss: 2.2932 - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1001 - val_loss: 2.2995 - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1000 - val_loss: 2.3233 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0980 - val_loss: 2.2868 - val_accuracy: 0.1429\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3033 - accuracy: 0.0973 - val_loss: 2.3132 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.0986 - val_loss: 2.3123 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0993 - val_loss: 2.2780 - val_accuracy: 0.1429\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0990 - val_loss: 2.3180 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1009 - val_loss: 2.3263 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.1015 - val_loss: 2.2883 - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.0988 - val_loss: 2.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.1004 - val_loss: 2.3153 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0960 - val_loss: 2.3011 - val_accuracy: 0.2857\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0988 - val_loss: 2.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3032 - accuracy: 0.0987 - val_loss: 2.3214 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1010 - val_loss: 2.3077 - val_accuracy: 0.1429\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0988 - val_loss: 2.3109 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0967 - val_loss: 2.3065 - val_accuracy: 0.1429\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.1002 - val_loss: 2.2924 - val_accuracy: 0.2857\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.1000 - val_loss: 2.3035 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3030 - accuracy: 0.1017 - val_loss: 2.2819 - val_accuracy: 0.1429\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3031 - accuracy: 0.0996 - val_loss: 2.3047 - val_accuracy: 0.1429\n",
      "Try 9/100: Best_val_acc: [2.3034284114837646, 0.09549999982118607], lr: 0.012867512874867768, Lambda: 2.319426361040266e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-7.0, 3.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-7,-2))\n",
    "    best_acc = basicHPCheckFCNN(100, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYWeXtnAkI-_"
   },
   "source": [
    "Lets take the below mentioned range now based on above results intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 655295,
     "status": "ok",
     "timestamp": 1594533671833,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "dwcrBN30V7LG",
    "outputId": "27b4a610-8463-432a-ee97-734d08dde721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3328 - accuracy: 0.1072 - val_loss: 2.2877 - val_accuracy: 0.1047\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2883 - accuracy: 0.1374 - val_loss: 2.2646 - val_accuracy: 0.1596\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2668 - accuracy: 0.1626 - val_loss: 2.2301 - val_accuracy: 0.2214\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2420 - accuracy: 0.1836 - val_loss: 2.2166 - val_accuracy: 0.2205\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2157 - accuracy: 0.2052 - val_loss: 2.1662 - val_accuracy: 0.2870\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1888 - accuracy: 0.2193 - val_loss: 2.1297 - val_accuracy: 0.3109\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1623 - accuracy: 0.2351 - val_loss: 2.0738 - val_accuracy: 0.3555\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1353 - accuracy: 0.2456 - val_loss: 2.0820 - val_accuracy: 0.3231\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1085 - accuracy: 0.2606 - val_loss: 2.0728 - val_accuracy: 0.3301\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0808 - accuracy: 0.2731 - val_loss: 2.0387 - val_accuracy: 0.3459\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0526 - accuracy: 0.2882 - val_loss: 2.0674 - val_accuracy: 0.3083\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0234 - accuracy: 0.3083 - val_loss: 1.9663 - val_accuracy: 0.4026\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9947 - accuracy: 0.3216 - val_loss: 2.0011 - val_accuracy: 0.3504\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9667 - accuracy: 0.3360 - val_loss: 1.9004 - val_accuracy: 0.4361\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9389 - accuracy: 0.3525 - val_loss: 1.8743 - val_accuracy: 0.4517\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9112 - accuracy: 0.3670 - val_loss: 1.8834 - val_accuracy: 0.4452\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8833 - accuracy: 0.3819 - val_loss: 1.8274 - val_accuracy: 0.4803\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8561 - accuracy: 0.4011 - val_loss: 1.8181 - val_accuracy: 0.4746\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8292 - accuracy: 0.4142 - val_loss: 1.8311 - val_accuracy: 0.4629\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8012 - accuracy: 0.4295 - val_loss: 1.7697 - val_accuracy: 0.4864\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7747 - accuracy: 0.4438 - val_loss: 1.7212 - val_accuracy: 0.5269\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7474 - accuracy: 0.4607 - val_loss: 1.6977 - val_accuracy: 0.5443\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7195 - accuracy: 0.4796 - val_loss: 1.6671 - val_accuracy: 0.5539\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6919 - accuracy: 0.4936 - val_loss: 1.6466 - val_accuracy: 0.5591\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6648 - accuracy: 0.5085 - val_loss: 1.6276 - val_accuracy: 0.5611\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6391 - accuracy: 0.5202 - val_loss: 1.6422 - val_accuracy: 0.5365\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6144 - accuracy: 0.5287 - val_loss: 1.6110 - val_accuracy: 0.5601\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5903 - accuracy: 0.5384 - val_loss: 1.5302 - val_accuracy: 0.5967\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5671 - accuracy: 0.5466 - val_loss: 1.5536 - val_accuracy: 0.5819\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5447 - accuracy: 0.5565 - val_loss: 1.5215 - val_accuracy: 0.5902\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5218 - accuracy: 0.5631 - val_loss: 1.4421 - val_accuracy: 0.6274\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5018 - accuracy: 0.5699 - val_loss: 1.4932 - val_accuracy: 0.5985\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4822 - accuracy: 0.5750 - val_loss: 1.4628 - val_accuracy: 0.6091\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4615 - accuracy: 0.5808 - val_loss: 1.4551 - val_accuracy: 0.5966\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4437 - accuracy: 0.5859 - val_loss: 1.5227 - val_accuracy: 0.5683\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4252 - accuracy: 0.5922 - val_loss: 1.3952 - val_accuracy: 0.6322\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4077 - accuracy: 0.5982 - val_loss: 1.3874 - val_accuracy: 0.6317\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3913 - accuracy: 0.6029 - val_loss: 1.3363 - val_accuracy: 0.6496\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3761 - accuracy: 0.6064 - val_loss: 1.3351 - val_accuracy: 0.6485\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3596 - accuracy: 0.6121 - val_loss: 1.3282 - val_accuracy: 0.6509\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3453 - accuracy: 0.6151 - val_loss: 1.3978 - val_accuracy: 0.6120\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3300 - accuracy: 0.6213 - val_loss: 1.2723 - val_accuracy: 0.6679\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3164 - accuracy: 0.6250 - val_loss: 1.3301 - val_accuracy: 0.6428\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3025 - accuracy: 0.6289 - val_loss: 1.2961 - val_accuracy: 0.6564\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2905 - accuracy: 0.6311 - val_loss: 1.3139 - val_accuracy: 0.6454\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2779 - accuracy: 0.6357 - val_loss: 1.3234 - val_accuracy: 0.6379\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2658 - accuracy: 0.6395 - val_loss: 1.2563 - val_accuracy: 0.6654\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2534 - accuracy: 0.6423 - val_loss: 1.2616 - val_accuracy: 0.6557\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2420 - accuracy: 0.6467 - val_loss: 1.2456 - val_accuracy: 0.6648\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2303 - accuracy: 0.6505 - val_loss: 1.2666 - val_accuracy: 0.6544\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2203 - accuracy: 0.6515 - val_loss: 1.2161 - val_accuracy: 0.6761\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2103 - accuracy: 0.6540 - val_loss: 1.2003 - val_accuracy: 0.6771\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1992 - accuracy: 0.6581 - val_loss: 1.2334 - val_accuracy: 0.6635\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1893 - accuracy: 0.6605 - val_loss: 1.1773 - val_accuracy: 0.6801\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1793 - accuracy: 0.6622 - val_loss: 1.1986 - val_accuracy: 0.6739\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1718 - accuracy: 0.6642 - val_loss: 1.1786 - val_accuracy: 0.6817\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1623 - accuracy: 0.6675 - val_loss: 1.1550 - val_accuracy: 0.6871\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1537 - accuracy: 0.6700 - val_loss: 1.2059 - val_accuracy: 0.6658\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1453 - accuracy: 0.6718 - val_loss: 1.1700 - val_accuracy: 0.6789\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1368 - accuracy: 0.6744 - val_loss: 1.1734 - val_accuracy: 0.6738\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1304 - accuracy: 0.6757 - val_loss: 1.1520 - val_accuracy: 0.6822\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1225 - accuracy: 0.6789 - val_loss: 1.1424 - val_accuracy: 0.6889\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1141 - accuracy: 0.6819 - val_loss: 1.0802 - val_accuracy: 0.7069\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1079 - accuracy: 0.6820 - val_loss: 1.1577 - val_accuracy: 0.6779\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1000 - accuracy: 0.6852 - val_loss: 1.1171 - val_accuracy: 0.6904\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0932 - accuracy: 0.6869 - val_loss: 1.1079 - val_accuracy: 0.7004\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0864 - accuracy: 0.6882 - val_loss: 1.1338 - val_accuracy: 0.6841\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0834 - accuracy: 0.6878 - val_loss: 1.1055 - val_accuracy: 0.6951\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0751 - accuracy: 0.6904 - val_loss: 1.0402 - val_accuracy: 0.7219\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0683 - accuracy: 0.6918 - val_loss: 1.0595 - val_accuracy: 0.7102\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0618 - accuracy: 0.6953 - val_loss: 1.0954 - val_accuracy: 0.6987\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0566 - accuracy: 0.6963 - val_loss: 1.0343 - val_accuracy: 0.7156\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0512 - accuracy: 0.6985 - val_loss: 1.1066 - val_accuracy: 0.6921\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0468 - accuracy: 0.7005 - val_loss: 1.0990 - val_accuracy: 0.6939\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0404 - accuracy: 0.7000 - val_loss: 1.0554 - val_accuracy: 0.7117\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0349 - accuracy: 0.7020 - val_loss: 1.1119 - val_accuracy: 0.6849\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0298 - accuracy: 0.7038 - val_loss: 1.0979 - val_accuracy: 0.6930\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0256 - accuracy: 0.7043 - val_loss: 1.0569 - val_accuracy: 0.7056\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0215 - accuracy: 0.7046 - val_loss: 1.0342 - val_accuracy: 0.7121\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0165 - accuracy: 0.7065 - val_loss: 1.0364 - val_accuracy: 0.7139\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0106 - accuracy: 0.7091 - val_loss: 1.0420 - val_accuracy: 0.7116\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0054 - accuracy: 0.7101 - val_loss: 1.0165 - val_accuracy: 0.7206\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0017 - accuracy: 0.7101 - val_loss: 1.0914 - val_accuracy: 0.6911\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9969 - accuracy: 0.7114 - val_loss: 1.0708 - val_accuracy: 0.6978\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9925 - accuracy: 0.7129 - val_loss: 1.0077 - val_accuracy: 0.7214\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9882 - accuracy: 0.7138 - val_loss: 0.9882 - val_accuracy: 0.7285\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9838 - accuracy: 0.7142 - val_loss: 1.0690 - val_accuracy: 0.6976\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9798 - accuracy: 0.7166 - val_loss: 1.0244 - val_accuracy: 0.7126\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9779 - accuracy: 0.7175 - val_loss: 1.0332 - val_accuracy: 0.7106\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9723 - accuracy: 0.7190 - val_loss: 0.9854 - val_accuracy: 0.7264\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9685 - accuracy: 0.7187 - val_loss: 1.0187 - val_accuracy: 0.7146\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9639 - accuracy: 0.7208 - val_loss: 0.9917 - val_accuracy: 0.7277\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9616 - accuracy: 0.7216 - val_loss: 1.0667 - val_accuracy: 0.6986\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9566 - accuracy: 0.7234 - val_loss: 1.0008 - val_accuracy: 0.7204\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9527 - accuracy: 0.7232 - val_loss: 1.0753 - val_accuracy: 0.6884\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9509 - accuracy: 0.7244 - val_loss: 0.9680 - val_accuracy: 0.7319\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9455 - accuracy: 0.7255 - val_loss: 0.9633 - val_accuracy: 0.7315\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9416 - accuracy: 0.7274 - val_loss: 0.9960 - val_accuracy: 0.7223\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9385 - accuracy: 0.7281 - val_loss: 0.9159 - val_accuracy: 0.7471\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9363 - accuracy: 0.7273 - val_loss: 1.0055 - val_accuracy: 0.7177\n",
      "Try 1/100: Best_val_acc: [0.9659203290939331, 0.7177222371101379], lr: 1.1573867554994972e-05, Lambda: 1.8914816019798962e-05\n",
      "\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2916 - accuracy: 0.1413 - val_loss: 2.2040 - val_accuracy: 0.3426\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1859 - accuracy: 0.2480 - val_loss: 2.0772 - val_accuracy: 0.3753\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0158 - accuracy: 0.3397 - val_loss: 1.8501 - val_accuracy: 0.4616\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8309 - accuracy: 0.4182 - val_loss: 1.7050 - val_accuracy: 0.5044\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6652 - accuracy: 0.4795 - val_loss: 1.6017 - val_accuracy: 0.5069\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5457 - accuracy: 0.5149 - val_loss: 1.5611 - val_accuracy: 0.5089\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4571 - accuracy: 0.5439 - val_loss: 1.4156 - val_accuracy: 0.5610\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3838 - accuracy: 0.5735 - val_loss: 1.3568 - val_accuracy: 0.5745\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3369 - accuracy: 0.5866 - val_loss: 1.3584 - val_accuracy: 0.5779\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2824 - accuracy: 0.6084 - val_loss: 1.3069 - val_accuracy: 0.6010\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2402 - accuracy: 0.6223 - val_loss: 1.3122 - val_accuracy: 0.5456\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2066 - accuracy: 0.6359 - val_loss: 1.2093 - val_accuracy: 0.6418\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1745 - accuracy: 0.6476 - val_loss: 1.1720 - val_accuracy: 0.6446\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1502 - accuracy: 0.6560 - val_loss: 1.1243 - val_accuracy: 0.6727\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1186 - accuracy: 0.6653 - val_loss: 1.1460 - val_accuracy: 0.6544\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0966 - accuracy: 0.6746 - val_loss: 1.0897 - val_accuracy: 0.6768\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0747 - accuracy: 0.6797 - val_loss: 1.0943 - val_accuracy: 0.6861\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0516 - accuracy: 0.6874 - val_loss: 1.0558 - val_accuracy: 0.7041\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0301 - accuracy: 0.6939 - val_loss: 1.0059 - val_accuracy: 0.7089\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0165 - accuracy: 0.6977 - val_loss: 1.0385 - val_accuracy: 0.7020\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9938 - accuracy: 0.7057 - val_loss: 1.0078 - val_accuracy: 0.7114\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9808 - accuracy: 0.7094 - val_loss: 0.9716 - val_accuracy: 0.7346\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9654 - accuracy: 0.7135 - val_loss: 0.9857 - val_accuracy: 0.7077\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9513 - accuracy: 0.7192 - val_loss: 0.9394 - val_accuracy: 0.7301\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9348 - accuracy: 0.7243 - val_loss: 1.0116 - val_accuracy: 0.7124\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9185 - accuracy: 0.7280 - val_loss: 0.9843 - val_accuracy: 0.7124\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9123 - accuracy: 0.7304 - val_loss: 0.8989 - val_accuracy: 0.7501\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9045 - accuracy: 0.7316 - val_loss: 0.8854 - val_accuracy: 0.7509\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8868 - accuracy: 0.7381 - val_loss: 0.8946 - val_accuracy: 0.7535\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8762 - accuracy: 0.7395 - val_loss: 0.9111 - val_accuracy: 0.7277\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8696 - accuracy: 0.7415 - val_loss: 0.8909 - val_accuracy: 0.7473\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8626 - accuracy: 0.7438 - val_loss: 0.8807 - val_accuracy: 0.7495\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8586 - accuracy: 0.7453 - val_loss: 0.9482 - val_accuracy: 0.7238\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8407 - accuracy: 0.7510 - val_loss: 0.8312 - val_accuracy: 0.7641\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8344 - accuracy: 0.7525 - val_loss: 0.9255 - val_accuracy: 0.7275\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8230 - accuracy: 0.7556 - val_loss: 0.8643 - val_accuracy: 0.7544\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8150 - accuracy: 0.7586 - val_loss: 0.8264 - val_accuracy: 0.7639\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8067 - accuracy: 0.7595 - val_loss: 0.8548 - val_accuracy: 0.7630\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7985 - accuracy: 0.7632 - val_loss: 0.7926 - val_accuracy: 0.7743\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7916 - accuracy: 0.7657 - val_loss: 0.8322 - val_accuracy: 0.7658\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7895 - accuracy: 0.7642 - val_loss: 0.8230 - val_accuracy: 0.7541\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7811 - accuracy: 0.7662 - val_loss: 0.8046 - val_accuracy: 0.7751\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7750 - accuracy: 0.7691 - val_loss: 0.8444 - val_accuracy: 0.7681\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7686 - accuracy: 0.7720 - val_loss: 0.8111 - val_accuracy: 0.7690\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7733 - accuracy: 0.7687 - val_loss: 0.7929 - val_accuracy: 0.7776\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7529 - accuracy: 0.7760 - val_loss: 0.8015 - val_accuracy: 0.7659\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7463 - accuracy: 0.7781 - val_loss: 0.7623 - val_accuracy: 0.7874\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7424 - accuracy: 0.7785 - val_loss: 0.7806 - val_accuracy: 0.7716\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7365 - accuracy: 0.7802 - val_loss: 0.7617 - val_accuracy: 0.7824\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7288 - accuracy: 0.7810 - val_loss: 0.7945 - val_accuracy: 0.7725\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7321 - accuracy: 0.7815 - val_loss: 0.7480 - val_accuracy: 0.7937\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7193 - accuracy: 0.7855 - val_loss: 0.7754 - val_accuracy: 0.7809\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7131 - accuracy: 0.7874 - val_loss: 0.7415 - val_accuracy: 0.7850\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7096 - accuracy: 0.7880 - val_loss: 0.7361 - val_accuracy: 0.7913\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7113 - accuracy: 0.7868 - val_loss: 0.7892 - val_accuracy: 0.7760\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7026 - accuracy: 0.7905 - val_loss: 0.7227 - val_accuracy: 0.7914\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6924 - accuracy: 0.7933 - val_loss: 0.7736 - val_accuracy: 0.7733\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6885 - accuracy: 0.7938 - val_loss: 0.7511 - val_accuracy: 0.7879\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6849 - accuracy: 0.7950 - val_loss: 0.7675 - val_accuracy: 0.7809\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6861 - accuracy: 0.7948 - val_loss: 0.7331 - val_accuracy: 0.7893\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6807 - accuracy: 0.7953 - val_loss: 0.7860 - val_accuracy: 0.7597\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6787 - accuracy: 0.7955 - val_loss: 0.7274 - val_accuracy: 0.7949\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6748 - accuracy: 0.7967 - val_loss: 0.7258 - val_accuracy: 0.7967\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6640 - accuracy: 0.8021 - val_loss: 0.7015 - val_accuracy: 0.7981\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6602 - accuracy: 0.8016 - val_loss: 0.6907 - val_accuracy: 0.8064\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6565 - accuracy: 0.8039 - val_loss: 0.6971 - val_accuracy: 0.7964\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6523 - accuracy: 0.8043 - val_loss: 0.6916 - val_accuracy: 0.8017\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6436 - accuracy: 0.8086 - val_loss: 0.6885 - val_accuracy: 0.8024\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6454 - accuracy: 0.8063 - val_loss: 0.6699 - val_accuracy: 0.8109\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6412 - accuracy: 0.8075 - val_loss: 0.6887 - val_accuracy: 0.8014\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6345 - accuracy: 0.8086 - val_loss: 0.6915 - val_accuracy: 0.7959\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6372 - accuracy: 0.8096 - val_loss: 0.6920 - val_accuracy: 0.8019\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6284 - accuracy: 0.8105 - val_loss: 0.6705 - val_accuracy: 0.8081\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6224 - accuracy: 0.8129 - val_loss: 0.6574 - val_accuracy: 0.8063\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6198 - accuracy: 0.8142 - val_loss: 0.6432 - val_accuracy: 0.8179\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6144 - accuracy: 0.8159 - val_loss: 0.6742 - val_accuracy: 0.7999\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6134 - accuracy: 0.8158 - val_loss: 0.6561 - val_accuracy: 0.8129\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6108 - accuracy: 0.8160 - val_loss: 0.6669 - val_accuracy: 0.8067\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6136 - accuracy: 0.8155 - val_loss: 0.6908 - val_accuracy: 0.8059\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6094 - accuracy: 0.8149 - val_loss: 0.6918 - val_accuracy: 0.7974\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5954 - accuracy: 0.8211 - val_loss: 0.6935 - val_accuracy: 0.7953\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5992 - accuracy: 0.8205 - val_loss: 0.6605 - val_accuracy: 0.8152\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5884 - accuracy: 0.8242 - val_loss: 0.6908 - val_accuracy: 0.7966\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5875 - accuracy: 0.8247 - val_loss: 0.6578 - val_accuracy: 0.8177\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5894 - accuracy: 0.8229 - val_loss: 0.6156 - val_accuracy: 0.8244\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5806 - accuracy: 0.8243 - val_loss: 0.6697 - val_accuracy: 0.8013\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5793 - accuracy: 0.8255 - val_loss: 0.6486 - val_accuracy: 0.8113\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5762 - accuracy: 0.8261 - val_loss: 0.6645 - val_accuracy: 0.8059\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5783 - accuracy: 0.8266 - val_loss: 0.6701 - val_accuracy: 0.8069\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5713 - accuracy: 0.8284 - val_loss: 0.6330 - val_accuracy: 0.8116\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5724 - accuracy: 0.8285 - val_loss: 0.6785 - val_accuracy: 0.7969\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5623 - accuracy: 0.8319 - val_loss: 0.6407 - val_accuracy: 0.8197\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5610 - accuracy: 0.8311 - val_loss: 0.6653 - val_accuracy: 0.8111\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5601 - accuracy: 0.8311 - val_loss: 0.6470 - val_accuracy: 0.8124\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5558 - accuracy: 0.8337 - val_loss: 0.6045 - val_accuracy: 0.8226\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5480 - accuracy: 0.8364 - val_loss: 0.6030 - val_accuracy: 0.8244\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5504 - accuracy: 0.8348 - val_loss: 0.6007 - val_accuracy: 0.8285\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5479 - accuracy: 0.8348 - val_loss: 0.5742 - val_accuracy: 0.8314\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5493 - accuracy: 0.8351 - val_loss: 0.6085 - val_accuracy: 0.8255\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5385 - accuracy: 0.8373 - val_loss: 0.6855 - val_accuracy: 0.8101\n",
      "Try 2/100: Best_val_acc: [0.7292283773422241, 0.792388916015625], lr: 5.767042362863121e-05, Lambda: 3.342640392314455e-05\n",
      "\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3096 - accuracy: 0.1318 - val_loss: 2.2440 - val_accuracy: 0.2532\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2426 - accuracy: 0.1811 - val_loss: 2.1714 - val_accuracy: 0.3166\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1642 - accuracy: 0.2390 - val_loss: 2.0688 - val_accuracy: 0.3736\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0770 - accuracy: 0.2940 - val_loss: 1.9554 - val_accuracy: 0.4243\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9809 - accuracy: 0.3317 - val_loss: 1.8661 - val_accuracy: 0.4415\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8805 - accuracy: 0.3738 - val_loss: 1.7782 - val_accuracy: 0.4781\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7853 - accuracy: 0.4085 - val_loss: 1.6525 - val_accuracy: 0.5344\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6998 - accuracy: 0.4450 - val_loss: 1.5634 - val_accuracy: 0.5786\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6250 - accuracy: 0.4792 - val_loss: 1.5210 - val_accuracy: 0.5789\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.5609 - accuracy: 0.5051 - val_loss: 1.5074 - val_accuracy: 0.5782\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5061 - accuracy: 0.5272 - val_loss: 1.4246 - val_accuracy: 0.5993\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4564 - accuracy: 0.5474 - val_loss: 1.3876 - val_accuracy: 0.5994\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4154 - accuracy: 0.5627 - val_loss: 1.4275 - val_accuracy: 0.5773\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3828 - accuracy: 0.5732 - val_loss: 1.3851 - val_accuracy: 0.5989\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3465 - accuracy: 0.5887 - val_loss: 1.3664 - val_accuracy: 0.6065\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3154 - accuracy: 0.6018 - val_loss: 1.2533 - val_accuracy: 0.6395\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2886 - accuracy: 0.6099 - val_loss: 1.2991 - val_accuracy: 0.6301\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2625 - accuracy: 0.6174 - val_loss: 1.2627 - val_accuracy: 0.6451\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2419 - accuracy: 0.6243 - val_loss: 1.2720 - val_accuracy: 0.6356\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2175 - accuracy: 0.6342 - val_loss: 1.2188 - val_accuracy: 0.6547\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1944 - accuracy: 0.6424 - val_loss: 1.1330 - val_accuracy: 0.6838\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1761 - accuracy: 0.6471 - val_loss: 1.1949 - val_accuracy: 0.6609\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1559 - accuracy: 0.6548 - val_loss: 1.1422 - val_accuracy: 0.6712\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1408 - accuracy: 0.6579 - val_loss: 1.2073 - val_accuracy: 0.6502\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1258 - accuracy: 0.6648 - val_loss: 1.1378 - val_accuracy: 0.6771\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1111 - accuracy: 0.6670 - val_loss: 1.1175 - val_accuracy: 0.6836\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0954 - accuracy: 0.6735 - val_loss: 1.1575 - val_accuracy: 0.6624\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0825 - accuracy: 0.6750 - val_loss: 1.1054 - val_accuracy: 0.6854\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0682 - accuracy: 0.6815 - val_loss: 1.1309 - val_accuracy: 0.6736\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0598 - accuracy: 0.6834 - val_loss: 1.1152 - val_accuracy: 0.6785\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0489 - accuracy: 0.6855 - val_loss: 1.0386 - val_accuracy: 0.7027\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0330 - accuracy: 0.6900 - val_loss: 1.1012 - val_accuracy: 0.6833\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0233 - accuracy: 0.6941 - val_loss: 1.0165 - val_accuracy: 0.7047\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0112 - accuracy: 0.6978 - val_loss: 1.0508 - val_accuracy: 0.6911\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0073 - accuracy: 0.6990 - val_loss: 1.0053 - val_accuracy: 0.7096\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9945 - accuracy: 0.7031 - val_loss: 1.0461 - val_accuracy: 0.7012\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9837 - accuracy: 0.7061 - val_loss: 1.0131 - val_accuracy: 0.7090\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9729 - accuracy: 0.7102 - val_loss: 0.9945 - val_accuracy: 0.7138\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9639 - accuracy: 0.7136 - val_loss: 0.9372 - val_accuracy: 0.7329\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9604 - accuracy: 0.7139 - val_loss: 0.9606 - val_accuracy: 0.7205\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9507 - accuracy: 0.7173 - val_loss: 0.9802 - val_accuracy: 0.7134\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9384 - accuracy: 0.7210 - val_loss: 1.0051 - val_accuracy: 0.7076\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9318 - accuracy: 0.7228 - val_loss: 1.0341 - val_accuracy: 0.6984\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9288 - accuracy: 0.7231 - val_loss: 0.9048 - val_accuracy: 0.7401\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9216 - accuracy: 0.7250 - val_loss: 0.9251 - val_accuracy: 0.7339\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9124 - accuracy: 0.7279 - val_loss: 0.9040 - val_accuracy: 0.7401\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9051 - accuracy: 0.7302 - val_loss: 0.9569 - val_accuracy: 0.7238\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8965 - accuracy: 0.7334 - val_loss: 0.8905 - val_accuracy: 0.7441\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8914 - accuracy: 0.7345 - val_loss: 0.8888 - val_accuracy: 0.7455\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8835 - accuracy: 0.7372 - val_loss: 0.8945 - val_accuracy: 0.7430\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8770 - accuracy: 0.7388 - val_loss: 0.9114 - val_accuracy: 0.7386\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8724 - accuracy: 0.7403 - val_loss: 0.8918 - val_accuracy: 0.7386\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8661 - accuracy: 0.7420 - val_loss: 0.9565 - val_accuracy: 0.7239\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8614 - accuracy: 0.7433 - val_loss: 0.8827 - val_accuracy: 0.7434\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8521 - accuracy: 0.7472 - val_loss: 0.8924 - val_accuracy: 0.7427\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8479 - accuracy: 0.7468 - val_loss: 0.9084 - val_accuracy: 0.7419\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8433 - accuracy: 0.7471 - val_loss: 0.8911 - val_accuracy: 0.7448\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8405 - accuracy: 0.7495 - val_loss: 0.8518 - val_accuracy: 0.7566\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8309 - accuracy: 0.7516 - val_loss: 0.8655 - val_accuracy: 0.7558\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8294 - accuracy: 0.7525 - val_loss: 0.8809 - val_accuracy: 0.7485\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8237 - accuracy: 0.7555 - val_loss: 0.8893 - val_accuracy: 0.7442\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8166 - accuracy: 0.7578 - val_loss: 0.8660 - val_accuracy: 0.7494\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8113 - accuracy: 0.7598 - val_loss: 0.8516 - val_accuracy: 0.7518\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8104 - accuracy: 0.7584 - val_loss: 0.8173 - val_accuracy: 0.7671\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8051 - accuracy: 0.7600 - val_loss: 0.8473 - val_accuracy: 0.7606\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8010 - accuracy: 0.7614 - val_loss: 0.8160 - val_accuracy: 0.7658\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7921 - accuracy: 0.7651 - val_loss: 0.8252 - val_accuracy: 0.7639\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7895 - accuracy: 0.7648 - val_loss: 0.8029 - val_accuracy: 0.7723\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7899 - accuracy: 0.7654 - val_loss: 0.8200 - val_accuracy: 0.7650\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7789 - accuracy: 0.7687 - val_loss: 0.7924 - val_accuracy: 0.7734\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7743 - accuracy: 0.7701 - val_loss: 0.7893 - val_accuracy: 0.7744\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7733 - accuracy: 0.7703 - val_loss: 0.8137 - val_accuracy: 0.7716\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7697 - accuracy: 0.7701 - val_loss: 0.8227 - val_accuracy: 0.7654\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7648 - accuracy: 0.7723 - val_loss: 0.7791 - val_accuracy: 0.7754\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7626 - accuracy: 0.7748 - val_loss: 0.7790 - val_accuracy: 0.7789\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7538 - accuracy: 0.7766 - val_loss: 0.7737 - val_accuracy: 0.7831\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7510 - accuracy: 0.7768 - val_loss: 0.7586 - val_accuracy: 0.7868\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7482 - accuracy: 0.7769 - val_loss: 0.8082 - val_accuracy: 0.7708\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7458 - accuracy: 0.7795 - val_loss: 0.8209 - val_accuracy: 0.7635\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7425 - accuracy: 0.7790 - val_loss: 0.7640 - val_accuracy: 0.7843\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7362 - accuracy: 0.7815 - val_loss: 0.7888 - val_accuracy: 0.7723\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7352 - accuracy: 0.7806 - val_loss: 0.7421 - val_accuracy: 0.7920\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7297 - accuracy: 0.7832 - val_loss: 0.8010 - val_accuracy: 0.7736\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7330 - accuracy: 0.7815 - val_loss: 0.7430 - val_accuracy: 0.7910\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7233 - accuracy: 0.7859 - val_loss: 0.7182 - val_accuracy: 0.7944\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7215 - accuracy: 0.7863 - val_loss: 0.7718 - val_accuracy: 0.7791\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7181 - accuracy: 0.7859 - val_loss: 0.7492 - val_accuracy: 0.7871\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7152 - accuracy: 0.7872 - val_loss: 0.7427 - val_accuracy: 0.7870\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7083 - accuracy: 0.7898 - val_loss: 0.7240 - val_accuracy: 0.7948\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7092 - accuracy: 0.7873 - val_loss: 0.7342 - val_accuracy: 0.7909\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7036 - accuracy: 0.7913 - val_loss: 0.7688 - val_accuracy: 0.7809\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7023 - accuracy: 0.7894 - val_loss: 0.7021 - val_accuracy: 0.8021\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6970 - accuracy: 0.7932 - val_loss: 0.7263 - val_accuracy: 0.7929\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6949 - accuracy: 0.7946 - val_loss: 0.7549 - val_accuracy: 0.7852\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6924 - accuracy: 0.7946 - val_loss: 0.7844 - val_accuracy: 0.7765\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6911 - accuracy: 0.7944 - val_loss: 0.7429 - val_accuracy: 0.7907\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6860 - accuracy: 0.7965 - val_loss: 0.7493 - val_accuracy: 0.7854\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6820 - accuracy: 0.7968 - val_loss: 0.7359 - val_accuracy: 0.7919\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6793 - accuracy: 0.7982 - val_loss: 0.7387 - val_accuracy: 0.7854\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6767 - accuracy: 0.7992 - val_loss: 0.7149 - val_accuracy: 0.7949\n",
      "Try 3/100: Best_val_acc: [0.7749811410903931, 0.7721111178398132], lr: 4.470762274499612e-05, Lambda: 0.00017723747307093723\n",
      "\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3544 - accuracy: 0.1003 - val_loss: 2.2823 - val_accuracy: 0.1972\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2958 - accuracy: 0.1216 - val_loss: 2.2513 - val_accuracy: 0.1888\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2774 - accuracy: 0.1480 - val_loss: 2.2219 - val_accuracy: 0.2246\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2526 - accuracy: 0.1738 - val_loss: 2.1842 - val_accuracy: 0.3013\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2212 - accuracy: 0.2059 - val_loss: 2.1205 - val_accuracy: 0.3669\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1834 - accuracy: 0.2397 - val_loss: 2.0739 - val_accuracy: 0.4199\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1405 - accuracy: 0.2659 - val_loss: 2.0162 - val_accuracy: 0.4475\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0940 - accuracy: 0.2901 - val_loss: 1.9682 - val_accuracy: 0.4681\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0488 - accuracy: 0.3078 - val_loss: 1.9080 - val_accuracy: 0.4686\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0022 - accuracy: 0.3255 - val_loss: 1.8716 - val_accuracy: 0.4700\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9568 - accuracy: 0.3465 - val_loss: 1.8434 - val_accuracy: 0.4950\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9111 - accuracy: 0.3711 - val_loss: 1.7935 - val_accuracy: 0.5002\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8656 - accuracy: 0.3973 - val_loss: 1.7693 - val_accuracy: 0.5045\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8230 - accuracy: 0.4182 - val_loss: 1.7165 - val_accuracy: 0.5094\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7831 - accuracy: 0.4357 - val_loss: 1.7085 - val_accuracy: 0.5059\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7442 - accuracy: 0.4560 - val_loss: 1.6693 - val_accuracy: 0.5271\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7081 - accuracy: 0.4709 - val_loss: 1.6574 - val_accuracy: 0.5155\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6738 - accuracy: 0.4851 - val_loss: 1.6309 - val_accuracy: 0.5137\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6407 - accuracy: 0.4993 - val_loss: 1.6017 - val_accuracy: 0.5168\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6080 - accuracy: 0.5117 - val_loss: 1.5784 - val_accuracy: 0.5295\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5787 - accuracy: 0.5236 - val_loss: 1.5693 - val_accuracy: 0.5246\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5502 - accuracy: 0.5370 - val_loss: 1.5476 - val_accuracy: 0.5355\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5219 - accuracy: 0.5480 - val_loss: 1.5405 - val_accuracy: 0.5380\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4947 - accuracy: 0.5601 - val_loss: 1.5189 - val_accuracy: 0.5469\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4699 - accuracy: 0.5721 - val_loss: 1.4850 - val_accuracy: 0.5692\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4455 - accuracy: 0.5810 - val_loss: 1.4957 - val_accuracy: 0.5560\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4215 - accuracy: 0.5911 - val_loss: 1.4241 - val_accuracy: 0.5944\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4014 - accuracy: 0.5962 - val_loss: 1.4412 - val_accuracy: 0.5836\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3804 - accuracy: 0.6041 - val_loss: 1.3956 - val_accuracy: 0.6096\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3610 - accuracy: 0.6103 - val_loss: 1.3985 - val_accuracy: 0.5999\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3437 - accuracy: 0.6157 - val_loss: 1.4102 - val_accuracy: 0.5841\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3260 - accuracy: 0.6201 - val_loss: 1.3985 - val_accuracy: 0.5904\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3086 - accuracy: 0.6270 - val_loss: 1.3545 - val_accuracy: 0.6126\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2934 - accuracy: 0.6321 - val_loss: 1.3386 - val_accuracy: 0.6144\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2808 - accuracy: 0.6332 - val_loss: 1.3212 - val_accuracy: 0.6231\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2660 - accuracy: 0.6386 - val_loss: 1.2983 - val_accuracy: 0.6259\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2534 - accuracy: 0.6401 - val_loss: 1.3270 - val_accuracy: 0.6074\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2397 - accuracy: 0.6457 - val_loss: 1.2948 - val_accuracy: 0.6243\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2276 - accuracy: 0.6489 - val_loss: 1.3012 - val_accuracy: 0.6183\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2173 - accuracy: 0.6506 - val_loss: 1.2694 - val_accuracy: 0.6354\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2057 - accuracy: 0.6538 - val_loss: 1.2628 - val_accuracy: 0.6397\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1966 - accuracy: 0.6561 - val_loss: 1.2876 - val_accuracy: 0.6254\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1856 - accuracy: 0.6606 - val_loss: 1.2196 - val_accuracy: 0.6534\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1748 - accuracy: 0.6643 - val_loss: 1.2456 - val_accuracy: 0.6367\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1660 - accuracy: 0.6659 - val_loss: 1.1795 - val_accuracy: 0.6634\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1578 - accuracy: 0.6680 - val_loss: 1.2172 - val_accuracy: 0.6461\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1489 - accuracy: 0.6704 - val_loss: 1.2239 - val_accuracy: 0.6496\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1395 - accuracy: 0.6727 - val_loss: 1.1828 - val_accuracy: 0.6593\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1322 - accuracy: 0.6755 - val_loss: 1.1976 - val_accuracy: 0.6560\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1237 - accuracy: 0.6787 - val_loss: 1.1981 - val_accuracy: 0.6510\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1161 - accuracy: 0.6796 - val_loss: 1.1948 - val_accuracy: 0.6533\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1090 - accuracy: 0.6804 - val_loss: 1.1669 - val_accuracy: 0.6624\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1018 - accuracy: 0.6835 - val_loss: 1.1483 - val_accuracy: 0.6710\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0941 - accuracy: 0.6859 - val_loss: 1.1414 - val_accuracy: 0.6719\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0859 - accuracy: 0.6889 - val_loss: 1.1523 - val_accuracy: 0.6636\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0800 - accuracy: 0.6890 - val_loss: 1.1563 - val_accuracy: 0.6644\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0727 - accuracy: 0.6917 - val_loss: 1.1334 - val_accuracy: 0.6716\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0673 - accuracy: 0.6933 - val_loss: 1.1353 - val_accuracy: 0.6703\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0612 - accuracy: 0.6950 - val_loss: 1.1240 - val_accuracy: 0.6746\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0537 - accuracy: 0.6969 - val_loss: 1.1273 - val_accuracy: 0.6684\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0485 - accuracy: 0.6978 - val_loss: 1.1068 - val_accuracy: 0.6732\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0425 - accuracy: 0.6997 - val_loss: 1.0941 - val_accuracy: 0.6850\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0383 - accuracy: 0.7014 - val_loss: 1.1017 - val_accuracy: 0.6747\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0300 - accuracy: 0.7034 - val_loss: 1.0921 - val_accuracy: 0.6797\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0263 - accuracy: 0.7042 - val_loss: 1.0737 - val_accuracy: 0.6913\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0208 - accuracy: 0.7069 - val_loss: 1.0816 - val_accuracy: 0.6859\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0155 - accuracy: 0.7082 - val_loss: 1.0915 - val_accuracy: 0.6773\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0094 - accuracy: 0.7090 - val_loss: 1.0927 - val_accuracy: 0.6802\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0047 - accuracy: 0.7101 - val_loss: 1.0441 - val_accuracy: 0.7015\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9994 - accuracy: 0.7130 - val_loss: 1.0388 - val_accuracy: 0.6976\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9947 - accuracy: 0.7133 - val_loss: 1.0907 - val_accuracy: 0.6768\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9906 - accuracy: 0.7153 - val_loss: 1.0342 - val_accuracy: 0.7011\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9852 - accuracy: 0.7157 - val_loss: 1.0382 - val_accuracy: 0.7005\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9809 - accuracy: 0.7163 - val_loss: 1.0413 - val_accuracy: 0.6944\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9775 - accuracy: 0.7171 - val_loss: 1.0645 - val_accuracy: 0.6880\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9722 - accuracy: 0.7199 - val_loss: 1.0234 - val_accuracy: 0.7017\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9672 - accuracy: 0.7223 - val_loss: 1.0199 - val_accuracy: 0.7047\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9621 - accuracy: 0.7220 - val_loss: 1.0345 - val_accuracy: 0.6921\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9614 - accuracy: 0.7222 - val_loss: 1.0116 - val_accuracy: 0.7114\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9539 - accuracy: 0.7254 - val_loss: 0.9856 - val_accuracy: 0.7195\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9495 - accuracy: 0.7253 - val_loss: 0.9932 - val_accuracy: 0.7135\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9446 - accuracy: 0.7278 - val_loss: 1.0198 - val_accuracy: 0.6998\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9413 - accuracy: 0.7295 - val_loss: 0.9802 - val_accuracy: 0.7196\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9369 - accuracy: 0.7285 - val_loss: 1.0360 - val_accuracy: 0.6922\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9325 - accuracy: 0.7319 - val_loss: 0.9871 - val_accuracy: 0.7131\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9303 - accuracy: 0.7316 - val_loss: 0.9874 - val_accuracy: 0.7126\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9251 - accuracy: 0.7337 - val_loss: 0.9876 - val_accuracy: 0.7161\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9214 - accuracy: 0.7346 - val_loss: 0.9918 - val_accuracy: 0.7080\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9178 - accuracy: 0.7337 - val_loss: 0.9822 - val_accuracy: 0.7144\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9147 - accuracy: 0.7354 - val_loss: 0.9705 - val_accuracy: 0.7184\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9105 - accuracy: 0.7369 - val_loss: 0.9844 - val_accuracy: 0.7121\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9063 - accuracy: 0.7377 - val_loss: 0.9474 - val_accuracy: 0.7282\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9041 - accuracy: 0.7390 - val_loss: 0.9646 - val_accuracy: 0.7202\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9001 - accuracy: 0.7392 - val_loss: 0.9431 - val_accuracy: 0.7271\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8967 - accuracy: 0.7399 - val_loss: 0.9691 - val_accuracy: 0.7164\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8934 - accuracy: 0.7416 - val_loss: 0.9386 - val_accuracy: 0.7296\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8886 - accuracy: 0.7434 - val_loss: 0.9699 - val_accuracy: 0.7172\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8866 - accuracy: 0.7425 - val_loss: 0.9468 - val_accuracy: 0.7279\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8837 - accuracy: 0.7444 - val_loss: 0.9533 - val_accuracy: 0.7208\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8792 - accuracy: 0.7464 - val_loss: 0.9364 - val_accuracy: 0.7270\n",
      "Try 4/100: Best_val_acc: [0.926321804523468, 0.7312222123146057], lr: 1.4467789313863729e-05, Lambda: 0.00020212387614047646\n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3181 - accuracy: 0.1040 - val_loss: 2.2617 - val_accuracy: 0.1876\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2802 - accuracy: 0.1324 - val_loss: 2.1015 - val_accuracy: 0.2167\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9930 - accuracy: 0.2778 - val_loss: 1.7352 - val_accuracy: 0.3936\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5996 - accuracy: 0.4553 - val_loss: 1.4136 - val_accuracy: 0.5266\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3792 - accuracy: 0.5480 - val_loss: 1.2834 - val_accuracy: 0.6014\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2384 - accuracy: 0.6046 - val_loss: 1.2529 - val_accuracy: 0.6037\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1587 - accuracy: 0.6354 - val_loss: 1.1150 - val_accuracy: 0.6411\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0915 - accuracy: 0.6603 - val_loss: 1.0922 - val_accuracy: 0.6507\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0335 - accuracy: 0.6778 - val_loss: 1.1354 - val_accuracy: 0.6494\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9933 - accuracy: 0.6891 - val_loss: 0.9973 - val_accuracy: 0.7034\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9318 - accuracy: 0.7120 - val_loss: 0.9045 - val_accuracy: 0.7281\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8985 - accuracy: 0.7213 - val_loss: 0.9062 - val_accuracy: 0.7377\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8747 - accuracy: 0.7284 - val_loss: 0.9087 - val_accuracy: 0.7243\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8248 - accuracy: 0.7431 - val_loss: 0.7966 - val_accuracy: 0.7676\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7895 - accuracy: 0.7549 - val_loss: 0.8429 - val_accuracy: 0.7439\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7519 - accuracy: 0.7681 - val_loss: 0.7781 - val_accuracy: 0.7729\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7344 - accuracy: 0.7742 - val_loss: 0.7642 - val_accuracy: 0.7788\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7198 - accuracy: 0.7773 - val_loss: 0.7695 - val_accuracy: 0.7725\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6820 - accuracy: 0.7901 - val_loss: 0.7739 - val_accuracy: 0.7712\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6728 - accuracy: 0.7909 - val_loss: 0.6905 - val_accuracy: 0.7967\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6601 - accuracy: 0.7951 - val_loss: 0.6627 - val_accuracy: 0.8077\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6425 - accuracy: 0.8007 - val_loss: 0.6180 - val_accuracy: 0.8177\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6125 - accuracy: 0.8122 - val_loss: 0.6511 - val_accuracy: 0.8111\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6111 - accuracy: 0.8092 - val_loss: 0.6385 - val_accuracy: 0.8157\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6038 - accuracy: 0.8131 - val_loss: 0.6377 - val_accuracy: 0.8092\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5825 - accuracy: 0.8175 - val_loss: 0.6180 - val_accuracy: 0.8182\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5774 - accuracy: 0.8198 - val_loss: 0.6280 - val_accuracy: 0.8156\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5542 - accuracy: 0.8261 - val_loss: 0.6201 - val_accuracy: 0.8184\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5419 - accuracy: 0.8299 - val_loss: 0.6230 - val_accuracy: 0.8166\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5432 - accuracy: 0.8292 - val_loss: 0.6149 - val_accuracy: 0.8183\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5202 - accuracy: 0.8382 - val_loss: 0.5459 - val_accuracy: 0.8439\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4996 - accuracy: 0.8446 - val_loss: 0.5482 - val_accuracy: 0.8399\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4895 - accuracy: 0.8459 - val_loss: 0.5648 - val_accuracy: 0.8375\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4755 - accuracy: 0.8511 - val_loss: 0.5761 - val_accuracy: 0.8274\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4732 - accuracy: 0.8505 - val_loss: 0.5176 - val_accuracy: 0.8451\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4755 - accuracy: 0.8513 - val_loss: 0.5800 - val_accuracy: 0.8252\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4688 - accuracy: 0.8511 - val_loss: 0.5917 - val_accuracy: 0.8241\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4442 - accuracy: 0.8593 - val_loss: 0.5658 - val_accuracy: 0.8307\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4356 - accuracy: 0.8625 - val_loss: 0.4969 - val_accuracy: 0.8538\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4427 - accuracy: 0.8607 - val_loss: 0.5172 - val_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4256 - accuracy: 0.8665 - val_loss: 0.4816 - val_accuracy: 0.8572\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4211 - accuracy: 0.8672 - val_loss: 0.5275 - val_accuracy: 0.8455\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4082 - accuracy: 0.8703 - val_loss: 0.5619 - val_accuracy: 0.8343\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4065 - accuracy: 0.8719 - val_loss: 0.4969 - val_accuracy: 0.8466\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4050 - accuracy: 0.8708 - val_loss: 0.4930 - val_accuracy: 0.8535\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3842 - accuracy: 0.8787 - val_loss: 0.5388 - val_accuracy: 0.8391\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3729 - accuracy: 0.8824 - val_loss: 0.4817 - val_accuracy: 0.8574\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3821 - accuracy: 0.8775 - val_loss: 0.4789 - val_accuracy: 0.8589\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3688 - accuracy: 0.8821 - val_loss: 0.5002 - val_accuracy: 0.8533\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3648 - accuracy: 0.8837 - val_loss: 0.4172 - val_accuracy: 0.8807\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3603 - accuracy: 0.8846 - val_loss: 0.4829 - val_accuracy: 0.8580\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3376 - accuracy: 0.8934 - val_loss: 0.4692 - val_accuracy: 0.8600\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3688 - accuracy: 0.8817 - val_loss: 0.4594 - val_accuracy: 0.8659\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3401 - accuracy: 0.8910 - val_loss: 0.5073 - val_accuracy: 0.8516\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3370 - accuracy: 0.8924 - val_loss: 0.4738 - val_accuracy: 0.8573\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3325 - accuracy: 0.8952 - val_loss: 0.4905 - val_accuracy: 0.8528\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3185 - accuracy: 0.8976 - val_loss: 0.4660 - val_accuracy: 0.8613\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3193 - accuracy: 0.8970 - val_loss: 0.4551 - val_accuracy: 0.8689\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3057 - accuracy: 0.9029 - val_loss: 0.4190 - val_accuracy: 0.8769\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3128 - accuracy: 0.8988 - val_loss: 0.4209 - val_accuracy: 0.8806\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3065 - accuracy: 0.9013 - val_loss: 0.3859 - val_accuracy: 0.8892\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3172 - accuracy: 0.8983 - val_loss: 0.4205 - val_accuracy: 0.8765\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2785 - accuracy: 0.9104 - val_loss: 0.4367 - val_accuracy: 0.8720\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2883 - accuracy: 0.9074 - val_loss: 0.4263 - val_accuracy: 0.8743\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2702 - accuracy: 0.9141 - val_loss: 0.4799 - val_accuracy: 0.8619\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2790 - accuracy: 0.9112 - val_loss: 0.4146 - val_accuracy: 0.8856\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2745 - accuracy: 0.9113 - val_loss: 0.4247 - val_accuracy: 0.8807\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2694 - accuracy: 0.9126 - val_loss: 0.4046 - val_accuracy: 0.8903\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2651 - accuracy: 0.9153 - val_loss: 0.4703 - val_accuracy: 0.8683\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2644 - accuracy: 0.9132 - val_loss: 0.4626 - val_accuracy: 0.8700\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2778 - accuracy: 0.9097 - val_loss: 0.5148 - val_accuracy: 0.8548\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2582 - accuracy: 0.9166 - val_loss: 0.3800 - val_accuracy: 0.8981\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2650 - accuracy: 0.9120 - val_loss: 0.3704 - val_accuracy: 0.8968\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2372 - accuracy: 0.9234 - val_loss: 0.4069 - val_accuracy: 0.8880\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2397 - accuracy: 0.9228 - val_loss: 0.5175 - val_accuracy: 0.8579\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2505 - accuracy: 0.9181 - val_loss: 0.4537 - val_accuracy: 0.8741\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2256 - accuracy: 0.9276 - val_loss: 0.4556 - val_accuracy: 0.8784\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2477 - accuracy: 0.9190 - val_loss: 0.4645 - val_accuracy: 0.8711\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2281 - accuracy: 0.9257 - val_loss: 0.3921 - val_accuracy: 0.8967\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2245 - accuracy: 0.9275 - val_loss: 0.4035 - val_accuracy: 0.8931\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2103 - accuracy: 0.9315 - val_loss: 0.4297 - val_accuracy: 0.8859\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2278 - accuracy: 0.9244 - val_loss: 0.4682 - val_accuracy: 0.8686\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2226 - accuracy: 0.9259 - val_loss: 0.3973 - val_accuracy: 0.8944\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2037 - accuracy: 0.9333 - val_loss: 0.3638 - val_accuracy: 0.9079\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2001 - accuracy: 0.9342 - val_loss: 0.3902 - val_accuracy: 0.8952\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1929 - accuracy: 0.9365 - val_loss: 0.3899 - val_accuracy: 0.9010\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2081 - accuracy: 0.9306 - val_loss: 0.3402 - val_accuracy: 0.9162\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1825 - accuracy: 0.9405 - val_loss: 0.3899 - val_accuracy: 0.9025\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1898 - accuracy: 0.9374 - val_loss: 0.3853 - val_accuracy: 0.9045\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2012 - accuracy: 0.9350 - val_loss: 0.3723 - val_accuracy: 0.9026\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2075 - accuracy: 0.9311 - val_loss: 0.3356 - val_accuracy: 0.9183\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1858 - accuracy: 0.9375 - val_loss: 0.3802 - val_accuracy: 0.9071\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1891 - accuracy: 0.9378 - val_loss: 0.3481 - val_accuracy: 0.9166\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1744 - accuracy: 0.9427 - val_loss: 0.3909 - val_accuracy: 0.9039\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1664 - accuracy: 0.9466 - val_loss: 0.3632 - val_accuracy: 0.9092\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1804 - accuracy: 0.9407 - val_loss: 0.4210 - val_accuracy: 0.8949\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1891 - accuracy: 0.9386 - val_loss: 0.4053 - val_accuracy: 0.9019\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1989 - accuracy: 0.9336 - val_loss: 0.4465 - val_accuracy: 0.8912\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1790 - accuracy: 0.9406 - val_loss: 0.4216 - val_accuracy: 0.8963\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1767 - accuracy: 0.9411 - val_loss: 0.4259 - val_accuracy: 0.8978\n",
      "Try 5/100: Best_val_acc: [0.8424423336982727, 0.8262222409248352], lr: 0.0009244310964017381, Lambda: 3.6124095136679596e-05\n",
      "\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3358 - accuracy: 0.1162 - val_loss: 2.2303 - val_accuracy: 0.1721\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2799 - accuracy: 0.1604 - val_loss: 2.2115 - val_accuracy: 0.2273\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2460 - accuracy: 0.2093 - val_loss: 2.1973 - val_accuracy: 0.2236\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1977 - accuracy: 0.2715 - val_loss: 2.0924 - val_accuracy: 0.3641\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1311 - accuracy: 0.3235 - val_loss: 2.0400 - val_accuracy: 0.3807\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0511 - accuracy: 0.3527 - val_loss: 1.9607 - val_accuracy: 0.3673\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9653 - accuracy: 0.3805 - val_loss: 1.8685 - val_accuracy: 0.4369\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8769 - accuracy: 0.4070 - val_loss: 1.8047 - val_accuracy: 0.4576\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7922 - accuracy: 0.4358 - val_loss: 1.7434 - val_accuracy: 0.4767\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7158 - accuracy: 0.4602 - val_loss: 1.6617 - val_accuracy: 0.5213\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6396 - accuracy: 0.4960 - val_loss: 1.6073 - val_accuracy: 0.5528\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5740 - accuracy: 0.5255 - val_loss: 1.5846 - val_accuracy: 0.5461\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5152 - accuracy: 0.5528 - val_loss: 1.5241 - val_accuracy: 0.5508\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4623 - accuracy: 0.5730 - val_loss: 1.4960 - val_accuracy: 0.5565\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4168 - accuracy: 0.5904 - val_loss: 1.4260 - val_accuracy: 0.5977\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3750 - accuracy: 0.6019 - val_loss: 1.3678 - val_accuracy: 0.6194\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3353 - accuracy: 0.6141 - val_loss: 1.3503 - val_accuracy: 0.6091\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3050 - accuracy: 0.6230 - val_loss: 1.3358 - val_accuracy: 0.6237\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2766 - accuracy: 0.6298 - val_loss: 1.2554 - val_accuracy: 0.6531\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2520 - accuracy: 0.6371 - val_loss: 1.2761 - val_accuracy: 0.6333\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2289 - accuracy: 0.6410 - val_loss: 1.2648 - val_accuracy: 0.6381\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2082 - accuracy: 0.6485 - val_loss: 1.2146 - val_accuracy: 0.6519\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1869 - accuracy: 0.6538 - val_loss: 1.2174 - val_accuracy: 0.6568\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1727 - accuracy: 0.6573 - val_loss: 1.2107 - val_accuracy: 0.6505\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1548 - accuracy: 0.6633 - val_loss: 1.1762 - val_accuracy: 0.6656\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1422 - accuracy: 0.6661 - val_loss: 1.1758 - val_accuracy: 0.6457\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1270 - accuracy: 0.6702 - val_loss: 1.1606 - val_accuracy: 0.6713\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1151 - accuracy: 0.6737 - val_loss: 1.1215 - val_accuracy: 0.6756\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1038 - accuracy: 0.6758 - val_loss: 1.0954 - val_accuracy: 0.6939\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0913 - accuracy: 0.6796 - val_loss: 1.2032 - val_accuracy: 0.6402\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0795 - accuracy: 0.6846 - val_loss: 1.1202 - val_accuracy: 0.6806\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0668 - accuracy: 0.6873 - val_loss: 1.1215 - val_accuracy: 0.6831\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0580 - accuracy: 0.6895 - val_loss: 1.0859 - val_accuracy: 0.6892\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0470 - accuracy: 0.6930 - val_loss: 1.1181 - val_accuracy: 0.6688\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0374 - accuracy: 0.6962 - val_loss: 1.0522 - val_accuracy: 0.7019\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0282 - accuracy: 0.6991 - val_loss: 1.0558 - val_accuracy: 0.7008\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0228 - accuracy: 0.6998 - val_loss: 1.0457 - val_accuracy: 0.7048\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0120 - accuracy: 0.7006 - val_loss: 1.0653 - val_accuracy: 0.6893\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0031 - accuracy: 0.7051 - val_loss: 1.0909 - val_accuracy: 0.6903\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9993 - accuracy: 0.7059 - val_loss: 0.9876 - val_accuracy: 0.7198\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9872 - accuracy: 0.7109 - val_loss: 1.0346 - val_accuracy: 0.7046\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9793 - accuracy: 0.7128 - val_loss: 1.0382 - val_accuracy: 0.6986\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9730 - accuracy: 0.7156 - val_loss: 1.0152 - val_accuracy: 0.7062\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9666 - accuracy: 0.7174 - val_loss: 1.0014 - val_accuracy: 0.7178\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9590 - accuracy: 0.7177 - val_loss: 1.0159 - val_accuracy: 0.7094\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9512 - accuracy: 0.7209 - val_loss: 1.0016 - val_accuracy: 0.7167\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9428 - accuracy: 0.7240 - val_loss: 0.9909 - val_accuracy: 0.7167\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9380 - accuracy: 0.7232 - val_loss: 1.0179 - val_accuracy: 0.7077\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9307 - accuracy: 0.7266 - val_loss: 0.9813 - val_accuracy: 0.7229\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9231 - accuracy: 0.7300 - val_loss: 0.9898 - val_accuracy: 0.7165\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9188 - accuracy: 0.7296 - val_loss: 0.9602 - val_accuracy: 0.7280\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9123 - accuracy: 0.7323 - val_loss: 0.9970 - val_accuracy: 0.7114\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9052 - accuracy: 0.7344 - val_loss: 0.9711 - val_accuracy: 0.7248\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9009 - accuracy: 0.7356 - val_loss: 0.9528 - val_accuracy: 0.7246\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8926 - accuracy: 0.7394 - val_loss: 0.9319 - val_accuracy: 0.7380\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8870 - accuracy: 0.7382 - val_loss: 0.9332 - val_accuracy: 0.7319\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8846 - accuracy: 0.7402 - val_loss: 0.9346 - val_accuracy: 0.7352\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8765 - accuracy: 0.7443 - val_loss: 0.9399 - val_accuracy: 0.7366\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8729 - accuracy: 0.7433 - val_loss: 0.9408 - val_accuracy: 0.7265\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8670 - accuracy: 0.7460 - val_loss: 0.8912 - val_accuracy: 0.7499\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8630 - accuracy: 0.7467 - val_loss: 0.9238 - val_accuracy: 0.7395\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8566 - accuracy: 0.7478 - val_loss: 0.9453 - val_accuracy: 0.7254\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8531 - accuracy: 0.7496 - val_loss: 0.9281 - val_accuracy: 0.7391\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8471 - accuracy: 0.7504 - val_loss: 0.9138 - val_accuracy: 0.7371\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8417 - accuracy: 0.7527 - val_loss: 0.8960 - val_accuracy: 0.7439\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8362 - accuracy: 0.7535 - val_loss: 0.8723 - val_accuracy: 0.7551\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8322 - accuracy: 0.7553 - val_loss: 0.8790 - val_accuracy: 0.7514\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8287 - accuracy: 0.7563 - val_loss: 0.9098 - val_accuracy: 0.7387\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8219 - accuracy: 0.7592 - val_loss: 0.8787 - val_accuracy: 0.7481\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8166 - accuracy: 0.7586 - val_loss: 0.8566 - val_accuracy: 0.7596\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8130 - accuracy: 0.7606 - val_loss: 0.8790 - val_accuracy: 0.7531\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8103 - accuracy: 0.7600 - val_loss: 0.8264 - val_accuracy: 0.7710\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8066 - accuracy: 0.7611 - val_loss: 0.8582 - val_accuracy: 0.7529\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8007 - accuracy: 0.7644 - val_loss: 0.8318 - val_accuracy: 0.7678\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7982 - accuracy: 0.7633 - val_loss: 0.8495 - val_accuracy: 0.7586\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7907 - accuracy: 0.7656 - val_loss: 0.8578 - val_accuracy: 0.7557\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7885 - accuracy: 0.7677 - val_loss: 0.8533 - val_accuracy: 0.7543\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7832 - accuracy: 0.7690 - val_loss: 0.8613 - val_accuracy: 0.7507\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7807 - accuracy: 0.7701 - val_loss: 0.8524 - val_accuracy: 0.7599\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7764 - accuracy: 0.7697 - val_loss: 0.8224 - val_accuracy: 0.7673\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7770 - accuracy: 0.7699 - val_loss: 0.8380 - val_accuracy: 0.7645\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7691 - accuracy: 0.7729 - val_loss: 0.8199 - val_accuracy: 0.7701\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7677 - accuracy: 0.7736 - val_loss: 0.7978 - val_accuracy: 0.7776\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7614 - accuracy: 0.7765 - val_loss: 0.8411 - val_accuracy: 0.7643\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7550 - accuracy: 0.7774 - val_loss: 0.8238 - val_accuracy: 0.7701\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7534 - accuracy: 0.7779 - val_loss: 0.8464 - val_accuracy: 0.7556\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7496 - accuracy: 0.7795 - val_loss: 0.8433 - val_accuracy: 0.7630\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7501 - accuracy: 0.7787 - val_loss: 0.7847 - val_accuracy: 0.7845\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7445 - accuracy: 0.7799 - val_loss: 0.8125 - val_accuracy: 0.7675\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7403 - accuracy: 0.7811 - val_loss: 0.7743 - val_accuracy: 0.7803\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7385 - accuracy: 0.7815 - val_loss: 0.7964 - val_accuracy: 0.7754\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7350 - accuracy: 0.7831 - val_loss: 0.7730 - val_accuracy: 0.7851\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7285 - accuracy: 0.7842 - val_loss: 0.7973 - val_accuracy: 0.7702\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7254 - accuracy: 0.7862 - val_loss: 0.8091 - val_accuracy: 0.7732\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7227 - accuracy: 0.7870 - val_loss: 0.8197 - val_accuracy: 0.7682\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7218 - accuracy: 0.7870 - val_loss: 0.8404 - val_accuracy: 0.7547\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7151 - accuracy: 0.7902 - val_loss: 0.7952 - val_accuracy: 0.7775\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7125 - accuracy: 0.7900 - val_loss: 0.7539 - val_accuracy: 0.7898\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7112 - accuracy: 0.7904 - val_loss: 0.7726 - val_accuracy: 0.7804\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7093 - accuracy: 0.7909 - val_loss: 0.7802 - val_accuracy: 0.7804\n",
      "Try 6/100: Best_val_acc: [0.8029659390449524, 0.7639444470405579], lr: 2.5449033511901445e-05, Lambda: 8.951291321433897e-05\n",
      "\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2482 - accuracy: 0.1720 - val_loss: 2.0745 - val_accuracy: 0.3529\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9768 - accuracy: 0.3139 - val_loss: 1.8354 - val_accuracy: 0.3742\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6804 - accuracy: 0.4490 - val_loss: 1.6582 - val_accuracy: 0.4209\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5096 - accuracy: 0.5242 - val_loss: 1.5009 - val_accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3792 - accuracy: 0.5747 - val_loss: 1.3754 - val_accuracy: 0.5645\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2765 - accuracy: 0.6111 - val_loss: 1.3169 - val_accuracy: 0.5853\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1879 - accuracy: 0.6419 - val_loss: 1.1828 - val_accuracy: 0.6307\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1275 - accuracy: 0.6617 - val_loss: 1.2277 - val_accuracy: 0.6114\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0684 - accuracy: 0.6816 - val_loss: 1.0771 - val_accuracy: 0.6815\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0194 - accuracy: 0.6957 - val_loss: 1.0239 - val_accuracy: 0.6941\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9811 - accuracy: 0.7087 - val_loss: 0.9624 - val_accuracy: 0.7193\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9540 - accuracy: 0.7162 - val_loss: 0.9084 - val_accuracy: 0.7403\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9224 - accuracy: 0.7253 - val_loss: 0.9281 - val_accuracy: 0.7283\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9010 - accuracy: 0.7311 - val_loss: 0.9065 - val_accuracy: 0.7286\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8698 - accuracy: 0.7394 - val_loss: 1.0176 - val_accuracy: 0.6927\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8555 - accuracy: 0.7448 - val_loss: 0.9237 - val_accuracy: 0.7274\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8346 - accuracy: 0.7503 - val_loss: 0.9235 - val_accuracy: 0.7260\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8267 - accuracy: 0.7509 - val_loss: 0.8531 - val_accuracy: 0.7481\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7969 - accuracy: 0.7619 - val_loss: 0.8404 - val_accuracy: 0.7524\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7925 - accuracy: 0.7637 - val_loss: 0.7975 - val_accuracy: 0.7648\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7735 - accuracy: 0.7676 - val_loss: 0.8948 - val_accuracy: 0.7309\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7581 - accuracy: 0.7732 - val_loss: 0.7975 - val_accuracy: 0.7709\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7461 - accuracy: 0.7779 - val_loss: 0.8745 - val_accuracy: 0.7351\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7334 - accuracy: 0.7799 - val_loss: 0.7834 - val_accuracy: 0.7721\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7228 - accuracy: 0.7812 - val_loss: 0.8805 - val_accuracy: 0.7346\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7016 - accuracy: 0.7894 - val_loss: 0.7542 - val_accuracy: 0.7771\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6994 - accuracy: 0.7885 - val_loss: 0.7014 - val_accuracy: 0.7978\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6792 - accuracy: 0.7958 - val_loss: 0.7727 - val_accuracy: 0.7744\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6680 - accuracy: 0.8000 - val_loss: 0.8160 - val_accuracy: 0.7564\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6571 - accuracy: 0.8032 - val_loss: 0.7617 - val_accuracy: 0.7766\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6443 - accuracy: 0.8084 - val_loss: 0.7037 - val_accuracy: 0.7910\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6412 - accuracy: 0.8085 - val_loss: 0.7092 - val_accuracy: 0.7944\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6166 - accuracy: 0.8158 - val_loss: 0.7472 - val_accuracy: 0.7835\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6169 - accuracy: 0.8144 - val_loss: 0.6664 - val_accuracy: 0.8044\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6121 - accuracy: 0.8161 - val_loss: 0.6476 - val_accuracy: 0.8119\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6051 - accuracy: 0.8178 - val_loss: 0.6569 - val_accuracy: 0.8104\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5932 - accuracy: 0.8215 - val_loss: 0.7316 - val_accuracy: 0.7876\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5854 - accuracy: 0.8244 - val_loss: 0.6736 - val_accuracy: 0.8019\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5709 - accuracy: 0.8308 - val_loss: 0.6754 - val_accuracy: 0.8056\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5646 - accuracy: 0.8313 - val_loss: 0.5810 - val_accuracy: 0.8324\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5550 - accuracy: 0.8359 - val_loss: 0.6669 - val_accuracy: 0.8056\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5500 - accuracy: 0.8355 - val_loss: 0.5827 - val_accuracy: 0.8331\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5346 - accuracy: 0.8413 - val_loss: 0.5566 - val_accuracy: 0.8364\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5355 - accuracy: 0.8411 - val_loss: 0.6142 - val_accuracy: 0.8222\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5223 - accuracy: 0.8444 - val_loss: 0.5772 - val_accuracy: 0.8334\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5235 - accuracy: 0.8440 - val_loss: 0.6177 - val_accuracy: 0.8206\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5180 - accuracy: 0.8448 - val_loss: 0.6543 - val_accuracy: 0.8088\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5158 - accuracy: 0.8452 - val_loss: 0.5890 - val_accuracy: 0.8258\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5071 - accuracy: 0.8474 - val_loss: 0.6004 - val_accuracy: 0.8246\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5008 - accuracy: 0.8504 - val_loss: 0.5381 - val_accuracy: 0.8421\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4907 - accuracy: 0.8518 - val_loss: 0.5699 - val_accuracy: 0.8311\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4926 - accuracy: 0.8518 - val_loss: 0.6355 - val_accuracy: 0.8130\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4773 - accuracy: 0.8565 - val_loss: 0.5375 - val_accuracy: 0.8444\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4756 - accuracy: 0.8566 - val_loss: 0.5673 - val_accuracy: 0.8332\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4596 - accuracy: 0.8610 - val_loss: 0.5224 - val_accuracy: 0.8489\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4589 - accuracy: 0.8641 - val_loss: 0.5841 - val_accuracy: 0.8286\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4536 - accuracy: 0.8628 - val_loss: 0.5202 - val_accuracy: 0.8501\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4398 - accuracy: 0.8687 - val_loss: 0.6553 - val_accuracy: 0.8006\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4441 - accuracy: 0.8677 - val_loss: 0.5406 - val_accuracy: 0.8439\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4348 - accuracy: 0.8698 - val_loss: 0.5283 - val_accuracy: 0.8459\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4469 - accuracy: 0.8663 - val_loss: 0.5091 - val_accuracy: 0.8539\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4200 - accuracy: 0.8758 - val_loss: 0.4628 - val_accuracy: 0.8676\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4189 - accuracy: 0.8752 - val_loss: 0.5671 - val_accuracy: 0.8347\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4247 - accuracy: 0.8723 - val_loss: 0.5363 - val_accuracy: 0.8450\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4074 - accuracy: 0.8775 - val_loss: 0.5858 - val_accuracy: 0.8315\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4073 - accuracy: 0.8784 - val_loss: 0.4939 - val_accuracy: 0.8586\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4043 - accuracy: 0.8786 - val_loss: 0.5526 - val_accuracy: 0.8389\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3896 - accuracy: 0.8850 - val_loss: 0.4865 - val_accuracy: 0.8559\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3974 - accuracy: 0.8794 - val_loss: 0.4463 - val_accuracy: 0.8731\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3837 - accuracy: 0.8850 - val_loss: 0.4807 - val_accuracy: 0.8626\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3874 - accuracy: 0.8840 - val_loss: 0.4496 - val_accuracy: 0.8681\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3815 - accuracy: 0.8849 - val_loss: 0.5179 - val_accuracy: 0.8502\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3788 - accuracy: 0.8847 - val_loss: 0.5024 - val_accuracy: 0.8524\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3610 - accuracy: 0.8926 - val_loss: 0.4854 - val_accuracy: 0.8613\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3662 - accuracy: 0.8905 - val_loss: 0.4701 - val_accuracy: 0.8671\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3592 - accuracy: 0.8936 - val_loss: 0.4553 - val_accuracy: 0.8727\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3601 - accuracy: 0.8907 - val_loss: 0.5074 - val_accuracy: 0.8502\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3628 - accuracy: 0.8906 - val_loss: 0.4274 - val_accuracy: 0.8768\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3544 - accuracy: 0.8925 - val_loss: 0.4391 - val_accuracy: 0.8731\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3425 - accuracy: 0.8967 - val_loss: 0.4962 - val_accuracy: 0.8595\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3448 - accuracy: 0.8978 - val_loss: 0.5353 - val_accuracy: 0.8432\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3370 - accuracy: 0.8992 - val_loss: 0.4486 - val_accuracy: 0.8705\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3301 - accuracy: 0.9010 - val_loss: 0.4491 - val_accuracy: 0.8714\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3315 - accuracy: 0.9006 - val_loss: 0.4598 - val_accuracy: 0.8709\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3251 - accuracy: 0.9028 - val_loss: 0.4471 - val_accuracy: 0.8712\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3177 - accuracy: 0.9047 - val_loss: 0.4511 - val_accuracy: 0.8729\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3150 - accuracy: 0.9058 - val_loss: 0.3853 - val_accuracy: 0.8925\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3189 - accuracy: 0.9038 - val_loss: 0.4930 - val_accuracy: 0.8593\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3069 - accuracy: 0.9078 - val_loss: 0.4875 - val_accuracy: 0.8621\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3144 - accuracy: 0.9044 - val_loss: 0.5551 - val_accuracy: 0.8384\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3280 - accuracy: 0.8998 - val_loss: 0.4557 - val_accuracy: 0.8673\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3018 - accuracy: 0.9099 - val_loss: 0.4048 - val_accuracy: 0.8853\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3033 - accuracy: 0.9080 - val_loss: 0.3989 - val_accuracy: 0.8889\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2902 - accuracy: 0.9139 - val_loss: 0.4429 - val_accuracy: 0.8730\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2904 - accuracy: 0.9127 - val_loss: 0.3820 - val_accuracy: 0.8943\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2829 - accuracy: 0.9151 - val_loss: 0.4176 - val_accuracy: 0.8814\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2811 - accuracy: 0.9151 - val_loss: 0.4880 - val_accuracy: 0.8611\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2863 - accuracy: 0.9134 - val_loss: 0.3654 - val_accuracy: 0.8999\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2815 - accuracy: 0.9152 - val_loss: 0.3776 - val_accuracy: 0.8944\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2815 - accuracy: 0.9146 - val_loss: 0.4170 - val_accuracy: 0.8833\n",
      "Try 7/100: Best_val_acc: [0.6557355523109436, 0.8266666531562805], lr: 0.00018720236405964409, Lambda: 3.893744621225725e-05\n",
      "\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3498 - accuracy: 0.1137 - val_loss: 2.3156 - val_accuracy: 0.0606\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2838 - accuracy: 0.1512 - val_loss: 2.2818 - val_accuracy: 0.1481\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2508 - accuracy: 0.1981 - val_loss: 2.2617 - val_accuracy: 0.1547\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2069 - accuracy: 0.2304 - val_loss: 2.2088 - val_accuracy: 0.1825\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1517 - accuracy: 0.2626 - val_loss: 2.1161 - val_accuracy: 0.3020\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0855 - accuracy: 0.3025 - val_loss: 2.0490 - val_accuracy: 0.3259\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0124 - accuracy: 0.3411 - val_loss: 1.9764 - val_accuracy: 0.4114\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9404 - accuracy: 0.3845 - val_loss: 1.9132 - val_accuracy: 0.4341\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8750 - accuracy: 0.4182 - val_loss: 1.8094 - val_accuracy: 0.5017\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8058 - accuracy: 0.4590 - val_loss: 1.7738 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7458 - accuracy: 0.4880 - val_loss: 1.7263 - val_accuracy: 0.5326\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6857 - accuracy: 0.5104 - val_loss: 1.6891 - val_accuracy: 0.5339\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6297 - accuracy: 0.5313 - val_loss: 1.6084 - val_accuracy: 0.5663\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5812 - accuracy: 0.5458 - val_loss: 1.5533 - val_accuracy: 0.5814\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5340 - accuracy: 0.5598 - val_loss: 1.5255 - val_accuracy: 0.5896\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4896 - accuracy: 0.5741 - val_loss: 1.4829 - val_accuracy: 0.6099\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4513 - accuracy: 0.5864 - val_loss: 1.4407 - val_accuracy: 0.6196\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4164 - accuracy: 0.5960 - val_loss: 1.3990 - val_accuracy: 0.6144\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3829 - accuracy: 0.6057 - val_loss: 1.4052 - val_accuracy: 0.6165\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3545 - accuracy: 0.6137 - val_loss: 1.3251 - val_accuracy: 0.6438\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3270 - accuracy: 0.6212 - val_loss: 1.3502 - val_accuracy: 0.6156\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3031 - accuracy: 0.6287 - val_loss: 1.2695 - val_accuracy: 0.6612\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2805 - accuracy: 0.6344 - val_loss: 1.2857 - val_accuracy: 0.6421\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2611 - accuracy: 0.6381 - val_loss: 1.2713 - val_accuracy: 0.6409\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2411 - accuracy: 0.6455 - val_loss: 1.2736 - val_accuracy: 0.6349\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2229 - accuracy: 0.6502 - val_loss: 1.3164 - val_accuracy: 0.6113\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2065 - accuracy: 0.6525 - val_loss: 1.2280 - val_accuracy: 0.6584\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1918 - accuracy: 0.6571 - val_loss: 1.2197 - val_accuracy: 0.6629\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1760 - accuracy: 0.6617 - val_loss: 1.1455 - val_accuracy: 0.6866\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1641 - accuracy: 0.6659 - val_loss: 1.1927 - val_accuracy: 0.6543\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1512 - accuracy: 0.6675 - val_loss: 1.1942 - val_accuracy: 0.6641\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1404 - accuracy: 0.6714 - val_loss: 1.1429 - val_accuracy: 0.6753\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1274 - accuracy: 0.6746 - val_loss: 1.1702 - val_accuracy: 0.6634\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1172 - accuracy: 0.6771 - val_loss: 1.1555 - val_accuracy: 0.6684\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1053 - accuracy: 0.6811 - val_loss: 1.1096 - val_accuracy: 0.6971\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0976 - accuracy: 0.6804 - val_loss: 1.1078 - val_accuracy: 0.6961\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0895 - accuracy: 0.6850 - val_loss: 1.1299 - val_accuracy: 0.6756\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0788 - accuracy: 0.6876 - val_loss: 1.1726 - val_accuracy: 0.6609\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0702 - accuracy: 0.6904 - val_loss: 1.1066 - val_accuracy: 0.6889\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0622 - accuracy: 0.6934 - val_loss: 1.0607 - val_accuracy: 0.7096\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0548 - accuracy: 0.6943 - val_loss: 1.1431 - val_accuracy: 0.6564\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0462 - accuracy: 0.6971 - val_loss: 1.0922 - val_accuracy: 0.6852\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0367 - accuracy: 0.7010 - val_loss: 1.0845 - val_accuracy: 0.6919\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0285 - accuracy: 0.7040 - val_loss: 1.0637 - val_accuracy: 0.6968\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0234 - accuracy: 0.7032 - val_loss: 1.0558 - val_accuracy: 0.7054\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0163 - accuracy: 0.7055 - val_loss: 1.0558 - val_accuracy: 0.7039\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0103 - accuracy: 0.7071 - val_loss: 1.0754 - val_accuracy: 0.6904\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0020 - accuracy: 0.7095 - val_loss: 1.0272 - val_accuracy: 0.7172\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9990 - accuracy: 0.7095 - val_loss: 1.0292 - val_accuracy: 0.7100\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9908 - accuracy: 0.7129 - val_loss: 1.0638 - val_accuracy: 0.6974\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9847 - accuracy: 0.7144 - val_loss: 1.0583 - val_accuracy: 0.6909\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9790 - accuracy: 0.7162 - val_loss: 1.0078 - val_accuracy: 0.7211\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9720 - accuracy: 0.7173 - val_loss: 0.9981 - val_accuracy: 0.7224\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9682 - accuracy: 0.7189 - val_loss: 0.9791 - val_accuracy: 0.7291\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9614 - accuracy: 0.7211 - val_loss: 1.0183 - val_accuracy: 0.7116\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9567 - accuracy: 0.7210 - val_loss: 0.9570 - val_accuracy: 0.7319\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9511 - accuracy: 0.7241 - val_loss: 0.9515 - val_accuracy: 0.7351\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9483 - accuracy: 0.7243 - val_loss: 0.9873 - val_accuracy: 0.7189\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9418 - accuracy: 0.7258 - val_loss: 0.9801 - val_accuracy: 0.7276\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9368 - accuracy: 0.7265 - val_loss: 1.0102 - val_accuracy: 0.7170\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9327 - accuracy: 0.7283 - val_loss: 0.9661 - val_accuracy: 0.7302\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9276 - accuracy: 0.7288 - val_loss: 0.9576 - val_accuracy: 0.7211\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9206 - accuracy: 0.7321 - val_loss: 0.9608 - val_accuracy: 0.7251\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9163 - accuracy: 0.7333 - val_loss: 0.9800 - val_accuracy: 0.7191\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9113 - accuracy: 0.7350 - val_loss: 0.9225 - val_accuracy: 0.7401\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9078 - accuracy: 0.7359 - val_loss: 0.9748 - val_accuracy: 0.7226\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9023 - accuracy: 0.7376 - val_loss: 0.9444 - val_accuracy: 0.7393\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9021 - accuracy: 0.7366 - val_loss: 0.9327 - val_accuracy: 0.7414\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8950 - accuracy: 0.7379 - val_loss: 0.9188 - val_accuracy: 0.7437\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8893 - accuracy: 0.7415 - val_loss: 0.9320 - val_accuracy: 0.7394\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8863 - accuracy: 0.7412 - val_loss: 0.9346 - val_accuracy: 0.7368\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8804 - accuracy: 0.7431 - val_loss: 0.9315 - val_accuracy: 0.7426\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8767 - accuracy: 0.7450 - val_loss: 0.9101 - val_accuracy: 0.7454\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8743 - accuracy: 0.7466 - val_loss: 0.9442 - val_accuracy: 0.7319\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8692 - accuracy: 0.7472 - val_loss: 0.9066 - val_accuracy: 0.7488\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8685 - accuracy: 0.7466 - val_loss: 0.9115 - val_accuracy: 0.7449\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8655 - accuracy: 0.7478 - val_loss: 0.8890 - val_accuracy: 0.7551\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8583 - accuracy: 0.7488 - val_loss: 0.8911 - val_accuracy: 0.7504\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8556 - accuracy: 0.7497 - val_loss: 0.8905 - val_accuracy: 0.7469\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8534 - accuracy: 0.7509 - val_loss: 0.9166 - val_accuracy: 0.7416\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8482 - accuracy: 0.7539 - val_loss: 0.8861 - val_accuracy: 0.7554\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8441 - accuracy: 0.7554 - val_loss: 0.8880 - val_accuracy: 0.7503\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8397 - accuracy: 0.7548 - val_loss: 0.8774 - val_accuracy: 0.7561\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8379 - accuracy: 0.7564 - val_loss: 0.9195 - val_accuracy: 0.7414\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8338 - accuracy: 0.7576 - val_loss: 0.9041 - val_accuracy: 0.7441\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8314 - accuracy: 0.7582 - val_loss: 0.8956 - val_accuracy: 0.7426\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8322 - accuracy: 0.7568 - val_loss: 0.8668 - val_accuracy: 0.7608\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8251 - accuracy: 0.7599 - val_loss: 0.8481 - val_accuracy: 0.7654\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8224 - accuracy: 0.7596 - val_loss: 0.8708 - val_accuracy: 0.7587\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8200 - accuracy: 0.7611 - val_loss: 0.8658 - val_accuracy: 0.7583\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8144 - accuracy: 0.7638 - val_loss: 0.8649 - val_accuracy: 0.7559\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8119 - accuracy: 0.7632 - val_loss: 0.8903 - val_accuracy: 0.7524\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8090 - accuracy: 0.7635 - val_loss: 0.8734 - val_accuracy: 0.7511\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8062 - accuracy: 0.7648 - val_loss: 0.8178 - val_accuracy: 0.7750\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8032 - accuracy: 0.7664 - val_loss: 0.8237 - val_accuracy: 0.7706\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8024 - accuracy: 0.7669 - val_loss: 0.8710 - val_accuracy: 0.7534\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7961 - accuracy: 0.7681 - val_loss: 0.8524 - val_accuracy: 0.7627\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7925 - accuracy: 0.7701 - val_loss: 0.9014 - val_accuracy: 0.7431\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7930 - accuracy: 0.7691 - val_loss: 0.8590 - val_accuracy: 0.7524\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7874 - accuracy: 0.7711 - val_loss: 0.8666 - val_accuracy: 0.7523\n",
      "Try 8/100: Best_val_acc: [0.8610916137695312, 0.7534999847412109], lr: 2.088496159297584e-05, Lambda: 0.0005295491519752289\n",
      "\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3034 - accuracy: 0.1098 - val_loss: 2.2678 - val_accuracy: 0.1509\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1396 - accuracy: 0.2241 - val_loss: 1.9201 - val_accuracy: 0.3302\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7800 - accuracy: 0.3830 - val_loss: 1.5871 - val_accuracy: 0.5211\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5412 - accuracy: 0.4922 - val_loss: 1.4265 - val_accuracy: 0.5718\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4109 - accuracy: 0.5417 - val_loss: 1.3055 - val_accuracy: 0.6014\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2820 - accuracy: 0.5972 - val_loss: 1.2726 - val_accuracy: 0.6170\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2094 - accuracy: 0.6219 - val_loss: 1.2161 - val_accuracy: 0.6294\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1373 - accuracy: 0.6507 - val_loss: 1.1671 - val_accuracy: 0.6564\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1010 - accuracy: 0.6625 - val_loss: 1.0672 - val_accuracy: 0.6825\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0574 - accuracy: 0.6750 - val_loss: 1.0684 - val_accuracy: 0.6907\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0290 - accuracy: 0.6855 - val_loss: 1.0766 - val_accuracy: 0.6791\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9995 - accuracy: 0.6941 - val_loss: 1.0568 - val_accuracy: 0.6873\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9687 - accuracy: 0.7019 - val_loss: 1.0019 - val_accuracy: 0.7098\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9356 - accuracy: 0.7148 - val_loss: 0.9978 - val_accuracy: 0.7034\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9137 - accuracy: 0.7225 - val_loss: 1.0214 - val_accuracy: 0.6973\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8837 - accuracy: 0.7305 - val_loss: 0.9489 - val_accuracy: 0.7199\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8739 - accuracy: 0.7335 - val_loss: 0.8758 - val_accuracy: 0.7454\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8314 - accuracy: 0.7466 - val_loss: 0.8611 - val_accuracy: 0.7458\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8218 - accuracy: 0.7497 - val_loss: 0.7716 - val_accuracy: 0.7762\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7955 - accuracy: 0.7580 - val_loss: 0.7977 - val_accuracy: 0.7687\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7769 - accuracy: 0.7639 - val_loss: 0.7536 - val_accuracy: 0.7832\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7636 - accuracy: 0.7658 - val_loss: 0.7422 - val_accuracy: 0.7807\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7434 - accuracy: 0.7717 - val_loss: 0.8882 - val_accuracy: 0.7226\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7174 - accuracy: 0.7795 - val_loss: 0.8019 - val_accuracy: 0.7655\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6995 - accuracy: 0.7863 - val_loss: 0.7210 - val_accuracy: 0.7937\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6789 - accuracy: 0.7921 - val_loss: 0.7541 - val_accuracy: 0.7831\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6750 - accuracy: 0.7930 - val_loss: 0.7126 - val_accuracy: 0.7899\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6534 - accuracy: 0.8008 - val_loss: 0.7537 - val_accuracy: 0.7789\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6304 - accuracy: 0.8095 - val_loss: 0.6817 - val_accuracy: 0.8042\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6212 - accuracy: 0.8101 - val_loss: 0.6629 - val_accuracy: 0.8070\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6187 - accuracy: 0.8097 - val_loss: 0.6763 - val_accuracy: 0.7992\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6075 - accuracy: 0.8131 - val_loss: 0.7512 - val_accuracy: 0.7812\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5951 - accuracy: 0.8179 - val_loss: 0.6499 - val_accuracy: 0.8076\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5874 - accuracy: 0.8206 - val_loss: 0.6741 - val_accuracy: 0.7986\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5617 - accuracy: 0.8289 - val_loss: 0.6888 - val_accuracy: 0.7997\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5617 - accuracy: 0.8280 - val_loss: 0.6295 - val_accuracy: 0.8124\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5489 - accuracy: 0.8325 - val_loss: 0.6694 - val_accuracy: 0.8031\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5491 - accuracy: 0.8311 - val_loss: 0.6553 - val_accuracy: 0.8053\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5347 - accuracy: 0.8352 - val_loss: 0.6065 - val_accuracy: 0.8244\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5106 - accuracy: 0.8449 - val_loss: 0.6711 - val_accuracy: 0.8041\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5209 - accuracy: 0.8388 - val_loss: 0.6214 - val_accuracy: 0.8173\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4973 - accuracy: 0.8481 - val_loss: 0.5881 - val_accuracy: 0.8259\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4851 - accuracy: 0.8509 - val_loss: 0.5616 - val_accuracy: 0.8360\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4943 - accuracy: 0.8481 - val_loss: 0.5851 - val_accuracy: 0.8301\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4722 - accuracy: 0.8547 - val_loss: 0.5400 - val_accuracy: 0.8400\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4700 - accuracy: 0.8549 - val_loss: 0.5510 - val_accuracy: 0.8379\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4658 - accuracy: 0.8568 - val_loss: 0.5324 - val_accuracy: 0.8454\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4658 - accuracy: 0.8565 - val_loss: 0.5765 - val_accuracy: 0.8339\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4535 - accuracy: 0.8603 - val_loss: 0.5867 - val_accuracy: 0.8280\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4381 - accuracy: 0.8646 - val_loss: 0.5075 - val_accuracy: 0.8541\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4416 - accuracy: 0.8636 - val_loss: 0.4989 - val_accuracy: 0.8558\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4318 - accuracy: 0.8660 - val_loss: 0.5278 - val_accuracy: 0.8437\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4263 - accuracy: 0.8681 - val_loss: 0.5035 - val_accuracy: 0.8534\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4097 - accuracy: 0.8729 - val_loss: 0.5556 - val_accuracy: 0.8343\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4079 - accuracy: 0.8724 - val_loss: 0.4609 - val_accuracy: 0.8649\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4086 - accuracy: 0.8718 - val_loss: 0.4978 - val_accuracy: 0.8566\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4038 - accuracy: 0.8750 - val_loss: 0.4986 - val_accuracy: 0.8566\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3860 - accuracy: 0.8795 - val_loss: 0.5156 - val_accuracy: 0.8481\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3804 - accuracy: 0.8813 - val_loss: 0.5229 - val_accuracy: 0.8471\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3777 - accuracy: 0.8810 - val_loss: 0.4341 - val_accuracy: 0.8729\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3748 - accuracy: 0.8847 - val_loss: 0.4678 - val_accuracy: 0.8670\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3656 - accuracy: 0.8864 - val_loss: 0.4178 - val_accuracy: 0.8826\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3558 - accuracy: 0.8905 - val_loss: 0.4581 - val_accuracy: 0.8670\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3650 - accuracy: 0.8857 - val_loss: 0.5140 - val_accuracy: 0.8525\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3429 - accuracy: 0.8931 - val_loss: 0.4969 - val_accuracy: 0.8586\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3473 - accuracy: 0.8921 - val_loss: 0.4411 - val_accuracy: 0.8754\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3494 - accuracy: 0.8902 - val_loss: 0.4792 - val_accuracy: 0.8609\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3416 - accuracy: 0.8919 - val_loss: 0.4901 - val_accuracy: 0.8556\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3428 - accuracy: 0.8913 - val_loss: 0.4621 - val_accuracy: 0.8683\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3231 - accuracy: 0.9001 - val_loss: 0.4637 - val_accuracy: 0.8680\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3198 - accuracy: 0.9004 - val_loss: 0.4392 - val_accuracy: 0.8759\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3134 - accuracy: 0.9015 - val_loss: 0.4690 - val_accuracy: 0.8651\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3132 - accuracy: 0.9022 - val_loss: 0.4727 - val_accuracy: 0.8630\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3098 - accuracy: 0.9031 - val_loss: 0.4678 - val_accuracy: 0.8654\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3059 - accuracy: 0.9040 - val_loss: 0.3932 - val_accuracy: 0.8871\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3100 - accuracy: 0.9040 - val_loss: 0.4710 - val_accuracy: 0.8691\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2886 - accuracy: 0.9119 - val_loss: 0.4305 - val_accuracy: 0.8777\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2948 - accuracy: 0.9078 - val_loss: 0.4431 - val_accuracy: 0.8759\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2793 - accuracy: 0.9126 - val_loss: 0.3997 - val_accuracy: 0.8886\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2778 - accuracy: 0.9125 - val_loss: 0.4217 - val_accuracy: 0.8849\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2711 - accuracy: 0.9143 - val_loss: 0.4122 - val_accuracy: 0.8844\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2691 - accuracy: 0.9159 - val_loss: 0.4307 - val_accuracy: 0.8783\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2787 - accuracy: 0.9112 - val_loss: 0.4871 - val_accuracy: 0.8616\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2688 - accuracy: 0.9144 - val_loss: 0.3964 - val_accuracy: 0.8899\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2656 - accuracy: 0.9154 - val_loss: 0.4707 - val_accuracy: 0.8658\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2514 - accuracy: 0.9206 - val_loss: 0.4362 - val_accuracy: 0.8804\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2515 - accuracy: 0.9209 - val_loss: 0.3921 - val_accuracy: 0.8922\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2513 - accuracy: 0.9220 - val_loss: 0.3978 - val_accuracy: 0.8921\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2409 - accuracy: 0.9238 - val_loss: 0.3848 - val_accuracy: 0.8948\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2459 - accuracy: 0.9229 - val_loss: 0.3642 - val_accuracy: 0.9025\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2451 - accuracy: 0.9217 - val_loss: 0.3939 - val_accuracy: 0.8920\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2376 - accuracy: 0.9251 - val_loss: 0.4507 - val_accuracy: 0.8716\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9279 - val_loss: 0.4129 - val_accuracy: 0.8914\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2191 - accuracy: 0.9316 - val_loss: 0.3586 - val_accuracy: 0.9066\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2320 - accuracy: 0.9264 - val_loss: 0.3888 - val_accuracy: 0.8966\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2262 - accuracy: 0.9284 - val_loss: 0.4927 - val_accuracy: 0.8692\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2141 - accuracy: 0.9339 - val_loss: 0.4644 - val_accuracy: 0.8766\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2357 - accuracy: 0.9250 - val_loss: 0.4090 - val_accuracy: 0.8959\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2138 - accuracy: 0.9335 - val_loss: 0.4106 - val_accuracy: 0.8925\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2095 - accuracy: 0.9353 - val_loss: 0.3527 - val_accuracy: 0.9072\n",
      "Try 9/100: Best_val_acc: [0.7090462446212769, 0.8300555348396301], lr: 0.0004740463722803667, Lambda: 2.694789339835453e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-5.0, -3.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,-3))\n",
    "    best_acc = basicHPCheckFCNN(100, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jc8y8vIOond6"
   },
   "source": [
    "Increasing the epochs to 200 from 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1289543,
     "status": "ok",
     "timestamp": 1594535397486,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "_ZiWOaGr8equ",
    "outputId": "0c027098-f443-40ef-c471-88fa528908e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2779 - accuracy: 0.1321 - val_loss: 2.2200 - val_accuracy: 0.1844\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0143 - accuracy: 0.2869 - val_loss: 1.8289 - val_accuracy: 0.4616\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7258 - accuracy: 0.4259 - val_loss: 1.6070 - val_accuracy: 0.5160\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4709 - accuracy: 0.5387 - val_loss: 1.5000 - val_accuracy: 0.5309\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3081 - accuracy: 0.5925 - val_loss: 1.2155 - val_accuracy: 0.6360\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2092 - accuracy: 0.6268 - val_loss: 1.2499 - val_accuracy: 0.6051\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1464 - accuracy: 0.6488 - val_loss: 1.1560 - val_accuracy: 0.6566\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1048 - accuracy: 0.6618 - val_loss: 1.1530 - val_accuracy: 0.6591\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0560 - accuracy: 0.6775 - val_loss: 1.0839 - val_accuracy: 0.6748\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0088 - accuracy: 0.6944 - val_loss: 1.0567 - val_accuracy: 0.6840\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9875 - accuracy: 0.7011 - val_loss: 0.9489 - val_accuracy: 0.7250\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9572 - accuracy: 0.7090 - val_loss: 0.9770 - val_accuracy: 0.7013\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9387 - accuracy: 0.7132 - val_loss: 0.9554 - val_accuracy: 0.7295\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9105 - accuracy: 0.7234 - val_loss: 0.9218 - val_accuracy: 0.7299\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8772 - accuracy: 0.7349 - val_loss: 0.9377 - val_accuracy: 0.7311\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8602 - accuracy: 0.7402 - val_loss: 0.9163 - val_accuracy: 0.7336\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8506 - accuracy: 0.7431 - val_loss: 0.8776 - val_accuracy: 0.7450\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8194 - accuracy: 0.7533 - val_loss: 0.9336 - val_accuracy: 0.7286\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8044 - accuracy: 0.7591 - val_loss: 0.8524 - val_accuracy: 0.7529\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7836 - accuracy: 0.7640 - val_loss: 0.8751 - val_accuracy: 0.7451\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7697 - accuracy: 0.7692 - val_loss: 0.8617 - val_accuracy: 0.7522\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7546 - accuracy: 0.7730 - val_loss: 0.7436 - val_accuracy: 0.7861\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7473 - accuracy: 0.7751 - val_loss: 0.7775 - val_accuracy: 0.7776\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7171 - accuracy: 0.7859 - val_loss: 0.8286 - val_accuracy: 0.7576\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7034 - accuracy: 0.7879 - val_loss: 0.7286 - val_accuracy: 0.7949\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6976 - accuracy: 0.7909 - val_loss: 0.7881 - val_accuracy: 0.7816\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6796 - accuracy: 0.7955 - val_loss: 0.7284 - val_accuracy: 0.7887\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6660 - accuracy: 0.8009 - val_loss: 0.7378 - val_accuracy: 0.7858\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6623 - accuracy: 0.8009 - val_loss: 0.7379 - val_accuracy: 0.7901\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6411 - accuracy: 0.8090 - val_loss: 0.7122 - val_accuracy: 0.7956\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6399 - accuracy: 0.8074 - val_loss: 0.6971 - val_accuracy: 0.8003\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6166 - accuracy: 0.8157 - val_loss: 0.7218 - val_accuracy: 0.7984\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6103 - accuracy: 0.8165 - val_loss: 0.6280 - val_accuracy: 0.8214\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5856 - accuracy: 0.8245 - val_loss: 0.7407 - val_accuracy: 0.7831\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5862 - accuracy: 0.8252 - val_loss: 0.6180 - val_accuracy: 0.8241\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5877 - accuracy: 0.8232 - val_loss: 0.6208 - val_accuracy: 0.8191\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5738 - accuracy: 0.8283 - val_loss: 0.5750 - val_accuracy: 0.8364\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5601 - accuracy: 0.8312 - val_loss: 0.6161 - val_accuracy: 0.8186\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5482 - accuracy: 0.8349 - val_loss: 0.5773 - val_accuracy: 0.8387\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5368 - accuracy: 0.8391 - val_loss: 0.5852 - val_accuracy: 0.8335\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5284 - accuracy: 0.8413 - val_loss: 0.6259 - val_accuracy: 0.8177\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5260 - accuracy: 0.8417 - val_loss: 0.6590 - val_accuracy: 0.8104\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5096 - accuracy: 0.8477 - val_loss: 0.5789 - val_accuracy: 0.8327\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5054 - accuracy: 0.8471 - val_loss: 0.5264 - val_accuracy: 0.8498\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5002 - accuracy: 0.8512 - val_loss: 0.5259 - val_accuracy: 0.8509\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4857 - accuracy: 0.8561 - val_loss: 0.5456 - val_accuracy: 0.8459\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4808 - accuracy: 0.8552 - val_loss: 0.5396 - val_accuracy: 0.8473\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4738 - accuracy: 0.8593 - val_loss: 0.5099 - val_accuracy: 0.8529\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4711 - accuracy: 0.8575 - val_loss: 0.5016 - val_accuracy: 0.8564\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4588 - accuracy: 0.8629 - val_loss: 0.6072 - val_accuracy: 0.8252\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4554 - accuracy: 0.8637 - val_loss: 0.5023 - val_accuracy: 0.8564\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4383 - accuracy: 0.8689 - val_loss: 0.5265 - val_accuracy: 0.8517\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4436 - accuracy: 0.8662 - val_loss: 0.6226 - val_accuracy: 0.8218\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4477 - accuracy: 0.8649 - val_loss: 0.5211 - val_accuracy: 0.8509\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4288 - accuracy: 0.8711 - val_loss: 0.5215 - val_accuracy: 0.8477\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4215 - accuracy: 0.8747 - val_loss: 0.5093 - val_accuracy: 0.8558\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4124 - accuracy: 0.8761 - val_loss: 0.5167 - val_accuracy: 0.8518\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4068 - accuracy: 0.8778 - val_loss: 0.5159 - val_accuracy: 0.8531\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4036 - accuracy: 0.8780 - val_loss: 0.5279 - val_accuracy: 0.8495\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3900 - accuracy: 0.8841 - val_loss: 0.5339 - val_accuracy: 0.8482\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3968 - accuracy: 0.8808 - val_loss: 0.5397 - val_accuracy: 0.8448\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3798 - accuracy: 0.8853 - val_loss: 0.5108 - val_accuracy: 0.8566\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3968 - accuracy: 0.8805 - val_loss: 0.4775 - val_accuracy: 0.8621\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3841 - accuracy: 0.8840 - val_loss: 0.4690 - val_accuracy: 0.8680\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3831 - accuracy: 0.8842 - val_loss: 0.4621 - val_accuracy: 0.8694\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3707 - accuracy: 0.8890 - val_loss: 0.5745 - val_accuracy: 0.8375\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3680 - accuracy: 0.8900 - val_loss: 0.4700 - val_accuracy: 0.8701\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3538 - accuracy: 0.8951 - val_loss: 0.4974 - val_accuracy: 0.8602\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3596 - accuracy: 0.8912 - val_loss: 0.4941 - val_accuracy: 0.8588\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3438 - accuracy: 0.8979 - val_loss: 0.4196 - val_accuracy: 0.8862\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3470 - accuracy: 0.8957 - val_loss: 0.4864 - val_accuracy: 0.8641\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3362 - accuracy: 0.9003 - val_loss: 0.4418 - val_accuracy: 0.8800\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3385 - accuracy: 0.8993 - val_loss: 0.4286 - val_accuracy: 0.8853\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3403 - accuracy: 0.8971 - val_loss: 0.4973 - val_accuracy: 0.8595\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3381 - accuracy: 0.8991 - val_loss: 0.4966 - val_accuracy: 0.8656\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3217 - accuracy: 0.9048 - val_loss: 0.4541 - val_accuracy: 0.8738\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3151 - accuracy: 0.9068 - val_loss: 0.4536 - val_accuracy: 0.8759\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3113 - accuracy: 0.9081 - val_loss: 0.4584 - val_accuracy: 0.8739\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3000 - accuracy: 0.9121 - val_loss: 0.4445 - val_accuracy: 0.8786\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3174 - accuracy: 0.9052 - val_loss: 0.4539 - val_accuracy: 0.8745\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3085 - accuracy: 0.9075 - val_loss: 0.4652 - val_accuracy: 0.8764\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2937 - accuracy: 0.9130 - val_loss: 0.4886 - val_accuracy: 0.8649\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3127 - accuracy: 0.9058 - val_loss: 0.3896 - val_accuracy: 0.8939\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2950 - accuracy: 0.9120 - val_loss: 0.3877 - val_accuracy: 0.8968\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2848 - accuracy: 0.9145 - val_loss: 0.3919 - val_accuracy: 0.8954\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2887 - accuracy: 0.9136 - val_loss: 0.3727 - val_accuracy: 0.9009\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2797 - accuracy: 0.9177 - val_loss: 0.4147 - val_accuracy: 0.8932\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2732 - accuracy: 0.9189 - val_loss: 0.4085 - val_accuracy: 0.8913\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2845 - accuracy: 0.9137 - val_loss: 0.4300 - val_accuracy: 0.8830\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2689 - accuracy: 0.9210 - val_loss: 0.4676 - val_accuracy: 0.8709\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2719 - accuracy: 0.9193 - val_loss: 0.4019 - val_accuracy: 0.8930\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2684 - accuracy: 0.9201 - val_loss: 0.4406 - val_accuracy: 0.8823\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2685 - accuracy: 0.9201 - val_loss: 0.4106 - val_accuracy: 0.8941\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2555 - accuracy: 0.9243 - val_loss: 0.4148 - val_accuracy: 0.8894\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2529 - accuracy: 0.9254 - val_loss: 0.3937 - val_accuracy: 0.8981\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2423 - accuracy: 0.9291 - val_loss: 0.4133 - val_accuracy: 0.8918\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2553 - accuracy: 0.9242 - val_loss: 0.3853 - val_accuracy: 0.9024\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2473 - accuracy: 0.9269 - val_loss: 0.4465 - val_accuracy: 0.8836\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2404 - accuracy: 0.9287 - val_loss: 0.4118 - val_accuracy: 0.8934\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2371 - accuracy: 0.9310 - val_loss: 0.3746 - val_accuracy: 0.9030\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2377 - accuracy: 0.9307 - val_loss: 0.4226 - val_accuracy: 0.8902\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2394 - accuracy: 0.9295 - val_loss: 0.3771 - val_accuracy: 0.9036\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2361 - accuracy: 0.9313 - val_loss: 0.4149 - val_accuracy: 0.8951\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2247 - accuracy: 0.9353 - val_loss: 0.4025 - val_accuracy: 0.8959\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2209 - accuracy: 0.9362 - val_loss: 0.3942 - val_accuracy: 0.9015\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2236 - accuracy: 0.9348 - val_loss: 0.3961 - val_accuracy: 0.8994\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2227 - accuracy: 0.9349 - val_loss: 0.3662 - val_accuracy: 0.9089\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2252 - accuracy: 0.9331 - val_loss: 0.3790 - val_accuracy: 0.9044\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2220 - accuracy: 0.9350 - val_loss: 0.3493 - val_accuracy: 0.9104\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2068 - accuracy: 0.9405 - val_loss: 0.4162 - val_accuracy: 0.8945\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2053 - accuracy: 0.9396 - val_loss: 0.3787 - val_accuracy: 0.9074\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2036 - accuracy: 0.9412 - val_loss: 0.3679 - val_accuracy: 0.9083\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2069 - accuracy: 0.9398 - val_loss: 0.4361 - val_accuracy: 0.8861\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1989 - accuracy: 0.9422 - val_loss: 0.3725 - val_accuracy: 0.9087\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2038 - accuracy: 0.9400 - val_loss: 0.4104 - val_accuracy: 0.8970\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1960 - accuracy: 0.9442 - val_loss: 0.4072 - val_accuracy: 0.9004\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1946 - accuracy: 0.9436 - val_loss: 0.3569 - val_accuracy: 0.9130\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1947 - accuracy: 0.9428 - val_loss: 0.4043 - val_accuracy: 0.9012\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1937 - accuracy: 0.9441 - val_loss: 0.3950 - val_accuracy: 0.9029\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1940 - accuracy: 0.9425 - val_loss: 0.4212 - val_accuracy: 0.8946\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1900 - accuracy: 0.9450 - val_loss: 0.3676 - val_accuracy: 0.9122\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2049 - accuracy: 0.9390 - val_loss: 0.4237 - val_accuracy: 0.8945\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1973 - accuracy: 0.9426 - val_loss: 0.4937 - val_accuracy: 0.8760\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1867 - accuracy: 0.9461 - val_loss: 0.3545 - val_accuracy: 0.9177\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1728 - accuracy: 0.9502 - val_loss: 0.3490 - val_accuracy: 0.9174\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1818 - accuracy: 0.9458 - val_loss: 0.3859 - val_accuracy: 0.9067\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1649 - accuracy: 0.9529 - val_loss: 0.4510 - val_accuracy: 0.8904\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1825 - accuracy: 0.9452 - val_loss: 0.4419 - val_accuracy: 0.8937\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1681 - accuracy: 0.9513 - val_loss: 0.3751 - val_accuracy: 0.9147\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1561 - accuracy: 0.9558 - val_loss: 0.3791 - val_accuracy: 0.9106\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1829 - accuracy: 0.9455 - val_loss: 0.3415 - val_accuracy: 0.9214\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1673 - accuracy: 0.9515 - val_loss: 0.4831 - val_accuracy: 0.8819\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1649 - accuracy: 0.9530 - val_loss: 0.3274 - val_accuracy: 0.9292\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1563 - accuracy: 0.9564 - val_loss: 0.4175 - val_accuracy: 0.9020\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1616 - accuracy: 0.9533 - val_loss: 0.3714 - val_accuracy: 0.9122\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1535 - accuracy: 0.9555 - val_loss: 0.3098 - val_accuracy: 0.9346\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1469 - accuracy: 0.9590 - val_loss: 0.3208 - val_accuracy: 0.9298\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1593 - accuracy: 0.9541 - val_loss: 0.3913 - val_accuracy: 0.9111\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1525 - accuracy: 0.9559 - val_loss: 0.4026 - val_accuracy: 0.9086\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1572 - accuracy: 0.9542 - val_loss: 0.4218 - val_accuracy: 0.9011\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1660 - accuracy: 0.9516 - val_loss: 0.3856 - val_accuracy: 0.9114\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1635 - accuracy: 0.9516 - val_loss: 0.3726 - val_accuracy: 0.9163\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1450 - accuracy: 0.9587 - val_loss: 0.3846 - val_accuracy: 0.9132\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1464 - accuracy: 0.9579 - val_loss: 0.3958 - val_accuracy: 0.9136\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1487 - accuracy: 0.9557 - val_loss: 0.3475 - val_accuracy: 0.9245\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1390 - accuracy: 0.9608 - val_loss: 0.3399 - val_accuracy: 0.9309\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1267 - accuracy: 0.9655 - val_loss: 0.3453 - val_accuracy: 0.9285\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1361 - accuracy: 0.9603 - val_loss: 0.3695 - val_accuracy: 0.9211\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1332 - accuracy: 0.9614 - val_loss: 0.3702 - val_accuracy: 0.9221\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1290 - accuracy: 0.9636 - val_loss: 0.3767 - val_accuracy: 0.9228\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1351 - accuracy: 0.9622 - val_loss: 0.3474 - val_accuracy: 0.9249\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1262 - accuracy: 0.9653 - val_loss: 0.3526 - val_accuracy: 0.9294\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1247 - accuracy: 0.9652 - val_loss: 0.3869 - val_accuracy: 0.9158\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1508 - accuracy: 0.9546 - val_loss: 0.3760 - val_accuracy: 0.9183\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1468 - accuracy: 0.9561 - val_loss: 0.3351 - val_accuracy: 0.9331\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1243 - accuracy: 0.9653 - val_loss: 0.3868 - val_accuracy: 0.9169\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1380 - accuracy: 0.9599 - val_loss: 0.3594 - val_accuracy: 0.9292\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1254 - accuracy: 0.9639 - val_loss: 0.4604 - val_accuracy: 0.8993\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1330 - accuracy: 0.9618 - val_loss: 0.3715 - val_accuracy: 0.9233\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1073 - accuracy: 0.9722 - val_loss: 0.3732 - val_accuracy: 0.9246\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1357 - accuracy: 0.9614 - val_loss: 0.3619 - val_accuracy: 0.9258\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1164 - accuracy: 0.9676 - val_loss: 0.3703 - val_accuracy: 0.9270\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1120 - accuracy: 0.9691 - val_loss: 0.3865 - val_accuracy: 0.9194\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1155 - accuracy: 0.9678 - val_loss: 0.3495 - val_accuracy: 0.9304\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1094 - accuracy: 0.9703 - val_loss: 0.3785 - val_accuracy: 0.9246\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1050 - accuracy: 0.9714 - val_loss: 0.3575 - val_accuracy: 0.9304\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1034 - accuracy: 0.9725 - val_loss: 0.4254 - val_accuracy: 0.9161\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1097 - accuracy: 0.9694 - val_loss: 0.3818 - val_accuracy: 0.9255\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1096 - accuracy: 0.9691 - val_loss: 0.3545 - val_accuracy: 0.9294\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1014 - accuracy: 0.9726 - val_loss: 0.3513 - val_accuracy: 0.9344\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1127 - accuracy: 0.9681 - val_loss: 0.4136 - val_accuracy: 0.9171\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1240 - accuracy: 0.9642 - val_loss: 0.4309 - val_accuracy: 0.9126\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1232 - accuracy: 0.9648 - val_loss: 0.3925 - val_accuracy: 0.9251\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1097 - accuracy: 0.9690 - val_loss: 0.3812 - val_accuracy: 0.9252\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1015 - accuracy: 0.9727 - val_loss: 0.3930 - val_accuracy: 0.9172\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1096 - accuracy: 0.9689 - val_loss: 0.3164 - val_accuracy: 0.9404\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0991 - accuracy: 0.9729 - val_loss: 0.3610 - val_accuracy: 0.9318\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1106 - accuracy: 0.9690 - val_loss: 0.3918 - val_accuracy: 0.9234\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0914 - accuracy: 0.9755 - val_loss: 0.3801 - val_accuracy: 0.9298\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0939 - accuracy: 0.9752 - val_loss: 0.4502 - val_accuracy: 0.9124\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0914 - accuracy: 0.9760 - val_loss: 0.3350 - val_accuracy: 0.9416\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1003 - accuracy: 0.9722 - val_loss: 0.4466 - val_accuracy: 0.9084\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1038 - accuracy: 0.9707 - val_loss: 0.4210 - val_accuracy: 0.9209\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1038 - accuracy: 0.9706 - val_loss: 0.4302 - val_accuracy: 0.9139\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0993 - accuracy: 0.9718 - val_loss: 0.4573 - val_accuracy: 0.9085\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0946 - accuracy: 0.9743 - val_loss: 0.4422 - val_accuracy: 0.9127\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1016 - accuracy: 0.9713 - val_loss: 0.3482 - val_accuracy: 0.9377\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0942 - accuracy: 0.9735 - val_loss: 0.3678 - val_accuracy: 0.9379\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0900 - accuracy: 0.9752 - val_loss: 0.4210 - val_accuracy: 0.9224\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0792 - accuracy: 0.9798 - val_loss: 0.3543 - val_accuracy: 0.9409\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0867 - accuracy: 0.9775 - val_loss: 0.3896 - val_accuracy: 0.9311\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0927 - accuracy: 0.9748 - val_loss: 0.4628 - val_accuracy: 0.9139\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1034 - accuracy: 0.9703 - val_loss: 0.3604 - val_accuracy: 0.9375\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0787 - accuracy: 0.9795 - val_loss: 0.4405 - val_accuracy: 0.9186\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0887 - accuracy: 0.9758 - val_loss: 0.3873 - val_accuracy: 0.9336\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0892 - accuracy: 0.9751 - val_loss: 0.4563 - val_accuracy: 0.9186\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0824 - accuracy: 0.9785 - val_loss: 0.3851 - val_accuracy: 0.9321\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0874 - accuracy: 0.9761 - val_loss: 0.3775 - val_accuracy: 0.9366\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0802 - accuracy: 0.9791 - val_loss: 0.3690 - val_accuracy: 0.9366\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0877 - accuracy: 0.9751 - val_loss: 0.3818 - val_accuracy: 0.9363\n",
      "Try 1/100: Best_val_acc: [1.0075159072875977, 0.8386666774749756], lr: 0.00033944317422263657, Lambda: 0.0006014286263922272\n",
      "\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2764 - accuracy: 0.1547 - val_loss: 2.2265 - val_accuracy: 0.2098\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1657 - accuracy: 0.2475 - val_loss: 2.0786 - val_accuracy: 0.3437\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0267 - accuracy: 0.3228 - val_loss: 1.9603 - val_accuracy: 0.4145\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8834 - accuracy: 0.3948 - val_loss: 1.8714 - val_accuracy: 0.4114\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7610 - accuracy: 0.4412 - val_loss: 1.7695 - val_accuracy: 0.4561\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6585 - accuracy: 0.4819 - val_loss: 1.6531 - val_accuracy: 0.4966\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5665 - accuracy: 0.5198 - val_loss: 1.5217 - val_accuracy: 0.5686\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4887 - accuracy: 0.5527 - val_loss: 1.4585 - val_accuracy: 0.5831\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4214 - accuracy: 0.5788 - val_loss: 1.3771 - val_accuracy: 0.6138\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3673 - accuracy: 0.5958 - val_loss: 1.3571 - val_accuracy: 0.6189\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3135 - accuracy: 0.6150 - val_loss: 1.3406 - val_accuracy: 0.6163\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2730 - accuracy: 0.6251 - val_loss: 1.2136 - val_accuracy: 0.6661\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2395 - accuracy: 0.6369 - val_loss: 1.1778 - val_accuracy: 0.6656\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2001 - accuracy: 0.6478 - val_loss: 1.0953 - val_accuracy: 0.7013\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1763 - accuracy: 0.6533 - val_loss: 1.1929 - val_accuracy: 0.6636\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1432 - accuracy: 0.6646 - val_loss: 1.1612 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1187 - accuracy: 0.6717 - val_loss: 1.1035 - val_accuracy: 0.6888\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1006 - accuracy: 0.6780 - val_loss: 1.0849 - val_accuracy: 0.6907\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0720 - accuracy: 0.6858 - val_loss: 1.1277 - val_accuracy: 0.6726\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0558 - accuracy: 0.6886 - val_loss: 1.0025 - val_accuracy: 0.7176\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0396 - accuracy: 0.6920 - val_loss: 0.9972 - val_accuracy: 0.7184\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0215 - accuracy: 0.6987 - val_loss: 0.9981 - val_accuracy: 0.7196\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0092 - accuracy: 0.6991 - val_loss: 1.0219 - val_accuracy: 0.7074\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9922 - accuracy: 0.7059 - val_loss: 0.9782 - val_accuracy: 0.7217\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9754 - accuracy: 0.7123 - val_loss: 1.0288 - val_accuracy: 0.7037\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9610 - accuracy: 0.7160 - val_loss: 0.9583 - val_accuracy: 0.7314\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9519 - accuracy: 0.7183 - val_loss: 0.9209 - val_accuracy: 0.7447\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9366 - accuracy: 0.7218 - val_loss: 0.9000 - val_accuracy: 0.7519\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9226 - accuracy: 0.7263 - val_loss: 0.9079 - val_accuracy: 0.7436\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9133 - accuracy: 0.7288 - val_loss: 1.0080 - val_accuracy: 0.7101\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9039 - accuracy: 0.7296 - val_loss: 0.9062 - val_accuracy: 0.7424\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8876 - accuracy: 0.7372 - val_loss: 0.8909 - val_accuracy: 0.7516\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8765 - accuracy: 0.7400 - val_loss: 0.9463 - val_accuracy: 0.7246\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8678 - accuracy: 0.7438 - val_loss: 0.8602 - val_accuracy: 0.7601\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8581 - accuracy: 0.7441 - val_loss: 0.8436 - val_accuracy: 0.7609\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8498 - accuracy: 0.7466 - val_loss: 0.8167 - val_accuracy: 0.7751\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8375 - accuracy: 0.7517 - val_loss: 0.8417 - val_accuracy: 0.7633\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8298 - accuracy: 0.7526 - val_loss: 0.8404 - val_accuracy: 0.7616\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8230 - accuracy: 0.7554 - val_loss: 0.7886 - val_accuracy: 0.7753\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8138 - accuracy: 0.7570 - val_loss: 0.7531 - val_accuracy: 0.7881\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8021 - accuracy: 0.7621 - val_loss: 0.7518 - val_accuracy: 0.7869\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7940 - accuracy: 0.7633 - val_loss: 0.8276 - val_accuracy: 0.7592\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7880 - accuracy: 0.7644 - val_loss: 0.8195 - val_accuracy: 0.7647\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7833 - accuracy: 0.7657 - val_loss: 0.8617 - val_accuracy: 0.7549\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.7695 - val_loss: 0.8408 - val_accuracy: 0.7565\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7658 - accuracy: 0.7704 - val_loss: 0.8107 - val_accuracy: 0.7654\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7563 - accuracy: 0.7750 - val_loss: 0.8290 - val_accuracy: 0.7607\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7572 - accuracy: 0.7733 - val_loss: 0.7301 - val_accuracy: 0.7965\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7806 - val_loss: 0.7860 - val_accuracy: 0.7753\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7364 - accuracy: 0.7813 - val_loss: 0.7815 - val_accuracy: 0.7775\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7349 - accuracy: 0.7809 - val_loss: 0.7796 - val_accuracy: 0.7784\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7280 - accuracy: 0.7832 - val_loss: 0.7684 - val_accuracy: 0.7819\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7169 - accuracy: 0.7884 - val_loss: 0.8043 - val_accuracy: 0.7712\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7138 - accuracy: 0.7881 - val_loss: 0.7347 - val_accuracy: 0.7931\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7062 - accuracy: 0.7915 - val_loss: 0.7589 - val_accuracy: 0.7851\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6991 - accuracy: 0.7928 - val_loss: 0.6734 - val_accuracy: 0.8086\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7006 - accuracy: 0.7903 - val_loss: 0.6783 - val_accuracy: 0.8125\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6933 - accuracy: 0.7959 - val_loss: 0.7044 - val_accuracy: 0.8013\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6847 - accuracy: 0.7970 - val_loss: 0.7734 - val_accuracy: 0.7771\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6852 - accuracy: 0.7965 - val_loss: 0.6665 - val_accuracy: 0.8115\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6749 - accuracy: 0.7998 - val_loss: 0.6945 - val_accuracy: 0.8026\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6714 - accuracy: 0.8011 - val_loss: 0.7227 - val_accuracy: 0.7948\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6692 - accuracy: 0.8024 - val_loss: 0.7512 - val_accuracy: 0.7859\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6625 - accuracy: 0.8042 - val_loss: 0.6955 - val_accuracy: 0.8037\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6556 - accuracy: 0.8064 - val_loss: 0.6712 - val_accuracy: 0.8109\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6529 - accuracy: 0.8062 - val_loss: 0.7103 - val_accuracy: 0.7981\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6476 - accuracy: 0.8085 - val_loss: 0.6739 - val_accuracy: 0.8111\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6491 - accuracy: 0.8065 - val_loss: 0.7235 - val_accuracy: 0.7927\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6417 - accuracy: 0.8097 - val_loss: 0.6712 - val_accuracy: 0.8077\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6337 - accuracy: 0.8135 - val_loss: 0.6812 - val_accuracy: 0.8076\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6316 - accuracy: 0.8109 - val_loss: 0.6368 - val_accuracy: 0.8228\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6295 - accuracy: 0.8141 - val_loss: 0.6218 - val_accuracy: 0.8246\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6230 - accuracy: 0.8139 - val_loss: 0.6669 - val_accuracy: 0.8118\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6159 - accuracy: 0.8174 - val_loss: 0.6728 - val_accuracy: 0.8117\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6158 - accuracy: 0.8168 - val_loss: 0.6772 - val_accuracy: 0.8109\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6136 - accuracy: 0.8180 - val_loss: 0.6351 - val_accuracy: 0.8251\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6089 - accuracy: 0.8207 - val_loss: 0.6387 - val_accuracy: 0.8211\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6000 - accuracy: 0.8235 - val_loss: 0.6321 - val_accuracy: 0.8226\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5981 - accuracy: 0.8238 - val_loss: 0.6405 - val_accuracy: 0.8219\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5975 - accuracy: 0.8244 - val_loss: 0.6614 - val_accuracy: 0.8126\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5922 - accuracy: 0.8252 - val_loss: 0.6533 - val_accuracy: 0.8143\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5939 - accuracy: 0.8228 - val_loss: 0.6766 - val_accuracy: 0.8101\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5853 - accuracy: 0.8280 - val_loss: 0.5895 - val_accuracy: 0.8353\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5861 - accuracy: 0.8265 - val_loss: 0.5997 - val_accuracy: 0.8294\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5796 - accuracy: 0.8281 - val_loss: 0.6296 - val_accuracy: 0.8216\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5755 - accuracy: 0.8297 - val_loss: 0.5765 - val_accuracy: 0.8414\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5762 - accuracy: 0.8287 - val_loss: 0.6266 - val_accuracy: 0.8251\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5699 - accuracy: 0.8319 - val_loss: 0.6059 - val_accuracy: 0.8294\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5675 - accuracy: 0.8308 - val_loss: 0.5889 - val_accuracy: 0.8318\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5670 - accuracy: 0.8303 - val_loss: 0.5807 - val_accuracy: 0.8391\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5579 - accuracy: 0.8332 - val_loss: 0.5822 - val_accuracy: 0.8374\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5580 - accuracy: 0.8361 - val_loss: 0.5802 - val_accuracy: 0.8399\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5526 - accuracy: 0.8370 - val_loss: 0.5922 - val_accuracy: 0.8339\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5540 - accuracy: 0.8353 - val_loss: 0.6127 - val_accuracy: 0.8265\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5554 - accuracy: 0.8347 - val_loss: 0.5685 - val_accuracy: 0.8409\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5483 - accuracy: 0.8370 - val_loss: 0.6748 - val_accuracy: 0.8051\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5447 - accuracy: 0.8380 - val_loss: 0.6133 - val_accuracy: 0.8270\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5385 - accuracy: 0.8404 - val_loss: 0.6877 - val_accuracy: 0.8070\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5353 - accuracy: 0.8414 - val_loss: 0.6892 - val_accuracy: 0.8013\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5357 - accuracy: 0.8410 - val_loss: 0.5722 - val_accuracy: 0.8394\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5324 - accuracy: 0.8410 - val_loss: 0.5964 - val_accuracy: 0.8353\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5268 - accuracy: 0.8439 - val_loss: 0.6052 - val_accuracy: 0.8286\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5304 - accuracy: 0.8426 - val_loss: 0.5414 - val_accuracy: 0.8506\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5289 - accuracy: 0.8413 - val_loss: 0.5992 - val_accuracy: 0.8332\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5222 - accuracy: 0.8449 - val_loss: 0.5872 - val_accuracy: 0.8311\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5198 - accuracy: 0.8447 - val_loss: 0.6056 - val_accuracy: 0.8297\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5207 - accuracy: 0.8447 - val_loss: 0.6303 - val_accuracy: 0.8197\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5123 - accuracy: 0.8492 - val_loss: 0.5867 - val_accuracy: 0.8382\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5129 - accuracy: 0.8478 - val_loss: 0.5973 - val_accuracy: 0.8321\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5061 - accuracy: 0.8504 - val_loss: 0.5428 - val_accuracy: 0.8464\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5091 - accuracy: 0.8485 - val_loss: 0.5173 - val_accuracy: 0.8526\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5049 - accuracy: 0.8507 - val_loss: 0.5611 - val_accuracy: 0.8447\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5028 - accuracy: 0.8503 - val_loss: 0.5159 - val_accuracy: 0.8564\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4985 - accuracy: 0.8519 - val_loss: 0.5491 - val_accuracy: 0.8460\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4948 - accuracy: 0.8534 - val_loss: 0.6230 - val_accuracy: 0.8259\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4928 - accuracy: 0.8535 - val_loss: 0.5866 - val_accuracy: 0.8317\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4920 - accuracy: 0.8537 - val_loss: 0.5238 - val_accuracy: 0.8544\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4902 - accuracy: 0.8550 - val_loss: 0.5339 - val_accuracy: 0.8514\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4901 - accuracy: 0.8551 - val_loss: 0.5572 - val_accuracy: 0.8416\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4872 - accuracy: 0.8551 - val_loss: 0.5353 - val_accuracy: 0.8519\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4838 - accuracy: 0.8563 - val_loss: 0.5851 - val_accuracy: 0.8353\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4784 - accuracy: 0.8580 - val_loss: 0.5912 - val_accuracy: 0.8333\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4793 - accuracy: 0.8575 - val_loss: 0.5706 - val_accuracy: 0.8411\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4800 - accuracy: 0.8571 - val_loss: 0.5660 - val_accuracy: 0.8369\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4733 - accuracy: 0.8612 - val_loss: 0.5057 - val_accuracy: 0.8566\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4693 - accuracy: 0.8615 - val_loss: 0.5959 - val_accuracy: 0.8306\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4698 - accuracy: 0.8618 - val_loss: 0.6337 - val_accuracy: 0.8219\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4757 - accuracy: 0.8586 - val_loss: 0.6390 - val_accuracy: 0.8124\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4647 - accuracy: 0.8627 - val_loss: 0.5172 - val_accuracy: 0.8591\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4648 - accuracy: 0.8619 - val_loss: 0.5461 - val_accuracy: 0.8452\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4611 - accuracy: 0.8633 - val_loss: 0.4924 - val_accuracy: 0.8633\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4560 - accuracy: 0.8653 - val_loss: 0.5332 - val_accuracy: 0.8483\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4577 - accuracy: 0.8642 - val_loss: 0.5401 - val_accuracy: 0.8481\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4533 - accuracy: 0.8649 - val_loss: 0.5217 - val_accuracy: 0.8549\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4546 - accuracy: 0.8656 - val_loss: 0.5081 - val_accuracy: 0.8586\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4514 - accuracy: 0.8651 - val_loss: 0.5093 - val_accuracy: 0.8586\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4555 - accuracy: 0.8636 - val_loss: 0.5089 - val_accuracy: 0.8594\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4499 - accuracy: 0.8668 - val_loss: 0.6514 - val_accuracy: 0.8127\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4515 - accuracy: 0.8656 - val_loss: 0.5463 - val_accuracy: 0.8474\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4404 - accuracy: 0.8699 - val_loss: 0.5325 - val_accuracy: 0.8506\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4412 - accuracy: 0.8685 - val_loss: 0.5474 - val_accuracy: 0.8464\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4407 - accuracy: 0.8695 - val_loss: 0.5245 - val_accuracy: 0.8523\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4363 - accuracy: 0.8708 - val_loss: 0.5056 - val_accuracy: 0.8586\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4324 - accuracy: 0.8714 - val_loss: 0.5677 - val_accuracy: 0.8400\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4357 - accuracy: 0.8715 - val_loss: 0.5034 - val_accuracy: 0.8596\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4306 - accuracy: 0.8726 - val_loss: 0.5301 - val_accuracy: 0.8531\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4300 - accuracy: 0.8721 - val_loss: 0.4540 - val_accuracy: 0.8716\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4316 - accuracy: 0.8725 - val_loss: 0.5502 - val_accuracy: 0.8440\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4251 - accuracy: 0.8740 - val_loss: 0.5640 - val_accuracy: 0.8402\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4268 - accuracy: 0.8736 - val_loss: 0.4896 - val_accuracy: 0.8613\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4233 - accuracy: 0.8755 - val_loss: 0.4913 - val_accuracy: 0.8630\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4212 - accuracy: 0.8753 - val_loss: 0.4508 - val_accuracy: 0.8739\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4213 - accuracy: 0.8745 - val_loss: 0.4890 - val_accuracy: 0.8589\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4153 - accuracy: 0.8770 - val_loss: 0.5219 - val_accuracy: 0.8534\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4171 - accuracy: 0.8756 - val_loss: 0.5010 - val_accuracy: 0.8581\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4143 - accuracy: 0.8771 - val_loss: 0.4990 - val_accuracy: 0.8594\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4155 - accuracy: 0.8767 - val_loss: 0.5433 - val_accuracy: 0.8456\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4136 - accuracy: 0.8783 - val_loss: 0.4981 - val_accuracy: 0.8623\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4076 - accuracy: 0.8800 - val_loss: 0.5323 - val_accuracy: 0.8482\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4053 - accuracy: 0.8805 - val_loss: 0.5255 - val_accuracy: 0.8545\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4038 - accuracy: 0.8796 - val_loss: 0.5582 - val_accuracy: 0.8405\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4016 - accuracy: 0.8814 - val_loss: 0.4777 - val_accuracy: 0.8638\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4046 - accuracy: 0.8793 - val_loss: 0.5633 - val_accuracy: 0.8375\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4003 - accuracy: 0.8813 - val_loss: 0.4921 - val_accuracy: 0.8624\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4041 - accuracy: 0.8792 - val_loss: 0.4794 - val_accuracy: 0.8661\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4035 - accuracy: 0.8808 - val_loss: 0.5242 - val_accuracy: 0.8507\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3984 - accuracy: 0.8819 - val_loss: 0.5356 - val_accuracy: 0.8429\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3933 - accuracy: 0.8842 - val_loss: 0.4997 - val_accuracy: 0.8621\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3858 - accuracy: 0.8853 - val_loss: 0.4846 - val_accuracy: 0.8608\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3906 - accuracy: 0.8841 - val_loss: 0.5351 - val_accuracy: 0.8480\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3926 - accuracy: 0.8840 - val_loss: 0.4878 - val_accuracy: 0.8646\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3882 - accuracy: 0.8848 - val_loss: 0.4480 - val_accuracy: 0.8758\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3908 - accuracy: 0.8834 - val_loss: 0.4515 - val_accuracy: 0.8751\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3851 - accuracy: 0.8852 - val_loss: 0.4689 - val_accuracy: 0.8694\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3831 - accuracy: 0.8859 - val_loss: 0.4952 - val_accuracy: 0.8618\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3798 - accuracy: 0.8876 - val_loss: 0.4813 - val_accuracy: 0.8653\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3823 - accuracy: 0.8862 - val_loss: 0.4563 - val_accuracy: 0.8745\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3879 - accuracy: 0.8849 - val_loss: 0.4324 - val_accuracy: 0.8794\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3786 - accuracy: 0.8867 - val_loss: 0.4695 - val_accuracy: 0.8723\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3740 - accuracy: 0.8892 - val_loss: 0.5079 - val_accuracy: 0.8590\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3699 - accuracy: 0.8904 - val_loss: 0.4589 - val_accuracy: 0.8746\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3697 - accuracy: 0.8920 - val_loss: 0.4686 - val_accuracy: 0.8674\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3716 - accuracy: 0.8898 - val_loss: 0.5415 - val_accuracy: 0.8476\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3755 - accuracy: 0.8883 - val_loss: 0.4579 - val_accuracy: 0.8714\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3679 - accuracy: 0.8909 - val_loss: 0.4640 - val_accuracy: 0.8726\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3648 - accuracy: 0.8931 - val_loss: 0.4671 - val_accuracy: 0.8693\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3638 - accuracy: 0.8913 - val_loss: 0.4827 - val_accuracy: 0.8646\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3640 - accuracy: 0.8918 - val_loss: 0.5160 - val_accuracy: 0.8584\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3582 - accuracy: 0.8937 - val_loss: 0.4954 - val_accuracy: 0.8637\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3602 - accuracy: 0.8934 - val_loss: 0.4557 - val_accuracy: 0.8738\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3566 - accuracy: 0.8950 - val_loss: 0.5685 - val_accuracy: 0.8399\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3546 - accuracy: 0.8951 - val_loss: 0.4924 - val_accuracy: 0.8634\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3575 - accuracy: 0.8940 - val_loss: 0.4176 - val_accuracy: 0.8836\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3529 - accuracy: 0.8956 - val_loss: 0.4542 - val_accuracy: 0.8773\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3529 - accuracy: 0.8960 - val_loss: 0.4369 - val_accuracy: 0.8796\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3489 - accuracy: 0.8964 - val_loss: 0.4402 - val_accuracy: 0.8780\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3480 - accuracy: 0.8962 - val_loss: 0.4415 - val_accuracy: 0.8820\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3504 - accuracy: 0.8959 - val_loss: 0.5168 - val_accuracy: 0.8546\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3516 - accuracy: 0.8940 - val_loss: 0.4692 - val_accuracy: 0.8695\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3472 - accuracy: 0.8962 - val_loss: 0.4814 - val_accuracy: 0.8670\n",
      "Try 2/100: Best_val_acc: [0.6670000553131104, 0.8220555782318115], lr: 5.032585663575004e-05, Lambda: 3.723855592309843e-05\n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3226 - accuracy: 0.1213 - val_loss: 2.3532 - val_accuracy: 0.0281\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2845 - accuracy: 0.1528 - val_loss: 2.3249 - val_accuracy: 0.0439\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2617 - accuracy: 0.1739 - val_loss: 2.2999 - val_accuracy: 0.0846\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2363 - accuracy: 0.2022 - val_loss: 2.2699 - val_accuracy: 0.1527\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2058 - accuracy: 0.2320 - val_loss: 2.2450 - val_accuracy: 0.1662\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1703 - accuracy: 0.2643 - val_loss: 2.2266 - val_accuracy: 0.1739\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1308 - accuracy: 0.3060 - val_loss: 2.1521 - val_accuracy: 0.2699\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0891 - accuracy: 0.3336 - val_loss: 2.0987 - val_accuracy: 0.3175\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0438 - accuracy: 0.3618 - val_loss: 2.0736 - val_accuracy: 0.3091\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9966 - accuracy: 0.3837 - val_loss: 2.0348 - val_accuracy: 0.3168\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9494 - accuracy: 0.4076 - val_loss: 1.9637 - val_accuracy: 0.3731\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9030 - accuracy: 0.4267 - val_loss: 1.9156 - val_accuracy: 0.4154\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8574 - accuracy: 0.4463 - val_loss: 1.8577 - val_accuracy: 0.4536\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8110 - accuracy: 0.4704 - val_loss: 1.8140 - val_accuracy: 0.4485\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7676 - accuracy: 0.4874 - val_loss: 1.7730 - val_accuracy: 0.4785\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7258 - accuracy: 0.5069 - val_loss: 1.7325 - val_accuracy: 0.5167\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6873 - accuracy: 0.5234 - val_loss: 1.7062 - val_accuracy: 0.5031\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6509 - accuracy: 0.5358 - val_loss: 1.6649 - val_accuracy: 0.5269\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6170 - accuracy: 0.5473 - val_loss: 1.6120 - val_accuracy: 0.5661\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5860 - accuracy: 0.5575 - val_loss: 1.5678 - val_accuracy: 0.5799\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5568 - accuracy: 0.5657 - val_loss: 1.5633 - val_accuracy: 0.5708\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5283 - accuracy: 0.5775 - val_loss: 1.5197 - val_accuracy: 0.5938\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5022 - accuracy: 0.5826 - val_loss: 1.5191 - val_accuracy: 0.5814\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4784 - accuracy: 0.5901 - val_loss: 1.4725 - val_accuracy: 0.5965\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4548 - accuracy: 0.5949 - val_loss: 1.4667 - val_accuracy: 0.6054\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4324 - accuracy: 0.6035 - val_loss: 1.4686 - val_accuracy: 0.5885\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4131 - accuracy: 0.6081 - val_loss: 1.4269 - val_accuracy: 0.6039\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3942 - accuracy: 0.6124 - val_loss: 1.4139 - val_accuracy: 0.6044\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3766 - accuracy: 0.6155 - val_loss: 1.4121 - val_accuracy: 0.6064\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3582 - accuracy: 0.6217 - val_loss: 1.3830 - val_accuracy: 0.6080\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3427 - accuracy: 0.6258 - val_loss: 1.3640 - val_accuracy: 0.6192\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3274 - accuracy: 0.6292 - val_loss: 1.3502 - val_accuracy: 0.6235\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3134 - accuracy: 0.6320 - val_loss: 1.3175 - val_accuracy: 0.6327\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2989 - accuracy: 0.6354 - val_loss: 1.3375 - val_accuracy: 0.6240\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2855 - accuracy: 0.6399 - val_loss: 1.3171 - val_accuracy: 0.6291\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2737 - accuracy: 0.6424 - val_loss: 1.3246 - val_accuracy: 0.6248\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2614 - accuracy: 0.6432 - val_loss: 1.3259 - val_accuracy: 0.6155\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2499 - accuracy: 0.6471 - val_loss: 1.3042 - val_accuracy: 0.6236\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2389 - accuracy: 0.6510 - val_loss: 1.2983 - val_accuracy: 0.6253\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2297 - accuracy: 0.6516 - val_loss: 1.2564 - val_accuracy: 0.6476\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2188 - accuracy: 0.6554 - val_loss: 1.2943 - val_accuracy: 0.6225\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2105 - accuracy: 0.6565 - val_loss: 1.2322 - val_accuracy: 0.6518\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2006 - accuracy: 0.6576 - val_loss: 1.2579 - val_accuracy: 0.6363\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1930 - accuracy: 0.6610 - val_loss: 1.2418 - val_accuracy: 0.6410\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1827 - accuracy: 0.6627 - val_loss: 1.2195 - val_accuracy: 0.6536\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1756 - accuracy: 0.6648 - val_loss: 1.2354 - val_accuracy: 0.6388\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1680 - accuracy: 0.6656 - val_loss: 1.2085 - val_accuracy: 0.6557\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1601 - accuracy: 0.6698 - val_loss: 1.2280 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1529 - accuracy: 0.6692 - val_loss: 1.2011 - val_accuracy: 0.6566\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1448 - accuracy: 0.6730 - val_loss: 1.1973 - val_accuracy: 0.6547\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1384 - accuracy: 0.6737 - val_loss: 1.1852 - val_accuracy: 0.6596\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1317 - accuracy: 0.6760 - val_loss: 1.1770 - val_accuracy: 0.6606\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1265 - accuracy: 0.6750 - val_loss: 1.1698 - val_accuracy: 0.6637\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1195 - accuracy: 0.6792 - val_loss: 1.1915 - val_accuracy: 0.6507\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1144 - accuracy: 0.6805 - val_loss: 1.1814 - val_accuracy: 0.6573\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1074 - accuracy: 0.6813 - val_loss: 1.1691 - val_accuracy: 0.6595\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1020 - accuracy: 0.6835 - val_loss: 1.1601 - val_accuracy: 0.6673\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0959 - accuracy: 0.6861 - val_loss: 1.1243 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0918 - accuracy: 0.6856 - val_loss: 1.1471 - val_accuracy: 0.6711\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0850 - accuracy: 0.6885 - val_loss: 1.1437 - val_accuracy: 0.6687\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0804 - accuracy: 0.6886 - val_loss: 1.1243 - val_accuracy: 0.6764\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0755 - accuracy: 0.6895 - val_loss: 1.1316 - val_accuracy: 0.6733\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0708 - accuracy: 0.6912 - val_loss: 1.1389 - val_accuracy: 0.6701\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0658 - accuracy: 0.6928 - val_loss: 1.1299 - val_accuracy: 0.6679\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0614 - accuracy: 0.6932 - val_loss: 1.1237 - val_accuracy: 0.6741\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0565 - accuracy: 0.6946 - val_loss: 1.1272 - val_accuracy: 0.6719\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0523 - accuracy: 0.6947 - val_loss: 1.0988 - val_accuracy: 0.6797\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0485 - accuracy: 0.6978 - val_loss: 1.0975 - val_accuracy: 0.6809\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0431 - accuracy: 0.6980 - val_loss: 1.0815 - val_accuracy: 0.6869\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0381 - accuracy: 0.6993 - val_loss: 1.0786 - val_accuracy: 0.6891\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0348 - accuracy: 0.6995 - val_loss: 1.0880 - val_accuracy: 0.6866\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0311 - accuracy: 0.7009 - val_loss: 1.0767 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0272 - accuracy: 0.7016 - val_loss: 1.0664 - val_accuracy: 0.6912\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0251 - accuracy: 0.7007 - val_loss: 1.0972 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0194 - accuracy: 0.7040 - val_loss: 1.0913 - val_accuracy: 0.6828\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0140 - accuracy: 0.7052 - val_loss: 1.0691 - val_accuracy: 0.6887\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0110 - accuracy: 0.7062 - val_loss: 1.0529 - val_accuracy: 0.6949\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0061 - accuracy: 0.7064 - val_loss: 1.0706 - val_accuracy: 0.6871\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0028 - accuracy: 0.7081 - val_loss: 1.0456 - val_accuracy: 0.7007\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9985 - accuracy: 0.7091 - val_loss: 1.0487 - val_accuracy: 0.6986\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9947 - accuracy: 0.7108 - val_loss: 1.0386 - val_accuracy: 0.7008\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9918 - accuracy: 0.7108 - val_loss: 1.0456 - val_accuracy: 0.6959\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9884 - accuracy: 0.7119 - val_loss: 1.0495 - val_accuracy: 0.6942\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9849 - accuracy: 0.7128 - val_loss: 1.0368 - val_accuracy: 0.7041\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9817 - accuracy: 0.7148 - val_loss: 1.0430 - val_accuracy: 0.6977\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9775 - accuracy: 0.7155 - val_loss: 1.0270 - val_accuracy: 0.7031\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9736 - accuracy: 0.7166 - val_loss: 1.0101 - val_accuracy: 0.7104\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9701 - accuracy: 0.7182 - val_loss: 1.0560 - val_accuracy: 0.6903\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9676 - accuracy: 0.7176 - val_loss: 1.0063 - val_accuracy: 0.7094\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9649 - accuracy: 0.7195 - val_loss: 1.0115 - val_accuracy: 0.7047\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9605 - accuracy: 0.7208 - val_loss: 0.9990 - val_accuracy: 0.7067\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9569 - accuracy: 0.7208 - val_loss: 1.0156 - val_accuracy: 0.7064\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9535 - accuracy: 0.7236 - val_loss: 1.0108 - val_accuracy: 0.7098\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9518 - accuracy: 0.7223 - val_loss: 1.0106 - val_accuracy: 0.7036\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9473 - accuracy: 0.7242 - val_loss: 0.9943 - val_accuracy: 0.7131\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9431 - accuracy: 0.7257 - val_loss: 1.0004 - val_accuracy: 0.7099\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9398 - accuracy: 0.7276 - val_loss: 1.0043 - val_accuracy: 0.7084\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9369 - accuracy: 0.7278 - val_loss: 1.0080 - val_accuracy: 0.7039\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9347 - accuracy: 0.7285 - val_loss: 1.0154 - val_accuracy: 0.6997\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9315 - accuracy: 0.7293 - val_loss: 0.9810 - val_accuracy: 0.7128\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9277 - accuracy: 0.7311 - val_loss: 0.9820 - val_accuracy: 0.7139\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9257 - accuracy: 0.7311 - val_loss: 1.0201 - val_accuracy: 0.7001\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9219 - accuracy: 0.7307 - val_loss: 0.9941 - val_accuracy: 0.7089\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9187 - accuracy: 0.7330 - val_loss: 0.9967 - val_accuracy: 0.7088\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9158 - accuracy: 0.7334 - val_loss: 0.9893 - val_accuracy: 0.7116\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9131 - accuracy: 0.7357 - val_loss: 0.9834 - val_accuracy: 0.7131\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9104 - accuracy: 0.7356 - val_loss: 0.9826 - val_accuracy: 0.7149\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9064 - accuracy: 0.7369 - val_loss: 0.9683 - val_accuracy: 0.7179\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9041 - accuracy: 0.7373 - val_loss: 0.9749 - val_accuracy: 0.7150\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9018 - accuracy: 0.7375 - val_loss: 0.9591 - val_accuracy: 0.7216\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9001 - accuracy: 0.7387 - val_loss: 0.9598 - val_accuracy: 0.7226\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8956 - accuracy: 0.7395 - val_loss: 0.9552 - val_accuracy: 0.7224\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8929 - accuracy: 0.7410 - val_loss: 0.9489 - val_accuracy: 0.7259\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8902 - accuracy: 0.7416 - val_loss: 0.9392 - val_accuracy: 0.7295\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8880 - accuracy: 0.7423 - val_loss: 0.9458 - val_accuracy: 0.7255\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8867 - accuracy: 0.7430 - val_loss: 0.9361 - val_accuracy: 0.7300\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8819 - accuracy: 0.7438 - val_loss: 0.9406 - val_accuracy: 0.7261\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8805 - accuracy: 0.7431 - val_loss: 0.9211 - val_accuracy: 0.7341\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8760 - accuracy: 0.7458 - val_loss: 0.9866 - val_accuracy: 0.7119\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8758 - accuracy: 0.7457 - val_loss: 0.9539 - val_accuracy: 0.7209\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8729 - accuracy: 0.7473 - val_loss: 0.9432 - val_accuracy: 0.7226\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8693 - accuracy: 0.7482 - val_loss: 0.9296 - val_accuracy: 0.7305\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8667 - accuracy: 0.7480 - val_loss: 0.9435 - val_accuracy: 0.7260\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8646 - accuracy: 0.7479 - val_loss: 0.8942 - val_accuracy: 0.7427\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8610 - accuracy: 0.7500 - val_loss: 0.9279 - val_accuracy: 0.7271\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8587 - accuracy: 0.7505 - val_loss: 0.9188 - val_accuracy: 0.7329\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8566 - accuracy: 0.7507 - val_loss: 0.9016 - val_accuracy: 0.7395\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8551 - accuracy: 0.7522 - val_loss: 0.9310 - val_accuracy: 0.7282\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8532 - accuracy: 0.7528 - val_loss: 0.8963 - val_accuracy: 0.7433\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8500 - accuracy: 0.7535 - val_loss: 0.9056 - val_accuracy: 0.7391\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8473 - accuracy: 0.7543 - val_loss: 0.9179 - val_accuracy: 0.7339\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8443 - accuracy: 0.7565 - val_loss: 0.9125 - val_accuracy: 0.7355\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8435 - accuracy: 0.7545 - val_loss: 0.8989 - val_accuracy: 0.7384\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8406 - accuracy: 0.7557 - val_loss: 0.8807 - val_accuracy: 0.7461\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8382 - accuracy: 0.7559 - val_loss: 0.8968 - val_accuracy: 0.7429\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8366 - accuracy: 0.7573 - val_loss: 0.8918 - val_accuracy: 0.7462\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8331 - accuracy: 0.7577 - val_loss: 0.8988 - val_accuracy: 0.7418\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8319 - accuracy: 0.7576 - val_loss: 0.8781 - val_accuracy: 0.7452\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8304 - accuracy: 0.7595 - val_loss: 0.8829 - val_accuracy: 0.7470\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8274 - accuracy: 0.7603 - val_loss: 0.8975 - val_accuracy: 0.7441\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8251 - accuracy: 0.7613 - val_loss: 0.8867 - val_accuracy: 0.7444\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8225 - accuracy: 0.7612 - val_loss: 0.8767 - val_accuracy: 0.7474\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8204 - accuracy: 0.7619 - val_loss: 0.8580 - val_accuracy: 0.7541\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8184 - accuracy: 0.7628 - val_loss: 0.8935 - val_accuracy: 0.7440\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8149 - accuracy: 0.7637 - val_loss: 0.8930 - val_accuracy: 0.7394\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8162 - accuracy: 0.7616 - val_loss: 0.8603 - val_accuracy: 0.7529\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8112 - accuracy: 0.7644 - val_loss: 0.8470 - val_accuracy: 0.7586\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8095 - accuracy: 0.7652 - val_loss: 0.8669 - val_accuracy: 0.7557\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8075 - accuracy: 0.7653 - val_loss: 0.8703 - val_accuracy: 0.7498\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8052 - accuracy: 0.7670 - val_loss: 0.8351 - val_accuracy: 0.7624\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8035 - accuracy: 0.7688 - val_loss: 0.8800 - val_accuracy: 0.7471\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8015 - accuracy: 0.7683 - val_loss: 0.8548 - val_accuracy: 0.7557\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7981 - accuracy: 0.7691 - val_loss: 0.8651 - val_accuracy: 0.7501\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7978 - accuracy: 0.7691 - val_loss: 0.8550 - val_accuracy: 0.7559\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7958 - accuracy: 0.7691 - val_loss: 0.8483 - val_accuracy: 0.7575\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7936 - accuracy: 0.7697 - val_loss: 0.8560 - val_accuracy: 0.7554\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7917 - accuracy: 0.7706 - val_loss: 0.8680 - val_accuracy: 0.7421\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7896 - accuracy: 0.7715 - val_loss: 0.8664 - val_accuracy: 0.7504\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7881 - accuracy: 0.7724 - val_loss: 0.8325 - val_accuracy: 0.7634\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7866 - accuracy: 0.7715 - val_loss: 0.8606 - val_accuracy: 0.7527\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7845 - accuracy: 0.7729 - val_loss: 0.8314 - val_accuracy: 0.7640\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7828 - accuracy: 0.7743 - val_loss: 0.8345 - val_accuracy: 0.7643\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7808 - accuracy: 0.7729 - val_loss: 0.8410 - val_accuracy: 0.7569\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7780 - accuracy: 0.7741 - val_loss: 0.8582 - val_accuracy: 0.7561\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7758 - accuracy: 0.7753 - val_loss: 0.8392 - val_accuracy: 0.7591\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7733 - accuracy: 0.7766 - val_loss: 0.8378 - val_accuracy: 0.7580\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7724 - accuracy: 0.7768 - val_loss: 0.8277 - val_accuracy: 0.7635\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7702 - accuracy: 0.7774 - val_loss: 0.8164 - val_accuracy: 0.7699\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7698 - accuracy: 0.7763 - val_loss: 0.8235 - val_accuracy: 0.7653\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7668 - accuracy: 0.7782 - val_loss: 0.8302 - val_accuracy: 0.7637\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.7773 - val_loss: 0.8626 - val_accuracy: 0.7559\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.7798 - val_loss: 0.8449 - val_accuracy: 0.7568\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7616 - accuracy: 0.7803 - val_loss: 0.8025 - val_accuracy: 0.7741\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7612 - accuracy: 0.7795 - val_loss: 0.8354 - val_accuracy: 0.7642\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7594 - accuracy: 0.7803 - val_loss: 0.8386 - val_accuracy: 0.7597\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7571 - accuracy: 0.7800 - val_loss: 0.8113 - val_accuracy: 0.7721\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7556 - accuracy: 0.7809 - val_loss: 0.8155 - val_accuracy: 0.7692\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7540 - accuracy: 0.7819 - val_loss: 0.8589 - val_accuracy: 0.7517\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7528 - accuracy: 0.7817 - val_loss: 0.8207 - val_accuracy: 0.7664\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7500 - accuracy: 0.7821 - val_loss: 0.8046 - val_accuracy: 0.7725\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7475 - accuracy: 0.7841 - val_loss: 0.8071 - val_accuracy: 0.7686\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7469 - accuracy: 0.7846 - val_loss: 0.8053 - val_accuracy: 0.7704\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7446 - accuracy: 0.7837 - val_loss: 0.8250 - val_accuracy: 0.7626\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7436 - accuracy: 0.7847 - val_loss: 0.7968 - val_accuracy: 0.7734\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7447 - accuracy: 0.7848 - val_loss: 0.7870 - val_accuracy: 0.7787\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7407 - accuracy: 0.7859 - val_loss: 0.7906 - val_accuracy: 0.7777\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7385 - accuracy: 0.7869 - val_loss: 0.7753 - val_accuracy: 0.7836\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7380 - accuracy: 0.7862 - val_loss: 0.8110 - val_accuracy: 0.7709\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7358 - accuracy: 0.7875 - val_loss: 0.7781 - val_accuracy: 0.7813\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7339 - accuracy: 0.7876 - val_loss: 0.7754 - val_accuracy: 0.7810\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7348 - accuracy: 0.7874 - val_loss: 0.7878 - val_accuracy: 0.7763\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7325 - accuracy: 0.7873 - val_loss: 0.7900 - val_accuracy: 0.7803\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7302 - accuracy: 0.7879 - val_loss: 0.8034 - val_accuracy: 0.7706\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7279 - accuracy: 0.7893 - val_loss: 0.7855 - val_accuracy: 0.7771\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7272 - accuracy: 0.7910 - val_loss: 0.7779 - val_accuracy: 0.7810\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7259 - accuracy: 0.7905 - val_loss: 0.7878 - val_accuracy: 0.7814\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7244 - accuracy: 0.7909 - val_loss: 0.7798 - val_accuracy: 0.7810\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7215 - accuracy: 0.7921 - val_loss: 0.7754 - val_accuracy: 0.7823\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7205 - accuracy: 0.7914 - val_loss: 0.7656 - val_accuracy: 0.7864\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7189 - accuracy: 0.7931 - val_loss: 0.7708 - val_accuracy: 0.7819\n",
      "Try 3/100: Best_val_acc: [0.8075202107429504, 0.7650555372238159], lr: 1.1696645739558355e-05, Lambda: 0.000499666067360816\n",
      "\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2648 - accuracy: 0.1552 - val_loss: 2.1141 - val_accuracy: 0.3263\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9513 - accuracy: 0.3171 - val_loss: 1.8309 - val_accuracy: 0.3263\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6518 - accuracy: 0.4477 - val_loss: 1.5331 - val_accuracy: 0.5289\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4630 - accuracy: 0.5348 - val_loss: 1.4842 - val_accuracy: 0.5206\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3493 - accuracy: 0.5779 - val_loss: 1.4409 - val_accuracy: 0.5474\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2482 - accuracy: 0.6169 - val_loss: 1.2350 - val_accuracy: 0.6314\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1821 - accuracy: 0.6374 - val_loss: 1.2905 - val_accuracy: 0.6152\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1298 - accuracy: 0.6512 - val_loss: 1.1027 - val_accuracy: 0.6724\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0749 - accuracy: 0.6728 - val_loss: 1.1929 - val_accuracy: 0.6340\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0318 - accuracy: 0.6862 - val_loss: 1.1078 - val_accuracy: 0.6646\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9939 - accuracy: 0.6964 - val_loss: 1.0338 - val_accuracy: 0.6910\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9577 - accuracy: 0.7099 - val_loss: 1.0341 - val_accuracy: 0.6833\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9328 - accuracy: 0.7158 - val_loss: 0.9929 - val_accuracy: 0.7091\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8992 - accuracy: 0.7263 - val_loss: 0.9929 - val_accuracy: 0.7031\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8678 - accuracy: 0.7362 - val_loss: 0.8650 - val_accuracy: 0.7480\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8627 - accuracy: 0.7358 - val_loss: 0.8207 - val_accuracy: 0.7596\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8162 - accuracy: 0.7542 - val_loss: 0.8111 - val_accuracy: 0.7613\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8054 - accuracy: 0.7556 - val_loss: 0.8671 - val_accuracy: 0.7377\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7846 - accuracy: 0.7632 - val_loss: 0.8118 - val_accuracy: 0.7672\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7683 - accuracy: 0.7677 - val_loss: 0.7741 - val_accuracy: 0.7770\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7498 - accuracy: 0.7733 - val_loss: 0.8713 - val_accuracy: 0.7354\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7407 - accuracy: 0.7765 - val_loss: 0.9198 - val_accuracy: 0.7242\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7231 - accuracy: 0.7803 - val_loss: 0.7529 - val_accuracy: 0.7779\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7135 - accuracy: 0.7822 - val_loss: 0.7150 - val_accuracy: 0.7941\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6904 - accuracy: 0.7923 - val_loss: 0.7593 - val_accuracy: 0.7781\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6799 - accuracy: 0.7945 - val_loss: 0.6651 - val_accuracy: 0.8089\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6757 - accuracy: 0.7959 - val_loss: 0.7325 - val_accuracy: 0.7826\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6827 - accuracy: 0.7935 - val_loss: 0.7697 - val_accuracy: 0.7796\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6470 - accuracy: 0.8046 - val_loss: 0.7290 - val_accuracy: 0.7888\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6458 - accuracy: 0.8032 - val_loss: 0.6188 - val_accuracy: 0.8218\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6241 - accuracy: 0.8124 - val_loss: 0.7434 - val_accuracy: 0.7777\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6198 - accuracy: 0.8116 - val_loss: 0.6048 - val_accuracy: 0.8266\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6081 - accuracy: 0.8153 - val_loss: 0.6289 - val_accuracy: 0.8186\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6024 - accuracy: 0.8167 - val_loss: 0.7474 - val_accuracy: 0.7774\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5866 - accuracy: 0.8221 - val_loss: 0.6391 - val_accuracy: 0.8147\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5827 - accuracy: 0.8230 - val_loss: 0.6925 - val_accuracy: 0.7976\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5692 - accuracy: 0.8262 - val_loss: 0.5512 - val_accuracy: 0.8454\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5687 - accuracy: 0.8265 - val_loss: 0.6479 - val_accuracy: 0.8119\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5643 - accuracy: 0.8277 - val_loss: 0.6958 - val_accuracy: 0.7976\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5486 - accuracy: 0.8334 - val_loss: 0.6354 - val_accuracy: 0.8112\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5495 - accuracy: 0.8327 - val_loss: 0.5697 - val_accuracy: 0.8316\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5360 - accuracy: 0.8375 - val_loss: 0.5471 - val_accuracy: 0.8429\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5361 - accuracy: 0.8356 - val_loss: 0.5686 - val_accuracy: 0.8335\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5254 - accuracy: 0.8394 - val_loss: 0.5992 - val_accuracy: 0.8244\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5168 - accuracy: 0.8429 - val_loss: 0.6836 - val_accuracy: 0.7986\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5046 - accuracy: 0.8455 - val_loss: 0.6334 - val_accuracy: 0.8166\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5039 - accuracy: 0.8443 - val_loss: 0.5894 - val_accuracy: 0.8306\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4920 - accuracy: 0.8489 - val_loss: 0.5414 - val_accuracy: 0.8394\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4864 - accuracy: 0.8508 - val_loss: 0.5444 - val_accuracy: 0.8420\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4787 - accuracy: 0.8538 - val_loss: 0.6197 - val_accuracy: 0.8210\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4746 - accuracy: 0.8539 - val_loss: 0.5685 - val_accuracy: 0.8349\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4789 - accuracy: 0.8529 - val_loss: 0.5878 - val_accuracy: 0.8298\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4571 - accuracy: 0.8605 - val_loss: 0.6577 - val_accuracy: 0.8053\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4628 - accuracy: 0.8588 - val_loss: 0.5039 - val_accuracy: 0.8578\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4455 - accuracy: 0.8635 - val_loss: 0.4543 - val_accuracy: 0.8659\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4498 - accuracy: 0.8623 - val_loss: 0.5629 - val_accuracy: 0.8334\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4338 - accuracy: 0.8675 - val_loss: 0.6420 - val_accuracy: 0.8111\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4382 - accuracy: 0.8657 - val_loss: 0.5272 - val_accuracy: 0.8444\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4280 - accuracy: 0.8695 - val_loss: 0.5459 - val_accuracy: 0.8344\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4268 - accuracy: 0.8710 - val_loss: 0.4670 - val_accuracy: 0.8661\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4243 - accuracy: 0.8701 - val_loss: 0.4396 - val_accuracy: 0.8699\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4156 - accuracy: 0.8733 - val_loss: 0.5372 - val_accuracy: 0.8444\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4054 - accuracy: 0.8766 - val_loss: 0.4734 - val_accuracy: 0.8644\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4113 - accuracy: 0.8745 - val_loss: 0.5279 - val_accuracy: 0.8445\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3941 - accuracy: 0.8798 - val_loss: 0.5019 - val_accuracy: 0.8536\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3955 - accuracy: 0.8790 - val_loss: 0.4762 - val_accuracy: 0.8617\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3883 - accuracy: 0.8817 - val_loss: 0.4453 - val_accuracy: 0.8713\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3823 - accuracy: 0.8840 - val_loss: 0.4950 - val_accuracy: 0.8566\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3817 - accuracy: 0.8841 - val_loss: 0.4283 - val_accuracy: 0.8779\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3808 - accuracy: 0.8833 - val_loss: 0.4586 - val_accuracy: 0.8691\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3744 - accuracy: 0.8843 - val_loss: 0.4691 - val_accuracy: 0.8653\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3777 - accuracy: 0.8833 - val_loss: 0.4801 - val_accuracy: 0.8629\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3664 - accuracy: 0.8880 - val_loss: 0.4624 - val_accuracy: 0.8684\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3602 - accuracy: 0.8899 - val_loss: 0.4598 - val_accuracy: 0.8694\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3571 - accuracy: 0.8907 - val_loss: 0.4369 - val_accuracy: 0.8768\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3517 - accuracy: 0.8919 - val_loss: 0.4726 - val_accuracy: 0.8656\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3431 - accuracy: 0.8950 - val_loss: 0.4653 - val_accuracy: 0.8639\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3526 - accuracy: 0.8918 - val_loss: 0.4882 - val_accuracy: 0.8599\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3430 - accuracy: 0.8958 - val_loss: 0.4995 - val_accuracy: 0.8524\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3439 - accuracy: 0.8953 - val_loss: 0.4223 - val_accuracy: 0.8814\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3328 - accuracy: 0.8979 - val_loss: 0.4439 - val_accuracy: 0.8736\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3258 - accuracy: 0.9007 - val_loss: 0.4325 - val_accuracy: 0.8766\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3269 - accuracy: 0.9004 - val_loss: 0.4387 - val_accuracy: 0.8779\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3171 - accuracy: 0.9046 - val_loss: 0.3788 - val_accuracy: 0.8938\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3192 - accuracy: 0.9031 - val_loss: 0.4398 - val_accuracy: 0.8762\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3170 - accuracy: 0.9041 - val_loss: 0.4532 - val_accuracy: 0.8717\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3172 - accuracy: 0.9036 - val_loss: 0.4528 - val_accuracy: 0.8714\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3037 - accuracy: 0.9086 - val_loss: 0.4108 - val_accuracy: 0.8864\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2978 - accuracy: 0.9089 - val_loss: 0.4285 - val_accuracy: 0.8802\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2952 - accuracy: 0.9098 - val_loss: 0.3928 - val_accuracy: 0.8929\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2903 - accuracy: 0.9134 - val_loss: 0.4159 - val_accuracy: 0.8854\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2925 - accuracy: 0.9100 - val_loss: 0.4181 - val_accuracy: 0.8824\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2933 - accuracy: 0.9111 - val_loss: 0.4607 - val_accuracy: 0.8668\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2870 - accuracy: 0.9123 - val_loss: 0.4168 - val_accuracy: 0.8846\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2874 - accuracy: 0.9133 - val_loss: 0.4653 - val_accuracy: 0.8659\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2786 - accuracy: 0.9163 - val_loss: 0.5073 - val_accuracy: 0.8548\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2882 - accuracy: 0.9111 - val_loss: 0.4135 - val_accuracy: 0.8859\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2867 - accuracy: 0.9113 - val_loss: 0.4009 - val_accuracy: 0.8901\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2691 - accuracy: 0.9190 - val_loss: 0.4294 - val_accuracy: 0.8786\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2721 - accuracy: 0.9173 - val_loss: 0.4033 - val_accuracy: 0.8904\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2613 - accuracy: 0.9211 - val_loss: 0.3866 - val_accuracy: 0.8939\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2642 - accuracy: 0.9194 - val_loss: 0.4408 - val_accuracy: 0.8748\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2567 - accuracy: 0.9220 - val_loss: 0.3597 - val_accuracy: 0.9029\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2569 - accuracy: 0.9222 - val_loss: 0.3804 - val_accuracy: 0.8976\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2551 - accuracy: 0.9238 - val_loss: 0.4105 - val_accuracy: 0.8876\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2509 - accuracy: 0.9237 - val_loss: 0.4434 - val_accuracy: 0.8759\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2570 - accuracy: 0.9216 - val_loss: 0.3799 - val_accuracy: 0.8949\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2464 - accuracy: 0.9251 - val_loss: 0.3597 - val_accuracy: 0.9027\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2397 - accuracy: 0.9271 - val_loss: 0.3635 - val_accuracy: 0.9015\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2385 - accuracy: 0.9281 - val_loss: 0.3967 - val_accuracy: 0.8891\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2450 - accuracy: 0.9243 - val_loss: 0.3946 - val_accuracy: 0.8914\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2371 - accuracy: 0.9270 - val_loss: 0.3567 - val_accuracy: 0.9034\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2310 - accuracy: 0.9309 - val_loss: 0.4870 - val_accuracy: 0.8659\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2376 - accuracy: 0.9264 - val_loss: 0.3645 - val_accuracy: 0.9022\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2307 - accuracy: 0.9299 - val_loss: 0.3612 - val_accuracy: 0.9014\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2292 - accuracy: 0.9300 - val_loss: 0.4131 - val_accuracy: 0.8919\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2180 - accuracy: 0.9335 - val_loss: 0.4117 - val_accuracy: 0.8847\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2218 - accuracy: 0.9327 - val_loss: 0.3698 - val_accuracy: 0.9030\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2364 - accuracy: 0.9267 - val_loss: 0.3516 - val_accuracy: 0.9092\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2117 - accuracy: 0.9360 - val_loss: 0.4068 - val_accuracy: 0.8910\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2116 - accuracy: 0.9359 - val_loss: 0.4347 - val_accuracy: 0.8841\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2161 - accuracy: 0.9339 - val_loss: 0.4522 - val_accuracy: 0.8776\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2208 - accuracy: 0.9325 - val_loss: 0.3976 - val_accuracy: 0.8936\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2119 - accuracy: 0.9345 - val_loss: 0.3534 - val_accuracy: 0.9081\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1991 - accuracy: 0.9398 - val_loss: 0.3791 - val_accuracy: 0.8986\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2026 - accuracy: 0.9374 - val_loss: 0.3932 - val_accuracy: 0.8957\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9405 - val_loss: 0.3810 - val_accuracy: 0.8995\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2045 - accuracy: 0.9360 - val_loss: 0.3968 - val_accuracy: 0.8956\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1999 - accuracy: 0.9394 - val_loss: 0.3776 - val_accuracy: 0.9000\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2001 - accuracy: 0.9383 - val_loss: 0.4024 - val_accuracy: 0.8917\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1963 - accuracy: 0.9410 - val_loss: 0.3953 - val_accuracy: 0.8962\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1911 - accuracy: 0.9425 - val_loss: 0.3490 - val_accuracy: 0.9105\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1856 - accuracy: 0.9433 - val_loss: 0.3638 - val_accuracy: 0.9081\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1739 - accuracy: 0.9476 - val_loss: 0.3799 - val_accuracy: 0.9035\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1843 - accuracy: 0.9439 - val_loss: 0.4140 - val_accuracy: 0.8945\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1799 - accuracy: 0.9460 - val_loss: 0.3279 - val_accuracy: 0.9181\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1759 - accuracy: 0.9475 - val_loss: 0.3402 - val_accuracy: 0.9137\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1702 - accuracy: 0.9490 - val_loss: 0.3835 - val_accuracy: 0.9045\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1746 - accuracy: 0.9472 - val_loss: 0.3768 - val_accuracy: 0.9052\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1682 - accuracy: 0.9484 - val_loss: 0.3689 - val_accuracy: 0.9058\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1653 - accuracy: 0.9500 - val_loss: 0.3444 - val_accuracy: 0.9148\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1677 - accuracy: 0.9480 - val_loss: 0.3294 - val_accuracy: 0.9156\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1732 - accuracy: 0.9470 - val_loss: 0.3781 - val_accuracy: 0.9034\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1572 - accuracy: 0.9526 - val_loss: 0.3568 - val_accuracy: 0.9124\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1587 - accuracy: 0.9511 - val_loss: 0.3243 - val_accuracy: 0.9208\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1703 - accuracy: 0.9471 - val_loss: 0.3983 - val_accuracy: 0.9011\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1635 - accuracy: 0.9497 - val_loss: 0.3446 - val_accuracy: 0.9164\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1651 - accuracy: 0.9488 - val_loss: 0.3687 - val_accuracy: 0.9056\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1518 - accuracy: 0.9551 - val_loss: 0.3681 - val_accuracy: 0.9111\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1531 - accuracy: 0.9535 - val_loss: 0.3742 - val_accuracy: 0.9099\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1558 - accuracy: 0.9532 - val_loss: 0.3458 - val_accuracy: 0.9156\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1479 - accuracy: 0.9545 - val_loss: 0.3416 - val_accuracy: 0.9174\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1543 - accuracy: 0.9522 - val_loss: 0.3518 - val_accuracy: 0.9144\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1403 - accuracy: 0.9579 - val_loss: 0.3456 - val_accuracy: 0.9161\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1396 - accuracy: 0.9576 - val_loss: 0.3621 - val_accuracy: 0.9124\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1419 - accuracy: 0.9563 - val_loss: 0.3126 - val_accuracy: 0.9279\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1390 - accuracy: 0.9584 - val_loss: 0.3701 - val_accuracy: 0.9121\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1533 - accuracy: 0.9525 - val_loss: 0.3719 - val_accuracy: 0.9102\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1453 - accuracy: 0.9559 - val_loss: 0.3426 - val_accuracy: 0.9180\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1419 - accuracy: 0.9570 - val_loss: 0.3255 - val_accuracy: 0.9242\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1299 - accuracy: 0.9618 - val_loss: 0.3291 - val_accuracy: 0.9244\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1442 - accuracy: 0.9554 - val_loss: 0.2928 - val_accuracy: 0.9334\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1285 - accuracy: 0.9614 - val_loss: 0.3300 - val_accuracy: 0.9223\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1306 - accuracy: 0.9606 - val_loss: 0.3407 - val_accuracy: 0.9172\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1207 - accuracy: 0.9642 - val_loss: 0.3422 - val_accuracy: 0.9219\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1284 - accuracy: 0.9610 - val_loss: 0.3680 - val_accuracy: 0.9147\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1211 - accuracy: 0.9635 - val_loss: 0.3563 - val_accuracy: 0.9161\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1308 - accuracy: 0.9607 - val_loss: 0.3228 - val_accuracy: 0.9291\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1297 - accuracy: 0.9594 - val_loss: 0.3367 - val_accuracy: 0.9253\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1218 - accuracy: 0.9627 - val_loss: 0.3569 - val_accuracy: 0.9178\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1148 - accuracy: 0.9660 - val_loss: 0.3039 - val_accuracy: 0.9349\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9632 - val_loss: 0.3815 - val_accuracy: 0.9124\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1161 - accuracy: 0.9653 - val_loss: 0.3529 - val_accuracy: 0.9184\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1175 - accuracy: 0.9646 - val_loss: 0.3590 - val_accuracy: 0.9206\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1102 - accuracy: 0.9674 - val_loss: 0.3706 - val_accuracy: 0.9161\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1092 - accuracy: 0.9662 - val_loss: 0.3449 - val_accuracy: 0.9227\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1144 - accuracy: 0.9652 - val_loss: 0.3930 - val_accuracy: 0.9052\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1253 - accuracy: 0.9606 - val_loss: 0.3422 - val_accuracy: 0.9233\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1167 - accuracy: 0.9643 - val_loss: 0.3453 - val_accuracy: 0.9249\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1062 - accuracy: 0.9677 - val_loss: 0.3395 - val_accuracy: 0.9270\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0967 - accuracy: 0.9717 - val_loss: 0.3336 - val_accuracy: 0.9267\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0991 - accuracy: 0.9713 - val_loss: 0.3455 - val_accuracy: 0.9283\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1063 - accuracy: 0.9684 - val_loss: 0.4028 - val_accuracy: 0.9049\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1097 - accuracy: 0.9667 - val_loss: 0.4203 - val_accuracy: 0.9073\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1125 - accuracy: 0.9642 - val_loss: 0.3965 - val_accuracy: 0.9153\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1081 - accuracy: 0.9670 - val_loss: 0.3707 - val_accuracy: 0.9173\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0926 - accuracy: 0.9725 - val_loss: 0.3453 - val_accuracy: 0.9261\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1016 - accuracy: 0.9694 - val_loss: 0.3230 - val_accuracy: 0.9350\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0975 - accuracy: 0.9708 - val_loss: 0.3623 - val_accuracy: 0.9187\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0902 - accuracy: 0.9730 - val_loss: 0.3864 - val_accuracy: 0.9204\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0917 - accuracy: 0.9727 - val_loss: 0.3785 - val_accuracy: 0.9235\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0980 - accuracy: 0.9699 - val_loss: 0.3147 - val_accuracy: 0.9365\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0996 - accuracy: 0.9686 - val_loss: 0.3529 - val_accuracy: 0.9255\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0895 - accuracy: 0.9735 - val_loss: 0.3566 - val_accuracy: 0.9274\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0898 - accuracy: 0.9735 - val_loss: 0.4230 - val_accuracy: 0.9084\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1024 - accuracy: 0.9678 - val_loss: 0.3549 - val_accuracy: 0.9251\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1006 - accuracy: 0.9685 - val_loss: 0.3551 - val_accuracy: 0.9273\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0922 - accuracy: 0.9716 - val_loss: 0.3475 - val_accuracy: 0.9293\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0854 - accuracy: 0.9744 - val_loss: 0.3644 - val_accuracy: 0.9242\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0921 - accuracy: 0.9722 - val_loss: 0.3443 - val_accuracy: 0.9309\n",
      "Try 4/100: Best_val_acc: [0.946337878704071, 0.8328889012336731], lr: 0.0002090231133835751, Lambda: 1.5882779128382753e-05\n",
      "\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3012 - accuracy: 0.1186 - val_loss: 2.3057 - val_accuracy: 0.1044\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0609 - accuracy: 0.2515 - val_loss: 1.8447 - val_accuracy: 0.4019\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6145 - accuracy: 0.4552 - val_loss: 1.4189 - val_accuracy: 0.5479\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3683 - accuracy: 0.5565 - val_loss: 1.3701 - val_accuracy: 0.5682\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2688 - accuracy: 0.5914 - val_loss: 1.1552 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1546 - accuracy: 0.6385 - val_loss: 1.1135 - val_accuracy: 0.6745\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1106 - accuracy: 0.6527 - val_loss: 1.2490 - val_accuracy: 0.6016\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0263 - accuracy: 0.6827 - val_loss: 0.9793 - val_accuracy: 0.7129\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9604 - accuracy: 0.7063 - val_loss: 1.0278 - val_accuracy: 0.6927\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9294 - accuracy: 0.7160 - val_loss: 0.9612 - val_accuracy: 0.7265\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8913 - accuracy: 0.7269 - val_loss: 0.8516 - val_accuracy: 0.7539\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8513 - accuracy: 0.7386 - val_loss: 0.8584 - val_accuracy: 0.7489\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8231 - accuracy: 0.7467 - val_loss: 0.9017 - val_accuracy: 0.7337\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7838 - accuracy: 0.7616 - val_loss: 0.7795 - val_accuracy: 0.7761\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7719 - accuracy: 0.7641 - val_loss: 0.7402 - val_accuracy: 0.7823\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7706 - val_loss: 0.7857 - val_accuracy: 0.7666\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7134 - accuracy: 0.7808 - val_loss: 0.7194 - val_accuracy: 0.7901\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6917 - accuracy: 0.7878 - val_loss: 0.7502 - val_accuracy: 0.7821\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6698 - accuracy: 0.7938 - val_loss: 0.7339 - val_accuracy: 0.7848\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6707 - accuracy: 0.7936 - val_loss: 0.6570 - val_accuracy: 0.8035\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6484 - accuracy: 0.8002 - val_loss: 0.6297 - val_accuracy: 0.8162\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6285 - accuracy: 0.8085 - val_loss: 0.6696 - val_accuracy: 0.8051\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6114 - accuracy: 0.8131 - val_loss: 0.6959 - val_accuracy: 0.7992\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6094 - accuracy: 0.8138 - val_loss: 0.7823 - val_accuracy: 0.7656\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5883 - accuracy: 0.8195 - val_loss: 0.6402 - val_accuracy: 0.8146\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5775 - accuracy: 0.8230 - val_loss: 0.5910 - val_accuracy: 0.8302\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5575 - accuracy: 0.8297 - val_loss: 0.6787 - val_accuracy: 0.7998\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5428 - accuracy: 0.8336 - val_loss: 0.6409 - val_accuracy: 0.8152\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5457 - accuracy: 0.8311 - val_loss: 0.5651 - val_accuracy: 0.8339\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5260 - accuracy: 0.8411 - val_loss: 0.6017 - val_accuracy: 0.8271\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5166 - accuracy: 0.8398 - val_loss: 0.5921 - val_accuracy: 0.8256\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5136 - accuracy: 0.8415 - val_loss: 0.5784 - val_accuracy: 0.8304\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5028 - accuracy: 0.8464 - val_loss: 0.6457 - val_accuracy: 0.8121\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4881 - accuracy: 0.8496 - val_loss: 0.5532 - val_accuracy: 0.8384\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4839 - accuracy: 0.8504 - val_loss: 0.5178 - val_accuracy: 0.8492\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4775 - accuracy: 0.8529 - val_loss: 0.5427 - val_accuracy: 0.8449\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4618 - accuracy: 0.8574 - val_loss: 0.5652 - val_accuracy: 0.8345\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4564 - accuracy: 0.8590 - val_loss: 0.5210 - val_accuracy: 0.8501\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4418 - accuracy: 0.8656 - val_loss: 0.6411 - val_accuracy: 0.8125\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4512 - accuracy: 0.8596 - val_loss: 0.5232 - val_accuracy: 0.8508\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4430 - accuracy: 0.8629 - val_loss: 0.5245 - val_accuracy: 0.8469\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4292 - accuracy: 0.8660 - val_loss: 0.5545 - val_accuracy: 0.8373\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4258 - accuracy: 0.8674 - val_loss: 0.5479 - val_accuracy: 0.8426\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4030 - accuracy: 0.8745 - val_loss: 0.5465 - val_accuracy: 0.8393\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3943 - accuracy: 0.8784 - val_loss: 0.5261 - val_accuracy: 0.8465\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3992 - accuracy: 0.8767 - val_loss: 0.5708 - val_accuracy: 0.8324\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3853 - accuracy: 0.8802 - val_loss: 0.4877 - val_accuracy: 0.8608\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3766 - accuracy: 0.8820 - val_loss: 0.5144 - val_accuracy: 0.8540\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3717 - accuracy: 0.8855 - val_loss: 0.4933 - val_accuracy: 0.8614\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3811 - accuracy: 0.8816 - val_loss: 0.4723 - val_accuracy: 0.8626\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3541 - accuracy: 0.8903 - val_loss: 0.4257 - val_accuracy: 0.8785\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3623 - accuracy: 0.8866 - val_loss: 0.4937 - val_accuracy: 0.8571\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3560 - accuracy: 0.8888 - val_loss: 0.4419 - val_accuracy: 0.8759\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3607 - accuracy: 0.8860 - val_loss: 0.4729 - val_accuracy: 0.8628\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3259 - accuracy: 0.9004 - val_loss: 0.4778 - val_accuracy: 0.8661\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3578 - accuracy: 0.8872 - val_loss: 0.4591 - val_accuracy: 0.8710\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3347 - accuracy: 0.8954 - val_loss: 0.4631 - val_accuracy: 0.8660\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3399 - accuracy: 0.8928 - val_loss: 0.4769 - val_accuracy: 0.8672\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3177 - accuracy: 0.9017 - val_loss: 0.4895 - val_accuracy: 0.8582\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3048 - accuracy: 0.9068 - val_loss: 0.4704 - val_accuracy: 0.8663\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3091 - accuracy: 0.9049 - val_loss: 0.3931 - val_accuracy: 0.8891\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3140 - accuracy: 0.9019 - val_loss: 0.4006 - val_accuracy: 0.8834\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2942 - accuracy: 0.9084 - val_loss: 0.4159 - val_accuracy: 0.8843\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2946 - accuracy: 0.9075 - val_loss: 0.4500 - val_accuracy: 0.8757\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2929 - accuracy: 0.9077 - val_loss: 0.3899 - val_accuracy: 0.8921\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2705 - accuracy: 0.9153 - val_loss: 0.3704 - val_accuracy: 0.9001\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2749 - accuracy: 0.9137 - val_loss: 0.3676 - val_accuracy: 0.9010\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2700 - accuracy: 0.9159 - val_loss: 0.4653 - val_accuracy: 0.8676\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2808 - accuracy: 0.9130 - val_loss: 0.4275 - val_accuracy: 0.8788\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2691 - accuracy: 0.9164 - val_loss: 0.4070 - val_accuracy: 0.8869\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2704 - accuracy: 0.9147 - val_loss: 0.4385 - val_accuracy: 0.8796\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2670 - accuracy: 0.9162 - val_loss: 0.4481 - val_accuracy: 0.8759\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2475 - accuracy: 0.9229 - val_loss: 0.4359 - val_accuracy: 0.8818\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2506 - accuracy: 0.9203 - val_loss: 0.4139 - val_accuracy: 0.8904\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2330 - accuracy: 0.9275 - val_loss: 0.3979 - val_accuracy: 0.8919\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2483 - accuracy: 0.9212 - val_loss: 0.4803 - val_accuracy: 0.8629\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2382 - accuracy: 0.9244 - val_loss: 0.3664 - val_accuracy: 0.9046\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2479 - accuracy: 0.9223 - val_loss: 0.3997 - val_accuracy: 0.8924\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2321 - accuracy: 0.9264 - val_loss: 0.3915 - val_accuracy: 0.8965\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2186 - accuracy: 0.9321 - val_loss: 0.4420 - val_accuracy: 0.8799\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2355 - accuracy: 0.9247 - val_loss: 0.3797 - val_accuracy: 0.9011\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2394 - accuracy: 0.9244 - val_loss: 0.4032 - val_accuracy: 0.8911\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2188 - accuracy: 0.9318 - val_loss: 0.3809 - val_accuracy: 0.8997\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2056 - accuracy: 0.9367 - val_loss: 0.3919 - val_accuracy: 0.9013\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2151 - accuracy: 0.9327 - val_loss: 0.4945 - val_accuracy: 0.8654\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2185 - accuracy: 0.9315 - val_loss: 0.3828 - val_accuracy: 0.9011\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2090 - accuracy: 0.9343 - val_loss: 0.3734 - val_accuracy: 0.9081\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1961 - accuracy: 0.9387 - val_loss: 0.3951 - val_accuracy: 0.9029\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1981 - accuracy: 0.9375 - val_loss: 0.4073 - val_accuracy: 0.8977\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1956 - accuracy: 0.9383 - val_loss: 0.3559 - val_accuracy: 0.9127\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1850 - accuracy: 0.9423 - val_loss: 0.3978 - val_accuracy: 0.9041\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1878 - accuracy: 0.9394 - val_loss: 0.3666 - val_accuracy: 0.9117\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1912 - accuracy: 0.9388 - val_loss: 0.4397 - val_accuracy: 0.8910\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1757 - accuracy: 0.9447 - val_loss: 0.4255 - val_accuracy: 0.8946\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9374 - val_loss: 0.3587 - val_accuracy: 0.9159\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1836 - accuracy: 0.9421 - val_loss: 0.3737 - val_accuracy: 0.9124\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1850 - accuracy: 0.9413 - val_loss: 0.3411 - val_accuracy: 0.9161\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1778 - accuracy: 0.9451 - val_loss: 0.4407 - val_accuracy: 0.8918\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1572 - accuracy: 0.9509 - val_loss: 0.3880 - val_accuracy: 0.9030\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1694 - accuracy: 0.9459 - val_loss: 0.4270 - val_accuracy: 0.8951\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1727 - accuracy: 0.9444 - val_loss: 0.3704 - val_accuracy: 0.9143\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1663 - accuracy: 0.9481 - val_loss: 0.4981 - val_accuracy: 0.8743\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1806 - accuracy: 0.9425 - val_loss: 0.3706 - val_accuracy: 0.9129\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1591 - accuracy: 0.9498 - val_loss: 0.4152 - val_accuracy: 0.9040\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1731 - accuracy: 0.9465 - val_loss: 0.3638 - val_accuracy: 0.9142\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1612 - accuracy: 0.9475 - val_loss: 0.4106 - val_accuracy: 0.9044\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1559 - accuracy: 0.9510 - val_loss: 0.4476 - val_accuracy: 0.8879\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1642 - accuracy: 0.9473 - val_loss: 0.3843 - val_accuracy: 0.9124\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1342 - accuracy: 0.9589 - val_loss: 0.3929 - val_accuracy: 0.9101\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1497 - accuracy: 0.9530 - val_loss: 0.3708 - val_accuracy: 0.9182\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1465 - accuracy: 0.9536 - val_loss: 0.4985 - val_accuracy: 0.8806\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1761 - accuracy: 0.9436 - val_loss: 0.4511 - val_accuracy: 0.8926\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1357 - accuracy: 0.9575 - val_loss: 0.4347 - val_accuracy: 0.8994\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1418 - accuracy: 0.9553 - val_loss: 0.3674 - val_accuracy: 0.9200\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1410 - accuracy: 0.9547 - val_loss: 0.3946 - val_accuracy: 0.9144\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1371 - accuracy: 0.9562 - val_loss: 0.4103 - val_accuracy: 0.9089\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1381 - accuracy: 0.9557 - val_loss: 0.3941 - val_accuracy: 0.9140\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1525 - accuracy: 0.9498 - val_loss: 0.4390 - val_accuracy: 0.9025\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1330 - accuracy: 0.9573 - val_loss: 0.4521 - val_accuracy: 0.9002\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1330 - accuracy: 0.9581 - val_loss: 0.3888 - val_accuracy: 0.9166\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1310 - accuracy: 0.9590 - val_loss: 0.3719 - val_accuracy: 0.9224\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1256 - accuracy: 0.9602 - val_loss: 0.3969 - val_accuracy: 0.9144\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1222 - accuracy: 0.9618 - val_loss: 0.3490 - val_accuracy: 0.9294\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1245 - accuracy: 0.9598 - val_loss: 0.4102 - val_accuracy: 0.9169\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1226 - accuracy: 0.9605 - val_loss: 0.3782 - val_accuracy: 0.9214\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1156 - accuracy: 0.9639 - val_loss: 0.3822 - val_accuracy: 0.9213\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1292 - accuracy: 0.9586 - val_loss: 0.3799 - val_accuracy: 0.9244\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1221 - accuracy: 0.9610 - val_loss: 0.3689 - val_accuracy: 0.9224\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1099 - accuracy: 0.9651 - val_loss: 0.3368 - val_accuracy: 0.9331\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1154 - accuracy: 0.9625 - val_loss: 0.5120 - val_accuracy: 0.8874\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1405 - accuracy: 0.9547 - val_loss: 0.4423 - val_accuracy: 0.8992\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1227 - accuracy: 0.9600 - val_loss: 0.4635 - val_accuracy: 0.8991\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1250 - accuracy: 0.9596 - val_loss: 0.4224 - val_accuracy: 0.9157\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1269 - accuracy: 0.9584 - val_loss: 0.3945 - val_accuracy: 0.9226\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0914 - accuracy: 0.9717 - val_loss: 0.4330 - val_accuracy: 0.9149\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1160 - accuracy: 0.9630 - val_loss: 0.3959 - val_accuracy: 0.9266\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1114 - accuracy: 0.9642 - val_loss: 0.4342 - val_accuracy: 0.9127\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0944 - accuracy: 0.9711 - val_loss: 0.4174 - val_accuracy: 0.9152\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1050 - accuracy: 0.9679 - val_loss: 0.4945 - val_accuracy: 0.8983\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1404 - accuracy: 0.9556 - val_loss: 0.4615 - val_accuracy: 0.9084\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1171 - accuracy: 0.9623 - val_loss: 0.4688 - val_accuracy: 0.9030\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1195 - accuracy: 0.9616 - val_loss: 0.4327 - val_accuracy: 0.9114\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1012 - accuracy: 0.9671 - val_loss: 0.4058 - val_accuracy: 0.9254\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0984 - accuracy: 0.9695 - val_loss: 0.3777 - val_accuracy: 0.9341\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1135 - accuracy: 0.9639 - val_loss: 0.3865 - val_accuracy: 0.9260\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0861 - accuracy: 0.9731 - val_loss: 0.4157 - val_accuracy: 0.9243\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0767 - accuracy: 0.9775 - val_loss: 0.3803 - val_accuracy: 0.9302\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0956 - accuracy: 0.9694 - val_loss: 0.4592 - val_accuracy: 0.9134\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0939 - accuracy: 0.9707 - val_loss: 0.4133 - val_accuracy: 0.9237\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0880 - accuracy: 0.9727 - val_loss: 0.4415 - val_accuracy: 0.9184\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0976 - accuracy: 0.9676 - val_loss: 0.3957 - val_accuracy: 0.9288\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1435 - accuracy: 0.9524 - val_loss: 0.5297 - val_accuracy: 0.8846\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0984 - accuracy: 0.9686 - val_loss: 0.4157 - val_accuracy: 0.9288\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0849 - accuracy: 0.9744 - val_loss: 0.4645 - val_accuracy: 0.9159\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0806 - accuracy: 0.9759 - val_loss: 0.3936 - val_accuracy: 0.9259\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1081 - accuracy: 0.9654 - val_loss: 0.3997 - val_accuracy: 0.9286\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1112 - accuracy: 0.9641 - val_loss: 0.4125 - val_accuracy: 0.9253\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0836 - accuracy: 0.9735 - val_loss: 0.3841 - val_accuracy: 0.9332\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0819 - accuracy: 0.9740 - val_loss: 0.4599 - val_accuracy: 0.9161\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0848 - accuracy: 0.9730 - val_loss: 0.4089 - val_accuracy: 0.9313\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1052 - accuracy: 0.9669 - val_loss: 0.4273 - val_accuracy: 0.9204\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1104 - accuracy: 0.9646 - val_loss: 0.4894 - val_accuracy: 0.9037\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0917 - accuracy: 0.9698 - val_loss: 0.4574 - val_accuracy: 0.9189\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0746 - accuracy: 0.9765 - val_loss: 0.4491 - val_accuracy: 0.9234\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0989 - accuracy: 0.9676 - val_loss: 0.4029 - val_accuracy: 0.9347\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0594 - accuracy: 0.9822 - val_loss: 0.5150 - val_accuracy: 0.9146\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0776 - accuracy: 0.9752 - val_loss: 0.4396 - val_accuracy: 0.9239\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0775 - accuracy: 0.9764 - val_loss: 0.3794 - val_accuracy: 0.9417\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0755 - accuracy: 0.9763 - val_loss: 0.4594 - val_accuracy: 0.9178\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0976 - accuracy: 0.9681 - val_loss: 0.3991 - val_accuracy: 0.9335\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0754 - accuracy: 0.9760 - val_loss: 0.4290 - val_accuracy: 0.9324\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1054 - accuracy: 0.9667 - val_loss: 0.4466 - val_accuracy: 0.9223\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0780 - accuracy: 0.9757 - val_loss: 0.4522 - val_accuracy: 0.9263\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 0.4205 - val_accuracy: 0.9265\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0889 - accuracy: 0.9708 - val_loss: 0.4241 - val_accuracy: 0.9318\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0642 - accuracy: 0.9808 - val_loss: 0.3969 - val_accuracy: 0.9399\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0656 - accuracy: 0.9802 - val_loss: 0.4659 - val_accuracy: 0.9279\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0489 - accuracy: 0.9859 - val_loss: 0.4263 - val_accuracy: 0.9340\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0711 - accuracy: 0.9770 - val_loss: 0.4832 - val_accuracy: 0.9134\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0868 - accuracy: 0.9724 - val_loss: 0.5065 - val_accuracy: 0.9114\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1331 - accuracy: 0.9572 - val_loss: 0.4729 - val_accuracy: 0.9107\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0939 - accuracy: 0.9698 - val_loss: 0.4425 - val_accuracy: 0.9281\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0849 - accuracy: 0.9729 - val_loss: 0.4873 - val_accuracy: 0.9192\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0781 - accuracy: 0.9749 - val_loss: 0.4580 - val_accuracy: 0.9284\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0784 - accuracy: 0.9749 - val_loss: 0.4394 - val_accuracy: 0.9279\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0664 - accuracy: 0.9797 - val_loss: 0.4885 - val_accuracy: 0.9197\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0506 - accuracy: 0.9842 - val_loss: 0.4703 - val_accuracy: 0.9279\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0556 - accuracy: 0.9828 - val_loss: 0.4011 - val_accuracy: 0.9437\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0656 - accuracy: 0.9800 - val_loss: 0.4487 - val_accuracy: 0.9329\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0734 - accuracy: 0.9765 - val_loss: 0.4927 - val_accuracy: 0.9172\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0923 - accuracy: 0.9703 - val_loss: 0.4298 - val_accuracy: 0.9367\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0516 - accuracy: 0.9848 - val_loss: 0.4863 - val_accuracy: 0.9277\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0608 - accuracy: 0.9813 - val_loss: 0.4881 - val_accuracy: 0.9237\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0809 - accuracy: 0.9739 - val_loss: 0.4417 - val_accuracy: 0.9309\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0639 - accuracy: 0.9795 - val_loss: 0.4356 - val_accuracy: 0.9344\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0559 - accuracy: 0.9829 - val_loss: 0.4316 - val_accuracy: 0.9372\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0840 - accuracy: 0.9739 - val_loss: 0.5450 - val_accuracy: 0.9086\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1431 - accuracy: 0.9549 - val_loss: 0.5201 - val_accuracy: 0.9046\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1213 - accuracy: 0.9620 - val_loss: 0.4400 - val_accuracy: 0.9304\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.4702 - val_accuracy: 0.9325\n",
      "Try 5/100: Best_val_acc: [1.206650733947754, 0.8359444737434387], lr: 0.0006478035487346598, Lambda: 0.00016886052270763873\n",
      "\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2993 - accuracy: 0.1269 - val_loss: 2.2526 - val_accuracy: 0.2442\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2512 - accuracy: 0.1817 - val_loss: 2.1664 - val_accuracy: 0.3299\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1755 - accuracy: 0.2376 - val_loss: 2.0621 - val_accuracy: 0.3339\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0694 - accuracy: 0.3074 - val_loss: 1.9524 - val_accuracy: 0.4229\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9362 - accuracy: 0.3856 - val_loss: 1.8058 - val_accuracy: 0.5061\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8097 - accuracy: 0.4424 - val_loss: 1.7112 - val_accuracy: 0.5399\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6952 - accuracy: 0.4927 - val_loss: 1.6739 - val_accuracy: 0.5241\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5971 - accuracy: 0.5294 - val_loss: 1.5650 - val_accuracy: 0.5586\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5108 - accuracy: 0.5621 - val_loss: 1.4708 - val_accuracy: 0.6024\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4441 - accuracy: 0.5838 - val_loss: 1.4860 - val_accuracy: 0.5714\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3852 - accuracy: 0.6035 - val_loss: 1.3695 - val_accuracy: 0.6327\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3341 - accuracy: 0.6216 - val_loss: 1.3412 - val_accuracy: 0.6304\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2964 - accuracy: 0.6295 - val_loss: 1.2868 - val_accuracy: 0.6483\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2543 - accuracy: 0.6415 - val_loss: 1.3227 - val_accuracy: 0.6198\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2243 - accuracy: 0.6494 - val_loss: 1.2335 - val_accuracy: 0.6526\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1951 - accuracy: 0.6567 - val_loss: 1.2279 - val_accuracy: 0.6608\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1685 - accuracy: 0.6633 - val_loss: 1.2115 - val_accuracy: 0.6572\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1440 - accuracy: 0.6700 - val_loss: 1.1850 - val_accuracy: 0.6612\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1221 - accuracy: 0.6760 - val_loss: 1.1569 - val_accuracy: 0.6774\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1023 - accuracy: 0.6812 - val_loss: 1.1347 - val_accuracy: 0.6819\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0828 - accuracy: 0.6866 - val_loss: 1.1299 - val_accuracy: 0.6813\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0651 - accuracy: 0.6914 - val_loss: 1.1344 - val_accuracy: 0.6742\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0502 - accuracy: 0.6959 - val_loss: 1.0785 - val_accuracy: 0.6947\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0373 - accuracy: 0.6969 - val_loss: 1.0468 - val_accuracy: 0.7089\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0228 - accuracy: 0.7030 - val_loss: 1.0045 - val_accuracy: 0.7227\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0091 - accuracy: 0.7061 - val_loss: 1.0627 - val_accuracy: 0.6986\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9926 - accuracy: 0.7117 - val_loss: 0.9985 - val_accuracy: 0.7238\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9883 - accuracy: 0.7108 - val_loss: 1.0079 - val_accuracy: 0.7160\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9690 - accuracy: 0.7160 - val_loss: 1.0208 - val_accuracy: 0.7096\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9611 - accuracy: 0.7170 - val_loss: 0.9582 - val_accuracy: 0.7316\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9504 - accuracy: 0.7221 - val_loss: 0.9480 - val_accuracy: 0.7343\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9407 - accuracy: 0.7245 - val_loss: 0.9957 - val_accuracy: 0.7175\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9293 - accuracy: 0.7276 - val_loss: 0.9684 - val_accuracy: 0.7241\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9180 - accuracy: 0.7300 - val_loss: 0.9731 - val_accuracy: 0.7225\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9114 - accuracy: 0.7325 - val_loss: 0.9805 - val_accuracy: 0.7211\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9003 - accuracy: 0.7353 - val_loss: 0.9288 - val_accuracy: 0.7377\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8951 - accuracy: 0.7349 - val_loss: 0.9337 - val_accuracy: 0.7334\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8847 - accuracy: 0.7389 - val_loss: 0.9119 - val_accuracy: 0.7444\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8757 - accuracy: 0.7429 - val_loss: 0.9879 - val_accuracy: 0.7147\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8726 - accuracy: 0.7422 - val_loss: 0.9267 - val_accuracy: 0.7369\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8611 - accuracy: 0.7460 - val_loss: 0.9130 - val_accuracy: 0.7351\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8530 - accuracy: 0.7480 - val_loss: 0.8914 - val_accuracy: 0.7437\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8453 - accuracy: 0.7499 - val_loss: 0.8549 - val_accuracy: 0.7570\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8406 - accuracy: 0.7515 - val_loss: 0.9054 - val_accuracy: 0.7379\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8325 - accuracy: 0.7542 - val_loss: 0.9683 - val_accuracy: 0.7196\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8277 - accuracy: 0.7543 - val_loss: 0.8907 - val_accuracy: 0.7457\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8209 - accuracy: 0.7555 - val_loss: 0.8396 - val_accuracy: 0.7651\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8126 - accuracy: 0.7596 - val_loss: 0.8521 - val_accuracy: 0.7564\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8075 - accuracy: 0.7609 - val_loss: 0.9207 - val_accuracy: 0.7288\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7990 - accuracy: 0.7638 - val_loss: 0.8507 - val_accuracy: 0.7586\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7942 - accuracy: 0.7650 - val_loss: 0.8076 - val_accuracy: 0.7761\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7911 - accuracy: 0.7656 - val_loss: 0.8333 - val_accuracy: 0.7643\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7861 - accuracy: 0.7670 - val_loss: 0.8382 - val_accuracy: 0.7641\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7795 - accuracy: 0.7684 - val_loss: 0.8126 - val_accuracy: 0.7729\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7723 - accuracy: 0.7718 - val_loss: 0.8406 - val_accuracy: 0.7576\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7626 - accuracy: 0.7741 - val_loss: 0.8112 - val_accuracy: 0.7727\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7614 - accuracy: 0.7733 - val_loss: 0.7990 - val_accuracy: 0.7714\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7553 - accuracy: 0.7763 - val_loss: 0.8301 - val_accuracy: 0.7658\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7546 - accuracy: 0.7762 - val_loss: 0.8071 - val_accuracy: 0.7739\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7461 - accuracy: 0.7792 - val_loss: 0.7823 - val_accuracy: 0.7786\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7394 - accuracy: 0.7805 - val_loss: 0.8364 - val_accuracy: 0.7578\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7357 - accuracy: 0.7820 - val_loss: 0.8074 - val_accuracy: 0.7713\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7300 - accuracy: 0.7851 - val_loss: 0.7290 - val_accuracy: 0.7979\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7307 - accuracy: 0.7840 - val_loss: 0.7394 - val_accuracy: 0.7955\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7220 - accuracy: 0.7869 - val_loss: 0.7841 - val_accuracy: 0.7802\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7171 - accuracy: 0.7892 - val_loss: 0.7420 - val_accuracy: 0.7948\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7108 - accuracy: 0.7881 - val_loss: 0.7398 - val_accuracy: 0.7935\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7107 - accuracy: 0.7885 - val_loss: 0.8053 - val_accuracy: 0.7662\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7027 - accuracy: 0.7914 - val_loss: 0.8281 - val_accuracy: 0.7642\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6966 - accuracy: 0.7949 - val_loss: 0.7578 - val_accuracy: 0.7863\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6938 - accuracy: 0.7952 - val_loss: 0.7302 - val_accuracy: 0.7962\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6869 - accuracy: 0.7969 - val_loss: 0.7606 - val_accuracy: 0.7826\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6863 - accuracy: 0.7982 - val_loss: 0.7190 - val_accuracy: 0.8017\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6814 - accuracy: 0.7991 - val_loss: 0.7629 - val_accuracy: 0.7873\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6782 - accuracy: 0.8009 - val_loss: 0.7380 - val_accuracy: 0.7874\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6733 - accuracy: 0.8025 - val_loss: 0.7465 - val_accuracy: 0.7892\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6660 - accuracy: 0.8034 - val_loss: 0.7488 - val_accuracy: 0.7861\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6653 - accuracy: 0.8044 - val_loss: 0.7721 - val_accuracy: 0.7815\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6588 - accuracy: 0.8066 - val_loss: 0.7054 - val_accuracy: 0.8014\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6559 - accuracy: 0.8075 - val_loss: 0.6894 - val_accuracy: 0.8077\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6558 - accuracy: 0.8070 - val_loss: 0.7352 - val_accuracy: 0.7899\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6507 - accuracy: 0.8079 - val_loss: 0.7738 - val_accuracy: 0.7761\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6480 - accuracy: 0.8091 - val_loss: 0.6653 - val_accuracy: 0.8144\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6406 - accuracy: 0.8109 - val_loss: 0.6826 - val_accuracy: 0.8116\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6442 - accuracy: 0.8100 - val_loss: 0.6847 - val_accuracy: 0.8084\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6411 - accuracy: 0.8111 - val_loss: 0.6879 - val_accuracy: 0.8080\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6331 - accuracy: 0.8122 - val_loss: 0.7077 - val_accuracy: 0.7941\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6277 - accuracy: 0.8145 - val_loss: 0.7100 - val_accuracy: 0.7990\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6249 - accuracy: 0.8170 - val_loss: 0.7049 - val_accuracy: 0.7991\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6220 - accuracy: 0.8162 - val_loss: 0.6947 - val_accuracy: 0.8023\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6216 - accuracy: 0.8168 - val_loss: 0.7007 - val_accuracy: 0.7972\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6187 - accuracy: 0.8176 - val_loss: 0.7360 - val_accuracy: 0.7911\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6127 - accuracy: 0.8184 - val_loss: 0.7330 - val_accuracy: 0.7909\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6114 - accuracy: 0.8195 - val_loss: 0.6940 - val_accuracy: 0.8027\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6051 - accuracy: 0.8210 - val_loss: 0.6097 - val_accuracy: 0.8317\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6035 - accuracy: 0.8220 - val_loss: 0.6873 - val_accuracy: 0.8059\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6012 - accuracy: 0.8221 - val_loss: 0.7128 - val_accuracy: 0.7941\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5963 - accuracy: 0.8246 - val_loss: 0.7137 - val_accuracy: 0.7974\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5913 - accuracy: 0.8249 - val_loss: 0.6174 - val_accuracy: 0.8280\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5936 - accuracy: 0.8245 - val_loss: 0.6750 - val_accuracy: 0.8069\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5899 - accuracy: 0.8250 - val_loss: 0.6788 - val_accuracy: 0.8069\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5861 - accuracy: 0.8257 - val_loss: 0.6104 - val_accuracy: 0.8304\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5833 - accuracy: 0.8270 - val_loss: 0.7319 - val_accuracy: 0.7881\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5844 - accuracy: 0.8275 - val_loss: 0.6718 - val_accuracy: 0.8093\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5798 - accuracy: 0.8280 - val_loss: 0.6690 - val_accuracy: 0.8101\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5740 - accuracy: 0.8302 - val_loss: 0.7184 - val_accuracy: 0.7881\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5687 - accuracy: 0.8322 - val_loss: 0.7441 - val_accuracy: 0.7854\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5700 - accuracy: 0.8316 - val_loss: 0.6439 - val_accuracy: 0.8214\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5641 - accuracy: 0.8331 - val_loss: 0.6551 - val_accuracy: 0.8176\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5667 - accuracy: 0.8324 - val_loss: 0.5823 - val_accuracy: 0.8392\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5645 - accuracy: 0.8332 - val_loss: 0.6818 - val_accuracy: 0.8044\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5587 - accuracy: 0.8346 - val_loss: 0.6114 - val_accuracy: 0.8254\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5600 - accuracy: 0.8341 - val_loss: 0.6387 - val_accuracy: 0.8224\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5520 - accuracy: 0.8374 - val_loss: 0.6598 - val_accuracy: 0.8124\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5508 - accuracy: 0.8364 - val_loss: 0.7340 - val_accuracy: 0.7844\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5493 - accuracy: 0.8381 - val_loss: 0.5969 - val_accuracy: 0.8343\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5463 - accuracy: 0.8380 - val_loss: 0.6415 - val_accuracy: 0.8180\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5436 - accuracy: 0.8399 - val_loss: 0.5911 - val_accuracy: 0.8360\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5420 - accuracy: 0.8396 - val_loss: 0.5912 - val_accuracy: 0.8344\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5372 - accuracy: 0.8402 - val_loss: 0.6023 - val_accuracy: 0.8316\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5365 - accuracy: 0.8412 - val_loss: 0.6137 - val_accuracy: 0.8268\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5309 - accuracy: 0.8418 - val_loss: 0.6222 - val_accuracy: 0.8232\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5345 - accuracy: 0.8408 - val_loss: 0.6258 - val_accuracy: 0.8260\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5316 - accuracy: 0.8430 - val_loss: 0.6049 - val_accuracy: 0.8314\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5253 - accuracy: 0.8444 - val_loss: 0.5850 - val_accuracy: 0.8321\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5261 - accuracy: 0.8443 - val_loss: 0.5573 - val_accuracy: 0.8461\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5194 - accuracy: 0.8462 - val_loss: 0.5623 - val_accuracy: 0.8463\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5207 - accuracy: 0.8449 - val_loss: 0.5401 - val_accuracy: 0.8469\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5191 - accuracy: 0.8463 - val_loss: 0.5748 - val_accuracy: 0.8418\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5178 - accuracy: 0.8469 - val_loss: 0.6211 - val_accuracy: 0.8265\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5133 - accuracy: 0.8473 - val_loss: 0.6209 - val_accuracy: 0.8228\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5090 - accuracy: 0.8488 - val_loss: 0.5852 - val_accuracy: 0.8359\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5107 - accuracy: 0.8482 - val_loss: 0.5745 - val_accuracy: 0.8389\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5086 - accuracy: 0.8491 - val_loss: 0.6218 - val_accuracy: 0.8205\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5071 - accuracy: 0.8511 - val_loss: 0.5626 - val_accuracy: 0.8430\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5037 - accuracy: 0.8510 - val_loss: 0.5903 - val_accuracy: 0.8330\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4985 - accuracy: 0.8518 - val_loss: 0.5631 - val_accuracy: 0.8419\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4977 - accuracy: 0.8529 - val_loss: 0.5817 - val_accuracy: 0.8325\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4970 - accuracy: 0.8523 - val_loss: 0.5334 - val_accuracy: 0.8502\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4931 - accuracy: 0.8539 - val_loss: 0.5515 - val_accuracy: 0.8461\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4979 - accuracy: 0.8527 - val_loss: 0.5950 - val_accuracy: 0.8319\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4873 - accuracy: 0.8556 - val_loss: 0.5591 - val_accuracy: 0.8426\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4874 - accuracy: 0.8546 - val_loss: 0.5928 - val_accuracy: 0.8321\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4854 - accuracy: 0.8561 - val_loss: 0.5717 - val_accuracy: 0.8371\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4857 - accuracy: 0.8571 - val_loss: 0.5639 - val_accuracy: 0.8419\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4837 - accuracy: 0.8554 - val_loss: 0.5487 - val_accuracy: 0.8453\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4780 - accuracy: 0.8576 - val_loss: 0.6146 - val_accuracy: 0.8257\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4818 - accuracy: 0.8567 - val_loss: 0.6029 - val_accuracy: 0.8294\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4775 - accuracy: 0.8567 - val_loss: 0.6129 - val_accuracy: 0.8256\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4764 - accuracy: 0.8582 - val_loss: 0.5791 - val_accuracy: 0.8366\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4732 - accuracy: 0.8589 - val_loss: 0.5629 - val_accuracy: 0.8402\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4711 - accuracy: 0.8614 - val_loss: 0.5920 - val_accuracy: 0.8280\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4733 - accuracy: 0.8587 - val_loss: 0.5844 - val_accuracy: 0.8351\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4697 - accuracy: 0.8618 - val_loss: 0.5425 - val_accuracy: 0.8495\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4648 - accuracy: 0.8633 - val_loss: 0.5012 - val_accuracy: 0.8604\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4660 - accuracy: 0.8605 - val_loss: 0.5696 - val_accuracy: 0.8376\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4597 - accuracy: 0.8649 - val_loss: 0.5431 - val_accuracy: 0.8481\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4587 - accuracy: 0.8644 - val_loss: 0.5586 - val_accuracy: 0.8440\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4579 - accuracy: 0.8641 - val_loss: 0.5311 - val_accuracy: 0.8502\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4578 - accuracy: 0.8628 - val_loss: 0.5553 - val_accuracy: 0.8454\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4548 - accuracy: 0.8661 - val_loss: 0.5374 - val_accuracy: 0.8481\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4530 - accuracy: 0.8660 - val_loss: 0.5515 - val_accuracy: 0.8429\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4518 - accuracy: 0.8664 - val_loss: 0.5122 - val_accuracy: 0.8573\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4558 - accuracy: 0.8645 - val_loss: 0.5862 - val_accuracy: 0.8308\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4516 - accuracy: 0.8661 - val_loss: 0.5364 - val_accuracy: 0.8511\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4430 - accuracy: 0.8699 - val_loss: 0.5150 - val_accuracy: 0.8561\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4434 - accuracy: 0.8685 - val_loss: 0.5196 - val_accuracy: 0.8548\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4418 - accuracy: 0.8695 - val_loss: 0.5268 - val_accuracy: 0.8531\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4416 - accuracy: 0.8690 - val_loss: 0.5451 - val_accuracy: 0.8452\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4435 - accuracy: 0.8674 - val_loss: 0.5170 - val_accuracy: 0.8536\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4376 - accuracy: 0.8703 - val_loss: 0.5660 - val_accuracy: 0.8414\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4370 - accuracy: 0.8692 - val_loss: 0.5668 - val_accuracy: 0.8388\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4362 - accuracy: 0.8701 - val_loss: 0.5474 - val_accuracy: 0.8443\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4325 - accuracy: 0.8717 - val_loss: 0.4815 - val_accuracy: 0.8671\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4297 - accuracy: 0.8725 - val_loss: 0.5349 - val_accuracy: 0.8475\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4275 - accuracy: 0.8738 - val_loss: 0.5137 - val_accuracy: 0.8536\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4330 - accuracy: 0.8702 - val_loss: 0.5004 - val_accuracy: 0.8602\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4276 - accuracy: 0.8739 - val_loss: 0.4831 - val_accuracy: 0.8666\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4225 - accuracy: 0.8768 - val_loss: 0.5116 - val_accuracy: 0.8556\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4220 - accuracy: 0.8745 - val_loss: 0.5494 - val_accuracy: 0.8424\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4316 - accuracy: 0.8722 - val_loss: 0.4809 - val_accuracy: 0.8644\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4214 - accuracy: 0.8763 - val_loss: 0.5167 - val_accuracy: 0.8556\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4212 - accuracy: 0.8751 - val_loss: 0.4746 - val_accuracy: 0.8682\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4171 - accuracy: 0.8763 - val_loss: 0.4931 - val_accuracy: 0.8622\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4166 - accuracy: 0.8771 - val_loss: 0.5718 - val_accuracy: 0.8346\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4204 - accuracy: 0.8748 - val_loss: 0.5427 - val_accuracy: 0.8432\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4130 - accuracy: 0.8790 - val_loss: 0.5497 - val_accuracy: 0.8465\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4089 - accuracy: 0.8791 - val_loss: 0.5062 - val_accuracy: 0.8581\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4090 - accuracy: 0.8790 - val_loss: 0.5348 - val_accuracy: 0.8485\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4110 - accuracy: 0.8782 - val_loss: 0.5198 - val_accuracy: 0.8552\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4090 - accuracy: 0.8789 - val_loss: 0.5136 - val_accuracy: 0.8569\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4052 - accuracy: 0.8789 - val_loss: 0.5505 - val_accuracy: 0.8431\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4041 - accuracy: 0.8810 - val_loss: 0.4855 - val_accuracy: 0.8670\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4022 - accuracy: 0.8809 - val_loss: 0.4984 - val_accuracy: 0.8604\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4007 - accuracy: 0.8822 - val_loss: 0.4737 - val_accuracy: 0.8687\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4020 - accuracy: 0.8798 - val_loss: 0.5350 - val_accuracy: 0.8476\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3990 - accuracy: 0.8818 - val_loss: 0.4873 - val_accuracy: 0.8651\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3987 - accuracy: 0.8815 - val_loss: 0.5080 - val_accuracy: 0.8599\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3954 - accuracy: 0.8840 - val_loss: 0.4793 - val_accuracy: 0.8665\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3982 - accuracy: 0.8814 - val_loss: 0.4884 - val_accuracy: 0.8637\n",
      "Try 6/100: Best_val_acc: [0.6498093008995056, 0.8173888921737671], lr: 3.898999731309411e-05, Lambda: 5.247041415261308e-05\n",
      "\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2909 - accuracy: 0.1168 - val_loss: 2.0691 - val_accuracy: 0.3374\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0659 - accuracy: 0.2429 - val_loss: 1.8793 - val_accuracy: 0.3528\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7876 - accuracy: 0.3691 - val_loss: 1.5271 - val_accuracy: 0.5154\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5513 - accuracy: 0.4855 - val_loss: 1.3970 - val_accuracy: 0.5691\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3747 - accuracy: 0.5582 - val_loss: 1.3193 - val_accuracy: 0.5919\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2477 - accuracy: 0.6080 - val_loss: 1.2025 - val_accuracy: 0.6379\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1821 - accuracy: 0.6305 - val_loss: 1.1756 - val_accuracy: 0.6559\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1084 - accuracy: 0.6548 - val_loss: 1.1150 - val_accuracy: 0.6694\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0649 - accuracy: 0.6694 - val_loss: 1.0941 - val_accuracy: 0.6638\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0284 - accuracy: 0.6808 - val_loss: 0.9591 - val_accuracy: 0.7106\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9925 - accuracy: 0.6950 - val_loss: 1.0306 - val_accuracy: 0.6881\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9704 - accuracy: 0.6998 - val_loss: 0.9369 - val_accuracy: 0.7185\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9548 - accuracy: 0.7060 - val_loss: 0.9777 - val_accuracy: 0.7092\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8978 - accuracy: 0.7231 - val_loss: 0.8671 - val_accuracy: 0.7395\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8665 - accuracy: 0.7350 - val_loss: 0.9568 - val_accuracy: 0.7184\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8428 - accuracy: 0.7415 - val_loss: 0.7927 - val_accuracy: 0.7726\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8173 - accuracy: 0.7483 - val_loss: 0.8142 - val_accuracy: 0.7582\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7965 - accuracy: 0.7553 - val_loss: 0.8113 - val_accuracy: 0.7599\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.7603 - val_loss: 0.8626 - val_accuracy: 0.7376\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7560 - accuracy: 0.7680 - val_loss: 0.8166 - val_accuracy: 0.7581\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7347 - accuracy: 0.7747 - val_loss: 0.7734 - val_accuracy: 0.7687\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7166 - accuracy: 0.7798 - val_loss: 0.7461 - val_accuracy: 0.7802\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6938 - accuracy: 0.7883 - val_loss: 0.6970 - val_accuracy: 0.7945\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6851 - accuracy: 0.7901 - val_loss: 0.7054 - val_accuracy: 0.7941\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6626 - accuracy: 0.7968 - val_loss: 0.7550 - val_accuracy: 0.7710\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6512 - accuracy: 0.7995 - val_loss: 0.6838 - val_accuracy: 0.8028\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6487 - accuracy: 0.8006 - val_loss: 0.8177 - val_accuracy: 0.7581\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6484 - accuracy: 0.8010 - val_loss: 0.6486 - val_accuracy: 0.8096\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6185 - accuracy: 0.8087 - val_loss: 0.7051 - val_accuracy: 0.7934\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6003 - accuracy: 0.8161 - val_loss: 0.6304 - val_accuracy: 0.8181\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5955 - accuracy: 0.8153 - val_loss: 0.6206 - val_accuracy: 0.8224\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5715 - accuracy: 0.8242 - val_loss: 0.6307 - val_accuracy: 0.8119\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5594 - accuracy: 0.8273 - val_loss: 0.6952 - val_accuracy: 0.7952\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5510 - accuracy: 0.8295 - val_loss: 0.6560 - val_accuracy: 0.8041\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5496 - accuracy: 0.8310 - val_loss: 0.6020 - val_accuracy: 0.8206\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5285 - accuracy: 0.8384 - val_loss: 0.7058 - val_accuracy: 0.7831\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5232 - accuracy: 0.8385 - val_loss: 0.5864 - val_accuracy: 0.8284\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5153 - accuracy: 0.8407 - val_loss: 0.5629 - val_accuracy: 0.8349\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5163 - accuracy: 0.8398 - val_loss: 0.5638 - val_accuracy: 0.8363\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5009 - accuracy: 0.8450 - val_loss: 0.6294 - val_accuracy: 0.8162\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4821 - accuracy: 0.8495 - val_loss: 0.5957 - val_accuracy: 0.8286\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4981 - accuracy: 0.8460 - val_loss: 0.5918 - val_accuracy: 0.8255\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4745 - accuracy: 0.8548 - val_loss: 0.5717 - val_accuracy: 0.8301\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4620 - accuracy: 0.8567 - val_loss: 0.5283 - val_accuracy: 0.8441\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4655 - accuracy: 0.8571 - val_loss: 0.5699 - val_accuracy: 0.8279\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4561 - accuracy: 0.8571 - val_loss: 0.5333 - val_accuracy: 0.8435\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4400 - accuracy: 0.8652 - val_loss: 0.5367 - val_accuracy: 0.8425\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4278 - accuracy: 0.8674 - val_loss: 0.6555 - val_accuracy: 0.8022\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4329 - accuracy: 0.8664 - val_loss: 0.5065 - val_accuracy: 0.8514\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4349 - accuracy: 0.8625 - val_loss: 0.4967 - val_accuracy: 0.8544\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4258 - accuracy: 0.8673 - val_loss: 0.5452 - val_accuracy: 0.8397\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4093 - accuracy: 0.8736 - val_loss: 0.4612 - val_accuracy: 0.8644\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3993 - accuracy: 0.8750 - val_loss: 0.5430 - val_accuracy: 0.8414\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4132 - accuracy: 0.8708 - val_loss: 0.5280 - val_accuracy: 0.8450\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3870 - accuracy: 0.8810 - val_loss: 0.4929 - val_accuracy: 0.8555\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3876 - accuracy: 0.8786 - val_loss: 0.4697 - val_accuracy: 0.8645\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3697 - accuracy: 0.8841 - val_loss: 0.5586 - val_accuracy: 0.8338\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3814 - accuracy: 0.8810 - val_loss: 0.5099 - val_accuracy: 0.8504\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3730 - accuracy: 0.8830 - val_loss: 0.4619 - val_accuracy: 0.8626\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3680 - accuracy: 0.8868 - val_loss: 0.4661 - val_accuracy: 0.8651\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3536 - accuracy: 0.8908 - val_loss: 0.4612 - val_accuracy: 0.8649\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3548 - accuracy: 0.8899 - val_loss: 0.4736 - val_accuracy: 0.8621\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3441 - accuracy: 0.8928 - val_loss: 0.4589 - val_accuracy: 0.8644\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3432 - accuracy: 0.8918 - val_loss: 0.4138 - val_accuracy: 0.8787\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3343 - accuracy: 0.8940 - val_loss: 0.4359 - val_accuracy: 0.8744\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3298 - accuracy: 0.8978 - val_loss: 0.4345 - val_accuracy: 0.8744\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3232 - accuracy: 0.8989 - val_loss: 0.4454 - val_accuracy: 0.8708\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3128 - accuracy: 0.9032 - val_loss: 0.4338 - val_accuracy: 0.8780\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3157 - accuracy: 0.8987 - val_loss: 0.4305 - val_accuracy: 0.8786\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3123 - accuracy: 0.9022 - val_loss: 0.4406 - val_accuracy: 0.8752\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2991 - accuracy: 0.9069 - val_loss: 0.4047 - val_accuracy: 0.8873\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3074 - accuracy: 0.9017 - val_loss: 0.4485 - val_accuracy: 0.8712\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2958 - accuracy: 0.9066 - val_loss: 0.4560 - val_accuracy: 0.8712\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3035 - accuracy: 0.9040 - val_loss: 0.4703 - val_accuracy: 0.8663\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2969 - accuracy: 0.9065 - val_loss: 0.4118 - val_accuracy: 0.8844\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2965 - accuracy: 0.9055 - val_loss: 0.4334 - val_accuracy: 0.8783\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2800 - accuracy: 0.9111 - val_loss: 0.4562 - val_accuracy: 0.8715\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2811 - accuracy: 0.9114 - val_loss: 0.4426 - val_accuracy: 0.8754\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2700 - accuracy: 0.9155 - val_loss: 0.3995 - val_accuracy: 0.8899\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2559 - accuracy: 0.9199 - val_loss: 0.3984 - val_accuracy: 0.8901\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2713 - accuracy: 0.9129 - val_loss: 0.4780 - val_accuracy: 0.8651\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2619 - accuracy: 0.9176 - val_loss: 0.4741 - val_accuracy: 0.8641\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2607 - accuracy: 0.9167 - val_loss: 0.4278 - val_accuracy: 0.8774\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2663 - accuracy: 0.9161 - val_loss: 0.4094 - val_accuracy: 0.8887\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2518 - accuracy: 0.9209 - val_loss: 0.3878 - val_accuracy: 0.8939\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2455 - accuracy: 0.9225 - val_loss: 0.4332 - val_accuracy: 0.8789\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2481 - accuracy: 0.9205 - val_loss: 0.3895 - val_accuracy: 0.8949\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2433 - accuracy: 0.9229 - val_loss: 0.4065 - val_accuracy: 0.8911\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2336 - accuracy: 0.9278 - val_loss: 0.3912 - val_accuracy: 0.8933\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2344 - accuracy: 0.9248 - val_loss: 0.3785 - val_accuracy: 0.8963\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2296 - accuracy: 0.9269 - val_loss: 0.3910 - val_accuracy: 0.8949\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2211 - accuracy: 0.9310 - val_loss: 0.3891 - val_accuracy: 0.8968\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2111 - accuracy: 0.9344 - val_loss: 0.4384 - val_accuracy: 0.8834\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2288 - accuracy: 0.9275 - val_loss: 0.3613 - val_accuracy: 0.9025\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2125 - accuracy: 0.9336 - val_loss: 0.4316 - val_accuracy: 0.8838\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2272 - accuracy: 0.9273 - val_loss: 0.3932 - val_accuracy: 0.8967\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2082 - accuracy: 0.9340 - val_loss: 0.3805 - val_accuracy: 0.8989\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2012 - accuracy: 0.9368 - val_loss: 0.4091 - val_accuracy: 0.8894\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2049 - accuracy: 0.9348 - val_loss: 0.3887 - val_accuracy: 0.8965\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1957 - accuracy: 0.9379 - val_loss: 0.3628 - val_accuracy: 0.9078\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1995 - accuracy: 0.9360 - val_loss: 0.3970 - val_accuracy: 0.8971\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1985 - accuracy: 0.9369 - val_loss: 0.3810 - val_accuracy: 0.9058\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2186 - accuracy: 0.9306 - val_loss: 0.4753 - val_accuracy: 0.8736\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2083 - accuracy: 0.9322 - val_loss: 0.4910 - val_accuracy: 0.8685\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1953 - accuracy: 0.9376 - val_loss: 0.4027 - val_accuracy: 0.8986\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1746 - accuracy: 0.9451 - val_loss: 0.3791 - val_accuracy: 0.9031\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1720 - accuracy: 0.9467 - val_loss: 0.3717 - val_accuracy: 0.9099\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1974 - accuracy: 0.9366 - val_loss: 0.4277 - val_accuracy: 0.8901\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1898 - accuracy: 0.9385 - val_loss: 0.3954 - val_accuracy: 0.8987\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1781 - accuracy: 0.9418 - val_loss: 0.3975 - val_accuracy: 0.9025\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1643 - accuracy: 0.9487 - val_loss: 0.3757 - val_accuracy: 0.9073\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1707 - accuracy: 0.9451 - val_loss: 0.3800 - val_accuracy: 0.9051\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1570 - accuracy: 0.9507 - val_loss: 0.3722 - val_accuracy: 0.9116\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1646 - accuracy: 0.9484 - val_loss: 0.4108 - val_accuracy: 0.8978\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1754 - accuracy: 0.9430 - val_loss: 0.4133 - val_accuracy: 0.8961\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1672 - accuracy: 0.9462 - val_loss: 0.3819 - val_accuracy: 0.9072\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1446 - accuracy: 0.9549 - val_loss: 0.3517 - val_accuracy: 0.9181\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1548 - accuracy: 0.9510 - val_loss: 0.3737 - val_accuracy: 0.9118\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1649 - accuracy: 0.9465 - val_loss: 0.3652 - val_accuracy: 0.9125\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1646 - accuracy: 0.9469 - val_loss: 0.3617 - val_accuracy: 0.9166\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1499 - accuracy: 0.9527 - val_loss: 0.3222 - val_accuracy: 0.9282\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1512 - accuracy: 0.9511 - val_loss: 0.3930 - val_accuracy: 0.9062\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1863 - accuracy: 0.9389 - val_loss: 0.3971 - val_accuracy: 0.9001\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1489 - accuracy: 0.9531 - val_loss: 0.3416 - val_accuracy: 0.9249\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1363 - accuracy: 0.9576 - val_loss: 0.3644 - val_accuracy: 0.9176\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1398 - accuracy: 0.9554 - val_loss: 0.3545 - val_accuracy: 0.9203\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1453 - accuracy: 0.9545 - val_loss: 0.3392 - val_accuracy: 0.9243\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1359 - accuracy: 0.9560 - val_loss: 0.3862 - val_accuracy: 0.9107\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1487 - accuracy: 0.9517 - val_loss: 0.3718 - val_accuracy: 0.9151\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1312 - accuracy: 0.9575 - val_loss: 0.3596 - val_accuracy: 0.9220\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1146 - accuracy: 0.9641 - val_loss: 0.3636 - val_accuracy: 0.9260\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1361 - accuracy: 0.9564 - val_loss: 0.3846 - val_accuracy: 0.9123\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1392 - accuracy: 0.9538 - val_loss: 0.4428 - val_accuracy: 0.9006\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1183 - accuracy: 0.9620 - val_loss: 0.4241 - val_accuracy: 0.9061\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1343 - accuracy: 0.9557 - val_loss: 0.3803 - val_accuracy: 0.9124\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1289 - accuracy: 0.9584 - val_loss: 0.3576 - val_accuracy: 0.9216\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1265 - accuracy: 0.9592 - val_loss: 0.4437 - val_accuracy: 0.9012\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1206 - accuracy: 0.9613 - val_loss: 0.3796 - val_accuracy: 0.9166\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1197 - accuracy: 0.9613 - val_loss: 0.4343 - val_accuracy: 0.9062\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.1187 - accuracy: 0.9612 - val_loss: 0.3889 - val_accuracy: 0.9227\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1053 - accuracy: 0.9667 - val_loss: 0.4312 - val_accuracy: 0.9068\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1272 - accuracy: 0.9582 - val_loss: 0.4089 - val_accuracy: 0.9116\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1164 - accuracy: 0.9623 - val_loss: 0.4219 - val_accuracy: 0.9104\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1066 - accuracy: 0.9663 - val_loss: 0.4037 - val_accuracy: 0.9142\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1246 - accuracy: 0.9597 - val_loss: 0.4522 - val_accuracy: 0.8974\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.4729 - val_accuracy: 0.8954\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1353 - accuracy: 0.9556 - val_loss: 0.3440 - val_accuracy: 0.9293\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1164 - accuracy: 0.9623 - val_loss: 0.3889 - val_accuracy: 0.9150\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1127 - accuracy: 0.9633 - val_loss: 0.3775 - val_accuracy: 0.9243\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0900 - accuracy: 0.9717 - val_loss: 0.3849 - val_accuracy: 0.9278\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1066 - accuracy: 0.9651 - val_loss: 0.4857 - val_accuracy: 0.9009\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1180 - accuracy: 0.9613 - val_loss: 0.4208 - val_accuracy: 0.9136\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1010 - accuracy: 0.9683 - val_loss: 0.3649 - val_accuracy: 0.9284\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0934 - accuracy: 0.9701 - val_loss: 0.3462 - val_accuracy: 0.9325\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0935 - accuracy: 0.9698 - val_loss: 0.3635 - val_accuracy: 0.9326\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0801 - accuracy: 0.9753 - val_loss: 0.3922 - val_accuracy: 0.9243\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0981 - accuracy: 0.9689 - val_loss: 0.4662 - val_accuracy: 0.9052\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1228 - accuracy: 0.9582 - val_loss: 0.4426 - val_accuracy: 0.9130\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1245 - accuracy: 0.9588 - val_loss: 0.4064 - val_accuracy: 0.9189\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1072 - accuracy: 0.9659 - val_loss: 0.3661 - val_accuracy: 0.9271\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0892 - accuracy: 0.9716 - val_loss: 0.4160 - val_accuracy: 0.9201\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0759 - accuracy: 0.9773 - val_loss: 0.3859 - val_accuracy: 0.9314\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0981 - accuracy: 0.9675 - val_loss: 0.3714 - val_accuracy: 0.9336\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0967 - accuracy: 0.9683 - val_loss: 0.3954 - val_accuracy: 0.9288\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0798 - accuracy: 0.9747 - val_loss: 0.3935 - val_accuracy: 0.9247\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0797 - accuracy: 0.9748 - val_loss: 0.3796 - val_accuracy: 0.9298\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0821 - accuracy: 0.9741 - val_loss: 0.4175 - val_accuracy: 0.9221\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0852 - accuracy: 0.9726 - val_loss: 0.4248 - val_accuracy: 0.9196\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1199 - accuracy: 0.9597 - val_loss: 0.3806 - val_accuracy: 0.9274\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0806 - accuracy: 0.9738 - val_loss: 0.3824 - val_accuracy: 0.9329\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0734 - accuracy: 0.9767 - val_loss: 0.4133 - val_accuracy: 0.9254\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0725 - accuracy: 0.9774 - val_loss: 0.4556 - val_accuracy: 0.9140\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0737 - accuracy: 0.9770 - val_loss: 0.4232 - val_accuracy: 0.9272\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0803 - accuracy: 0.9740 - val_loss: 0.4415 - val_accuracy: 0.9205\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1243 - accuracy: 0.9592 - val_loss: 0.4252 - val_accuracy: 0.9160\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0862 - accuracy: 0.9712 - val_loss: 0.3773 - val_accuracy: 0.9354\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0752 - accuracy: 0.9763 - val_loss: 0.3852 - val_accuracy: 0.9307\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0805 - accuracy: 0.9739 - val_loss: 0.4588 - val_accuracy: 0.9188\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0723 - accuracy: 0.9763 - val_loss: 0.3822 - val_accuracy: 0.9401\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0681 - accuracy: 0.9782 - val_loss: 0.4327 - val_accuracy: 0.9210\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0734 - accuracy: 0.9761 - val_loss: 0.3710 - val_accuracy: 0.9394\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0786 - accuracy: 0.9741 - val_loss: 0.4120 - val_accuracy: 0.9298\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0898 - accuracy: 0.9707 - val_loss: 0.4836 - val_accuracy: 0.9111\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0793 - accuracy: 0.9739 - val_loss: 0.4390 - val_accuracy: 0.9253\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0730 - accuracy: 0.9765 - val_loss: 0.4434 - val_accuracy: 0.9221\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0897 - accuracy: 0.9710 - val_loss: 0.7138 - val_accuracy: 0.8549\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.4687 - val_accuracy: 0.9147\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0819 - accuracy: 0.9731 - val_loss: 0.4279 - val_accuracy: 0.9262\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0889 - accuracy: 0.9707 - val_loss: 0.5980 - val_accuracy: 0.8880\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0899 - accuracy: 0.9700 - val_loss: 0.4871 - val_accuracy: 0.9159\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0581 - accuracy: 0.9818 - val_loss: 0.4338 - val_accuracy: 0.9306\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 0.5263 - val_accuracy: 0.9138\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0645 - accuracy: 0.9797 - val_loss: 0.4914 - val_accuracy: 0.9174\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0741 - accuracy: 0.9749 - val_loss: 0.4407 - val_accuracy: 0.9260\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0830 - accuracy: 0.9720 - val_loss: 0.4459 - val_accuracy: 0.9304\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0582 - accuracy: 0.9820 - val_loss: 0.3776 - val_accuracy: 0.9427\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.4564 - val_accuracy: 0.9276\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0917 - accuracy: 0.9697 - val_loss: 0.4558 - val_accuracy: 0.9275\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0881 - accuracy: 0.9707 - val_loss: 0.4872 - val_accuracy: 0.9212\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0625 - accuracy: 0.9798 - val_loss: 0.4116 - val_accuracy: 0.9374\n",
      "Try 7/100: Best_val_acc: [1.2308975458145142, 0.8292222023010254], lr: 0.0005300389958274892, Lambda: 5.60341603286307e-05\n",
      "\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3268 - accuracy: 0.1066 - val_loss: 2.2864 - val_accuracy: 0.1449\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2950 - accuracy: 0.1323 - val_loss: 2.2844 - val_accuracy: 0.1553\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2785 - accuracy: 0.1579 - val_loss: 2.2606 - val_accuracy: 0.1971\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2581 - accuracy: 0.1746 - val_loss: 2.2197 - val_accuracy: 0.2224\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2345 - accuracy: 0.1976 - val_loss: 2.2142 - val_accuracy: 0.2022\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2078 - accuracy: 0.2220 - val_loss: 2.1957 - val_accuracy: 0.2144\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1796 - accuracy: 0.2458 - val_loss: 2.1525 - val_accuracy: 0.2804\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1501 - accuracy: 0.2761 - val_loss: 2.1444 - val_accuracy: 0.2708\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1185 - accuracy: 0.3025 - val_loss: 2.1050 - val_accuracy: 0.2997\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0844 - accuracy: 0.3268 - val_loss: 2.0735 - val_accuracy: 0.3201\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0493 - accuracy: 0.3481 - val_loss: 2.0744 - val_accuracy: 0.2976\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0121 - accuracy: 0.3737 - val_loss: 2.0163 - val_accuracy: 0.3431\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9740 - accuracy: 0.3963 - val_loss: 1.9702 - val_accuracy: 0.3809\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9337 - accuracy: 0.4157 - val_loss: 1.9436 - val_accuracy: 0.4026\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8942 - accuracy: 0.4347 - val_loss: 1.9186 - val_accuracy: 0.4004\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8538 - accuracy: 0.4479 - val_loss: 1.8820 - val_accuracy: 0.4239\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8136 - accuracy: 0.4662 - val_loss: 1.8503 - val_accuracy: 0.4448\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7732 - accuracy: 0.4794 - val_loss: 1.7834 - val_accuracy: 0.4966\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7342 - accuracy: 0.4949 - val_loss: 1.7997 - val_accuracy: 0.4412\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6966 - accuracy: 0.5096 - val_loss: 1.6852 - val_accuracy: 0.5324\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6601 - accuracy: 0.5252 - val_loss: 1.7394 - val_accuracy: 0.4621\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6259 - accuracy: 0.5384 - val_loss: 1.6733 - val_accuracy: 0.5086\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5919 - accuracy: 0.5510 - val_loss: 1.6343 - val_accuracy: 0.5351\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5608 - accuracy: 0.5600 - val_loss: 1.5975 - val_accuracy: 0.5524\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5296 - accuracy: 0.5720 - val_loss: 1.5983 - val_accuracy: 0.5329\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4998 - accuracy: 0.5797 - val_loss: 1.4895 - val_accuracy: 0.6004\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4706 - accuracy: 0.5870 - val_loss: 1.5323 - val_accuracy: 0.5573\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4443 - accuracy: 0.5968 - val_loss: 1.4432 - val_accuracy: 0.6049\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4196 - accuracy: 0.6017 - val_loss: 1.5023 - val_accuracy: 0.5589\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3966 - accuracy: 0.6078 - val_loss: 1.4491 - val_accuracy: 0.5869\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3728 - accuracy: 0.6140 - val_loss: 1.4308 - val_accuracy: 0.5897\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3520 - accuracy: 0.6201 - val_loss: 1.4001 - val_accuracy: 0.6048\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3328 - accuracy: 0.6246 - val_loss: 1.4072 - val_accuracy: 0.5916\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3147 - accuracy: 0.6296 - val_loss: 1.3765 - val_accuracy: 0.5986\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2979 - accuracy: 0.6332 - val_loss: 1.3494 - val_accuracy: 0.6159\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2817 - accuracy: 0.6374 - val_loss: 1.3129 - val_accuracy: 0.6271\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2662 - accuracy: 0.6408 - val_loss: 1.3176 - val_accuracy: 0.6159\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2529 - accuracy: 0.6440 - val_loss: 1.3260 - val_accuracy: 0.6072\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2394 - accuracy: 0.6479 - val_loss: 1.3251 - val_accuracy: 0.6148\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2267 - accuracy: 0.6494 - val_loss: 1.2696 - val_accuracy: 0.6394\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2144 - accuracy: 0.6533 - val_loss: 1.2721 - val_accuracy: 0.6313\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2037 - accuracy: 0.6545 - val_loss: 1.2791 - val_accuracy: 0.6218\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1915 - accuracy: 0.6573 - val_loss: 1.2756 - val_accuracy: 0.6202\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1805 - accuracy: 0.6612 - val_loss: 1.2017 - val_accuracy: 0.6575\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1715 - accuracy: 0.6630 - val_loss: 1.2185 - val_accuracy: 0.6524\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1617 - accuracy: 0.6659 - val_loss: 1.1821 - val_accuracy: 0.6552\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1531 - accuracy: 0.6667 - val_loss: 1.2153 - val_accuracy: 0.6409\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1438 - accuracy: 0.6683 - val_loss: 1.1996 - val_accuracy: 0.6482\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1367 - accuracy: 0.6725 - val_loss: 1.2098 - val_accuracy: 0.6424\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1291 - accuracy: 0.6727 - val_loss: 1.1945 - val_accuracy: 0.6483\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1216 - accuracy: 0.6752 - val_loss: 1.2406 - val_accuracy: 0.6200\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1156 - accuracy: 0.6752 - val_loss: 1.1569 - val_accuracy: 0.6644\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1074 - accuracy: 0.6790 - val_loss: 1.1663 - val_accuracy: 0.6640\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1001 - accuracy: 0.6811 - val_loss: 1.1660 - val_accuracy: 0.6615\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0944 - accuracy: 0.6823 - val_loss: 1.1778 - val_accuracy: 0.6480\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0883 - accuracy: 0.6835 - val_loss: 1.1252 - val_accuracy: 0.6748\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0821 - accuracy: 0.6855 - val_loss: 1.1079 - val_accuracy: 0.6817\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0754 - accuracy: 0.6874 - val_loss: 1.1280 - val_accuracy: 0.6713\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0710 - accuracy: 0.6886 - val_loss: 1.1063 - val_accuracy: 0.6835\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0640 - accuracy: 0.6901 - val_loss: 1.1211 - val_accuracy: 0.6709\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0597 - accuracy: 0.6916 - val_loss: 1.1186 - val_accuracy: 0.6734\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0547 - accuracy: 0.6920 - val_loss: 1.0741 - val_accuracy: 0.6946\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0483 - accuracy: 0.6949 - val_loss: 1.1423 - val_accuracy: 0.6621\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0445 - accuracy: 0.6949 - val_loss: 1.1000 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0393 - accuracy: 0.6974 - val_loss: 1.0837 - val_accuracy: 0.6878\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0349 - accuracy: 0.6981 - val_loss: 1.0717 - val_accuracy: 0.6899\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0287 - accuracy: 0.6996 - val_loss: 1.0792 - val_accuracy: 0.6836\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0237 - accuracy: 0.7010 - val_loss: 1.0931 - val_accuracy: 0.6774\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0190 - accuracy: 0.7028 - val_loss: 1.0845 - val_accuracy: 0.6856\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0153 - accuracy: 0.7034 - val_loss: 1.0290 - val_accuracy: 0.7098\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0112 - accuracy: 0.7046 - val_loss: 1.0441 - val_accuracy: 0.7047\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0071 - accuracy: 0.7059 - val_loss: 1.0216 - val_accuracy: 0.7089\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0025 - accuracy: 0.7073 - val_loss: 1.0446 - val_accuracy: 0.7024\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9981 - accuracy: 0.7087 - val_loss: 1.0499 - val_accuracy: 0.6909\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9939 - accuracy: 0.7102 - val_loss: 1.0174 - val_accuracy: 0.7100\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9895 - accuracy: 0.7105 - val_loss: 1.0535 - val_accuracy: 0.6907\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9858 - accuracy: 0.7122 - val_loss: 1.0439 - val_accuracy: 0.7005\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9817 - accuracy: 0.7132 - val_loss: 1.0326 - val_accuracy: 0.7008\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9786 - accuracy: 0.7146 - val_loss: 1.0494 - val_accuracy: 0.6963\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9743 - accuracy: 0.7148 - val_loss: 1.0268 - val_accuracy: 0.7021\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9698 - accuracy: 0.7173 - val_loss: 0.9930 - val_accuracy: 0.7209\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9666 - accuracy: 0.7181 - val_loss: 1.0212 - val_accuracy: 0.7027\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9619 - accuracy: 0.7195 - val_loss: 1.0087 - val_accuracy: 0.7109\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9583 - accuracy: 0.7205 - val_loss: 1.0200 - val_accuracy: 0.7051\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9554 - accuracy: 0.7212 - val_loss: 0.9842 - val_accuracy: 0.7221\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9520 - accuracy: 0.7226 - val_loss: 0.9885 - val_accuracy: 0.7191\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9481 - accuracy: 0.7234 - val_loss: 1.0018 - val_accuracy: 0.7134\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9445 - accuracy: 0.7248 - val_loss: 0.9919 - val_accuracy: 0.7178\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9410 - accuracy: 0.7248 - val_loss: 0.9980 - val_accuracy: 0.7140\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9367 - accuracy: 0.7268 - val_loss: 0.9599 - val_accuracy: 0.7305\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9349 - accuracy: 0.7269 - val_loss: 0.9653 - val_accuracy: 0.7259\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9305 - accuracy: 0.7275 - val_loss: 1.0143 - val_accuracy: 0.7065\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9272 - accuracy: 0.7301 - val_loss: 0.9566 - val_accuracy: 0.7278\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9247 - accuracy: 0.7305 - val_loss: 0.9630 - val_accuracy: 0.7285\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9200 - accuracy: 0.7322 - val_loss: 0.9831 - val_accuracy: 0.7191\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9168 - accuracy: 0.7327 - val_loss: 0.9907 - val_accuracy: 0.7111\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9149 - accuracy: 0.7315 - val_loss: 0.9433 - val_accuracy: 0.7325\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9104 - accuracy: 0.7350 - val_loss: 0.9713 - val_accuracy: 0.7184\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9076 - accuracy: 0.7346 - val_loss: 0.9481 - val_accuracy: 0.7319\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9047 - accuracy: 0.7356 - val_loss: 0.9632 - val_accuracy: 0.7257\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9025 - accuracy: 0.7372 - val_loss: 0.9558 - val_accuracy: 0.7261\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8996 - accuracy: 0.7378 - val_loss: 0.9710 - val_accuracy: 0.7216\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8957 - accuracy: 0.7386 - val_loss: 0.9536 - val_accuracy: 0.7225\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8938 - accuracy: 0.7387 - val_loss: 0.9002 - val_accuracy: 0.7447\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8913 - accuracy: 0.7401 - val_loss: 0.9152 - val_accuracy: 0.7438\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8889 - accuracy: 0.7397 - val_loss: 0.9412 - val_accuracy: 0.7265\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8850 - accuracy: 0.7415 - val_loss: 0.9123 - val_accuracy: 0.7436\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8823 - accuracy: 0.7427 - val_loss: 0.9088 - val_accuracy: 0.7433\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8791 - accuracy: 0.7432 - val_loss: 0.9639 - val_accuracy: 0.7223\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8759 - accuracy: 0.7445 - val_loss: 0.9091 - val_accuracy: 0.7445\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8732 - accuracy: 0.7445 - val_loss: 0.9181 - val_accuracy: 0.7401\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8698 - accuracy: 0.7454 - val_loss: 0.9195 - val_accuracy: 0.7379\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8682 - accuracy: 0.7463 - val_loss: 0.9003 - val_accuracy: 0.7455\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8657 - accuracy: 0.7464 - val_loss: 0.8762 - val_accuracy: 0.7560\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8629 - accuracy: 0.7479 - val_loss: 0.9275 - val_accuracy: 0.7292\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8597 - accuracy: 0.7485 - val_loss: 0.9003 - val_accuracy: 0.7436\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8578 - accuracy: 0.7490 - val_loss: 0.9208 - val_accuracy: 0.7338\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8543 - accuracy: 0.7511 - val_loss: 0.9230 - val_accuracy: 0.7337\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8520 - accuracy: 0.7512 - val_loss: 0.8960 - val_accuracy: 0.7409\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8492 - accuracy: 0.7526 - val_loss: 0.8903 - val_accuracy: 0.7486\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8473 - accuracy: 0.7514 - val_loss: 0.9014 - val_accuracy: 0.7429\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8442 - accuracy: 0.7550 - val_loss: 0.9273 - val_accuracy: 0.7335\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8412 - accuracy: 0.7561 - val_loss: 0.8947 - val_accuracy: 0.7456\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8397 - accuracy: 0.7544 - val_loss: 0.9298 - val_accuracy: 0.7310\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8385 - accuracy: 0.7550 - val_loss: 0.8781 - val_accuracy: 0.7514\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8347 - accuracy: 0.7553 - val_loss: 0.9200 - val_accuracy: 0.7349\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8326 - accuracy: 0.7572 - val_loss: 0.9117 - val_accuracy: 0.7356\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8297 - accuracy: 0.7571 - val_loss: 0.8500 - val_accuracy: 0.7609\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8275 - accuracy: 0.7585 - val_loss: 0.9123 - val_accuracy: 0.7334\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8256 - accuracy: 0.7586 - val_loss: 0.8979 - val_accuracy: 0.7423\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8235 - accuracy: 0.7593 - val_loss: 0.8642 - val_accuracy: 0.7544\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8205 - accuracy: 0.7610 - val_loss: 0.8548 - val_accuracy: 0.7581\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8180 - accuracy: 0.7612 - val_loss: 0.8532 - val_accuracy: 0.7580\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8168 - accuracy: 0.7612 - val_loss: 0.8516 - val_accuracy: 0.7582\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8146 - accuracy: 0.7615 - val_loss: 0.9073 - val_accuracy: 0.7351\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8127 - accuracy: 0.7628 - val_loss: 0.8487 - val_accuracy: 0.7583\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8096 - accuracy: 0.7630 - val_loss: 0.8773 - val_accuracy: 0.7506\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8073 - accuracy: 0.7650 - val_loss: 0.8753 - val_accuracy: 0.7486\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8052 - accuracy: 0.7649 - val_loss: 0.8736 - val_accuracy: 0.7492\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8041 - accuracy: 0.7654 - val_loss: 0.7993 - val_accuracy: 0.7748\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8016 - accuracy: 0.7654 - val_loss: 0.8417 - val_accuracy: 0.7611\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7996 - accuracy: 0.7651 - val_loss: 0.8362 - val_accuracy: 0.7619\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7983 - accuracy: 0.7667 - val_loss: 0.8144 - val_accuracy: 0.7709\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7957 - accuracy: 0.7680 - val_loss: 0.8522 - val_accuracy: 0.7571\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7929 - accuracy: 0.7680 - val_loss: 0.8467 - val_accuracy: 0.7609\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7915 - accuracy: 0.7686 - val_loss: 0.8477 - val_accuracy: 0.7564\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7888 - accuracy: 0.7693 - val_loss: 0.8477 - val_accuracy: 0.7559\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7873 - accuracy: 0.7695 - val_loss: 0.8608 - val_accuracy: 0.7512\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7843 - accuracy: 0.7712 - val_loss: 0.8534 - val_accuracy: 0.7561\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7833 - accuracy: 0.7701 - val_loss: 0.7990 - val_accuracy: 0.7776\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7807 - accuracy: 0.7719 - val_loss: 0.8034 - val_accuracy: 0.7717\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7792 - accuracy: 0.7732 - val_loss: 0.8498 - val_accuracy: 0.7532\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7770 - accuracy: 0.7720 - val_loss: 0.7988 - val_accuracy: 0.7754\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7754 - accuracy: 0.7740 - val_loss: 0.8212 - val_accuracy: 0.7661\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7731 - accuracy: 0.7745 - val_loss: 0.8255 - val_accuracy: 0.7643\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7708 - accuracy: 0.7756 - val_loss: 0.8280 - val_accuracy: 0.7636\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7697 - accuracy: 0.7751 - val_loss: 0.7912 - val_accuracy: 0.7754\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7670 - accuracy: 0.7765 - val_loss: 0.8245 - val_accuracy: 0.7614\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7666 - accuracy: 0.7760 - val_loss: 0.8459 - val_accuracy: 0.7553\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7642 - accuracy: 0.7761 - val_loss: 0.8115 - val_accuracy: 0.7716\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7623 - accuracy: 0.7780 - val_loss: 0.8190 - val_accuracy: 0.7666\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7604 - accuracy: 0.7775 - val_loss: 0.8215 - val_accuracy: 0.7696\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7593 - accuracy: 0.7786 - val_loss: 0.8205 - val_accuracy: 0.7666\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7567 - accuracy: 0.7792 - val_loss: 0.8252 - val_accuracy: 0.7618\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7546 - accuracy: 0.7797 - val_loss: 0.8001 - val_accuracy: 0.7731\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7547 - accuracy: 0.7791 - val_loss: 0.8087 - val_accuracy: 0.7687\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7501 - accuracy: 0.7808 - val_loss: 0.8083 - val_accuracy: 0.7714\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7498 - accuracy: 0.7802 - val_loss: 0.8066 - val_accuracy: 0.7723\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7478 - accuracy: 0.7812 - val_loss: 0.7727 - val_accuracy: 0.7857\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7459 - accuracy: 0.7809 - val_loss: 0.8090 - val_accuracy: 0.7688\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7436 - accuracy: 0.7826 - val_loss: 0.8042 - val_accuracy: 0.7749\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7432 - accuracy: 0.7835 - val_loss: 0.8151 - val_accuracy: 0.7645\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7415 - accuracy: 0.7827 - val_loss: 0.7850 - val_accuracy: 0.7776\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7399 - accuracy: 0.7829 - val_loss: 0.7861 - val_accuracy: 0.7765\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7380 - accuracy: 0.7840 - val_loss: 0.7945 - val_accuracy: 0.7659\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7363 - accuracy: 0.7847 - val_loss: 0.7942 - val_accuracy: 0.7714\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7343 - accuracy: 0.7853 - val_loss: 0.7817 - val_accuracy: 0.7766\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7320 - accuracy: 0.7858 - val_loss: 0.7506 - val_accuracy: 0.7899\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7313 - accuracy: 0.7859 - val_loss: 0.7988 - val_accuracy: 0.7734\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7308 - accuracy: 0.7853 - val_loss: 0.7865 - val_accuracy: 0.7739\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7287 - accuracy: 0.7866 - val_loss: 0.7944 - val_accuracy: 0.7711\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7280 - accuracy: 0.7865 - val_loss: 0.7748 - val_accuracy: 0.7794\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7251 - accuracy: 0.7880 - val_loss: 0.8037 - val_accuracy: 0.7695\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7255 - accuracy: 0.7864 - val_loss: 0.7442 - val_accuracy: 0.7903\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7214 - accuracy: 0.7889 - val_loss: 0.7811 - val_accuracy: 0.7784\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7193 - accuracy: 0.7894 - val_loss: 0.7863 - val_accuracy: 0.7721\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7198 - accuracy: 0.7893 - val_loss: 0.7830 - val_accuracy: 0.7793\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7177 - accuracy: 0.7896 - val_loss: 0.7662 - val_accuracy: 0.7835\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7165 - accuracy: 0.7898 - val_loss: 0.7705 - val_accuracy: 0.7796\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7146 - accuracy: 0.7906 - val_loss: 0.7467 - val_accuracy: 0.7905\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7129 - accuracy: 0.7908 - val_loss: 0.7717 - val_accuracy: 0.7816\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7123 - accuracy: 0.7925 - val_loss: 0.7836 - val_accuracy: 0.7752\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7093 - accuracy: 0.7929 - val_loss: 0.7900 - val_accuracy: 0.7724\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7086 - accuracy: 0.7919 - val_loss: 0.7592 - val_accuracy: 0.7821\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7076 - accuracy: 0.7928 - val_loss: 0.7600 - val_accuracy: 0.7829\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7049 - accuracy: 0.7939 - val_loss: 0.7405 - val_accuracy: 0.7894\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7051 - accuracy: 0.7930 - val_loss: 0.7333 - val_accuracy: 0.7945\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7037 - accuracy: 0.7930 - val_loss: 0.7883 - val_accuracy: 0.7713\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7020 - accuracy: 0.7933 - val_loss: 0.7550 - val_accuracy: 0.7849\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6990 - accuracy: 0.7943 - val_loss: 0.7598 - val_accuracy: 0.7829\n",
      "Try 8/100: Best_val_acc: [0.7914428114891052, 0.7720555663108826], lr: 1.1467762865610072e-05, Lambda: 0.00010320485946497927\n",
      "\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 2.3986 - accuracy: 0.1103 - val_loss: 2.3010 - val_accuracy: 0.1250\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2939 - accuracy: 0.1383 - val_loss: 2.2849 - val_accuracy: 0.1448\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2721 - accuracy: 0.1674 - val_loss: 2.2611 - val_accuracy: 0.1606\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2476 - accuracy: 0.1964 - val_loss: 2.2389 - val_accuracy: 0.1846\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2214 - accuracy: 0.2182 - val_loss: 2.1841 - val_accuracy: 0.2354\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1900 - accuracy: 0.2365 - val_loss: 2.1650 - val_accuracy: 0.2332\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1559 - accuracy: 0.2506 - val_loss: 2.1253 - val_accuracy: 0.2776\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1208 - accuracy: 0.2662 - val_loss: 2.0567 - val_accuracy: 0.3281\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0861 - accuracy: 0.2861 - val_loss: 2.0439 - val_accuracy: 0.3250\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0500 - accuracy: 0.3062 - val_loss: 2.0215 - val_accuracy: 0.3147\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0150 - accuracy: 0.3245 - val_loss: 1.9762 - val_accuracy: 0.3346\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9824 - accuracy: 0.3426 - val_loss: 1.9265 - val_accuracy: 0.3926\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9508 - accuracy: 0.3576 - val_loss: 1.9055 - val_accuracy: 0.3924\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9201 - accuracy: 0.3732 - val_loss: 1.9106 - val_accuracy: 0.3745\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8907 - accuracy: 0.3865 - val_loss: 1.8375 - val_accuracy: 0.4234\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8623 - accuracy: 0.3991 - val_loss: 1.8480 - val_accuracy: 0.3935\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8346 - accuracy: 0.4107 - val_loss: 1.8084 - val_accuracy: 0.4301\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8071 - accuracy: 0.4248 - val_loss: 1.7578 - val_accuracy: 0.4657\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7809 - accuracy: 0.4347 - val_loss: 1.7397 - val_accuracy: 0.4848\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7552 - accuracy: 0.4439 - val_loss: 1.7465 - val_accuracy: 0.4639\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7283 - accuracy: 0.4555 - val_loss: 1.6882 - val_accuracy: 0.5006\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7040 - accuracy: 0.4623 - val_loss: 1.6341 - val_accuracy: 0.5459\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6803 - accuracy: 0.4767 - val_loss: 1.6680 - val_accuracy: 0.5114\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6586 - accuracy: 0.4807 - val_loss: 1.6020 - val_accuracy: 0.5634\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6358 - accuracy: 0.4905 - val_loss: 1.5906 - val_accuracy: 0.5678\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6149 - accuracy: 0.4997 - val_loss: 1.5710 - val_accuracy: 0.5806\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5954 - accuracy: 0.5078 - val_loss: 1.5327 - val_accuracy: 0.5984\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5756 - accuracy: 0.5170 - val_loss: 1.5465 - val_accuracy: 0.5839\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5557 - accuracy: 0.5238 - val_loss: 1.5257 - val_accuracy: 0.5875\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5370 - accuracy: 0.5310 - val_loss: 1.4815 - val_accuracy: 0.6116\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5191 - accuracy: 0.5376 - val_loss: 1.4680 - val_accuracy: 0.6244\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5020 - accuracy: 0.5463 - val_loss: 1.4438 - val_accuracy: 0.6256\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4846 - accuracy: 0.5534 - val_loss: 1.4292 - val_accuracy: 0.6268\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4683 - accuracy: 0.5583 - val_loss: 1.4064 - val_accuracy: 0.6368\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4523 - accuracy: 0.5664 - val_loss: 1.4039 - val_accuracy: 0.6356\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4370 - accuracy: 0.5736 - val_loss: 1.3960 - val_accuracy: 0.6362\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4212 - accuracy: 0.5787 - val_loss: 1.3919 - val_accuracy: 0.6321\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4074 - accuracy: 0.5846 - val_loss: 1.3767 - val_accuracy: 0.6362\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3928 - accuracy: 0.5905 - val_loss: 1.4199 - val_accuracy: 0.6069\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3788 - accuracy: 0.5954 - val_loss: 1.3576 - val_accuracy: 0.6338\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3665 - accuracy: 0.6004 - val_loss: 1.3501 - val_accuracy: 0.6343\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3529 - accuracy: 0.6070 - val_loss: 1.3426 - val_accuracy: 0.6366\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3406 - accuracy: 0.6101 - val_loss: 1.3374 - val_accuracy: 0.6409\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3281 - accuracy: 0.6181 - val_loss: 1.3053 - val_accuracy: 0.6481\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3159 - accuracy: 0.6221 - val_loss: 1.3056 - val_accuracy: 0.6461\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3042 - accuracy: 0.6276 - val_loss: 1.3121 - val_accuracy: 0.6399\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2928 - accuracy: 0.6302 - val_loss: 1.3308 - val_accuracy: 0.6286\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2818 - accuracy: 0.6350 - val_loss: 1.2960 - val_accuracy: 0.6413\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2711 - accuracy: 0.6382 - val_loss: 1.2782 - val_accuracy: 0.6543\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2594 - accuracy: 0.6445 - val_loss: 1.2829 - val_accuracy: 0.6385\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2500 - accuracy: 0.6453 - val_loss: 1.2520 - val_accuracy: 0.6546\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2381 - accuracy: 0.6500 - val_loss: 1.2835 - val_accuracy: 0.6397\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2294 - accuracy: 0.6531 - val_loss: 1.2099 - val_accuracy: 0.6721\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2194 - accuracy: 0.6560 - val_loss: 1.2470 - val_accuracy: 0.6513\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2103 - accuracy: 0.6577 - val_loss: 1.1936 - val_accuracy: 0.6763\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2005 - accuracy: 0.6603 - val_loss: 1.2097 - val_accuracy: 0.6651\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1917 - accuracy: 0.6643 - val_loss: 1.1839 - val_accuracy: 0.6813\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1836 - accuracy: 0.6663 - val_loss: 1.1871 - val_accuracy: 0.6771\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1748 - accuracy: 0.6681 - val_loss: 1.2008 - val_accuracy: 0.6631\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1662 - accuracy: 0.6722 - val_loss: 1.1808 - val_accuracy: 0.6735\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1581 - accuracy: 0.6740 - val_loss: 1.2075 - val_accuracy: 0.6568\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1483 - accuracy: 0.6764 - val_loss: 1.1613 - val_accuracy: 0.6794\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1421 - accuracy: 0.6783 - val_loss: 1.1499 - val_accuracy: 0.6808\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1333 - accuracy: 0.6807 - val_loss: 1.1269 - val_accuracy: 0.6881\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1262 - accuracy: 0.6816 - val_loss: 1.1285 - val_accuracy: 0.6888\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1199 - accuracy: 0.6836 - val_loss: 1.1061 - val_accuracy: 0.6989\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1138 - accuracy: 0.6863 - val_loss: 1.1031 - val_accuracy: 0.6961\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1058 - accuracy: 0.6870 - val_loss: 1.1331 - val_accuracy: 0.6804\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0987 - accuracy: 0.6890 - val_loss: 1.1059 - val_accuracy: 0.6936\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0922 - accuracy: 0.6891 - val_loss: 1.1049 - val_accuracy: 0.6901\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0854 - accuracy: 0.6918 - val_loss: 1.0998 - val_accuracy: 0.6918\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0798 - accuracy: 0.6937 - val_loss: 1.0937 - val_accuracy: 0.6945\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0746 - accuracy: 0.6948 - val_loss: 1.0996 - val_accuracy: 0.6918\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0673 - accuracy: 0.6973 - val_loss: 1.0988 - val_accuracy: 0.6915\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0628 - accuracy: 0.6977 - val_loss: 1.0743 - val_accuracy: 0.7003\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0567 - accuracy: 0.6992 - val_loss: 1.0897 - val_accuracy: 0.6909\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0518 - accuracy: 0.6997 - val_loss: 1.0683 - val_accuracy: 0.6999\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0488 - accuracy: 0.7000 - val_loss: 1.0871 - val_accuracy: 0.6881\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0412 - accuracy: 0.7035 - val_loss: 1.1055 - val_accuracy: 0.6831\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0369 - accuracy: 0.7034 - val_loss: 1.0884 - val_accuracy: 0.6888\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0321 - accuracy: 0.7047 - val_loss: 1.0248 - val_accuracy: 0.7159\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0268 - accuracy: 0.7059 - val_loss: 1.0796 - val_accuracy: 0.6886\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0222 - accuracy: 0.7064 - val_loss: 1.0268 - val_accuracy: 0.7124\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0175 - accuracy: 0.7084 - val_loss: 1.0651 - val_accuracy: 0.6934\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0129 - accuracy: 0.7098 - val_loss: 1.0371 - val_accuracy: 0.7081\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0091 - accuracy: 0.7096 - val_loss: 1.0142 - val_accuracy: 0.7153\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0037 - accuracy: 0.7115 - val_loss: 1.0308 - val_accuracy: 0.7075\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0011 - accuracy: 0.7132 - val_loss: 1.0027 - val_accuracy: 0.7181\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9955 - accuracy: 0.7142 - val_loss: 0.9924 - val_accuracy: 0.7213\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9905 - accuracy: 0.7154 - val_loss: 1.0027 - val_accuracy: 0.7146\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9871 - accuracy: 0.7157 - val_loss: 0.9956 - val_accuracy: 0.7201\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9833 - accuracy: 0.7173 - val_loss: 1.0236 - val_accuracy: 0.7079\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9794 - accuracy: 0.7178 - val_loss: 0.9903 - val_accuracy: 0.7218\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9750 - accuracy: 0.7185 - val_loss: 1.0097 - val_accuracy: 0.7118\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9705 - accuracy: 0.7200 - val_loss: 0.9990 - val_accuracy: 0.7172\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9666 - accuracy: 0.7218 - val_loss: 1.0242 - val_accuracy: 0.7010\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9644 - accuracy: 0.7215 - val_loss: 0.9878 - val_accuracy: 0.7206\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9599 - accuracy: 0.7227 - val_loss: 0.9903 - val_accuracy: 0.7186\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9583 - accuracy: 0.7228 - val_loss: 0.9715 - val_accuracy: 0.7233\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9539 - accuracy: 0.7236 - val_loss: 0.9980 - val_accuracy: 0.7138\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9490 - accuracy: 0.7249 - val_loss: 1.0089 - val_accuracy: 0.7066\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9466 - accuracy: 0.7262 - val_loss: 0.9669 - val_accuracy: 0.7228\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9433 - accuracy: 0.7272 - val_loss: 0.9847 - val_accuracy: 0.7171\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9384 - accuracy: 0.7282 - val_loss: 0.9494 - val_accuracy: 0.7332\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9360 - accuracy: 0.7280 - val_loss: 0.9203 - val_accuracy: 0.7456\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9333 - accuracy: 0.7296 - val_loss: 1.0001 - val_accuracy: 0.7094\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9285 - accuracy: 0.7311 - val_loss: 0.9209 - val_accuracy: 0.7408\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9263 - accuracy: 0.7317 - val_loss: 0.9604 - val_accuracy: 0.7284\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9242 - accuracy: 0.7316 - val_loss: 0.9718 - val_accuracy: 0.7191\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9198 - accuracy: 0.7328 - val_loss: 0.9566 - val_accuracy: 0.7286\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9172 - accuracy: 0.7332 - val_loss: 0.9392 - val_accuracy: 0.7341\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9136 - accuracy: 0.7348 - val_loss: 0.9446 - val_accuracy: 0.7322\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9103 - accuracy: 0.7356 - val_loss: 0.9198 - val_accuracy: 0.7442\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9080 - accuracy: 0.7358 - val_loss: 0.9589 - val_accuracy: 0.7259\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9056 - accuracy: 0.7360 - val_loss: 0.9149 - val_accuracy: 0.7456\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9024 - accuracy: 0.7373 - val_loss: 0.9783 - val_accuracy: 0.7164\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8994 - accuracy: 0.7388 - val_loss: 0.9284 - val_accuracy: 0.7388\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8965 - accuracy: 0.7386 - val_loss: 0.9343 - val_accuracy: 0.7341\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8939 - accuracy: 0.7397 - val_loss: 0.9049 - val_accuracy: 0.7425\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8920 - accuracy: 0.7399 - val_loss: 0.8926 - val_accuracy: 0.7510\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8884 - accuracy: 0.7410 - val_loss: 0.9266 - val_accuracy: 0.7364\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8857 - accuracy: 0.7426 - val_loss: 0.9298 - val_accuracy: 0.7366\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8828 - accuracy: 0.7427 - val_loss: 0.9168 - val_accuracy: 0.7389\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8808 - accuracy: 0.7430 - val_loss: 0.9225 - val_accuracy: 0.7356\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8784 - accuracy: 0.7441 - val_loss: 0.9510 - val_accuracy: 0.7210\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8758 - accuracy: 0.7446 - val_loss: 0.8720 - val_accuracy: 0.7570\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8727 - accuracy: 0.7446 - val_loss: 0.8781 - val_accuracy: 0.7557\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8704 - accuracy: 0.7466 - val_loss: 0.9243 - val_accuracy: 0.7339\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8677 - accuracy: 0.7468 - val_loss: 0.8693 - val_accuracy: 0.7575\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8650 - accuracy: 0.7483 - val_loss: 0.9212 - val_accuracy: 0.7357\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8626 - accuracy: 0.7495 - val_loss: 0.9049 - val_accuracy: 0.7421\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8610 - accuracy: 0.7482 - val_loss: 0.9038 - val_accuracy: 0.7397\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8579 - accuracy: 0.7497 - val_loss: 0.8936 - val_accuracy: 0.7482\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8578 - accuracy: 0.7485 - val_loss: 0.8928 - val_accuracy: 0.7462\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8524 - accuracy: 0.7516 - val_loss: 0.9067 - val_accuracy: 0.7405\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8509 - accuracy: 0.7524 - val_loss: 0.8742 - val_accuracy: 0.7561\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8483 - accuracy: 0.7524 - val_loss: 0.8985 - val_accuracy: 0.7441\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8455 - accuracy: 0.7527 - val_loss: 0.9072 - val_accuracy: 0.7391\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8443 - accuracy: 0.7533 - val_loss: 0.8626 - val_accuracy: 0.7575\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8426 - accuracy: 0.7532 - val_loss: 0.8624 - val_accuracy: 0.7585\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8396 - accuracy: 0.7551 - val_loss: 0.8788 - val_accuracy: 0.7475\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8376 - accuracy: 0.7550 - val_loss: 0.8865 - val_accuracy: 0.7466\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8362 - accuracy: 0.7556 - val_loss: 0.9110 - val_accuracy: 0.7376\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8341 - accuracy: 0.7566 - val_loss: 0.8739 - val_accuracy: 0.7502\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8315 - accuracy: 0.7565 - val_loss: 0.8655 - val_accuracy: 0.7567\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8284 - accuracy: 0.7577 - val_loss: 0.8728 - val_accuracy: 0.7516\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8273 - accuracy: 0.7575 - val_loss: 0.8882 - val_accuracy: 0.7464\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8248 - accuracy: 0.7589 - val_loss: 0.8501 - val_accuracy: 0.7603\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8224 - accuracy: 0.7593 - val_loss: 0.8567 - val_accuracy: 0.7585\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8212 - accuracy: 0.7590 - val_loss: 0.8539 - val_accuracy: 0.7586\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8187 - accuracy: 0.7612 - val_loss: 0.8701 - val_accuracy: 0.7516\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8175 - accuracy: 0.7608 - val_loss: 0.8671 - val_accuracy: 0.7516\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8159 - accuracy: 0.7617 - val_loss: 0.8289 - val_accuracy: 0.7696\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8118 - accuracy: 0.7628 - val_loss: 0.8509 - val_accuracy: 0.7584\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8112 - accuracy: 0.7620 - val_loss: 0.8505 - val_accuracy: 0.7554\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.7620 - val_loss: 0.8353 - val_accuracy: 0.7652\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8077 - accuracy: 0.7635 - val_loss: 0.8282 - val_accuracy: 0.7660\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8056 - accuracy: 0.7638 - val_loss: 0.8460 - val_accuracy: 0.7614\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8039 - accuracy: 0.7648 - val_loss: 0.8513 - val_accuracy: 0.7549\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8020 - accuracy: 0.7652 - val_loss: 0.8116 - val_accuracy: 0.7713\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8011 - accuracy: 0.7657 - val_loss: 0.8390 - val_accuracy: 0.7623\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7972 - accuracy: 0.7670 - val_loss: 0.8360 - val_accuracy: 0.7637\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7976 - accuracy: 0.7652 - val_loss: 0.8246 - val_accuracy: 0.7688\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7933 - accuracy: 0.7679 - val_loss: 0.8195 - val_accuracy: 0.7679\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7935 - accuracy: 0.7676 - val_loss: 0.8297 - val_accuracy: 0.7634\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7910 - accuracy: 0.7691 - val_loss: 0.8184 - val_accuracy: 0.7708\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7892 - accuracy: 0.7690 - val_loss: 0.8054 - val_accuracy: 0.7743\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7869 - accuracy: 0.7686 - val_loss: 0.8299 - val_accuracy: 0.7635\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7850 - accuracy: 0.7701 - val_loss: 0.8062 - val_accuracy: 0.7732\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7842 - accuracy: 0.7697 - val_loss: 0.8129 - val_accuracy: 0.7722\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7816 - accuracy: 0.7710 - val_loss: 0.8658 - val_accuracy: 0.7524\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7799 - accuracy: 0.7707 - val_loss: 0.7991 - val_accuracy: 0.7749\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7773 - accuracy: 0.7727 - val_loss: 0.8047 - val_accuracy: 0.7722\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7759 - accuracy: 0.7729 - val_loss: 0.8122 - val_accuracy: 0.7695\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7750 - accuracy: 0.7735 - val_loss: 0.8169 - val_accuracy: 0.7693\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7728 - accuracy: 0.7748 - val_loss: 0.8154 - val_accuracy: 0.7694\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7714 - accuracy: 0.7733 - val_loss: 0.8159 - val_accuracy: 0.7688\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7708 - accuracy: 0.7745 - val_loss: 0.7986 - val_accuracy: 0.7759\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7682 - accuracy: 0.7743 - val_loss: 0.8465 - val_accuracy: 0.7574\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7664 - accuracy: 0.7758 - val_loss: 0.8013 - val_accuracy: 0.7726\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7648 - accuracy: 0.7763 - val_loss: 0.7967 - val_accuracy: 0.7745\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7628 - accuracy: 0.7764 - val_loss: 0.7893 - val_accuracy: 0.7792\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7618 - accuracy: 0.7772 - val_loss: 0.8031 - val_accuracy: 0.7737\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7616 - accuracy: 0.7768 - val_loss: 0.8034 - val_accuracy: 0.7743\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7580 - accuracy: 0.7784 - val_loss: 0.8023 - val_accuracy: 0.7717\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7585 - accuracy: 0.7784 - val_loss: 0.8000 - val_accuracy: 0.7744\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7551 - accuracy: 0.7799 - val_loss: 0.8212 - val_accuracy: 0.7670\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.7785 - val_loss: 0.7912 - val_accuracy: 0.7784\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7526 - accuracy: 0.7787 - val_loss: 0.7689 - val_accuracy: 0.7856\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7503 - accuracy: 0.7796 - val_loss: 0.7726 - val_accuracy: 0.7832\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7493 - accuracy: 0.7806 - val_loss: 0.8098 - val_accuracy: 0.7674\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7488 - accuracy: 0.7809 - val_loss: 0.8021 - val_accuracy: 0.7734\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7464 - accuracy: 0.7808 - val_loss: 0.8153 - val_accuracy: 0.7676\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7447 - accuracy: 0.7821 - val_loss: 0.7579 - val_accuracy: 0.7892\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7432 - accuracy: 0.7819 - val_loss: 0.7688 - val_accuracy: 0.7829\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7822 - val_loss: 0.8276 - val_accuracy: 0.7619\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7406 - accuracy: 0.7824 - val_loss: 0.7898 - val_accuracy: 0.7771\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7389 - accuracy: 0.7830 - val_loss: 0.7993 - val_accuracy: 0.7729\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7374 - accuracy: 0.7842 - val_loss: 0.7746 - val_accuracy: 0.7816\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7369 - accuracy: 0.7835 - val_loss: 0.7983 - val_accuracy: 0.7701\n",
      "Try 9/100: Best_val_acc: [0.8177157640457153, 0.7600555419921875], lr: 1.0127156731454229e-05, Lambda: 5.742480308769984e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-5.0, -3.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,-3))\n",
    "    best_acc = basicHPCheckFCNN(200, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 653470,
     "status": "ok",
     "timestamp": 1594536339410,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "sdWeveU3_Ji8",
    "outputId": "61fde033-ebb4-4245-db7a-f69d4865499f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3282 - accuracy: 0.1173 - val_loss: 2.3261 - val_accuracy: 0.1302\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2860 - accuracy: 0.1511 - val_loss: 2.2934 - val_accuracy: 0.1529\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2654 - accuracy: 0.1788 - val_loss: 2.2407 - val_accuracy: 0.2025\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2462 - accuracy: 0.2016 - val_loss: 2.2318 - val_accuracy: 0.2178\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2246 - accuracy: 0.2242 - val_loss: 2.2104 - val_accuracy: 0.2616\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1995 - accuracy: 0.2482 - val_loss: 2.1751 - val_accuracy: 0.3031\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1713 - accuracy: 0.2727 - val_loss: 2.1554 - val_accuracy: 0.3115\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1409 - accuracy: 0.2960 - val_loss: 2.1392 - val_accuracy: 0.3183\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1077 - accuracy: 0.3187 - val_loss: 2.0960 - val_accuracy: 0.3544\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0732 - accuracy: 0.3366 - val_loss: 2.0830 - val_accuracy: 0.3516\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0365 - accuracy: 0.3554 - val_loss: 2.0182 - val_accuracy: 0.3899\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0000 - accuracy: 0.3726 - val_loss: 1.9965 - val_accuracy: 0.3931\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9618 - accuracy: 0.3904 - val_loss: 1.9425 - val_accuracy: 0.4206\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9242 - accuracy: 0.4063 - val_loss: 1.9249 - val_accuracy: 0.4424\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8842 - accuracy: 0.4268 - val_loss: 1.8824 - val_accuracy: 0.4596\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8448 - accuracy: 0.4430 - val_loss: 1.8324 - val_accuracy: 0.4859\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8041 - accuracy: 0.4597 - val_loss: 1.7871 - val_accuracy: 0.4972\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7632 - accuracy: 0.4772 - val_loss: 1.7573 - val_accuracy: 0.5110\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7247 - accuracy: 0.4915 - val_loss: 1.6973 - val_accuracy: 0.5387\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6866 - accuracy: 0.5053 - val_loss: 1.6968 - val_accuracy: 0.5194\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6524 - accuracy: 0.5181 - val_loss: 1.6433 - val_accuracy: 0.5556\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6182 - accuracy: 0.5295 - val_loss: 1.6263 - val_accuracy: 0.5556\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5874 - accuracy: 0.5420 - val_loss: 1.6279 - val_accuracy: 0.5407\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5579 - accuracy: 0.5501 - val_loss: 1.6129 - val_accuracy: 0.5510\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5307 - accuracy: 0.5589 - val_loss: 1.5165 - val_accuracy: 0.5951\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5041 - accuracy: 0.5665 - val_loss: 1.5199 - val_accuracy: 0.5809\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4799 - accuracy: 0.5742 - val_loss: 1.4894 - val_accuracy: 0.5946\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4557 - accuracy: 0.5823 - val_loss: 1.4396 - val_accuracy: 0.6164\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4332 - accuracy: 0.5894 - val_loss: 1.4594 - val_accuracy: 0.6019\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4121 - accuracy: 0.5938 - val_loss: 1.4363 - val_accuracy: 0.6035\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3916 - accuracy: 0.5996 - val_loss: 1.4116 - val_accuracy: 0.6029\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3727 - accuracy: 0.6058 - val_loss: 1.3884 - val_accuracy: 0.6209\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3549 - accuracy: 0.6112 - val_loss: 1.3942 - val_accuracy: 0.6108\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3362 - accuracy: 0.6177 - val_loss: 1.3361 - val_accuracy: 0.6344\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3219 - accuracy: 0.6205 - val_loss: 1.3294 - val_accuracy: 0.6377\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3055 - accuracy: 0.6255 - val_loss: 1.3350 - val_accuracy: 0.6311\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2918 - accuracy: 0.6283 - val_loss: 1.2970 - val_accuracy: 0.6434\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2787 - accuracy: 0.6321 - val_loss: 1.3149 - val_accuracy: 0.6335\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2649 - accuracy: 0.6361 - val_loss: 1.2713 - val_accuracy: 0.6531\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2520 - accuracy: 0.6402 - val_loss: 1.2573 - val_accuracy: 0.6563\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2417 - accuracy: 0.6416 - val_loss: 1.2867 - val_accuracy: 0.6416\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2300 - accuracy: 0.6440 - val_loss: 1.2327 - val_accuracy: 0.6626\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2197 - accuracy: 0.6464 - val_loss: 1.2669 - val_accuracy: 0.6424\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2087 - accuracy: 0.6504 - val_loss: 1.2431 - val_accuracy: 0.6481\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1987 - accuracy: 0.6528 - val_loss: 1.2073 - val_accuracy: 0.6676\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1901 - accuracy: 0.6541 - val_loss: 1.2216 - val_accuracy: 0.6592\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1814 - accuracy: 0.6590 - val_loss: 1.1903 - val_accuracy: 0.6707\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1718 - accuracy: 0.6605 - val_loss: 1.1951 - val_accuracy: 0.6697\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1633 - accuracy: 0.6630 - val_loss: 1.1905 - val_accuracy: 0.6651\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1549 - accuracy: 0.6649 - val_loss: 1.1882 - val_accuracy: 0.6686\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1475 - accuracy: 0.6667 - val_loss: 1.1689 - val_accuracy: 0.6739\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1400 - accuracy: 0.6696 - val_loss: 1.1632 - val_accuracy: 0.6726\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1322 - accuracy: 0.6708 - val_loss: 1.1513 - val_accuracy: 0.6784\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1267 - accuracy: 0.6719 - val_loss: 1.1684 - val_accuracy: 0.6666\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1199 - accuracy: 0.6729 - val_loss: 1.1325 - val_accuracy: 0.6836\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1123 - accuracy: 0.6767 - val_loss: 1.1433 - val_accuracy: 0.6758\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1055 - accuracy: 0.6790 - val_loss: 1.1391 - val_accuracy: 0.6811\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0992 - accuracy: 0.6795 - val_loss: 1.1421 - val_accuracy: 0.6761\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0934 - accuracy: 0.6819 - val_loss: 1.0782 - val_accuracy: 0.7008\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0878 - accuracy: 0.6844 - val_loss: 1.1284 - val_accuracy: 0.6784\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0821 - accuracy: 0.6846 - val_loss: 1.0911 - val_accuracy: 0.6954\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0763 - accuracy: 0.6865 - val_loss: 1.1112 - val_accuracy: 0.6879\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0716 - accuracy: 0.6874 - val_loss: 1.0919 - val_accuracy: 0.6932\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0663 - accuracy: 0.6896 - val_loss: 1.1028 - val_accuracy: 0.6892\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0606 - accuracy: 0.6904 - val_loss: 1.0857 - val_accuracy: 0.6929\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0552 - accuracy: 0.6932 - val_loss: 1.0757 - val_accuracy: 0.6975\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0497 - accuracy: 0.6951 - val_loss: 1.0807 - val_accuracy: 0.6932\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0452 - accuracy: 0.6961 - val_loss: 1.0635 - val_accuracy: 0.7021\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0398 - accuracy: 0.6981 - val_loss: 1.0881 - val_accuracy: 0.6905\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0357 - accuracy: 0.6993 - val_loss: 1.0819 - val_accuracy: 0.6960\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0299 - accuracy: 0.7004 - val_loss: 1.0433 - val_accuracy: 0.7042\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0269 - accuracy: 0.7005 - val_loss: 1.0715 - val_accuracy: 0.6969\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0211 - accuracy: 0.7037 - val_loss: 1.0494 - val_accuracy: 0.7047\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0170 - accuracy: 0.7047 - val_loss: 1.0467 - val_accuracy: 0.7085\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0136 - accuracy: 0.7055 - val_loss: 1.0449 - val_accuracy: 0.7100\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0086 - accuracy: 0.7059 - val_loss: 1.0512 - val_accuracy: 0.7043\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0052 - accuracy: 0.7077 - val_loss: 1.0588 - val_accuracy: 0.6971\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0010 - accuracy: 0.7081 - val_loss: 1.0272 - val_accuracy: 0.7122\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9963 - accuracy: 0.7105 - val_loss: 1.0278 - val_accuracy: 0.7115\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9930 - accuracy: 0.7115 - val_loss: 1.0272 - val_accuracy: 0.7117\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9902 - accuracy: 0.7114 - val_loss: 1.0551 - val_accuracy: 0.7018\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9845 - accuracy: 0.7151 - val_loss: 1.0025 - val_accuracy: 0.7193\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9806 - accuracy: 0.7150 - val_loss: 1.0181 - val_accuracy: 0.7164\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9782 - accuracy: 0.7157 - val_loss: 1.0164 - val_accuracy: 0.7163\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9738 - accuracy: 0.7162 - val_loss: 0.9903 - val_accuracy: 0.7245\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9700 - accuracy: 0.7183 - val_loss: 1.0298 - val_accuracy: 0.7054\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9668 - accuracy: 0.7182 - val_loss: 0.9891 - val_accuracy: 0.7230\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9637 - accuracy: 0.7193 - val_loss: 0.9708 - val_accuracy: 0.7318\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9602 - accuracy: 0.7210 - val_loss: 0.9951 - val_accuracy: 0.7218\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9564 - accuracy: 0.7214 - val_loss: 0.9582 - val_accuracy: 0.7351\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9523 - accuracy: 0.7233 - val_loss: 0.9737 - val_accuracy: 0.7274\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9486 - accuracy: 0.7250 - val_loss: 0.9825 - val_accuracy: 0.7230\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9470 - accuracy: 0.7234 - val_loss: 0.9746 - val_accuracy: 0.7274\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9435 - accuracy: 0.7254 - val_loss: 1.0018 - val_accuracy: 0.7149\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9398 - accuracy: 0.7264 - val_loss: 0.9934 - val_accuracy: 0.7181\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9365 - accuracy: 0.7281 - val_loss: 0.9856 - val_accuracy: 0.7211\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9339 - accuracy: 0.7282 - val_loss: 0.9876 - val_accuracy: 0.7205\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9305 - accuracy: 0.7302 - val_loss: 0.9522 - val_accuracy: 0.7336\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9270 - accuracy: 0.7302 - val_loss: 0.9745 - val_accuracy: 0.7250\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9234 - accuracy: 0.7330 - val_loss: 0.9623 - val_accuracy: 0.7286\n",
      "Try 1/100: Best_val_acc: [0.95527583360672, 0.7194444537162781], lr: 1.1123887770229007e-05, Lambda: 6.35471209525405e-05\n",
      "\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3031 - accuracy: 0.1241 - val_loss: 2.2060 - val_accuracy: 0.2255\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.1885 - accuracy: 0.2290 - val_loss: 2.0911 - val_accuracy: 0.2751\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.9985 - accuracy: 0.3446 - val_loss: 1.8748 - val_accuracy: 0.4124\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7628 - accuracy: 0.4426 - val_loss: 1.7081 - val_accuracy: 0.4681\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5604 - accuracy: 0.5220 - val_loss: 1.5804 - val_accuracy: 0.5067\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4277 - accuracy: 0.5680 - val_loss: 1.4587 - val_accuracy: 0.5448\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3306 - accuracy: 0.6020 - val_loss: 1.4198 - val_accuracy: 0.5694\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2644 - accuracy: 0.6218 - val_loss: 1.2795 - val_accuracy: 0.6249\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1999 - accuracy: 0.6436 - val_loss: 1.2473 - val_accuracy: 0.6236\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1507 - accuracy: 0.6580 - val_loss: 1.2127 - val_accuracy: 0.6345\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1144 - accuracy: 0.6684 - val_loss: 1.1399 - val_accuracy: 0.6674\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0810 - accuracy: 0.6760 - val_loss: 1.1205 - val_accuracy: 0.6686\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0496 - accuracy: 0.6867 - val_loss: 1.0164 - val_accuracy: 0.7111\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0177 - accuracy: 0.6975 - val_loss: 1.1356 - val_accuracy: 0.6567\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9953 - accuracy: 0.7037 - val_loss: 1.0217 - val_accuracy: 0.7022\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9696 - accuracy: 0.7116 - val_loss: 0.9964 - val_accuracy: 0.7076\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9466 - accuracy: 0.7180 - val_loss: 1.0541 - val_accuracy: 0.6875\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9285 - accuracy: 0.7225 - val_loss: 0.9234 - val_accuracy: 0.7381\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9152 - accuracy: 0.7256 - val_loss: 0.9898 - val_accuracy: 0.6969\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8950 - accuracy: 0.7334 - val_loss: 0.9082 - val_accuracy: 0.7389\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8783 - accuracy: 0.7367 - val_loss: 0.9384 - val_accuracy: 0.7231\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8651 - accuracy: 0.7402 - val_loss: 0.9010 - val_accuracy: 0.7379\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8521 - accuracy: 0.7455 - val_loss: 0.9343 - val_accuracy: 0.7276\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8323 - accuracy: 0.7515 - val_loss: 0.9465 - val_accuracy: 0.7246\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8185 - accuracy: 0.7542 - val_loss: 0.9053 - val_accuracy: 0.7417\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8084 - accuracy: 0.7585 - val_loss: 0.8614 - val_accuracy: 0.7524\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7940 - accuracy: 0.7638 - val_loss: 0.9271 - val_accuracy: 0.7266\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7888 - accuracy: 0.7627 - val_loss: 0.7584 - val_accuracy: 0.7839\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7672 - accuracy: 0.7705 - val_loss: 0.8362 - val_accuracy: 0.7611\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7650 - accuracy: 0.7728 - val_loss: 0.8383 - val_accuracy: 0.7576\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7566 - accuracy: 0.7741 - val_loss: 0.8642 - val_accuracy: 0.7409\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7406 - accuracy: 0.7801 - val_loss: 0.8262 - val_accuracy: 0.7629\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7375 - accuracy: 0.7775 - val_loss: 0.8743 - val_accuracy: 0.7494\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7269 - accuracy: 0.7819 - val_loss: 0.8100 - val_accuracy: 0.7596\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7099 - accuracy: 0.7875 - val_loss: 0.6944 - val_accuracy: 0.8019\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6995 - accuracy: 0.7911 - val_loss: 0.8055 - val_accuracy: 0.7617\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6988 - accuracy: 0.7916 - val_loss: 0.7108 - val_accuracy: 0.7962\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6825 - accuracy: 0.7961 - val_loss: 0.7437 - val_accuracy: 0.7865\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6811 - accuracy: 0.7967 - val_loss: 0.7095 - val_accuracy: 0.7971\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6669 - accuracy: 0.8013 - val_loss: 0.6720 - val_accuracy: 0.8101\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6579 - accuracy: 0.8025 - val_loss: 0.7233 - val_accuracy: 0.7889\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6550 - accuracy: 0.8037 - val_loss: 0.7116 - val_accuracy: 0.7976\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6445 - accuracy: 0.8080 - val_loss: 0.7141 - val_accuracy: 0.7941\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6352 - accuracy: 0.8122 - val_loss: 0.6315 - val_accuracy: 0.8204\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6319 - accuracy: 0.8111 - val_loss: 0.6846 - val_accuracy: 0.8005\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6212 - accuracy: 0.8148 - val_loss: 0.6727 - val_accuracy: 0.8061\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6118 - accuracy: 0.8164 - val_loss: 0.6910 - val_accuracy: 0.8019\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6110 - accuracy: 0.8175 - val_loss: 0.7081 - val_accuracy: 0.7969\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6012 - accuracy: 0.8210 - val_loss: 0.6067 - val_accuracy: 0.8230\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6021 - accuracy: 0.8179 - val_loss: 0.7242 - val_accuracy: 0.7866\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5994 - accuracy: 0.8201 - val_loss: 0.6192 - val_accuracy: 0.8220\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5873 - accuracy: 0.8252 - val_loss: 0.6742 - val_accuracy: 0.8051\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5799 - accuracy: 0.8278 - val_loss: 0.6923 - val_accuracy: 0.7999\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5778 - accuracy: 0.8259 - val_loss: 0.6457 - val_accuracy: 0.8144\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5636 - accuracy: 0.8314 - val_loss: 0.6462 - val_accuracy: 0.8139\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5745 - accuracy: 0.8279 - val_loss: 0.5977 - val_accuracy: 0.8269\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5605 - accuracy: 0.8338 - val_loss: 0.6268 - val_accuracy: 0.8174\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5544 - accuracy: 0.8339 - val_loss: 0.6148 - val_accuracy: 0.8261\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5509 - accuracy: 0.8354 - val_loss: 0.6102 - val_accuracy: 0.8271\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5408 - accuracy: 0.8393 - val_loss: 0.6701 - val_accuracy: 0.8096\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5408 - accuracy: 0.8399 - val_loss: 0.5597 - val_accuracy: 0.8397\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5448 - accuracy: 0.8360 - val_loss: 0.6564 - val_accuracy: 0.8108\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5337 - accuracy: 0.8424 - val_loss: 0.5718 - val_accuracy: 0.8376\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5292 - accuracy: 0.8413 - val_loss: 0.6074 - val_accuracy: 0.8262\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5222 - accuracy: 0.8433 - val_loss: 0.6022 - val_accuracy: 0.8274\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5132 - accuracy: 0.8465 - val_loss: 0.5930 - val_accuracy: 0.8324\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5094 - accuracy: 0.8480 - val_loss: 0.5974 - val_accuracy: 0.8283\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5114 - accuracy: 0.8473 - val_loss: 0.5535 - val_accuracy: 0.8439\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5054 - accuracy: 0.8480 - val_loss: 0.5517 - val_accuracy: 0.8420\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5054 - accuracy: 0.8496 - val_loss: 0.6054 - val_accuracy: 0.8236\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4971 - accuracy: 0.8524 - val_loss: 0.5517 - val_accuracy: 0.8408\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4891 - accuracy: 0.8528 - val_loss: 0.6221 - val_accuracy: 0.8206\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4889 - accuracy: 0.8544 - val_loss: 0.5421 - val_accuracy: 0.8441\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4814 - accuracy: 0.8552 - val_loss: 0.5252 - val_accuracy: 0.8510\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4744 - accuracy: 0.8593 - val_loss: 0.4961 - val_accuracy: 0.8590\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4784 - accuracy: 0.8575 - val_loss: 0.5907 - val_accuracy: 0.8272\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4746 - accuracy: 0.8599 - val_loss: 0.5409 - val_accuracy: 0.8456\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4666 - accuracy: 0.8598 - val_loss: 0.5694 - val_accuracy: 0.8374\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4665 - accuracy: 0.8600 - val_loss: 0.5188 - val_accuracy: 0.8526\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4589 - accuracy: 0.8631 - val_loss: 0.5107 - val_accuracy: 0.8534\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4569 - accuracy: 0.8634 - val_loss: 0.6313 - val_accuracy: 0.8151\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4558 - accuracy: 0.8634 - val_loss: 0.5334 - val_accuracy: 0.8471\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4564 - accuracy: 0.8635 - val_loss: 0.5234 - val_accuracy: 0.8495\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4481 - accuracy: 0.8655 - val_loss: 0.5132 - val_accuracy: 0.8532\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4445 - accuracy: 0.8676 - val_loss: 0.4879 - val_accuracy: 0.8630\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4402 - accuracy: 0.8689 - val_loss: 0.5005 - val_accuracy: 0.8605\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4355 - accuracy: 0.8690 - val_loss: 0.5364 - val_accuracy: 0.8487\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4309 - accuracy: 0.8710 - val_loss: 0.5254 - val_accuracy: 0.8527\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4267 - accuracy: 0.8716 - val_loss: 0.4966 - val_accuracy: 0.8590\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4252 - accuracy: 0.8724 - val_loss: 0.5069 - val_accuracy: 0.8584\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4203 - accuracy: 0.8751 - val_loss: 0.5170 - val_accuracy: 0.8534\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4227 - accuracy: 0.8741 - val_loss: 0.4671 - val_accuracy: 0.8708\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4127 - accuracy: 0.8777 - val_loss: 0.4829 - val_accuracy: 0.8628\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4150 - accuracy: 0.8768 - val_loss: 0.4991 - val_accuracy: 0.8577\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4129 - accuracy: 0.8761 - val_loss: 0.4773 - val_accuracy: 0.8612\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4078 - accuracy: 0.8783 - val_loss: 0.5241 - val_accuracy: 0.8506\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4056 - accuracy: 0.8788 - val_loss: 0.5004 - val_accuracy: 0.8600\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4045 - accuracy: 0.8790 - val_loss: 0.4517 - val_accuracy: 0.8737\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3974 - accuracy: 0.8816 - val_loss: 0.4511 - val_accuracy: 0.8737\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4005 - accuracy: 0.8796 - val_loss: 0.5865 - val_accuracy: 0.8311\n",
      "Try 2/100: Best_val_acc: [0.664990246295929, 0.8119999766349792], lr: 9.569243866395219e-05, Lambda: 1.404906680567885e-05\n",
      "\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2853 - accuracy: 0.1419 - val_loss: 2.1470 - val_accuracy: 0.3626\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1759 - accuracy: 0.2439 - val_loss: 1.9923 - val_accuracy: 0.4449\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0260 - accuracy: 0.3289 - val_loss: 1.8706 - val_accuracy: 0.4153\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8652 - accuracy: 0.4098 - val_loss: 1.6885 - val_accuracy: 0.5081\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7183 - accuracy: 0.4657 - val_loss: 1.5537 - val_accuracy: 0.5761\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5982 - accuracy: 0.5113 - val_loss: 1.4783 - val_accuracy: 0.5769\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5000 - accuracy: 0.5477 - val_loss: 1.4470 - val_accuracy: 0.5811\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4222 - accuracy: 0.5768 - val_loss: 1.4489 - val_accuracy: 0.5759\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3543 - accuracy: 0.6016 - val_loss: 1.3161 - val_accuracy: 0.6226\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3013 - accuracy: 0.6177 - val_loss: 1.2547 - val_accuracy: 0.6322\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2593 - accuracy: 0.6292 - val_loss: 1.2821 - val_accuracy: 0.6320\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2241 - accuracy: 0.6387 - val_loss: 1.2471 - val_accuracy: 0.6394\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1883 - accuracy: 0.6529 - val_loss: 1.2504 - val_accuracy: 0.6459\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1593 - accuracy: 0.6628 - val_loss: 1.2149 - val_accuracy: 0.6494\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1378 - accuracy: 0.6659 - val_loss: 1.1454 - val_accuracy: 0.6714\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1119 - accuracy: 0.6745 - val_loss: 1.1118 - val_accuracy: 0.6787\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0921 - accuracy: 0.6800 - val_loss: 1.2089 - val_accuracy: 0.6446\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0736 - accuracy: 0.6848 - val_loss: 1.0975 - val_accuracy: 0.6860\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0534 - accuracy: 0.6905 - val_loss: 1.0704 - val_accuracy: 0.6940\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0365 - accuracy: 0.6969 - val_loss: 1.1354 - val_accuracy: 0.6687\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0259 - accuracy: 0.6972 - val_loss: 1.0878 - val_accuracy: 0.6854\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0081 - accuracy: 0.7022 - val_loss: 1.0730 - val_accuracy: 0.6941\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9982 - accuracy: 0.7061 - val_loss: 1.0864 - val_accuracy: 0.6833\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9831 - accuracy: 0.7093 - val_loss: 1.0607 - val_accuracy: 0.6907\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9697 - accuracy: 0.7135 - val_loss: 1.0392 - val_accuracy: 0.6989\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9595 - accuracy: 0.7151 - val_loss: 0.9472 - val_accuracy: 0.7266\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9443 - accuracy: 0.7218 - val_loss: 0.9924 - val_accuracy: 0.7137\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9372 - accuracy: 0.7234 - val_loss: 0.9641 - val_accuracy: 0.7275\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9234 - accuracy: 0.7274 - val_loss: 0.9895 - val_accuracy: 0.7153\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9173 - accuracy: 0.7270 - val_loss: 0.9485 - val_accuracy: 0.7237\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9065 - accuracy: 0.7300 - val_loss: 0.9897 - val_accuracy: 0.7104\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8977 - accuracy: 0.7343 - val_loss: 0.9504 - val_accuracy: 0.7217\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8840 - accuracy: 0.7390 - val_loss: 0.9632 - val_accuracy: 0.7219\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8789 - accuracy: 0.7390 - val_loss: 0.9634 - val_accuracy: 0.7208\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8704 - accuracy: 0.7420 - val_loss: 0.9072 - val_accuracy: 0.7414\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8622 - accuracy: 0.7441 - val_loss: 0.8668 - val_accuracy: 0.7541\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8528 - accuracy: 0.7469 - val_loss: 0.8971 - val_accuracy: 0.7426\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8426 - accuracy: 0.7507 - val_loss: 0.9383 - val_accuracy: 0.7290\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8376 - accuracy: 0.7493 - val_loss: 0.9225 - val_accuracy: 0.7344\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8298 - accuracy: 0.7528 - val_loss: 0.9257 - val_accuracy: 0.7349\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8237 - accuracy: 0.7555 - val_loss: 0.9048 - val_accuracy: 0.7437\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8146 - accuracy: 0.7570 - val_loss: 0.8194 - val_accuracy: 0.7628\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8076 - accuracy: 0.7587 - val_loss: 0.9221 - val_accuracy: 0.7350\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7983 - accuracy: 0.7632 - val_loss: 0.8165 - val_accuracy: 0.7678\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7925 - accuracy: 0.7651 - val_loss: 0.8117 - val_accuracy: 0.7643\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7885 - accuracy: 0.7635 - val_loss: 0.8844 - val_accuracy: 0.7519\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7791 - accuracy: 0.7681 - val_loss: 0.9194 - val_accuracy: 0.7360\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7738 - accuracy: 0.7690 - val_loss: 0.8189 - val_accuracy: 0.7635\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7665 - accuracy: 0.7719 - val_loss: 0.8435 - val_accuracy: 0.7574\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7585 - accuracy: 0.7746 - val_loss: 0.8217 - val_accuracy: 0.7663\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7525 - accuracy: 0.7765 - val_loss: 0.8218 - val_accuracy: 0.7688\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7509 - accuracy: 0.7772 - val_loss: 0.7958 - val_accuracy: 0.7740\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7412 - accuracy: 0.7790 - val_loss: 0.7839 - val_accuracy: 0.7751\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7370 - accuracy: 0.7802 - val_loss: 0.8843 - val_accuracy: 0.7461\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7344 - accuracy: 0.7809 - val_loss: 0.8752 - val_accuracy: 0.7507\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7259 - accuracy: 0.7837 - val_loss: 0.7594 - val_accuracy: 0.7864\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7198 - accuracy: 0.7856 - val_loss: 0.7474 - val_accuracy: 0.7880\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7131 - accuracy: 0.7880 - val_loss: 0.7611 - val_accuracy: 0.7836\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7101 - accuracy: 0.7894 - val_loss: 0.7164 - val_accuracy: 0.7946\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7067 - accuracy: 0.7887 - val_loss: 0.7554 - val_accuracy: 0.7836\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7037 - accuracy: 0.7900 - val_loss: 0.7753 - val_accuracy: 0.7778\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6953 - accuracy: 0.7947 - val_loss: 0.7188 - val_accuracy: 0.7974\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6878 - accuracy: 0.7965 - val_loss: 0.7819 - val_accuracy: 0.7764\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6848 - accuracy: 0.7971 - val_loss: 0.8412 - val_accuracy: 0.7554\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6823 - accuracy: 0.7975 - val_loss: 0.7516 - val_accuracy: 0.7848\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6766 - accuracy: 0.7992 - val_loss: 0.7392 - val_accuracy: 0.7899\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6707 - accuracy: 0.8008 - val_loss: 0.7576 - val_accuracy: 0.7840\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6680 - accuracy: 0.8025 - val_loss: 0.7041 - val_accuracy: 0.8011\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6618 - accuracy: 0.8030 - val_loss: 0.7212 - val_accuracy: 0.7899\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6626 - accuracy: 0.8035 - val_loss: 0.6865 - val_accuracy: 0.8076\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6541 - accuracy: 0.8056 - val_loss: 0.8204 - val_accuracy: 0.7644\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6530 - accuracy: 0.8072 - val_loss: 0.7028 - val_accuracy: 0.8029\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6462 - accuracy: 0.8080 - val_loss: 0.7218 - val_accuracy: 0.7932\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6439 - accuracy: 0.8091 - val_loss: 0.7094 - val_accuracy: 0.8009\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6433 - accuracy: 0.8089 - val_loss: 0.6443 - val_accuracy: 0.8174\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6355 - accuracy: 0.8125 - val_loss: 0.7467 - val_accuracy: 0.7843\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6340 - accuracy: 0.8116 - val_loss: 0.7569 - val_accuracy: 0.7804\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6262 - accuracy: 0.8148 - val_loss: 0.6981 - val_accuracy: 0.8031\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6272 - accuracy: 0.8133 - val_loss: 0.7342 - val_accuracy: 0.7890\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6289 - accuracy: 0.8113 - val_loss: 0.6654 - val_accuracy: 0.8129\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6209 - accuracy: 0.8153 - val_loss: 0.7068 - val_accuracy: 0.7986\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6146 - accuracy: 0.8181 - val_loss: 0.7213 - val_accuracy: 0.7939\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6116 - accuracy: 0.8184 - val_loss: 0.6712 - val_accuracy: 0.8090\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6072 - accuracy: 0.8201 - val_loss: 0.6843 - val_accuracy: 0.8071\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6052 - accuracy: 0.8207 - val_loss: 0.6771 - val_accuracy: 0.8059\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6001 - accuracy: 0.8232 - val_loss: 0.6633 - val_accuracy: 0.8121\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5947 - accuracy: 0.8249 - val_loss: 0.6643 - val_accuracy: 0.8121\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5986 - accuracy: 0.8225 - val_loss: 0.6156 - val_accuracy: 0.8238\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5927 - accuracy: 0.8250 - val_loss: 0.7078 - val_accuracy: 0.7980\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5864 - accuracy: 0.8259 - val_loss: 0.6232 - val_accuracy: 0.8233\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5849 - accuracy: 0.8264 - val_loss: 0.6631 - val_accuracy: 0.8100\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5820 - accuracy: 0.8278 - val_loss: 0.7182 - val_accuracy: 0.7934\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5759 - accuracy: 0.8300 - val_loss: 0.6664 - val_accuracy: 0.8129\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5752 - accuracy: 0.8315 - val_loss: 0.6224 - val_accuracy: 0.8236\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5717 - accuracy: 0.8308 - val_loss: 0.6755 - val_accuracy: 0.8057\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5710 - accuracy: 0.8320 - val_loss: 0.6151 - val_accuracy: 0.8243\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5655 - accuracy: 0.8321 - val_loss: 0.6272 - val_accuracy: 0.8205\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5596 - accuracy: 0.8343 - val_loss: 0.5812 - val_accuracy: 0.8374\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5594 - accuracy: 0.8354 - val_loss: 0.6078 - val_accuracy: 0.8288\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5548 - accuracy: 0.8361 - val_loss: 0.6795 - val_accuracy: 0.8045\n",
      "Try 3/100: Best_val_acc: [0.7082420587539673, 0.7937777638435364], lr: 4.9168629613697246e-05, Lambda: 2.6991000919530432e-05\n",
      "\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3256 - accuracy: 0.1184 - val_loss: 2.2656 - val_accuracy: 0.2256\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2668 - accuracy: 0.1805 - val_loss: 2.2624 - val_accuracy: 0.2189\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2210 - accuracy: 0.2339 - val_loss: 2.1812 - val_accuracy: 0.3733\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1601 - accuracy: 0.2798 - val_loss: 2.1109 - val_accuracy: 0.3984\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0847 - accuracy: 0.3214 - val_loss: 2.0921 - val_accuracy: 0.3902\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0031 - accuracy: 0.3515 - val_loss: 1.9573 - val_accuracy: 0.4110\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9215 - accuracy: 0.3826 - val_loss: 1.8948 - val_accuracy: 0.4739\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8385 - accuracy: 0.4230 - val_loss: 1.8311 - val_accuracy: 0.4872\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7559 - accuracy: 0.4611 - val_loss: 1.7456 - val_accuracy: 0.5215\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6760 - accuracy: 0.4936 - val_loss: 1.6923 - val_accuracy: 0.5094\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6011 - accuracy: 0.5215 - val_loss: 1.6085 - val_accuracy: 0.5409\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5383 - accuracy: 0.5397 - val_loss: 1.5065 - val_accuracy: 0.5865\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4773 - accuracy: 0.5616 - val_loss: 1.4876 - val_accuracy: 0.5684\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4247 - accuracy: 0.5778 - val_loss: 1.4114 - val_accuracy: 0.5993\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3839 - accuracy: 0.5897 - val_loss: 1.4590 - val_accuracy: 0.5569\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3461 - accuracy: 0.6000 - val_loss: 1.3859 - val_accuracy: 0.5789\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3147 - accuracy: 0.6079 - val_loss: 1.2950 - val_accuracy: 0.6227\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2863 - accuracy: 0.6165 - val_loss: 1.3215 - val_accuracy: 0.6054\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2610 - accuracy: 0.6255 - val_loss: 1.2725 - val_accuracy: 0.6191\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2369 - accuracy: 0.6320 - val_loss: 1.2413 - val_accuracy: 0.6294\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2216 - accuracy: 0.6354 - val_loss: 1.3231 - val_accuracy: 0.5899\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2032 - accuracy: 0.6404 - val_loss: 1.3279 - val_accuracy: 0.5831\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1847 - accuracy: 0.6473 - val_loss: 1.2498 - val_accuracy: 0.6286\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1701 - accuracy: 0.6527 - val_loss: 1.2410 - val_accuracy: 0.6261\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1557 - accuracy: 0.6569 - val_loss: 1.1992 - val_accuracy: 0.6436\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1428 - accuracy: 0.6607 - val_loss: 1.1301 - val_accuracy: 0.6669\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1298 - accuracy: 0.6637 - val_loss: 1.1836 - val_accuracy: 0.6442\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1173 - accuracy: 0.6685 - val_loss: 1.1774 - val_accuracy: 0.6407\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1065 - accuracy: 0.6702 - val_loss: 1.1766 - val_accuracy: 0.6458\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0956 - accuracy: 0.6753 - val_loss: 1.1765 - val_accuracy: 0.6477\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0873 - accuracy: 0.6770 - val_loss: 1.1918 - val_accuracy: 0.6323\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0751 - accuracy: 0.6807 - val_loss: 1.1319 - val_accuracy: 0.6677\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0682 - accuracy: 0.6817 - val_loss: 1.1327 - val_accuracy: 0.6571\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0576 - accuracy: 0.6866 - val_loss: 1.0907 - val_accuracy: 0.6714\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0535 - accuracy: 0.6860 - val_loss: 1.0881 - val_accuracy: 0.6809\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0429 - accuracy: 0.6895 - val_loss: 1.0839 - val_accuracy: 0.6786\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0353 - accuracy: 0.6922 - val_loss: 1.0717 - val_accuracy: 0.6854\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0280 - accuracy: 0.6945 - val_loss: 1.0764 - val_accuracy: 0.6769\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0183 - accuracy: 0.6978 - val_loss: 1.0728 - val_accuracy: 0.6830\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0114 - accuracy: 0.6999 - val_loss: 1.0919 - val_accuracy: 0.6739\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0059 - accuracy: 0.7000 - val_loss: 1.0478 - val_accuracy: 0.6880\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9989 - accuracy: 0.7049 - val_loss: 1.0864 - val_accuracy: 0.6721\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9889 - accuracy: 0.7058 - val_loss: 1.0534 - val_accuracy: 0.6895\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9864 - accuracy: 0.7071 - val_loss: 1.0091 - val_accuracy: 0.7033\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9776 - accuracy: 0.7101 - val_loss: 1.0421 - val_accuracy: 0.6892\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9735 - accuracy: 0.7106 - val_loss: 1.0181 - val_accuracy: 0.6987\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9623 - accuracy: 0.7146 - val_loss: 0.9862 - val_accuracy: 0.7121\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9602 - accuracy: 0.7150 - val_loss: 0.9996 - val_accuracy: 0.7045\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9542 - accuracy: 0.7171 - val_loss: 0.9875 - val_accuracy: 0.7101\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9456 - accuracy: 0.7190 - val_loss: 0.9922 - val_accuracy: 0.7103\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9391 - accuracy: 0.7218 - val_loss: 1.0051 - val_accuracy: 0.6994\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9343 - accuracy: 0.7225 - val_loss: 0.9869 - val_accuracy: 0.7061\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9293 - accuracy: 0.7250 - val_loss: 0.9954 - val_accuracy: 0.7051\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9257 - accuracy: 0.7259 - val_loss: 0.9810 - val_accuracy: 0.7089\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9180 - accuracy: 0.7278 - val_loss: 0.9994 - val_accuracy: 0.7049\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9121 - accuracy: 0.7303 - val_loss: 0.9629 - val_accuracy: 0.7194\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9077 - accuracy: 0.7311 - val_loss: 0.9621 - val_accuracy: 0.7148\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9000 - accuracy: 0.7334 - val_loss: 0.9378 - val_accuracy: 0.7220\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8975 - accuracy: 0.7337 - val_loss: 1.0168 - val_accuracy: 0.6963\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8941 - accuracy: 0.7352 - val_loss: 0.9239 - val_accuracy: 0.7301\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8866 - accuracy: 0.7370 - val_loss: 0.9623 - val_accuracy: 0.7126\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8814 - accuracy: 0.7400 - val_loss: 0.9043 - val_accuracy: 0.7336\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8774 - accuracy: 0.7386 - val_loss: 0.9769 - val_accuracy: 0.7066\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8707 - accuracy: 0.7407 - val_loss: 0.9041 - val_accuracy: 0.7382\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8662 - accuracy: 0.7433 - val_loss: 0.8937 - val_accuracy: 0.7384\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8619 - accuracy: 0.7450 - val_loss: 0.9193 - val_accuracy: 0.7302\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8570 - accuracy: 0.7469 - val_loss: 0.9199 - val_accuracy: 0.7273\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8520 - accuracy: 0.7490 - val_loss: 0.9535 - val_accuracy: 0.7141\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8476 - accuracy: 0.7493 - val_loss: 0.8994 - val_accuracy: 0.7391\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8432 - accuracy: 0.7494 - val_loss: 0.8794 - val_accuracy: 0.7408\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8386 - accuracy: 0.7528 - val_loss: 0.9301 - val_accuracy: 0.7254\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8333 - accuracy: 0.7526 - val_loss: 0.9073 - val_accuracy: 0.7334\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8276 - accuracy: 0.7544 - val_loss: 0.9035 - val_accuracy: 0.7300\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8233 - accuracy: 0.7561 - val_loss: 0.8690 - val_accuracy: 0.7517\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8215 - accuracy: 0.7575 - val_loss: 0.9031 - val_accuracy: 0.7339\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8147 - accuracy: 0.7585 - val_loss: 0.8753 - val_accuracy: 0.7427\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8116 - accuracy: 0.7598 - val_loss: 0.8642 - val_accuracy: 0.7459\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8097 - accuracy: 0.7609 - val_loss: 0.9033 - val_accuracy: 0.7359\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8057 - accuracy: 0.7615 - val_loss: 0.8890 - val_accuracy: 0.7366\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7974 - accuracy: 0.7645 - val_loss: 0.8548 - val_accuracy: 0.7494\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7937 - accuracy: 0.7649 - val_loss: 0.8549 - val_accuracy: 0.7511\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7906 - accuracy: 0.7665 - val_loss: 0.8515 - val_accuracy: 0.7538\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7875 - accuracy: 0.7676 - val_loss: 0.8883 - val_accuracy: 0.7339\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7839 - accuracy: 0.7685 - val_loss: 0.8550 - val_accuracy: 0.7515\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7791 - accuracy: 0.7696 - val_loss: 0.8170 - val_accuracy: 0.7682\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7755 - accuracy: 0.7701 - val_loss: 0.8292 - val_accuracy: 0.7576\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7716 - accuracy: 0.7718 - val_loss: 0.8579 - val_accuracy: 0.7503\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7670 - accuracy: 0.7732 - val_loss: 0.7794 - val_accuracy: 0.7785\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7638 - accuracy: 0.7747 - val_loss: 0.8538 - val_accuracy: 0.7489\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7597 - accuracy: 0.7764 - val_loss: 0.8620 - val_accuracy: 0.7455\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7544 - accuracy: 0.7774 - val_loss: 0.7977 - val_accuracy: 0.7681\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7538 - accuracy: 0.7783 - val_loss: 0.8113 - val_accuracy: 0.7659\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7503 - accuracy: 0.7804 - val_loss: 0.8282 - val_accuracy: 0.7586\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7451 - accuracy: 0.7813 - val_loss: 0.8234 - val_accuracy: 0.7617\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7423 - accuracy: 0.7814 - val_loss: 0.7898 - val_accuracy: 0.7712\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7383 - accuracy: 0.7837 - val_loss: 0.7688 - val_accuracy: 0.7794\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7371 - accuracy: 0.7837 - val_loss: 0.8550 - val_accuracy: 0.7486\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7301 - accuracy: 0.7860 - val_loss: 0.7887 - val_accuracy: 0.7739\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7285 - accuracy: 0.7860 - val_loss: 0.7859 - val_accuracy: 0.7760\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7256 - accuracy: 0.7870 - val_loss: 0.7814 - val_accuracy: 0.7713\n",
      "Try 4/100: Best_val_acc: [0.8154158592224121, 0.761222243309021], lr: 2.5290886646866885e-05, Lambda: 6.74344996532513e-05\n",
      "\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3018 - accuracy: 0.1192 - val_loss: 2.2412 - val_accuracy: 0.2245\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2510 - accuracy: 0.1852 - val_loss: 2.2104 - val_accuracy: 0.2287\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1829 - accuracy: 0.2645 - val_loss: 2.0886 - val_accuracy: 0.4176\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0870 - accuracy: 0.3262 - val_loss: 1.9899 - val_accuracy: 0.4743\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9687 - accuracy: 0.3790 - val_loss: 1.8512 - val_accuracy: 0.5273\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8485 - accuracy: 0.4290 - val_loss: 1.7757 - val_accuracy: 0.5299\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7354 - accuracy: 0.4685 - val_loss: 1.6796 - val_accuracy: 0.5318\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6331 - accuracy: 0.5077 - val_loss: 1.5783 - val_accuracy: 0.5709\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5516 - accuracy: 0.5356 - val_loss: 1.4880 - val_accuracy: 0.5954\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4771 - accuracy: 0.5652 - val_loss: 1.3866 - val_accuracy: 0.6349\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4162 - accuracy: 0.5842 - val_loss: 1.3720 - val_accuracy: 0.6099\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3667 - accuracy: 0.5998 - val_loss: 1.3885 - val_accuracy: 0.6030\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3232 - accuracy: 0.6123 - val_loss: 1.3095 - val_accuracy: 0.6325\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2875 - accuracy: 0.6202 - val_loss: 1.2584 - val_accuracy: 0.6552\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2545 - accuracy: 0.6313 - val_loss: 1.2571 - val_accuracy: 0.6513\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2285 - accuracy: 0.6390 - val_loss: 1.2664 - val_accuracy: 0.6372\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2048 - accuracy: 0.6427 - val_loss: 1.1731 - val_accuracy: 0.6780\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1805 - accuracy: 0.6500 - val_loss: 1.2072 - val_accuracy: 0.6514\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1587 - accuracy: 0.6577 - val_loss: 1.1704 - val_accuracy: 0.6695\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1396 - accuracy: 0.6626 - val_loss: 1.1657 - val_accuracy: 0.6619\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1227 - accuracy: 0.6667 - val_loss: 1.0955 - val_accuracy: 0.6953\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1058 - accuracy: 0.6724 - val_loss: 1.1465 - val_accuracy: 0.6756\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0923 - accuracy: 0.6755 - val_loss: 1.1051 - val_accuracy: 0.6891\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0729 - accuracy: 0.6822 - val_loss: 1.0982 - val_accuracy: 0.6907\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0633 - accuracy: 0.6859 - val_loss: 1.0492 - val_accuracy: 0.7086\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0483 - accuracy: 0.6889 - val_loss: 1.0932 - val_accuracy: 0.6924\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0361 - accuracy: 0.6912 - val_loss: 1.0645 - val_accuracy: 0.6983\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0216 - accuracy: 0.6982 - val_loss: 1.0505 - val_accuracy: 0.6993\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0120 - accuracy: 0.7011 - val_loss: 1.0217 - val_accuracy: 0.7090\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0035 - accuracy: 0.7018 - val_loss: 1.0210 - val_accuracy: 0.7155\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9883 - accuracy: 0.7082 - val_loss: 1.0049 - val_accuracy: 0.7203\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9782 - accuracy: 0.7118 - val_loss: 1.0170 - val_accuracy: 0.7098\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9680 - accuracy: 0.7129 - val_loss: 0.9796 - val_accuracy: 0.7221\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9586 - accuracy: 0.7187 - val_loss: 0.9881 - val_accuracy: 0.7157\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9494 - accuracy: 0.7208 - val_loss: 1.0086 - val_accuracy: 0.7126\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9372 - accuracy: 0.7239 - val_loss: 0.9868 - val_accuracy: 0.7202\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9323 - accuracy: 0.7251 - val_loss: 0.9593 - val_accuracy: 0.7298\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9216 - accuracy: 0.7300 - val_loss: 0.9815 - val_accuracy: 0.7201\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9136 - accuracy: 0.7307 - val_loss: 0.9596 - val_accuracy: 0.7246\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9039 - accuracy: 0.7352 - val_loss: 0.9816 - val_accuracy: 0.7206\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8949 - accuracy: 0.7375 - val_loss: 0.9185 - val_accuracy: 0.7420\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8880 - accuracy: 0.7382 - val_loss: 0.9706 - val_accuracy: 0.7114\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8786 - accuracy: 0.7420 - val_loss: 0.9424 - val_accuracy: 0.7338\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8733 - accuracy: 0.7422 - val_loss: 0.9762 - val_accuracy: 0.7160\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8634 - accuracy: 0.7469 - val_loss: 0.8901 - val_accuracy: 0.7498\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8587 - accuracy: 0.7475 - val_loss: 0.9208 - val_accuracy: 0.7362\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8545 - accuracy: 0.7500 - val_loss: 0.8545 - val_accuracy: 0.7622\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8450 - accuracy: 0.7508 - val_loss: 0.8882 - val_accuracy: 0.7491\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8378 - accuracy: 0.7529 - val_loss: 0.8294 - val_accuracy: 0.7644\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8327 - accuracy: 0.7543 - val_loss: 0.8695 - val_accuracy: 0.7572\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8244 - accuracy: 0.7568 - val_loss: 0.8713 - val_accuracy: 0.7501\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8161 - accuracy: 0.7590 - val_loss: 0.8259 - val_accuracy: 0.7623\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8097 - accuracy: 0.7623 - val_loss: 0.8444 - val_accuracy: 0.7572\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8054 - accuracy: 0.7623 - val_loss: 0.8571 - val_accuracy: 0.7595\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7997 - accuracy: 0.7658 - val_loss: 0.8566 - val_accuracy: 0.7546\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7922 - accuracy: 0.7653 - val_loss: 0.8666 - val_accuracy: 0.7441\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7874 - accuracy: 0.7667 - val_loss: 0.7918 - val_accuracy: 0.7748\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7810 - accuracy: 0.7715 - val_loss: 0.7977 - val_accuracy: 0.7754\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7755 - accuracy: 0.7715 - val_loss: 0.8522 - val_accuracy: 0.7502\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7720 - accuracy: 0.7731 - val_loss: 0.8186 - val_accuracy: 0.7632\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7666 - accuracy: 0.7741 - val_loss: 0.8566 - val_accuracy: 0.7552\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7599 - accuracy: 0.7770 - val_loss: 0.8147 - val_accuracy: 0.7675\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7642 - accuracy: 0.7735 - val_loss: 0.7816 - val_accuracy: 0.7787\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7517 - accuracy: 0.7786 - val_loss: 0.8144 - val_accuracy: 0.7724\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7464 - accuracy: 0.7819 - val_loss: 0.7716 - val_accuracy: 0.7839\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7424 - accuracy: 0.7815 - val_loss: 0.7936 - val_accuracy: 0.7774\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7371 - accuracy: 0.7838 - val_loss: 0.7897 - val_accuracy: 0.7784\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7334 - accuracy: 0.7840 - val_loss: 0.8109 - val_accuracy: 0.7658\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7323 - accuracy: 0.7829 - val_loss: 0.7271 - val_accuracy: 0.7960\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7255 - accuracy: 0.7856 - val_loss: 0.7768 - val_accuracy: 0.7788\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7199 - accuracy: 0.7887 - val_loss: 0.7842 - val_accuracy: 0.7772\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7171 - accuracy: 0.7892 - val_loss: 0.7512 - val_accuracy: 0.7861\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7130 - accuracy: 0.7895 - val_loss: 0.7859 - val_accuracy: 0.7794\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7143 - accuracy: 0.7883 - val_loss: 0.7901 - val_accuracy: 0.7736\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7061 - accuracy: 0.7908 - val_loss: 0.7485 - val_accuracy: 0.7891\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7036 - accuracy: 0.7937 - val_loss: 0.7864 - val_accuracy: 0.7717\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7026 - accuracy: 0.7926 - val_loss: 0.7492 - val_accuracy: 0.7870\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6945 - accuracy: 0.7955 - val_loss: 0.7772 - val_accuracy: 0.7749\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6891 - accuracy: 0.7979 - val_loss: 0.7729 - val_accuracy: 0.7751\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6833 - accuracy: 0.7996 - val_loss: 0.7229 - val_accuracy: 0.7903\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6827 - accuracy: 0.7984 - val_loss: 0.7650 - val_accuracy: 0.7799\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6784 - accuracy: 0.8003 - val_loss: 0.7303 - val_accuracy: 0.7899\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6752 - accuracy: 0.8003 - val_loss: 0.7288 - val_accuracy: 0.7884\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6744 - accuracy: 0.8006 - val_loss: 0.7274 - val_accuracy: 0.7939\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6686 - accuracy: 0.8029 - val_loss: 0.7455 - val_accuracy: 0.7805\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6700 - accuracy: 0.8033 - val_loss: 0.7611 - val_accuracy: 0.7794\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6616 - accuracy: 0.8046 - val_loss: 0.7254 - val_accuracy: 0.7926\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6617 - accuracy: 0.8053 - val_loss: 0.6823 - val_accuracy: 0.8096\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6599 - accuracy: 0.8054 - val_loss: 0.7421 - val_accuracy: 0.7854\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6551 - accuracy: 0.8054 - val_loss: 0.7128 - val_accuracy: 0.7949\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6532 - accuracy: 0.8078 - val_loss: 0.7099 - val_accuracy: 0.7998\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6504 - accuracy: 0.8081 - val_loss: 0.7347 - val_accuracy: 0.7881\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6448 - accuracy: 0.8094 - val_loss: 0.7288 - val_accuracy: 0.7950\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6425 - accuracy: 0.8105 - val_loss: 0.7112 - val_accuracy: 0.7982\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6374 - accuracy: 0.8111 - val_loss: 0.7007 - val_accuracy: 0.7984\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6385 - accuracy: 0.8120 - val_loss: 0.7085 - val_accuracy: 0.7982\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6327 - accuracy: 0.8127 - val_loss: 0.6682 - val_accuracy: 0.8092\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6282 - accuracy: 0.8150 - val_loss: 0.7031 - val_accuracy: 0.7971\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6274 - accuracy: 0.8153 - val_loss: 0.7132 - val_accuracy: 0.7942\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6313 - accuracy: 0.8128 - val_loss: 0.6626 - val_accuracy: 0.8111\n",
      "Try 5/100: Best_val_acc: [0.7411400675773621, 0.7842222452163696], lr: 3.370271576584011e-05, Lambda: 7.735841166853925e-05\n",
      "\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3053 - accuracy: 0.1190 - val_loss: 2.2723 - val_accuracy: 0.1654\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2599 - accuracy: 0.1754 - val_loss: 2.2374 - val_accuracy: 0.1811\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2186 - accuracy: 0.2210 - val_loss: 2.1831 - val_accuracy: 0.2486\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1739 - accuracy: 0.2672 - val_loss: 2.1031 - val_accuracy: 0.3689\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.1232 - accuracy: 0.3159 - val_loss: 2.0719 - val_accuracy: 0.3876\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.0671 - accuracy: 0.3523 - val_loss: 2.0063 - val_accuracy: 0.4226\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.0051 - accuracy: 0.3852 - val_loss: 1.9148 - val_accuracy: 0.4627\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9403 - accuracy: 0.4132 - val_loss: 1.8525 - val_accuracy: 0.4832\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8764 - accuracy: 0.4400 - val_loss: 1.7897 - val_accuracy: 0.5256\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8152 - accuracy: 0.4653 - val_loss: 1.7025 - val_accuracy: 0.5576\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7589 - accuracy: 0.4801 - val_loss: 1.6322 - val_accuracy: 0.5878\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.7030 - accuracy: 0.5033 - val_loss: 1.6480 - val_accuracy: 0.5603\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6535 - accuracy: 0.5203 - val_loss: 1.5532 - val_accuracy: 0.6008\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6074 - accuracy: 0.5362 - val_loss: 1.5561 - val_accuracy: 0.5820\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5621 - accuracy: 0.5523 - val_loss: 1.4773 - val_accuracy: 0.6239\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5231 - accuracy: 0.5627 - val_loss: 1.4496 - val_accuracy: 0.6134\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4840 - accuracy: 0.5768 - val_loss: 1.4256 - val_accuracy: 0.6214\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.4497 - accuracy: 0.5843 - val_loss: 1.4393 - val_accuracy: 0.6073\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4158 - accuracy: 0.5977 - val_loss: 1.3839 - val_accuracy: 0.6284\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3857 - accuracy: 0.6062 - val_loss: 1.3775 - val_accuracy: 0.6295\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3550 - accuracy: 0.6178 - val_loss: 1.2979 - val_accuracy: 0.6560\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3271 - accuracy: 0.6265 - val_loss: 1.3363 - val_accuracy: 0.6389\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3037 - accuracy: 0.6306 - val_loss: 1.2766 - val_accuracy: 0.6631\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2818 - accuracy: 0.6383 - val_loss: 1.2264 - val_accuracy: 0.6765\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2611 - accuracy: 0.6453 - val_loss: 1.2553 - val_accuracy: 0.6614\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2418 - accuracy: 0.6497 - val_loss: 1.2287 - val_accuracy: 0.6714\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2232 - accuracy: 0.6532 - val_loss: 1.1940 - val_accuracy: 0.6845\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2061 - accuracy: 0.6573 - val_loss: 1.2007 - val_accuracy: 0.6739\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1892 - accuracy: 0.6626 - val_loss: 1.1587 - val_accuracy: 0.6923\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1740 - accuracy: 0.6656 - val_loss: 1.1703 - val_accuracy: 0.6794\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1613 - accuracy: 0.6690 - val_loss: 1.1422 - val_accuracy: 0.6904\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1490 - accuracy: 0.6722 - val_loss: 1.1581 - val_accuracy: 0.6837\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1356 - accuracy: 0.6758 - val_loss: 1.1009 - val_accuracy: 0.7043\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1230 - accuracy: 0.6801 - val_loss: 1.1259 - val_accuracy: 0.6869\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1146 - accuracy: 0.6809 - val_loss: 1.1506 - val_accuracy: 0.6744\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1032 - accuracy: 0.6824 - val_loss: 1.1422 - val_accuracy: 0.6815\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0913 - accuracy: 0.6873 - val_loss: 1.0863 - val_accuracy: 0.7061\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0827 - accuracy: 0.6875 - val_loss: 1.0667 - val_accuracy: 0.7085\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0720 - accuracy: 0.6921 - val_loss: 1.0741 - val_accuracy: 0.7056\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0630 - accuracy: 0.6934 - val_loss: 1.0648 - val_accuracy: 0.7047\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0555 - accuracy: 0.6963 - val_loss: 1.0685 - val_accuracy: 0.7017\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0462 - accuracy: 0.6973 - val_loss: 1.0789 - val_accuracy: 0.7006\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0386 - accuracy: 0.7018 - val_loss: 1.0672 - val_accuracy: 0.6998\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0314 - accuracy: 0.7020 - val_loss: 1.0293 - val_accuracy: 0.7180\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0248 - accuracy: 0.7030 - val_loss: 0.9917 - val_accuracy: 0.7312\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0176 - accuracy: 0.7048 - val_loss: 1.0296 - val_accuracy: 0.7169\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0110 - accuracy: 0.7062 - val_loss: 0.9940 - val_accuracy: 0.7255\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0026 - accuracy: 0.7091 - val_loss: 1.0029 - val_accuracy: 0.7263\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9979 - accuracy: 0.7098 - val_loss: 1.0244 - val_accuracy: 0.7087\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9902 - accuracy: 0.7140 - val_loss: 0.9945 - val_accuracy: 0.7243\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9847 - accuracy: 0.7140 - val_loss: 0.9970 - val_accuracy: 0.7244\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9785 - accuracy: 0.7159 - val_loss: 0.9863 - val_accuracy: 0.7263\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9729 - accuracy: 0.7174 - val_loss: 0.9994 - val_accuracy: 0.7202\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9680 - accuracy: 0.7192 - val_loss: 0.9926 - val_accuracy: 0.7201\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9636 - accuracy: 0.7192 - val_loss: 0.9811 - val_accuracy: 0.7226\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9578 - accuracy: 0.7209 - val_loss: 0.9591 - val_accuracy: 0.7317\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9508 - accuracy: 0.7229 - val_loss: 0.9795 - val_accuracy: 0.7191\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9465 - accuracy: 0.7228 - val_loss: 0.9766 - val_accuracy: 0.7249\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9404 - accuracy: 0.7265 - val_loss: 0.9476 - val_accuracy: 0.7352\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9352 - accuracy: 0.7274 - val_loss: 0.9691 - val_accuracy: 0.7281\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9307 - accuracy: 0.7287 - val_loss: 0.9494 - val_accuracy: 0.7360\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9257 - accuracy: 0.7296 - val_loss: 0.9271 - val_accuracy: 0.7434\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9211 - accuracy: 0.7305 - val_loss: 0.9437 - val_accuracy: 0.7356\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9179 - accuracy: 0.7333 - val_loss: 0.8985 - val_accuracy: 0.7494\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9139 - accuracy: 0.7322 - val_loss: 0.8900 - val_accuracy: 0.7538\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9095 - accuracy: 0.7335 - val_loss: 0.9063 - val_accuracy: 0.7484\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9032 - accuracy: 0.7372 - val_loss: 0.9336 - val_accuracy: 0.7331\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8995 - accuracy: 0.7360 - val_loss: 0.8808 - val_accuracy: 0.7554\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8960 - accuracy: 0.7379 - val_loss: 0.8849 - val_accuracy: 0.7538\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8910 - accuracy: 0.7385 - val_loss: 0.9606 - val_accuracy: 0.7246\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8866 - accuracy: 0.7402 - val_loss: 0.9127 - val_accuracy: 0.7421\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8820 - accuracy: 0.7413 - val_loss: 0.8838 - val_accuracy: 0.7513\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8784 - accuracy: 0.7442 - val_loss: 0.8557 - val_accuracy: 0.7671\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8745 - accuracy: 0.7439 - val_loss: 0.8596 - val_accuracy: 0.7599\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8711 - accuracy: 0.7444 - val_loss: 0.8941 - val_accuracy: 0.7459\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8667 - accuracy: 0.7461 - val_loss: 0.8868 - val_accuracy: 0.7493\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8642 - accuracy: 0.7454 - val_loss: 0.9007 - val_accuracy: 0.7457\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8589 - accuracy: 0.7466 - val_loss: 0.8526 - val_accuracy: 0.7639\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8569 - accuracy: 0.7474 - val_loss: 0.8923 - val_accuracy: 0.7473\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8542 - accuracy: 0.7500 - val_loss: 0.8712 - val_accuracy: 0.7551\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8492 - accuracy: 0.7496 - val_loss: 0.8648 - val_accuracy: 0.7566\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8446 - accuracy: 0.7510 - val_loss: 0.8931 - val_accuracy: 0.7468\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8406 - accuracy: 0.7529 - val_loss: 0.8738 - val_accuracy: 0.7557\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8392 - accuracy: 0.7516 - val_loss: 0.8835 - val_accuracy: 0.7491\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8343 - accuracy: 0.7546 - val_loss: 0.8424 - val_accuracy: 0.7643\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8299 - accuracy: 0.7566 - val_loss: 0.8346 - val_accuracy: 0.7661\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8270 - accuracy: 0.7568 - val_loss: 0.8731 - val_accuracy: 0.7513\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8240 - accuracy: 0.7577 - val_loss: 0.8623 - val_accuracy: 0.7529\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8206 - accuracy: 0.7584 - val_loss: 0.8650 - val_accuracy: 0.7558\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8196 - accuracy: 0.7587 - val_loss: 0.8191 - val_accuracy: 0.7706\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8167 - accuracy: 0.7584 - val_loss: 0.8381 - val_accuracy: 0.7649\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8133 - accuracy: 0.7608 - val_loss: 0.8702 - val_accuracy: 0.7509\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8089 - accuracy: 0.7615 - val_loss: 0.8272 - val_accuracy: 0.7683\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8046 - accuracy: 0.7630 - val_loss: 0.8354 - val_accuracy: 0.7647\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8010 - accuracy: 0.7635 - val_loss: 0.8432 - val_accuracy: 0.7596\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7981 - accuracy: 0.7655 - val_loss: 0.8512 - val_accuracy: 0.7575\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7941 - accuracy: 0.7656 - val_loss: 0.8185 - val_accuracy: 0.7725\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7928 - accuracy: 0.7661 - val_loss: 0.8208 - val_accuracy: 0.7688\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7923 - accuracy: 0.7649 - val_loss: 0.8072 - val_accuracy: 0.7721\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7865 - accuracy: 0.7679 - val_loss: 0.8255 - val_accuracy: 0.7693\n",
      "Try 6/100: Best_val_acc: [0.854948878288269, 0.7484999895095825], lr: 1.6953874865865752e-05, Lambda: 4.390281873208347e-05\n",
      "\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2788 - accuracy: 0.1518 - val_loss: 2.1560 - val_accuracy: 0.3457\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1593 - accuracy: 0.2734 - val_loss: 2.0079 - val_accuracy: 0.4371\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9364 - accuracy: 0.4004 - val_loss: 1.8730 - val_accuracy: 0.4529\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6912 - accuracy: 0.4914 - val_loss: 1.7212 - val_accuracy: 0.4793\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5034 - accuracy: 0.5498 - val_loss: 1.4295 - val_accuracy: 0.6224\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3740 - accuracy: 0.5897 - val_loss: 1.2750 - val_accuracy: 0.6604\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2801 - accuracy: 0.6227 - val_loss: 1.2581 - val_accuracy: 0.6546\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2135 - accuracy: 0.6380 - val_loss: 1.3124 - val_accuracy: 0.6070\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1609 - accuracy: 0.6529 - val_loss: 1.1296 - val_accuracy: 0.6778\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1140 - accuracy: 0.6696 - val_loss: 1.1590 - val_accuracy: 0.6631\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0860 - accuracy: 0.6741 - val_loss: 1.1219 - val_accuracy: 0.6732\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0538 - accuracy: 0.6848 - val_loss: 1.1006 - val_accuracy: 0.6722\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0265 - accuracy: 0.6920 - val_loss: 1.0943 - val_accuracy: 0.6650\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0035 - accuracy: 0.6975 - val_loss: 1.0321 - val_accuracy: 0.7025\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9779 - accuracy: 0.7060 - val_loss: 1.0890 - val_accuracy: 0.6793\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9521 - accuracy: 0.7143 - val_loss: 0.9900 - val_accuracy: 0.7131\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9427 - accuracy: 0.7152 - val_loss: 0.9337 - val_accuracy: 0.7353\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9212 - accuracy: 0.7221 - val_loss: 0.9699 - val_accuracy: 0.6999\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9023 - accuracy: 0.7275 - val_loss: 0.9002 - val_accuracy: 0.7414\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8813 - accuracy: 0.7350 - val_loss: 0.8950 - val_accuracy: 0.7433\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8678 - accuracy: 0.7390 - val_loss: 0.9322 - val_accuracy: 0.7294\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8510 - accuracy: 0.7448 - val_loss: 1.0026 - val_accuracy: 0.7076\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8402 - accuracy: 0.7463 - val_loss: 0.8104 - val_accuracy: 0.7659\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8279 - accuracy: 0.7518 - val_loss: 0.8849 - val_accuracy: 0.7464\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8098 - accuracy: 0.7554 - val_loss: 0.9231 - val_accuracy: 0.7166\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8111 - accuracy: 0.7546 - val_loss: 0.8075 - val_accuracy: 0.7691\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7951 - accuracy: 0.7598 - val_loss: 0.9387 - val_accuracy: 0.7201\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7854 - accuracy: 0.7628 - val_loss: 0.7986 - val_accuracy: 0.7709\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7752 - accuracy: 0.7646 - val_loss: 0.7622 - val_accuracy: 0.7791\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7645 - accuracy: 0.7688 - val_loss: 0.7729 - val_accuracy: 0.7759\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7529 - accuracy: 0.7725 - val_loss: 0.7341 - val_accuracy: 0.7899\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7421 - accuracy: 0.7759 - val_loss: 0.7721 - val_accuracy: 0.7771\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7372 - accuracy: 0.7766 - val_loss: 0.7878 - val_accuracy: 0.7654\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7277 - accuracy: 0.7763 - val_loss: 0.7496 - val_accuracy: 0.7803\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7190 - accuracy: 0.7824 - val_loss: 0.7523 - val_accuracy: 0.7817\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7081 - accuracy: 0.7858 - val_loss: 0.7591 - val_accuracy: 0.7790\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6971 - accuracy: 0.7900 - val_loss: 0.8210 - val_accuracy: 0.7551\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6868 - accuracy: 0.7938 - val_loss: 0.7503 - val_accuracy: 0.7829\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6829 - accuracy: 0.7940 - val_loss: 0.7490 - val_accuracy: 0.7856\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6798 - accuracy: 0.7941 - val_loss: 0.8121 - val_accuracy: 0.7614\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6752 - accuracy: 0.7963 - val_loss: 0.7366 - val_accuracy: 0.7871\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6623 - accuracy: 0.8013 - val_loss: 0.6751 - val_accuracy: 0.8024\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6577 - accuracy: 0.8006 - val_loss: 0.7303 - val_accuracy: 0.7852\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6574 - accuracy: 0.8010 - val_loss: 0.7606 - val_accuracy: 0.7761\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6402 - accuracy: 0.8060 - val_loss: 0.6530 - val_accuracy: 0.8124\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6327 - accuracy: 0.8098 - val_loss: 0.6904 - val_accuracy: 0.8013\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6305 - accuracy: 0.8097 - val_loss: 0.6645 - val_accuracy: 0.8054\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6226 - accuracy: 0.8128 - val_loss: 0.6836 - val_accuracy: 0.8059\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6159 - accuracy: 0.8143 - val_loss: 0.7310 - val_accuracy: 0.7862\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6171 - accuracy: 0.8139 - val_loss: 0.6091 - val_accuracy: 0.8221\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6066 - accuracy: 0.8182 - val_loss: 0.6159 - val_accuracy: 0.8224\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5993 - accuracy: 0.8199 - val_loss: 0.7689 - val_accuracy: 0.7794\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5988 - accuracy: 0.8194 - val_loss: 0.6819 - val_accuracy: 0.7995\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5897 - accuracy: 0.8204 - val_loss: 0.6778 - val_accuracy: 0.8001\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5902 - accuracy: 0.8204 - val_loss: 0.6305 - val_accuracy: 0.8167\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5839 - accuracy: 0.8243 - val_loss: 0.6580 - val_accuracy: 0.8101\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5740 - accuracy: 0.8262 - val_loss: 0.5909 - val_accuracy: 0.8286\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5714 - accuracy: 0.8277 - val_loss: 0.6294 - val_accuracy: 0.8173\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5652 - accuracy: 0.8295 - val_loss: 0.6392 - val_accuracy: 0.8093\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5644 - accuracy: 0.8293 - val_loss: 0.6100 - val_accuracy: 0.8230\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5565 - accuracy: 0.8311 - val_loss: 0.6636 - val_accuracy: 0.8074\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5538 - accuracy: 0.8334 - val_loss: 0.6442 - val_accuracy: 0.8154\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5514 - accuracy: 0.8346 - val_loss: 0.6198 - val_accuracy: 0.8239\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5398 - accuracy: 0.8368 - val_loss: 0.6247 - val_accuracy: 0.8211\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5343 - accuracy: 0.8390 - val_loss: 0.6050 - val_accuracy: 0.8240\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5342 - accuracy: 0.8386 - val_loss: 0.5993 - val_accuracy: 0.8255\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5287 - accuracy: 0.8418 - val_loss: 0.6414 - val_accuracy: 0.8121\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5194 - accuracy: 0.8440 - val_loss: 0.5759 - val_accuracy: 0.8321\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5249 - accuracy: 0.8410 - val_loss: 0.5635 - val_accuracy: 0.8339\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5125 - accuracy: 0.8445 - val_loss: 0.5424 - val_accuracy: 0.8442\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5157 - accuracy: 0.8450 - val_loss: 0.6134 - val_accuracy: 0.8202\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5103 - accuracy: 0.8473 - val_loss: 0.6172 - val_accuracy: 0.8233\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4988 - accuracy: 0.8508 - val_loss: 0.6059 - val_accuracy: 0.8237\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4957 - accuracy: 0.8505 - val_loss: 0.6145 - val_accuracy: 0.8249\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5034 - accuracy: 0.8487 - val_loss: 0.5807 - val_accuracy: 0.8326\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4851 - accuracy: 0.8556 - val_loss: 0.5335 - val_accuracy: 0.8485\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4795 - accuracy: 0.8556 - val_loss: 0.5492 - val_accuracy: 0.8409\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4853 - accuracy: 0.8538 - val_loss: 0.5823 - val_accuracy: 0.8306\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4755 - accuracy: 0.8577 - val_loss: 0.5143 - val_accuracy: 0.8536\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4750 - accuracy: 0.8571 - val_loss: 0.5640 - val_accuracy: 0.8381\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4787 - accuracy: 0.8555 - val_loss: 0.5870 - val_accuracy: 0.8321\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4727 - accuracy: 0.8583 - val_loss: 0.6089 - val_accuracy: 0.8171\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4700 - accuracy: 0.8587 - val_loss: 0.5071 - val_accuracy: 0.8556\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4614 - accuracy: 0.8618 - val_loss: 0.5473 - val_accuracy: 0.8400\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4573 - accuracy: 0.8632 - val_loss: 0.5340 - val_accuracy: 0.8416\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4575 - accuracy: 0.8626 - val_loss: 0.5134 - val_accuracy: 0.8499\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4569 - accuracy: 0.8632 - val_loss: 0.5364 - val_accuracy: 0.8481\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4495 - accuracy: 0.8650 - val_loss: 0.5266 - val_accuracy: 0.8506\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4405 - accuracy: 0.8682 - val_loss: 0.5033 - val_accuracy: 0.8586\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4387 - accuracy: 0.8683 - val_loss: 0.4802 - val_accuracy: 0.8625\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4402 - accuracy: 0.8682 - val_loss: 0.5643 - val_accuracy: 0.8389\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4345 - accuracy: 0.8695 - val_loss: 0.5052 - val_accuracy: 0.8536\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4286 - accuracy: 0.8722 - val_loss: 0.4788 - val_accuracy: 0.8667\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4303 - accuracy: 0.8706 - val_loss: 0.4821 - val_accuracy: 0.8613\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4289 - accuracy: 0.8724 - val_loss: 0.6964 - val_accuracy: 0.7978\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4215 - accuracy: 0.8742 - val_loss: 0.5307 - val_accuracy: 0.8492\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4187 - accuracy: 0.8745 - val_loss: 0.5323 - val_accuracy: 0.8420\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4239 - accuracy: 0.8721 - val_loss: 0.5364 - val_accuracy: 0.8451\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4206 - accuracy: 0.8741 - val_loss: 0.5088 - val_accuracy: 0.8539\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4093 - accuracy: 0.8765 - val_loss: 0.4638 - val_accuracy: 0.8694\n",
      "Try 7/100: Best_val_acc: [0.6370719075202942, 0.8233888745307922], lr: 9.082639786220927e-05, Lambda: 5.6629681169034375e-05\n",
      "\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3047 - accuracy: 0.1200 - val_loss: 2.3133 - val_accuracy: 0.1527\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2710 - accuracy: 0.1634 - val_loss: 2.2872 - val_accuracy: 0.2069\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2379 - accuracy: 0.2131 - val_loss: 2.2245 - val_accuracy: 0.2799\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1967 - accuracy: 0.2531 - val_loss: 2.1957 - val_accuracy: 0.2546\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1438 - accuracy: 0.2877 - val_loss: 2.1571 - val_accuracy: 0.2559\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0828 - accuracy: 0.3215 - val_loss: 2.1319 - val_accuracy: 0.2791\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0141 - accuracy: 0.3590 - val_loss: 2.0540 - val_accuracy: 0.3386\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9410 - accuracy: 0.3975 - val_loss: 1.9542 - val_accuracy: 0.4001\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8640 - accuracy: 0.4380 - val_loss: 1.8709 - val_accuracy: 0.4668\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7923 - accuracy: 0.4686 - val_loss: 1.8096 - val_accuracy: 0.4931\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7246 - accuracy: 0.4955 - val_loss: 1.7147 - val_accuracy: 0.5362\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6643 - accuracy: 0.5151 - val_loss: 1.7146 - val_accuracy: 0.5151\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6101 - accuracy: 0.5323 - val_loss: 1.6096 - val_accuracy: 0.5599\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5609 - accuracy: 0.5482 - val_loss: 1.5775 - val_accuracy: 0.5769\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5189 - accuracy: 0.5607 - val_loss: 1.5533 - val_accuracy: 0.5623\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4788 - accuracy: 0.5740 - val_loss: 1.4836 - val_accuracy: 0.5969\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4448 - accuracy: 0.5808 - val_loss: 1.4523 - val_accuracy: 0.6014\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4148 - accuracy: 0.5916 - val_loss: 1.5077 - val_accuracy: 0.5541\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3870 - accuracy: 0.5939 - val_loss: 1.4360 - val_accuracy: 0.5845\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3609 - accuracy: 0.6034 - val_loss: 1.3858 - val_accuracy: 0.6150\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3390 - accuracy: 0.6091 - val_loss: 1.3802 - val_accuracy: 0.6079\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3174 - accuracy: 0.6149 - val_loss: 1.3325 - val_accuracy: 0.6281\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2985 - accuracy: 0.6199 - val_loss: 1.3214 - val_accuracy: 0.6274\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2806 - accuracy: 0.6242 - val_loss: 1.3355 - val_accuracy: 0.6174\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2641 - accuracy: 0.6287 - val_loss: 1.2927 - val_accuracy: 0.6386\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2478 - accuracy: 0.6351 - val_loss: 1.2638 - val_accuracy: 0.6482\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2388 - accuracy: 0.6352 - val_loss: 1.3128 - val_accuracy: 0.6119\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2220 - accuracy: 0.6414 - val_loss: 1.2284 - val_accuracy: 0.6596\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2085 - accuracy: 0.6447 - val_loss: 1.2629 - val_accuracy: 0.6291\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1971 - accuracy: 0.6496 - val_loss: 1.1726 - val_accuracy: 0.6713\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1855 - accuracy: 0.6513 - val_loss: 1.1994 - val_accuracy: 0.6626\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1747 - accuracy: 0.6543 - val_loss: 1.2144 - val_accuracy: 0.6548\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1645 - accuracy: 0.6569 - val_loss: 1.1563 - val_accuracy: 0.6799\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1535 - accuracy: 0.6615 - val_loss: 1.1842 - val_accuracy: 0.6631\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1446 - accuracy: 0.6619 - val_loss: 1.1781 - val_accuracy: 0.6651\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1363 - accuracy: 0.6651 - val_loss: 1.1928 - val_accuracy: 0.6609\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1270 - accuracy: 0.6675 - val_loss: 1.2011 - val_accuracy: 0.6534\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1169 - accuracy: 0.6706 - val_loss: 1.1362 - val_accuracy: 0.6798\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1090 - accuracy: 0.6734 - val_loss: 1.1311 - val_accuracy: 0.6795\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1016 - accuracy: 0.6755 - val_loss: 1.1134 - val_accuracy: 0.6810\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0940 - accuracy: 0.6765 - val_loss: 1.1323 - val_accuracy: 0.6776\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0857 - accuracy: 0.6800 - val_loss: 1.1390 - val_accuracy: 0.6743\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0788 - accuracy: 0.6813 - val_loss: 1.0894 - val_accuracy: 0.6915\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0706 - accuracy: 0.6851 - val_loss: 1.1629 - val_accuracy: 0.6592\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0645 - accuracy: 0.6871 - val_loss: 1.1544 - val_accuracy: 0.6650\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0594 - accuracy: 0.6880 - val_loss: 1.0978 - val_accuracy: 0.6841\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0527 - accuracy: 0.6889 - val_loss: 1.1074 - val_accuracy: 0.6828\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0445 - accuracy: 0.6925 - val_loss: 1.1194 - val_accuracy: 0.6777\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0383 - accuracy: 0.6927 - val_loss: 1.1398 - val_accuracy: 0.6722\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0324 - accuracy: 0.6950 - val_loss: 1.0469 - val_accuracy: 0.7048\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0247 - accuracy: 0.6974 - val_loss: 1.1356 - val_accuracy: 0.6654\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0188 - accuracy: 0.6990 - val_loss: 1.1094 - val_accuracy: 0.6816\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0134 - accuracy: 0.7007 - val_loss: 1.0484 - val_accuracy: 0.7022\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0088 - accuracy: 0.7028 - val_loss: 1.0450 - val_accuracy: 0.7013\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0022 - accuracy: 0.7041 - val_loss: 1.0253 - val_accuracy: 0.7060\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9966 - accuracy: 0.7060 - val_loss: 1.0662 - val_accuracy: 0.6926\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9913 - accuracy: 0.7080 - val_loss: 1.0554 - val_accuracy: 0.6941\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9855 - accuracy: 0.7093 - val_loss: 1.0630 - val_accuracy: 0.6934\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9801 - accuracy: 0.7110 - val_loss: 1.0176 - val_accuracy: 0.7107\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9746 - accuracy: 0.7124 - val_loss: 0.9838 - val_accuracy: 0.7264\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9699 - accuracy: 0.7138 - val_loss: 1.0143 - val_accuracy: 0.7126\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9676 - accuracy: 0.7141 - val_loss: 1.0315 - val_accuracy: 0.7043\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9612 - accuracy: 0.7173 - val_loss: 1.0876 - val_accuracy: 0.6799\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9560 - accuracy: 0.7178 - val_loss: 1.0208 - val_accuracy: 0.7068\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9507 - accuracy: 0.7185 - val_loss: 0.9980 - val_accuracy: 0.7110\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9467 - accuracy: 0.7213 - val_loss: 0.9885 - val_accuracy: 0.7200\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9410 - accuracy: 0.7221 - val_loss: 0.9853 - val_accuracy: 0.7226\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9364 - accuracy: 0.7238 - val_loss: 1.0462 - val_accuracy: 0.6924\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9363 - accuracy: 0.7242 - val_loss: 0.9912 - val_accuracy: 0.7168\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9296 - accuracy: 0.7261 - val_loss: 0.9701 - val_accuracy: 0.7214\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9249 - accuracy: 0.7274 - val_loss: 0.9590 - val_accuracy: 0.7286\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9214 - accuracy: 0.7291 - val_loss: 0.9520 - val_accuracy: 0.7315\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9175 - accuracy: 0.7277 - val_loss: 0.9700 - val_accuracy: 0.7245\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9124 - accuracy: 0.7304 - val_loss: 0.9684 - val_accuracy: 0.7255\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9095 - accuracy: 0.7322 - val_loss: 0.9207 - val_accuracy: 0.7410\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9048 - accuracy: 0.7329 - val_loss: 0.9725 - val_accuracy: 0.7197\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9010 - accuracy: 0.7339 - val_loss: 0.9273 - val_accuracy: 0.7385\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8967 - accuracy: 0.7349 - val_loss: 1.0174 - val_accuracy: 0.7031\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8958 - accuracy: 0.7350 - val_loss: 0.9226 - val_accuracy: 0.7414\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8897 - accuracy: 0.7370 - val_loss: 0.9380 - val_accuracy: 0.7325\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8858 - accuracy: 0.7384 - val_loss: 0.9398 - val_accuracy: 0.7295\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8844 - accuracy: 0.7391 - val_loss: 0.9169 - val_accuracy: 0.7411\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8818 - accuracy: 0.7398 - val_loss: 0.9552 - val_accuracy: 0.7277\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8784 - accuracy: 0.7390 - val_loss: 0.8975 - val_accuracy: 0.7485\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8722 - accuracy: 0.7433 - val_loss: 0.8886 - val_accuracy: 0.7512\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8693 - accuracy: 0.7433 - val_loss: 0.8676 - val_accuracy: 0.7579\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8675 - accuracy: 0.7428 - val_loss: 0.9109 - val_accuracy: 0.7434\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8622 - accuracy: 0.7458 - val_loss: 0.9550 - val_accuracy: 0.7260\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8585 - accuracy: 0.7472 - val_loss: 0.8874 - val_accuracy: 0.7484\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8564 - accuracy: 0.7480 - val_loss: 0.8857 - val_accuracy: 0.7526\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8512 - accuracy: 0.7495 - val_loss: 0.9205 - val_accuracy: 0.7329\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8503 - accuracy: 0.7505 - val_loss: 0.9094 - val_accuracy: 0.7411\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8458 - accuracy: 0.7505 - val_loss: 0.9516 - val_accuracy: 0.7262\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8429 - accuracy: 0.7518 - val_loss: 0.9239 - val_accuracy: 0.7371\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8395 - accuracy: 0.7520 - val_loss: 0.8922 - val_accuracy: 0.7461\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8357 - accuracy: 0.7529 - val_loss: 0.8725 - val_accuracy: 0.7522\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8370 - accuracy: 0.7527 - val_loss: 0.9151 - val_accuracy: 0.7329\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8305 - accuracy: 0.7545 - val_loss: 0.8826 - val_accuracy: 0.7504\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8288 - accuracy: 0.7548 - val_loss: 0.8474 - val_accuracy: 0.7624\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8273 - accuracy: 0.7546 - val_loss: 0.8671 - val_accuracy: 0.7511\n",
      "Try 8/100: Best_val_acc: [0.8893121480941772, 0.7367222309112549], lr: 1.790203722303483e-05, Lambda: 7.018394056438098e-05\n",
      "\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3693 - accuracy: 0.1023 - val_loss: 2.2374 - val_accuracy: 0.2350\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2988 - accuracy: 0.1112 - val_loss: 2.2433 - val_accuracy: 0.2079\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2871 - accuracy: 0.1154 - val_loss: 2.2354 - val_accuracy: 0.2094\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2761 - accuracy: 0.1280 - val_loss: 2.2198 - val_accuracy: 0.2370\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2628 - accuracy: 0.1390 - val_loss: 2.2104 - val_accuracy: 0.2444\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2477 - accuracy: 0.1491 - val_loss: 2.1837 - val_accuracy: 0.2654\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2322 - accuracy: 0.1615 - val_loss: 2.1615 - val_accuracy: 0.2758\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2150 - accuracy: 0.1770 - val_loss: 2.1443 - val_accuracy: 0.2984\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1957 - accuracy: 0.1923 - val_loss: 2.1099 - val_accuracy: 0.3354\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1733 - accuracy: 0.2160 - val_loss: 2.0939 - val_accuracy: 0.3571\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1484 - accuracy: 0.2440 - val_loss: 2.0728 - val_accuracy: 0.4006\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1213 - accuracy: 0.2744 - val_loss: 2.0224 - val_accuracy: 0.4624\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0917 - accuracy: 0.3046 - val_loss: 2.0092 - val_accuracy: 0.4660\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0599 - accuracy: 0.3323 - val_loss: 1.9472 - val_accuracy: 0.5359\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0257 - accuracy: 0.3591 - val_loss: 1.9335 - val_accuracy: 0.5224\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9902 - accuracy: 0.3860 - val_loss: 1.9147 - val_accuracy: 0.5230\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9553 - accuracy: 0.4088 - val_loss: 1.8628 - val_accuracy: 0.5642\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9196 - accuracy: 0.4276 - val_loss: 1.8590 - val_accuracy: 0.5387\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8828 - accuracy: 0.4461 - val_loss: 1.8192 - val_accuracy: 0.5589\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8475 - accuracy: 0.4630 - val_loss: 1.7815 - val_accuracy: 0.5649\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8096 - accuracy: 0.4789 - val_loss: 1.7352 - val_accuracy: 0.5904\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7743 - accuracy: 0.4923 - val_loss: 1.7317 - val_accuracy: 0.5669\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7402 - accuracy: 0.5060 - val_loss: 1.6846 - val_accuracy: 0.5841\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7060 - accuracy: 0.5161 - val_loss: 1.6516 - val_accuracy: 0.5845\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6746 - accuracy: 0.5259 - val_loss: 1.6371 - val_accuracy: 0.5819\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6448 - accuracy: 0.5360 - val_loss: 1.6125 - val_accuracy: 0.5793\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6157 - accuracy: 0.5460 - val_loss: 1.5879 - val_accuracy: 0.5854\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5885 - accuracy: 0.5545 - val_loss: 1.5550 - val_accuracy: 0.5954\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5628 - accuracy: 0.5599 - val_loss: 1.5177 - val_accuracy: 0.6041\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5387 - accuracy: 0.5654 - val_loss: 1.4857 - val_accuracy: 0.6235\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5152 - accuracy: 0.5731 - val_loss: 1.4922 - val_accuracy: 0.6049\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4949 - accuracy: 0.5776 - val_loss: 1.4836 - val_accuracy: 0.6058\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4734 - accuracy: 0.5809 - val_loss: 1.4696 - val_accuracy: 0.6023\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4537 - accuracy: 0.5880 - val_loss: 1.4369 - val_accuracy: 0.6023\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4354 - accuracy: 0.5915 - val_loss: 1.4238 - val_accuracy: 0.6092\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4176 - accuracy: 0.5965 - val_loss: 1.4067 - val_accuracy: 0.6094\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4017 - accuracy: 0.6005 - val_loss: 1.3745 - val_accuracy: 0.6256\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3868 - accuracy: 0.6037 - val_loss: 1.3836 - val_accuracy: 0.6080\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3711 - accuracy: 0.6070 - val_loss: 1.3891 - val_accuracy: 0.6056\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3571 - accuracy: 0.6115 - val_loss: 1.3736 - val_accuracy: 0.6086\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3441 - accuracy: 0.6132 - val_loss: 1.3326 - val_accuracy: 0.6281\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3305 - accuracy: 0.6187 - val_loss: 1.3162 - val_accuracy: 0.6342\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3175 - accuracy: 0.6218 - val_loss: 1.3124 - val_accuracy: 0.6318\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3057 - accuracy: 0.6227 - val_loss: 1.2896 - val_accuracy: 0.6391\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2946 - accuracy: 0.6253 - val_loss: 1.3379 - val_accuracy: 0.6086\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2832 - accuracy: 0.6294 - val_loss: 1.2844 - val_accuracy: 0.6281\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2729 - accuracy: 0.6312 - val_loss: 1.2932 - val_accuracy: 0.6189\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2637 - accuracy: 0.6329 - val_loss: 1.2758 - val_accuracy: 0.6285\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2527 - accuracy: 0.6361 - val_loss: 1.2582 - val_accuracy: 0.6414\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2433 - accuracy: 0.6395 - val_loss: 1.2561 - val_accuracy: 0.6381\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2348 - accuracy: 0.6403 - val_loss: 1.2389 - val_accuracy: 0.6436\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2266 - accuracy: 0.6418 - val_loss: 1.2314 - val_accuracy: 0.6459\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2177 - accuracy: 0.6455 - val_loss: 1.2213 - val_accuracy: 0.6446\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2101 - accuracy: 0.6462 - val_loss: 1.2524 - val_accuracy: 0.6312\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2030 - accuracy: 0.6497 - val_loss: 1.2042 - val_accuracy: 0.6509\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1946 - accuracy: 0.6508 - val_loss: 1.1963 - val_accuracy: 0.6589\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1884 - accuracy: 0.6522 - val_loss: 1.2171 - val_accuracy: 0.6410\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1817 - accuracy: 0.6546 - val_loss: 1.1934 - val_accuracy: 0.6573\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1748 - accuracy: 0.6563 - val_loss: 1.1848 - val_accuracy: 0.6639\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1678 - accuracy: 0.6579 - val_loss: 1.1887 - val_accuracy: 0.6524\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1609 - accuracy: 0.6597 - val_loss: 1.1885 - val_accuracy: 0.6561\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1561 - accuracy: 0.6611 - val_loss: 1.1825 - val_accuracy: 0.6519\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1500 - accuracy: 0.6626 - val_loss: 1.1896 - val_accuracy: 0.6529\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1446 - accuracy: 0.6631 - val_loss: 1.1934 - val_accuracy: 0.6517\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1388 - accuracy: 0.6667 - val_loss: 1.1730 - val_accuracy: 0.6562\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1346 - accuracy: 0.6652 - val_loss: 1.1425 - val_accuracy: 0.6691\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1283 - accuracy: 0.6681 - val_loss: 1.1302 - val_accuracy: 0.6717\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1232 - accuracy: 0.6695 - val_loss: 1.1435 - val_accuracy: 0.6714\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1187 - accuracy: 0.6709 - val_loss: 1.1157 - val_accuracy: 0.6793\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1134 - accuracy: 0.6725 - val_loss: 1.1432 - val_accuracy: 0.6680\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1088 - accuracy: 0.6724 - val_loss: 1.1468 - val_accuracy: 0.6651\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1049 - accuracy: 0.6732 - val_loss: 1.1397 - val_accuracy: 0.6706\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1000 - accuracy: 0.6752 - val_loss: 1.1314 - val_accuracy: 0.6729\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0949 - accuracy: 0.6762 - val_loss: 1.1163 - val_accuracy: 0.6794\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0909 - accuracy: 0.6765 - val_loss: 1.1149 - val_accuracy: 0.6749\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0864 - accuracy: 0.6791 - val_loss: 1.1081 - val_accuracy: 0.6744\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0816 - accuracy: 0.6800 - val_loss: 1.0994 - val_accuracy: 0.6813\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0783 - accuracy: 0.6812 - val_loss: 1.0957 - val_accuracy: 0.6821\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0739 - accuracy: 0.6821 - val_loss: 1.0853 - val_accuracy: 0.6873\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0697 - accuracy: 0.6834 - val_loss: 1.0993 - val_accuracy: 0.6796\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0660 - accuracy: 0.6845 - val_loss: 1.0956 - val_accuracy: 0.6847\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0629 - accuracy: 0.6851 - val_loss: 1.0588 - val_accuracy: 0.7009\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0588 - accuracy: 0.6868 - val_loss: 1.0824 - val_accuracy: 0.6832\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0556 - accuracy: 0.6868 - val_loss: 1.0970 - val_accuracy: 0.6803\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0520 - accuracy: 0.6871 - val_loss: 1.0866 - val_accuracy: 0.6883\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0473 - accuracy: 0.6905 - val_loss: 1.0750 - val_accuracy: 0.6891\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0448 - accuracy: 0.6909 - val_loss: 1.0786 - val_accuracy: 0.6877\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0407 - accuracy: 0.6911 - val_loss: 1.0655 - val_accuracy: 0.6979\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0368 - accuracy: 0.6928 - val_loss: 1.0623 - val_accuracy: 0.6984\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0336 - accuracy: 0.6942 - val_loss: 1.0870 - val_accuracy: 0.6905\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0314 - accuracy: 0.6943 - val_loss: 1.0870 - val_accuracy: 0.6811\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0280 - accuracy: 0.6950 - val_loss: 1.0696 - val_accuracy: 0.6944\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0241 - accuracy: 0.6963 - val_loss: 1.0655 - val_accuracy: 0.6969\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0205 - accuracy: 0.6970 - val_loss: 1.0526 - val_accuracy: 0.6971\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0181 - accuracy: 0.6984 - val_loss: 1.0260 - val_accuracy: 0.7114\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0152 - accuracy: 0.7003 - val_loss: 1.0617 - val_accuracy: 0.6948\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0114 - accuracy: 0.7001 - val_loss: 1.0498 - val_accuracy: 0.7001\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0086 - accuracy: 0.7010 - val_loss: 1.0349 - val_accuracy: 0.7025\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0053 - accuracy: 0.7025 - val_loss: 1.0616 - val_accuracy: 0.6951\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0030 - accuracy: 0.7018 - val_loss: 1.0405 - val_accuracy: 0.7014\n",
      "Try 9/100: Best_val_acc: [1.0324205160140991, 0.6980000138282776], lr: 1.0069269286149196e-05, Lambda: 3.841232793146588e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-5.0, -4.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5.0,-4.0))\n",
    "    best_acc = basicHPCheckFCNN(100, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1281912,
     "status": "ok",
     "timestamp": 1594538655156,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "im7MgOFoBsQ6",
    "outputId": "a2f2a8da-90af-4417-fd27-592854e1d717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2934 - accuracy: 0.1360 - val_loss: 2.2209 - val_accuracy: 0.2622\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1994 - accuracy: 0.2295 - val_loss: 2.1005 - val_accuracy: 0.2954\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0523 - accuracy: 0.3310 - val_loss: 1.9516 - val_accuracy: 0.4512\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8639 - accuracy: 0.4219 - val_loss: 1.6906 - val_accuracy: 0.5521\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6873 - accuracy: 0.4808 - val_loss: 1.5480 - val_accuracy: 0.5851\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5479 - accuracy: 0.5272 - val_loss: 1.4894 - val_accuracy: 0.5734\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4493 - accuracy: 0.5593 - val_loss: 1.3706 - val_accuracy: 0.5997\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3721 - accuracy: 0.5852 - val_loss: 1.3508 - val_accuracy: 0.5959\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3099 - accuracy: 0.6052 - val_loss: 1.2604 - val_accuracy: 0.6266\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2603 - accuracy: 0.6210 - val_loss: 1.2345 - val_accuracy: 0.6447\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2189 - accuracy: 0.6347 - val_loss: 1.2425 - val_accuracy: 0.6310\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1782 - accuracy: 0.6491 - val_loss: 1.1574 - val_accuracy: 0.6430\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1437 - accuracy: 0.6561 - val_loss: 1.1883 - val_accuracy: 0.6409\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1172 - accuracy: 0.6664 - val_loss: 1.1204 - val_accuracy: 0.6746\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0893 - accuracy: 0.6757 - val_loss: 1.0801 - val_accuracy: 0.6803\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0606 - accuracy: 0.6854 - val_loss: 1.0812 - val_accuracy: 0.6784\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0397 - accuracy: 0.6910 - val_loss: 0.9938 - val_accuracy: 0.7139\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0263 - accuracy: 0.6932 - val_loss: 1.0006 - val_accuracy: 0.7116\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0000 - accuracy: 0.7045 - val_loss: 1.0210 - val_accuracy: 0.7096\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9843 - accuracy: 0.7072 - val_loss: 0.9639 - val_accuracy: 0.7188\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9654 - accuracy: 0.7127 - val_loss: 0.9946 - val_accuracy: 0.7132\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9513 - accuracy: 0.7187 - val_loss: 0.9624 - val_accuracy: 0.7284\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9421 - accuracy: 0.7191 - val_loss: 0.9421 - val_accuracy: 0.7303\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9196 - accuracy: 0.7263 - val_loss: 0.9272 - val_accuracy: 0.7354\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9074 - accuracy: 0.7301 - val_loss: 0.9554 - val_accuracy: 0.7343\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8967 - accuracy: 0.7340 - val_loss: 0.9268 - val_accuracy: 0.7392\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8779 - accuracy: 0.7398 - val_loss: 0.8693 - val_accuracy: 0.7527\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8686 - accuracy: 0.7427 - val_loss: 0.9739 - val_accuracy: 0.7227\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8585 - accuracy: 0.7447 - val_loss: 0.9216 - val_accuracy: 0.7432\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8462 - accuracy: 0.7478 - val_loss: 0.8926 - val_accuracy: 0.7373\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8414 - accuracy: 0.7501 - val_loss: 0.9731 - val_accuracy: 0.7196\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8288 - accuracy: 0.7538 - val_loss: 0.8866 - val_accuracy: 0.7397\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8152 - accuracy: 0.7581 - val_loss: 0.8812 - val_accuracy: 0.7543\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8113 - accuracy: 0.7588 - val_loss: 0.9741 - val_accuracy: 0.7168\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7978 - accuracy: 0.7627 - val_loss: 0.8700 - val_accuracy: 0.7480\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7871 - accuracy: 0.7666 - val_loss: 0.7937 - val_accuracy: 0.7800\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7780 - accuracy: 0.7694 - val_loss: 0.7795 - val_accuracy: 0.7843\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7772 - accuracy: 0.7682 - val_loss: 0.7716 - val_accuracy: 0.7814\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7650 - accuracy: 0.7731 - val_loss: 0.7838 - val_accuracy: 0.7796\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7552 - accuracy: 0.7763 - val_loss: 0.8100 - val_accuracy: 0.7664\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7475 - accuracy: 0.7765 - val_loss: 0.7579 - val_accuracy: 0.7848\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7393 - accuracy: 0.7796 - val_loss: 0.7944 - val_accuracy: 0.7752\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7297 - accuracy: 0.7834 - val_loss: 0.7398 - val_accuracy: 0.7890\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7241 - accuracy: 0.7836 - val_loss: 0.7411 - val_accuracy: 0.7835\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7282 - accuracy: 0.7821 - val_loss: 0.6962 - val_accuracy: 0.8051\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7103 - accuracy: 0.7895 - val_loss: 0.7937 - val_accuracy: 0.7776\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7126 - accuracy: 0.7879 - val_loss: 0.7524 - val_accuracy: 0.7801\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7022 - accuracy: 0.7900 - val_loss: 0.6824 - val_accuracy: 0.8116\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6978 - accuracy: 0.7925 - val_loss: 0.6915 - val_accuracy: 0.8007\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6952 - accuracy: 0.7930 - val_loss: 0.7390 - val_accuracy: 0.7882\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6836 - accuracy: 0.7967 - val_loss: 0.7315 - val_accuracy: 0.7976\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6810 - accuracy: 0.7983 - val_loss: 0.7242 - val_accuracy: 0.8008\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6750 - accuracy: 0.8000 - val_loss: 0.7238 - val_accuracy: 0.7963\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6666 - accuracy: 0.8018 - val_loss: 0.6970 - val_accuracy: 0.7953\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6619 - accuracy: 0.8036 - val_loss: 0.7836 - val_accuracy: 0.7752\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6537 - accuracy: 0.8057 - val_loss: 0.6501 - val_accuracy: 0.8172\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6542 - accuracy: 0.8046 - val_loss: 0.6603 - val_accuracy: 0.8151\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6453 - accuracy: 0.8088 - val_loss: 0.7307 - val_accuracy: 0.7987\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6430 - accuracy: 0.8090 - val_loss: 0.7248 - val_accuracy: 0.7979\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6353 - accuracy: 0.8120 - val_loss: 0.6571 - val_accuracy: 0.8151\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6311 - accuracy: 0.8124 - val_loss: 0.7283 - val_accuracy: 0.7944\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6254 - accuracy: 0.8140 - val_loss: 0.7013 - val_accuracy: 0.8012\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6208 - accuracy: 0.8163 - val_loss: 0.6689 - val_accuracy: 0.8119\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6160 - accuracy: 0.8166 - val_loss: 0.7327 - val_accuracy: 0.7914\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6197 - accuracy: 0.8153 - val_loss: 0.6172 - val_accuracy: 0.8196\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6153 - accuracy: 0.8174 - val_loss: 0.6380 - val_accuracy: 0.8222\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6049 - accuracy: 0.8226 - val_loss: 0.6517 - val_accuracy: 0.8111\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5987 - accuracy: 0.8232 - val_loss: 0.6026 - val_accuracy: 0.8332\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6005 - accuracy: 0.8217 - val_loss: 0.6643 - val_accuracy: 0.8138\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5980 - accuracy: 0.8208 - val_loss: 0.6309 - val_accuracy: 0.8226\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5882 - accuracy: 0.8249 - val_loss: 0.6252 - val_accuracy: 0.8286\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5830 - accuracy: 0.8261 - val_loss: 0.6692 - val_accuracy: 0.8146\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5829 - accuracy: 0.8260 - val_loss: 0.6129 - val_accuracy: 0.8255\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5721 - accuracy: 0.8312 - val_loss: 0.6525 - val_accuracy: 0.8174\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5749 - accuracy: 0.8289 - val_loss: 0.6104 - val_accuracy: 0.8319\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5659 - accuracy: 0.8328 - val_loss: 0.6126 - val_accuracy: 0.8299\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5644 - accuracy: 0.8331 - val_loss: 0.6324 - val_accuracy: 0.8204\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5640 - accuracy: 0.8345 - val_loss: 0.6475 - val_accuracy: 0.8199\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5591 - accuracy: 0.8351 - val_loss: 0.5983 - val_accuracy: 0.8343\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5531 - accuracy: 0.8363 - val_loss: 0.5856 - val_accuracy: 0.8371\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5507 - accuracy: 0.8369 - val_loss: 0.5941 - val_accuracy: 0.8380\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5422 - accuracy: 0.8400 - val_loss: 0.5996 - val_accuracy: 0.8339\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5434 - accuracy: 0.8392 - val_loss: 0.6376 - val_accuracy: 0.8182\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5510 - accuracy: 0.8359 - val_loss: 0.6539 - val_accuracy: 0.8110\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5438 - accuracy: 0.8383 - val_loss: 0.6249 - val_accuracy: 0.8231\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5376 - accuracy: 0.8403 - val_loss: 0.6319 - val_accuracy: 0.8250\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5338 - accuracy: 0.8418 - val_loss: 0.5763 - val_accuracy: 0.8324\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5279 - accuracy: 0.8439 - val_loss: 0.5840 - val_accuracy: 0.8326\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5325 - accuracy: 0.8417 - val_loss: 0.6370 - val_accuracy: 0.8263\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5200 - accuracy: 0.8462 - val_loss: 0.5761 - val_accuracy: 0.8291\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5256 - accuracy: 0.8443 - val_loss: 0.6139 - val_accuracy: 0.8249\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5142 - accuracy: 0.8473 - val_loss: 0.5771 - val_accuracy: 0.8399\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5116 - accuracy: 0.8478 - val_loss: 0.5606 - val_accuracy: 0.8454\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5123 - accuracy: 0.8486 - val_loss: 0.5906 - val_accuracy: 0.8336\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5099 - accuracy: 0.8497 - val_loss: 0.5546 - val_accuracy: 0.8468\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5008 - accuracy: 0.8516 - val_loss: 0.5860 - val_accuracy: 0.8280\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5061 - accuracy: 0.8490 - val_loss: 0.5975 - val_accuracy: 0.8321\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4977 - accuracy: 0.8532 - val_loss: 0.5600 - val_accuracy: 0.8446\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4967 - accuracy: 0.8514 - val_loss: 0.4991 - val_accuracy: 0.8592\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5010 - accuracy: 0.8510 - val_loss: 0.5719 - val_accuracy: 0.8404\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4936 - accuracy: 0.8527 - val_loss: 0.5338 - val_accuracy: 0.8525\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4849 - accuracy: 0.8576 - val_loss: 0.5323 - val_accuracy: 0.8464\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4839 - accuracy: 0.8577 - val_loss: 0.5892 - val_accuracy: 0.8346\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4833 - accuracy: 0.8575 - val_loss: 0.5347 - val_accuracy: 0.8493\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4773 - accuracy: 0.8590 - val_loss: 0.5755 - val_accuracy: 0.8385\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4782 - accuracy: 0.8596 - val_loss: 0.5413 - val_accuracy: 0.8454\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4758 - accuracy: 0.8595 - val_loss: 0.5532 - val_accuracy: 0.8409\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4722 - accuracy: 0.8600 - val_loss: 0.5063 - val_accuracy: 0.8585\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4695 - accuracy: 0.8611 - val_loss: 0.5288 - val_accuracy: 0.8459\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4660 - accuracy: 0.8620 - val_loss: 0.5541 - val_accuracy: 0.8474\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4610 - accuracy: 0.8639 - val_loss: 0.5358 - val_accuracy: 0.8479\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4631 - accuracy: 0.8630 - val_loss: 0.5238 - val_accuracy: 0.8516\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4581 - accuracy: 0.8631 - val_loss: 0.4868 - val_accuracy: 0.8674\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4570 - accuracy: 0.8659 - val_loss: 0.5139 - val_accuracy: 0.8541\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4603 - accuracy: 0.8628 - val_loss: 0.5063 - val_accuracy: 0.8550\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4542 - accuracy: 0.8652 - val_loss: 0.5387 - val_accuracy: 0.8452\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4494 - accuracy: 0.8671 - val_loss: 0.5281 - val_accuracy: 0.8532\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4464 - accuracy: 0.8686 - val_loss: 0.5387 - val_accuracy: 0.8464\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4517 - accuracy: 0.8654 - val_loss: 0.5258 - val_accuracy: 0.8496\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4512 - accuracy: 0.8663 - val_loss: 0.5376 - val_accuracy: 0.8461\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4428 - accuracy: 0.8689 - val_loss: 0.4844 - val_accuracy: 0.8606\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4435 - accuracy: 0.8696 - val_loss: 0.5568 - val_accuracy: 0.8434\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4365 - accuracy: 0.8702 - val_loss: 0.4678 - val_accuracy: 0.8681\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4345 - accuracy: 0.8714 - val_loss: 0.5400 - val_accuracy: 0.8441\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4361 - accuracy: 0.8715 - val_loss: 0.5582 - val_accuracy: 0.8442\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4311 - accuracy: 0.8719 - val_loss: 0.4654 - val_accuracy: 0.8716\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4274 - accuracy: 0.8732 - val_loss: 0.4688 - val_accuracy: 0.8668\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4217 - accuracy: 0.8754 - val_loss: 0.4737 - val_accuracy: 0.8664\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4204 - accuracy: 0.8762 - val_loss: 0.5072 - val_accuracy: 0.8588\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4210 - accuracy: 0.8765 - val_loss: 0.5103 - val_accuracy: 0.8550\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4187 - accuracy: 0.8758 - val_loss: 0.5025 - val_accuracy: 0.8541\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4188 - accuracy: 0.8758 - val_loss: 0.4866 - val_accuracy: 0.8626\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4184 - accuracy: 0.8769 - val_loss: 0.4789 - val_accuracy: 0.8669\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4134 - accuracy: 0.8782 - val_loss: 0.5166 - val_accuracy: 0.8516\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4123 - accuracy: 0.8792 - val_loss: 0.4746 - val_accuracy: 0.8689\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4076 - accuracy: 0.8799 - val_loss: 0.5283 - val_accuracy: 0.8485\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4071 - accuracy: 0.8797 - val_loss: 0.5597 - val_accuracy: 0.8396\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4043 - accuracy: 0.8809 - val_loss: 0.4880 - val_accuracy: 0.8649\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4113 - accuracy: 0.8783 - val_loss: 0.4966 - val_accuracy: 0.8604\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4015 - accuracy: 0.8813 - val_loss: 0.5332 - val_accuracy: 0.8455\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3996 - accuracy: 0.8817 - val_loss: 0.4845 - val_accuracy: 0.8645\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4028 - accuracy: 0.8794 - val_loss: 0.4806 - val_accuracy: 0.8661\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3989 - accuracy: 0.8824 - val_loss: 0.5128 - val_accuracy: 0.8525\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3891 - accuracy: 0.8846 - val_loss: 0.5112 - val_accuracy: 0.8515\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3947 - accuracy: 0.8828 - val_loss: 0.5758 - val_accuracy: 0.8378\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3883 - accuracy: 0.8860 - val_loss: 0.5204 - val_accuracy: 0.8545\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3865 - accuracy: 0.8869 - val_loss: 0.5144 - val_accuracy: 0.8552\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3872 - accuracy: 0.8858 - val_loss: 0.4957 - val_accuracy: 0.8599\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3893 - accuracy: 0.8849 - val_loss: 0.4773 - val_accuracy: 0.8646\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3831 - accuracy: 0.8875 - val_loss: 0.5231 - val_accuracy: 0.8473\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3837 - accuracy: 0.8860 - val_loss: 0.4858 - val_accuracy: 0.8650\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3843 - accuracy: 0.8865 - val_loss: 0.4514 - val_accuracy: 0.8764\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3770 - accuracy: 0.8895 - val_loss: 0.4963 - val_accuracy: 0.8621\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3737 - accuracy: 0.8902 - val_loss: 0.4888 - val_accuracy: 0.8641\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3708 - accuracy: 0.8912 - val_loss: 0.4531 - val_accuracy: 0.8749\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3710 - accuracy: 0.8912 - val_loss: 0.4357 - val_accuracy: 0.8779\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3713 - accuracy: 0.8901 - val_loss: 0.4820 - val_accuracy: 0.8674\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3690 - accuracy: 0.8918 - val_loss: 0.5050 - val_accuracy: 0.8590\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3683 - accuracy: 0.8919 - val_loss: 0.4610 - val_accuracy: 0.8734\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3686 - accuracy: 0.8914 - val_loss: 0.4944 - val_accuracy: 0.8626\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3622 - accuracy: 0.8949 - val_loss: 0.4593 - val_accuracy: 0.8707\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3677 - accuracy: 0.8911 - val_loss: 0.5044 - val_accuracy: 0.8587\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3573 - accuracy: 0.8952 - val_loss: 0.4101 - val_accuracy: 0.8905\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3648 - accuracy: 0.8921 - val_loss: 0.4492 - val_accuracy: 0.8746\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3627 - accuracy: 0.8933 - val_loss: 0.4535 - val_accuracy: 0.8735\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3531 - accuracy: 0.8963 - val_loss: 0.4496 - val_accuracy: 0.8738\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3513 - accuracy: 0.8964 - val_loss: 0.4521 - val_accuracy: 0.8766\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3535 - accuracy: 0.8957 - val_loss: 0.4443 - val_accuracy: 0.8785\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3456 - accuracy: 0.8997 - val_loss: 0.4279 - val_accuracy: 0.8820\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3490 - accuracy: 0.8987 - val_loss: 0.4487 - val_accuracy: 0.8754\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3453 - accuracy: 0.8995 - val_loss: 0.4175 - val_accuracy: 0.8839\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3442 - accuracy: 0.8991 - val_loss: 0.5176 - val_accuracy: 0.8564\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3441 - accuracy: 0.8992 - val_loss: 0.5308 - val_accuracy: 0.8513\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3443 - accuracy: 0.8985 - val_loss: 0.4883 - val_accuracy: 0.8642\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3425 - accuracy: 0.8986 - val_loss: 0.4087 - val_accuracy: 0.8877\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3398 - accuracy: 0.9015 - val_loss: 0.4248 - val_accuracy: 0.8836\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3342 - accuracy: 0.9021 - val_loss: 0.4588 - val_accuracy: 0.8733\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3357 - accuracy: 0.9018 - val_loss: 0.4225 - val_accuracy: 0.8856\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3338 - accuracy: 0.9022 - val_loss: 0.4758 - val_accuracy: 0.8681\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3330 - accuracy: 0.9027 - val_loss: 0.4401 - val_accuracy: 0.8749\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3396 - accuracy: 0.8999 - val_loss: 0.4385 - val_accuracy: 0.8813\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3322 - accuracy: 0.9018 - val_loss: 0.5155 - val_accuracy: 0.8538\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3282 - accuracy: 0.9040 - val_loss: 0.4366 - val_accuracy: 0.8799\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3262 - accuracy: 0.9045 - val_loss: 0.4287 - val_accuracy: 0.8804\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3263 - accuracy: 0.9051 - val_loss: 0.4490 - val_accuracy: 0.8761\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3257 - accuracy: 0.9060 - val_loss: 0.4770 - val_accuracy: 0.8665\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3234 - accuracy: 0.9049 - val_loss: 0.4148 - val_accuracy: 0.8844\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3155 - accuracy: 0.9097 - val_loss: 0.4562 - val_accuracy: 0.8744\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3196 - accuracy: 0.9069 - val_loss: 0.4089 - val_accuracy: 0.8856\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3158 - accuracy: 0.9093 - val_loss: 0.3966 - val_accuracy: 0.8924\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3219 - accuracy: 0.9047 - val_loss: 0.4205 - val_accuracy: 0.8839\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3170 - accuracy: 0.9077 - val_loss: 0.4339 - val_accuracy: 0.8813\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3148 - accuracy: 0.9084 - val_loss: 0.4529 - val_accuracy: 0.8742\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3150 - accuracy: 0.9080 - val_loss: 0.4140 - val_accuracy: 0.8837\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3079 - accuracy: 0.9119 - val_loss: 0.4522 - val_accuracy: 0.8751\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3086 - accuracy: 0.9101 - val_loss: 0.4318 - val_accuracy: 0.8810\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3015 - accuracy: 0.9141 - val_loss: 0.4195 - val_accuracy: 0.8875\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3049 - accuracy: 0.9121 - val_loss: 0.4099 - val_accuracy: 0.8845\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3102 - accuracy: 0.9091 - val_loss: 0.4554 - val_accuracy: 0.8749\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3036 - accuracy: 0.9121 - val_loss: 0.3915 - val_accuracy: 0.8928\n",
      "Try 1/100: Best_val_acc: [0.6310116648674011, 0.8339999914169312], lr: 6.300137040801173e-05, Lambda: 5.927995073366053e-05\n",
      "\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.2883 - accuracy: 0.1457 - val_loss: 2.2533 - val_accuracy: 0.2029\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2047 - accuracy: 0.2422 - val_loss: 2.0736 - val_accuracy: 0.4218\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0458 - accuracy: 0.3362 - val_loss: 1.8624 - val_accuracy: 0.4842\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8483 - accuracy: 0.4271 - val_loss: 1.6758 - val_accuracy: 0.5581\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6665 - accuracy: 0.5043 - val_loss: 1.5976 - val_accuracy: 0.5237\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5290 - accuracy: 0.5441 - val_loss: 1.4095 - val_accuracy: 0.6147\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4282 - accuracy: 0.5696 - val_loss: 1.4370 - val_accuracy: 0.5755\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3536 - accuracy: 0.5910 - val_loss: 1.3033 - val_accuracy: 0.6244\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2998 - accuracy: 0.6053 - val_loss: 1.3475 - val_accuracy: 0.6023\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2528 - accuracy: 0.6219 - val_loss: 1.1817 - val_accuracy: 0.6704\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2158 - accuracy: 0.6336 - val_loss: 1.1618 - val_accuracy: 0.6703\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1815 - accuracy: 0.6438 - val_loss: 1.1776 - val_accuracy: 0.6598\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1512 - accuracy: 0.6540 - val_loss: 1.1557 - val_accuracy: 0.6649\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1213 - accuracy: 0.6639 - val_loss: 1.0959 - val_accuracy: 0.6794\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0961 - accuracy: 0.6713 - val_loss: 1.0675 - val_accuracy: 0.6990\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0699 - accuracy: 0.6793 - val_loss: 1.1249 - val_accuracy: 0.6638\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0423 - accuracy: 0.6899 - val_loss: 1.0517 - val_accuracy: 0.6961\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0234 - accuracy: 0.6950 - val_loss: 1.0888 - val_accuracy: 0.6836\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0052 - accuracy: 0.7000 - val_loss: 1.0616 - val_accuracy: 0.6882\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9855 - accuracy: 0.7083 - val_loss: 1.0168 - val_accuracy: 0.7059\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9650 - accuracy: 0.7135 - val_loss: 1.0080 - val_accuracy: 0.7095\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9476 - accuracy: 0.7187 - val_loss: 0.9656 - val_accuracy: 0.7184\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9316 - accuracy: 0.7241 - val_loss: 0.9461 - val_accuracy: 0.7254\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9173 - accuracy: 0.7276 - val_loss: 0.9423 - val_accuracy: 0.7302\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8972 - accuracy: 0.7326 - val_loss: 0.9233 - val_accuracy: 0.7336\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8830 - accuracy: 0.7372 - val_loss: 0.8845 - val_accuracy: 0.7426\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8677 - accuracy: 0.7422 - val_loss: 0.9014 - val_accuracy: 0.7382\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8635 - accuracy: 0.7430 - val_loss: 0.9051 - val_accuracy: 0.7391\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8484 - accuracy: 0.7487 - val_loss: 0.8681 - val_accuracy: 0.7476\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8323 - accuracy: 0.7537 - val_loss: 0.8860 - val_accuracy: 0.7391\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8207 - accuracy: 0.7562 - val_loss: 0.9204 - val_accuracy: 0.7309\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8137 - accuracy: 0.7595 - val_loss: 0.8715 - val_accuracy: 0.7450\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7988 - accuracy: 0.7624 - val_loss: 0.8598 - val_accuracy: 0.7459\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7926 - accuracy: 0.7645 - val_loss: 0.7663 - val_accuracy: 0.7840\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7795 - accuracy: 0.7700 - val_loss: 0.8513 - val_accuracy: 0.7542\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7715 - accuracy: 0.7712 - val_loss: 0.7822 - val_accuracy: 0.7763\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7606 - accuracy: 0.7739 - val_loss: 0.8786 - val_accuracy: 0.7342\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7598 - accuracy: 0.7731 - val_loss: 0.8027 - val_accuracy: 0.7681\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7464 - accuracy: 0.7774 - val_loss: 0.8007 - val_accuracy: 0.7644\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7431 - accuracy: 0.7776 - val_loss: 0.7269 - val_accuracy: 0.7929\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7351 - accuracy: 0.7812 - val_loss: 0.8031 - val_accuracy: 0.7647\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7247 - accuracy: 0.7838 - val_loss: 0.7692 - val_accuracy: 0.7699\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7136 - accuracy: 0.7885 - val_loss: 0.8242 - val_accuracy: 0.7577\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7125 - accuracy: 0.7872 - val_loss: 0.7501 - val_accuracy: 0.7826\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7037 - accuracy: 0.7906 - val_loss: 0.7221 - val_accuracy: 0.7916\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6977 - accuracy: 0.7905 - val_loss: 0.7723 - val_accuracy: 0.7661\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6941 - accuracy: 0.7924 - val_loss: 0.7984 - val_accuracy: 0.7693\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6868 - accuracy: 0.7944 - val_loss: 0.7133 - val_accuracy: 0.7936\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6802 - accuracy: 0.7984 - val_loss: 0.7427 - val_accuracy: 0.7803\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6731 - accuracy: 0.7996 - val_loss: 0.6976 - val_accuracy: 0.7975\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6705 - accuracy: 0.8003 - val_loss: 0.6858 - val_accuracy: 0.8004\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6624 - accuracy: 0.8014 - val_loss: 0.7690 - val_accuracy: 0.7766\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6554 - accuracy: 0.8041 - val_loss: 0.7098 - val_accuracy: 0.7924\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6524 - accuracy: 0.8049 - val_loss: 0.6920 - val_accuracy: 0.7988\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6453 - accuracy: 0.8076 - val_loss: 0.6927 - val_accuracy: 0.8002\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6388 - accuracy: 0.8092 - val_loss: 0.6895 - val_accuracy: 0.8027\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6416 - accuracy: 0.8080 - val_loss: 0.8193 - val_accuracy: 0.7602\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6325 - accuracy: 0.8105 - val_loss: 0.7026 - val_accuracy: 0.7876\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6293 - accuracy: 0.8120 - val_loss: 0.6608 - val_accuracy: 0.8117\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6226 - accuracy: 0.8145 - val_loss: 0.7201 - val_accuracy: 0.7893\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6155 - accuracy: 0.8152 - val_loss: 0.6193 - val_accuracy: 0.8234\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6110 - accuracy: 0.8198 - val_loss: 0.6481 - val_accuracy: 0.8166\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6084 - accuracy: 0.8179 - val_loss: 0.7210 - val_accuracy: 0.7931\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6020 - accuracy: 0.8205 - val_loss: 0.6575 - val_accuracy: 0.8101\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6047 - accuracy: 0.8191 - val_loss: 0.6472 - val_accuracy: 0.8129\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5974 - accuracy: 0.8223 - val_loss: 0.6857 - val_accuracy: 0.8045\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5924 - accuracy: 0.8223 - val_loss: 0.5840 - val_accuracy: 0.8368\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5953 - accuracy: 0.8216 - val_loss: 0.6488 - val_accuracy: 0.8114\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5815 - accuracy: 0.8268 - val_loss: 0.6468 - val_accuracy: 0.8122\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5880 - accuracy: 0.8229 - val_loss: 0.6565 - val_accuracy: 0.8124\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5765 - accuracy: 0.8274 - val_loss: 0.6234 - val_accuracy: 0.8223\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5692 - accuracy: 0.8293 - val_loss: 0.6358 - val_accuracy: 0.8200\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5656 - accuracy: 0.8312 - val_loss: 0.6722 - val_accuracy: 0.8000\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5635 - accuracy: 0.8319 - val_loss: 0.6349 - val_accuracy: 0.8155\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5652 - accuracy: 0.8313 - val_loss: 0.6285 - val_accuracy: 0.8147\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5584 - accuracy: 0.8345 - val_loss: 0.6849 - val_accuracy: 0.7951\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5526 - accuracy: 0.8345 - val_loss: 0.5713 - val_accuracy: 0.8365\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5539 - accuracy: 0.8347 - val_loss: 0.6974 - val_accuracy: 0.7991\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5461 - accuracy: 0.8372 - val_loss: 0.6161 - val_accuracy: 0.8238\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5435 - accuracy: 0.8385 - val_loss: 0.5948 - val_accuracy: 0.8304\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5478 - accuracy: 0.8355 - val_loss: 0.5945 - val_accuracy: 0.8281\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5423 - accuracy: 0.8385 - val_loss: 0.5844 - val_accuracy: 0.8350\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5324 - accuracy: 0.8419 - val_loss: 0.5486 - val_accuracy: 0.8454\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5294 - accuracy: 0.8421 - val_loss: 0.5959 - val_accuracy: 0.8284\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5328 - accuracy: 0.8394 - val_loss: 0.5621 - val_accuracy: 0.8381\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5267 - accuracy: 0.8429 - val_loss: 0.6075 - val_accuracy: 0.8275\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5228 - accuracy: 0.8429 - val_loss: 0.5550 - val_accuracy: 0.8394\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5205 - accuracy: 0.8444 - val_loss: 0.5937 - val_accuracy: 0.8296\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5143 - accuracy: 0.8450 - val_loss: 0.5501 - val_accuracy: 0.8455\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5105 - accuracy: 0.8481 - val_loss: 0.6228 - val_accuracy: 0.8218\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5099 - accuracy: 0.8483 - val_loss: 0.6113 - val_accuracy: 0.8247\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5098 - accuracy: 0.8488 - val_loss: 0.6250 - val_accuracy: 0.8234\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5065 - accuracy: 0.8477 - val_loss: 0.6068 - val_accuracy: 0.8290\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4988 - accuracy: 0.8502 - val_loss: 0.6133 - val_accuracy: 0.8199\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4990 - accuracy: 0.8498 - val_loss: 0.5875 - val_accuracy: 0.8327\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4925 - accuracy: 0.8530 - val_loss: 0.5780 - val_accuracy: 0.8372\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4965 - accuracy: 0.8514 - val_loss: 0.5418 - val_accuracy: 0.8455\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4879 - accuracy: 0.8551 - val_loss: 0.5466 - val_accuracy: 0.8471\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4854 - accuracy: 0.8546 - val_loss: 0.5635 - val_accuracy: 0.8390\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4872 - accuracy: 0.8544 - val_loss: 0.5059 - val_accuracy: 0.8536\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4799 - accuracy: 0.8573 - val_loss: 0.5915 - val_accuracy: 0.8310\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4783 - accuracy: 0.8565 - val_loss: 0.4982 - val_accuracy: 0.8601\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4773 - accuracy: 0.8566 - val_loss: 0.5559 - val_accuracy: 0.8406\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4753 - accuracy: 0.8570 - val_loss: 0.5676 - val_accuracy: 0.8340\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4714 - accuracy: 0.8596 - val_loss: 0.5580 - val_accuracy: 0.8387\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4663 - accuracy: 0.8608 - val_loss: 0.4701 - val_accuracy: 0.8659\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4737 - accuracy: 0.8581 - val_loss: 0.4886 - val_accuracy: 0.8615\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4636 - accuracy: 0.8613 - val_loss: 0.5194 - val_accuracy: 0.8529\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4623 - accuracy: 0.8617 - val_loss: 0.5320 - val_accuracy: 0.8503\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4558 - accuracy: 0.8626 - val_loss: 0.5744 - val_accuracy: 0.8314\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4553 - accuracy: 0.8634 - val_loss: 0.5521 - val_accuracy: 0.8383\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4550 - accuracy: 0.8625 - val_loss: 0.5583 - val_accuracy: 0.8391\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4573 - accuracy: 0.8622 - val_loss: 0.5522 - val_accuracy: 0.8404\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4546 - accuracy: 0.8624 - val_loss: 0.6099 - val_accuracy: 0.8228\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4481 - accuracy: 0.8659 - val_loss: 0.5592 - val_accuracy: 0.8396\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4483 - accuracy: 0.8667 - val_loss: 0.5227 - val_accuracy: 0.8514\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4433 - accuracy: 0.8675 - val_loss: 0.5526 - val_accuracy: 0.8391\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4390 - accuracy: 0.8691 - val_loss: 0.5219 - val_accuracy: 0.8519\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4357 - accuracy: 0.8709 - val_loss: 0.5532 - val_accuracy: 0.8399\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4369 - accuracy: 0.8693 - val_loss: 0.5339 - val_accuracy: 0.8448\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4344 - accuracy: 0.8694 - val_loss: 0.4737 - val_accuracy: 0.8664\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4289 - accuracy: 0.8727 - val_loss: 0.5430 - val_accuracy: 0.8407\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4288 - accuracy: 0.8713 - val_loss: 0.5364 - val_accuracy: 0.8461\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4291 - accuracy: 0.8713 - val_loss: 0.4755 - val_accuracy: 0.8685\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4303 - accuracy: 0.8718 - val_loss: 0.5130 - val_accuracy: 0.8500\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4273 - accuracy: 0.8705 - val_loss: 0.4635 - val_accuracy: 0.8689\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4200 - accuracy: 0.8768 - val_loss: 0.4812 - val_accuracy: 0.8619\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4190 - accuracy: 0.8744 - val_loss: 0.5200 - val_accuracy: 0.8491\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4135 - accuracy: 0.8766 - val_loss: 0.5010 - val_accuracy: 0.8568\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4187 - accuracy: 0.8741 - val_loss: 0.4940 - val_accuracy: 0.8604\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4129 - accuracy: 0.8769 - val_loss: 0.5193 - val_accuracy: 0.8521\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4114 - accuracy: 0.8763 - val_loss: 0.5069 - val_accuracy: 0.8541\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4093 - accuracy: 0.8762 - val_loss: 0.5484 - val_accuracy: 0.8436\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4071 - accuracy: 0.8777 - val_loss: 0.5192 - val_accuracy: 0.8541\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4080 - accuracy: 0.8777 - val_loss: 0.5213 - val_accuracy: 0.8475\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4017 - accuracy: 0.8790 - val_loss: 0.5032 - val_accuracy: 0.8550\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3943 - accuracy: 0.8820 - val_loss: 0.5517 - val_accuracy: 0.8404\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4036 - accuracy: 0.8784 - val_loss: 0.4883 - val_accuracy: 0.8624\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3969 - accuracy: 0.8808 - val_loss: 0.4572 - val_accuracy: 0.8697\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3962 - accuracy: 0.8816 - val_loss: 0.5054 - val_accuracy: 0.8578\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3891 - accuracy: 0.8831 - val_loss: 0.4713 - val_accuracy: 0.8667\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3911 - accuracy: 0.8831 - val_loss: 0.4409 - val_accuracy: 0.8756\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3849 - accuracy: 0.8854 - val_loss: 0.4677 - val_accuracy: 0.8678\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3806 - accuracy: 0.8867 - val_loss: 0.4676 - val_accuracy: 0.8699\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3862 - accuracy: 0.8845 - val_loss: 0.4540 - val_accuracy: 0.8715\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3790 - accuracy: 0.8869 - val_loss: 0.4876 - val_accuracy: 0.8616\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3790 - accuracy: 0.8871 - val_loss: 0.5368 - val_accuracy: 0.8490\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3801 - accuracy: 0.8858 - val_loss: 0.4954 - val_accuracy: 0.8600\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3772 - accuracy: 0.8880 - val_loss: 0.4382 - val_accuracy: 0.8796\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3740 - accuracy: 0.8884 - val_loss: 0.4437 - val_accuracy: 0.8763\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3772 - accuracy: 0.8865 - val_loss: 0.4484 - val_accuracy: 0.8719\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3733 - accuracy: 0.8879 - val_loss: 0.4812 - val_accuracy: 0.8656\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3701 - accuracy: 0.8898 - val_loss: 0.4450 - val_accuracy: 0.8756\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3675 - accuracy: 0.8905 - val_loss: 0.4594 - val_accuracy: 0.8689\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3801 - accuracy: 0.8852 - val_loss: 0.4628 - val_accuracy: 0.8716\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3676 - accuracy: 0.8888 - val_loss: 0.4772 - val_accuracy: 0.8651\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3589 - accuracy: 0.8934 - val_loss: 0.4758 - val_accuracy: 0.8609\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3586 - accuracy: 0.8923 - val_loss: 0.4509 - val_accuracy: 0.8741\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3556 - accuracy: 0.8940 - val_loss: 0.4599 - val_accuracy: 0.8720\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3611 - accuracy: 0.8915 - val_loss: 0.4595 - val_accuracy: 0.8712\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3560 - accuracy: 0.8939 - val_loss: 0.5274 - val_accuracy: 0.8512\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3544 - accuracy: 0.8938 - val_loss: 0.5346 - val_accuracy: 0.8481\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3528 - accuracy: 0.8941 - val_loss: 0.4286 - val_accuracy: 0.8804\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3535 - accuracy: 0.8940 - val_loss: 0.4022 - val_accuracy: 0.8889\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3567 - accuracy: 0.8920 - val_loss: 0.4884 - val_accuracy: 0.8612\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3541 - accuracy: 0.8932 - val_loss: 0.4718 - val_accuracy: 0.8689\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3439 - accuracy: 0.8976 - val_loss: 0.4255 - val_accuracy: 0.8822\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3418 - accuracy: 0.8994 - val_loss: 0.4584 - val_accuracy: 0.8717\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3444 - accuracy: 0.8969 - val_loss: 0.4410 - val_accuracy: 0.8779\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3432 - accuracy: 0.8973 - val_loss: 0.4261 - val_accuracy: 0.8808\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3375 - accuracy: 0.8999 - val_loss: 0.4264 - val_accuracy: 0.8826\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3307 - accuracy: 0.9034 - val_loss: 0.4305 - val_accuracy: 0.8792\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3371 - accuracy: 0.8995 - val_loss: 0.4625 - val_accuracy: 0.8694\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3310 - accuracy: 0.9021 - val_loss: 0.4238 - val_accuracy: 0.8826\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3271 - accuracy: 0.9030 - val_loss: 0.4313 - val_accuracy: 0.8798\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3286 - accuracy: 0.9024 - val_loss: 0.4451 - val_accuracy: 0.8767\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3257 - accuracy: 0.9045 - val_loss: 0.4741 - val_accuracy: 0.8671\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3259 - accuracy: 0.9033 - val_loss: 0.4779 - val_accuracy: 0.8665\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3276 - accuracy: 0.9021 - val_loss: 0.4636 - val_accuracy: 0.8714\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3252 - accuracy: 0.9032 - val_loss: 0.4925 - val_accuracy: 0.8606\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3197 - accuracy: 0.9052 - val_loss: 0.4695 - val_accuracy: 0.8674\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3208 - accuracy: 0.9039 - val_loss: 0.4374 - val_accuracy: 0.8781\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3218 - accuracy: 0.9043 - val_loss: 0.4681 - val_accuracy: 0.8641\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3206 - accuracy: 0.9046 - val_loss: 0.4611 - val_accuracy: 0.8707\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3139 - accuracy: 0.9057 - val_loss: 0.4636 - val_accuracy: 0.8679\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3293 - accuracy: 0.9016 - val_loss: 0.4306 - val_accuracy: 0.8809\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3154 - accuracy: 0.9060 - val_loss: 0.4599 - val_accuracy: 0.8688\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3115 - accuracy: 0.9071 - val_loss: 0.4807 - val_accuracy: 0.8647\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3115 - accuracy: 0.9071 - val_loss: 0.4869 - val_accuracy: 0.8641\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3070 - accuracy: 0.9100 - val_loss: 0.4494 - val_accuracy: 0.8714\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3032 - accuracy: 0.9108 - val_loss: 0.4736 - val_accuracy: 0.8656\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3043 - accuracy: 0.9125 - val_loss: 0.4143 - val_accuracy: 0.8834\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3065 - accuracy: 0.9085 - val_loss: 0.4151 - val_accuracy: 0.8863\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3122 - accuracy: 0.9065 - val_loss: 0.5212 - val_accuracy: 0.8516\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3116 - accuracy: 0.9056 - val_loss: 0.4288 - val_accuracy: 0.8824\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2994 - accuracy: 0.9121 - val_loss: 0.4468 - val_accuracy: 0.8759\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2982 - accuracy: 0.9113 - val_loss: 0.4278 - val_accuracy: 0.8821\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3006 - accuracy: 0.9114 - val_loss: 0.4607 - val_accuracy: 0.8700\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2949 - accuracy: 0.9127 - val_loss: 0.4287 - val_accuracy: 0.8829\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2966 - accuracy: 0.9122 - val_loss: 0.3827 - val_accuracy: 0.8946\n",
      "Try 2/100: Best_val_acc: [0.6322535276412964, 0.832277774810791], lr: 6.226740683093598e-05, Lambda: 6.289113767791814e-05\n",
      "\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2871 - accuracy: 0.1400 - val_loss: 2.2043 - val_accuracy: 0.2776\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1787 - accuracy: 0.2474 - val_loss: 2.1513 - val_accuracy: 0.2961\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0371 - accuracy: 0.3315 - val_loss: 1.9841 - val_accuracy: 0.3888\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8852 - accuracy: 0.3986 - val_loss: 1.8076 - val_accuracy: 0.4866\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7278 - accuracy: 0.4647 - val_loss: 1.6839 - val_accuracy: 0.5030\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5935 - accuracy: 0.5155 - val_loss: 1.5650 - val_accuracy: 0.5407\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4836 - accuracy: 0.5599 - val_loss: 1.4777 - val_accuracy: 0.5779\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3979 - accuracy: 0.5883 - val_loss: 1.4495 - val_accuracy: 0.5673\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3225 - accuracy: 0.6146 - val_loss: 1.2952 - val_accuracy: 0.6324\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2614 - accuracy: 0.6314 - val_loss: 1.2547 - val_accuracy: 0.6334\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2114 - accuracy: 0.6434 - val_loss: 1.2308 - val_accuracy: 0.6411\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1668 - accuracy: 0.6570 - val_loss: 1.2096 - val_accuracy: 0.6568\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1346 - accuracy: 0.6670 - val_loss: 1.1594 - val_accuracy: 0.6688\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0999 - accuracy: 0.6767 - val_loss: 1.0600 - val_accuracy: 0.7056\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0739 - accuracy: 0.6839 - val_loss: 1.1023 - val_accuracy: 0.6855\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0447 - accuracy: 0.6938 - val_loss: 1.0629 - val_accuracy: 0.7011\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0236 - accuracy: 0.6996 - val_loss: 1.0418 - val_accuracy: 0.7017\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0082 - accuracy: 0.7007 - val_loss: 1.0251 - val_accuracy: 0.7085\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9842 - accuracy: 0.7106 - val_loss: 0.9923 - val_accuracy: 0.7176\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9702 - accuracy: 0.7128 - val_loss: 0.9629 - val_accuracy: 0.7299\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9480 - accuracy: 0.7212 - val_loss: 0.9619 - val_accuracy: 0.7286\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9354 - accuracy: 0.7238 - val_loss: 0.9913 - val_accuracy: 0.7152\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9225 - accuracy: 0.7282 - val_loss: 0.9750 - val_accuracy: 0.7180\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9060 - accuracy: 0.7340 - val_loss: 0.9238 - val_accuracy: 0.7332\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8960 - accuracy: 0.7339 - val_loss: 0.9473 - val_accuracy: 0.7294\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8815 - accuracy: 0.7404 - val_loss: 0.8916 - val_accuracy: 0.7473\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8678 - accuracy: 0.7455 - val_loss: 0.9725 - val_accuracy: 0.7140\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8567 - accuracy: 0.7462 - val_loss: 0.9166 - val_accuracy: 0.7374\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8502 - accuracy: 0.7468 - val_loss: 0.8598 - val_accuracy: 0.7566\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8360 - accuracy: 0.7531 - val_loss: 0.8799 - val_accuracy: 0.7474\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8266 - accuracy: 0.7546 - val_loss: 0.8652 - val_accuracy: 0.7526\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8113 - accuracy: 0.7596 - val_loss: 0.8574 - val_accuracy: 0.7551\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8080 - accuracy: 0.7615 - val_loss: 0.8102 - val_accuracy: 0.7726\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7928 - accuracy: 0.7651 - val_loss: 0.7798 - val_accuracy: 0.7817\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7941 - accuracy: 0.7648 - val_loss: 0.8135 - val_accuracy: 0.7637\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7773 - accuracy: 0.7706 - val_loss: 0.8228 - val_accuracy: 0.7636\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7710 - accuracy: 0.7725 - val_loss: 0.8367 - val_accuracy: 0.7566\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7659 - accuracy: 0.7723 - val_loss: 0.7649 - val_accuracy: 0.7853\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7570 - accuracy: 0.7751 - val_loss: 0.8132 - val_accuracy: 0.7690\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7458 - accuracy: 0.7791 - val_loss: 0.8091 - val_accuracy: 0.7679\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7408 - accuracy: 0.7801 - val_loss: 0.7811 - val_accuracy: 0.7762\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7345 - accuracy: 0.7820 - val_loss: 0.7827 - val_accuracy: 0.7767\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7231 - accuracy: 0.7860 - val_loss: 0.7167 - val_accuracy: 0.8011\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7174 - accuracy: 0.7891 - val_loss: 0.7905 - val_accuracy: 0.7727\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7106 - accuracy: 0.7883 - val_loss: 0.7776 - val_accuracy: 0.7748\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7036 - accuracy: 0.7919 - val_loss: 0.7703 - val_accuracy: 0.7756\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7004 - accuracy: 0.7911 - val_loss: 0.8213 - val_accuracy: 0.7503\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6920 - accuracy: 0.7943 - val_loss: 0.7621 - val_accuracy: 0.7826\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6913 - accuracy: 0.7942 - val_loss: 0.7542 - val_accuracy: 0.7839\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6799 - accuracy: 0.7976 - val_loss: 0.7329 - val_accuracy: 0.7911\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6722 - accuracy: 0.8011 - val_loss: 0.7191 - val_accuracy: 0.7976\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6719 - accuracy: 0.8018 - val_loss: 0.7579 - val_accuracy: 0.7806\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6637 - accuracy: 0.8032 - val_loss: 0.7366 - val_accuracy: 0.7920\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6559 - accuracy: 0.8070 - val_loss: 0.7514 - val_accuracy: 0.7784\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6523 - accuracy: 0.8072 - val_loss: 0.7646 - val_accuracy: 0.7809\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6454 - accuracy: 0.8094 - val_loss: 0.6827 - val_accuracy: 0.8076\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6413 - accuracy: 0.8103 - val_loss: 0.7864 - val_accuracy: 0.7752\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6390 - accuracy: 0.8102 - val_loss: 0.7908 - val_accuracy: 0.7726\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6324 - accuracy: 0.8118 - val_loss: 0.6675 - val_accuracy: 0.8130\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6308 - accuracy: 0.8116 - val_loss: 0.7000 - val_accuracy: 0.7977\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6234 - accuracy: 0.8153 - val_loss: 0.6906 - val_accuracy: 0.7997\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6162 - accuracy: 0.8182 - val_loss: 0.6662 - val_accuracy: 0.8120\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6121 - accuracy: 0.8200 - val_loss: 0.6961 - val_accuracy: 0.7985\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6083 - accuracy: 0.8187 - val_loss: 0.6586 - val_accuracy: 0.8116\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6026 - accuracy: 0.8214 - val_loss: 0.6984 - val_accuracy: 0.8039\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5998 - accuracy: 0.8223 - val_loss: 0.6392 - val_accuracy: 0.8224\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5927 - accuracy: 0.8257 - val_loss: 0.6935 - val_accuracy: 0.7981\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5916 - accuracy: 0.8258 - val_loss: 0.6648 - val_accuracy: 0.8118\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5894 - accuracy: 0.8245 - val_loss: 0.7064 - val_accuracy: 0.7963\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5811 - accuracy: 0.8294 - val_loss: 0.6531 - val_accuracy: 0.8116\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5772 - accuracy: 0.8292 - val_loss: 0.6824 - val_accuracy: 0.8051\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5728 - accuracy: 0.8302 - val_loss: 0.6577 - val_accuracy: 0.8148\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5743 - accuracy: 0.8288 - val_loss: 0.5813 - val_accuracy: 0.8352\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5669 - accuracy: 0.8324 - val_loss: 0.6611 - val_accuracy: 0.8134\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5622 - accuracy: 0.8343 - val_loss: 0.6336 - val_accuracy: 0.8201\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5569 - accuracy: 0.8358 - val_loss: 0.6266 - val_accuracy: 0.8206\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5573 - accuracy: 0.8345 - val_loss: 0.6740 - val_accuracy: 0.8069\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5525 - accuracy: 0.8375 - val_loss: 0.6086 - val_accuracy: 0.8275\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5501 - accuracy: 0.8371 - val_loss: 0.5735 - val_accuracy: 0.8394\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5416 - accuracy: 0.8396 - val_loss: 0.6189 - val_accuracy: 0.8258\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5422 - accuracy: 0.8395 - val_loss: 0.6623 - val_accuracy: 0.8089\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5358 - accuracy: 0.8409 - val_loss: 0.5950 - val_accuracy: 0.8324\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5364 - accuracy: 0.8407 - val_loss: 0.5900 - val_accuracy: 0.8338\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5294 - accuracy: 0.8429 - val_loss: 0.6222 - val_accuracy: 0.8226\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5289 - accuracy: 0.8437 - val_loss: 0.6318 - val_accuracy: 0.8202\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5213 - accuracy: 0.8450 - val_loss: 0.6301 - val_accuracy: 0.8199\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5182 - accuracy: 0.8472 - val_loss: 0.5985 - val_accuracy: 0.8255\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5156 - accuracy: 0.8476 - val_loss: 0.5765 - val_accuracy: 0.8346\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5168 - accuracy: 0.8460 - val_loss: 0.6050 - val_accuracy: 0.8256\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5181 - accuracy: 0.8468 - val_loss: 0.6158 - val_accuracy: 0.8221\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5082 - accuracy: 0.8499 - val_loss: 0.6074 - val_accuracy: 0.8278\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5011 - accuracy: 0.8523 - val_loss: 0.5309 - val_accuracy: 0.8521\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5000 - accuracy: 0.8522 - val_loss: 0.6223 - val_accuracy: 0.8204\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4980 - accuracy: 0.8538 - val_loss: 0.5756 - val_accuracy: 0.8344\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4934 - accuracy: 0.8545 - val_loss: 0.5400 - val_accuracy: 0.8475\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4955 - accuracy: 0.8528 - val_loss: 0.5705 - val_accuracy: 0.8364\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4908 - accuracy: 0.8540 - val_loss: 0.6039 - val_accuracy: 0.8296\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4841 - accuracy: 0.8574 - val_loss: 0.5216 - val_accuracy: 0.8558\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4849 - accuracy: 0.8573 - val_loss: 0.6588 - val_accuracy: 0.8088\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4854 - accuracy: 0.8571 - val_loss: 0.5944 - val_accuracy: 0.8308\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4793 - accuracy: 0.8587 - val_loss: 0.5585 - val_accuracy: 0.8408\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4783 - accuracy: 0.8588 - val_loss: 0.5835 - val_accuracy: 0.8315\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4749 - accuracy: 0.8588 - val_loss: 0.5439 - val_accuracy: 0.8488\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4696 - accuracy: 0.8610 - val_loss: 0.5707 - val_accuracy: 0.8370\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4704 - accuracy: 0.8597 - val_loss: 0.6327 - val_accuracy: 0.8181\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4633 - accuracy: 0.8634 - val_loss: 0.5308 - val_accuracy: 0.8489\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4610 - accuracy: 0.8625 - val_loss: 0.6079 - val_accuracy: 0.8221\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4597 - accuracy: 0.8631 - val_loss: 0.5276 - val_accuracy: 0.8487\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4549 - accuracy: 0.8651 - val_loss: 0.5843 - val_accuracy: 0.8358\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4555 - accuracy: 0.8655 - val_loss: 0.5542 - val_accuracy: 0.8427\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4507 - accuracy: 0.8669 - val_loss: 0.5738 - val_accuracy: 0.8363\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4506 - accuracy: 0.8671 - val_loss: 0.5179 - val_accuracy: 0.8524\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4496 - accuracy: 0.8662 - val_loss: 0.5640 - val_accuracy: 0.8413\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4485 - accuracy: 0.8668 - val_loss: 0.5526 - val_accuracy: 0.8390\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4393 - accuracy: 0.8702 - val_loss: 0.5495 - val_accuracy: 0.8451\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4364 - accuracy: 0.8717 - val_loss: 0.5886 - val_accuracy: 0.8277\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4405 - accuracy: 0.8678 - val_loss: 0.6039 - val_accuracy: 0.8251\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4335 - accuracy: 0.8714 - val_loss: 0.5924 - val_accuracy: 0.8309\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4340 - accuracy: 0.8718 - val_loss: 0.5635 - val_accuracy: 0.8359\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4332 - accuracy: 0.8718 - val_loss: 0.5440 - val_accuracy: 0.8454\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4263 - accuracy: 0.8745 - val_loss: 0.5655 - val_accuracy: 0.8399\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4224 - accuracy: 0.8765 - val_loss: 0.5003 - val_accuracy: 0.8584\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4203 - accuracy: 0.8760 - val_loss: 0.5340 - val_accuracy: 0.8497\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4213 - accuracy: 0.8752 - val_loss: 0.5002 - val_accuracy: 0.8624\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4209 - accuracy: 0.8762 - val_loss: 0.5644 - val_accuracy: 0.8402\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4163 - accuracy: 0.8763 - val_loss: 0.4739 - val_accuracy: 0.8681\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4244 - accuracy: 0.8739 - val_loss: 0.5610 - val_accuracy: 0.8425\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4117 - accuracy: 0.8784 - val_loss: 0.5435 - val_accuracy: 0.8464\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4107 - accuracy: 0.8789 - val_loss: 0.5096 - val_accuracy: 0.8563\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4071 - accuracy: 0.8808 - val_loss: 0.5137 - val_accuracy: 0.8511\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4070 - accuracy: 0.8798 - val_loss: 0.5493 - val_accuracy: 0.8456\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4038 - accuracy: 0.8799 - val_loss: 0.4802 - val_accuracy: 0.8667\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4040 - accuracy: 0.8803 - val_loss: 0.4959 - val_accuracy: 0.8618\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4017 - accuracy: 0.8806 - val_loss: 0.4987 - val_accuracy: 0.8606\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3973 - accuracy: 0.8823 - val_loss: 0.5519 - val_accuracy: 0.8448\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3925 - accuracy: 0.8859 - val_loss: 0.5144 - val_accuracy: 0.8551\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3934 - accuracy: 0.8848 - val_loss: 0.5208 - val_accuracy: 0.8554\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3907 - accuracy: 0.8848 - val_loss: 0.5196 - val_accuracy: 0.8555\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3851 - accuracy: 0.8863 - val_loss: 0.5453 - val_accuracy: 0.8467\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3938 - accuracy: 0.8832 - val_loss: 0.5054 - val_accuracy: 0.8581\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3809 - accuracy: 0.8877 - val_loss: 0.5324 - val_accuracy: 0.8483\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3815 - accuracy: 0.8872 - val_loss: 0.4718 - val_accuracy: 0.8671\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3819 - accuracy: 0.8886 - val_loss: 0.5514 - val_accuracy: 0.8456\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3810 - accuracy: 0.8870 - val_loss: 0.4385 - val_accuracy: 0.8795\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3856 - accuracy: 0.8869 - val_loss: 0.4756 - val_accuracy: 0.8701\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3769 - accuracy: 0.8898 - val_loss: 0.4773 - val_accuracy: 0.8686\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3762 - accuracy: 0.8882 - val_loss: 0.4595 - val_accuracy: 0.8726\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3683 - accuracy: 0.8925 - val_loss: 0.4713 - val_accuracy: 0.8681\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3704 - accuracy: 0.8916 - val_loss: 0.5114 - val_accuracy: 0.8581\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3672 - accuracy: 0.8922 - val_loss: 0.4467 - val_accuracy: 0.8746\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3665 - accuracy: 0.8923 - val_loss: 0.4882 - val_accuracy: 0.8609\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.8944 - val_loss: 0.4676 - val_accuracy: 0.8709\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3661 - accuracy: 0.8929 - val_loss: 0.4654 - val_accuracy: 0.8689\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.8939 - val_loss: 0.4700 - val_accuracy: 0.8667\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3582 - accuracy: 0.8953 - val_loss: 0.4339 - val_accuracy: 0.8788\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3529 - accuracy: 0.8964 - val_loss: 0.4817 - val_accuracy: 0.8661\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3540 - accuracy: 0.8963 - val_loss: 0.5146 - val_accuracy: 0.8546\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3541 - accuracy: 0.8944 - val_loss: 0.4613 - val_accuracy: 0.8716\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3522 - accuracy: 0.8962 - val_loss: 0.4656 - val_accuracy: 0.8715\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3492 - accuracy: 0.8975 - val_loss: 0.4896 - val_accuracy: 0.8628\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3534 - accuracy: 0.8962 - val_loss: 0.4963 - val_accuracy: 0.8574\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3467 - accuracy: 0.8989 - val_loss: 0.4731 - val_accuracy: 0.8683\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3486 - accuracy: 0.8971 - val_loss: 0.5318 - val_accuracy: 0.8496\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3489 - accuracy: 0.8964 - val_loss: 0.4708 - val_accuracy: 0.8699\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3399 - accuracy: 0.9004 - val_loss: 0.4556 - val_accuracy: 0.8718\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3389 - accuracy: 0.9000 - val_loss: 0.4425 - val_accuracy: 0.8756\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3377 - accuracy: 0.9010 - val_loss: 0.4717 - val_accuracy: 0.8696\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3340 - accuracy: 0.9022 - val_loss: 0.4595 - val_accuracy: 0.8744\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3301 - accuracy: 0.9038 - val_loss: 0.4114 - val_accuracy: 0.8876\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3314 - accuracy: 0.9019 - val_loss: 0.4619 - val_accuracy: 0.8729\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3289 - accuracy: 0.9028 - val_loss: 0.4409 - val_accuracy: 0.8776\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3386 - accuracy: 0.9005 - val_loss: 0.4438 - val_accuracy: 0.8762\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3264 - accuracy: 0.9041 - val_loss: 0.4747 - val_accuracy: 0.8684\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3245 - accuracy: 0.9046 - val_loss: 0.4244 - val_accuracy: 0.8861\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3251 - accuracy: 0.9057 - val_loss: 0.4115 - val_accuracy: 0.8893\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3301 - accuracy: 0.9023 - val_loss: 0.4618 - val_accuracy: 0.8742\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3227 - accuracy: 0.9048 - val_loss: 0.4680 - val_accuracy: 0.8727\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3203 - accuracy: 0.9058 - val_loss: 0.4203 - val_accuracy: 0.8852\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3234 - accuracy: 0.9039 - val_loss: 0.4723 - val_accuracy: 0.8681\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3178 - accuracy: 0.9067 - val_loss: 0.4694 - val_accuracy: 0.8699\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3199 - accuracy: 0.9068 - val_loss: 0.4663 - val_accuracy: 0.8729\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3150 - accuracy: 0.9083 - val_loss: 0.4966 - val_accuracy: 0.8615\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3110 - accuracy: 0.9090 - val_loss: 0.4521 - val_accuracy: 0.8764\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3149 - accuracy: 0.9069 - val_loss: 0.4267 - val_accuracy: 0.8869\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3067 - accuracy: 0.9095 - val_loss: 0.4150 - val_accuracy: 0.8879\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3122 - accuracy: 0.9095 - val_loss: 0.4631 - val_accuracy: 0.8746\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3027 - accuracy: 0.9125 - val_loss: 0.4214 - val_accuracy: 0.8875\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3105 - accuracy: 0.9087 - val_loss: 0.3962 - val_accuracy: 0.8956\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3064 - accuracy: 0.9096 - val_loss: 0.4774 - val_accuracy: 0.8690\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3039 - accuracy: 0.9105 - val_loss: 0.4326 - val_accuracy: 0.8802\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3038 - accuracy: 0.9103 - val_loss: 0.4808 - val_accuracy: 0.8655\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3041 - accuracy: 0.9100 - val_loss: 0.4068 - val_accuracy: 0.8884\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3002 - accuracy: 0.9123 - val_loss: 0.4644 - val_accuracy: 0.8732\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2969 - accuracy: 0.9138 - val_loss: 0.3999 - val_accuracy: 0.8923\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2992 - accuracy: 0.9126 - val_loss: 0.4394 - val_accuracy: 0.8794\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2929 - accuracy: 0.9156 - val_loss: 0.4062 - val_accuracy: 0.8929\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2907 - accuracy: 0.9162 - val_loss: 0.4177 - val_accuracy: 0.8874\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2932 - accuracy: 0.9138 - val_loss: 0.3951 - val_accuracy: 0.8945\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2944 - accuracy: 0.9144 - val_loss: 0.4233 - val_accuracy: 0.8878\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2863 - accuracy: 0.9175 - val_loss: 0.3934 - val_accuracy: 0.8943\n",
      "Try 3/100: Best_val_acc: [0.64641273021698, 0.8303333520889282], lr: 5.751850243797588e-05, Lambda: 6.074172215908568e-05\n",
      "\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3037 - accuracy: 0.1335 - val_loss: 2.1958 - val_accuracy: 0.3286\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2203 - accuracy: 0.2315 - val_loss: 2.0867 - val_accuracy: 0.4286\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1072 - accuracy: 0.3270 - val_loss: 1.9524 - val_accuracy: 0.4757\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9535 - accuracy: 0.3964 - val_loss: 1.7404 - val_accuracy: 0.5446\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7949 - accuracy: 0.4523 - val_loss: 1.6622 - val_accuracy: 0.5451\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6433 - accuracy: 0.5073 - val_loss: 1.5315 - val_accuracy: 0.5682\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5275 - accuracy: 0.5376 - val_loss: 1.4619 - val_accuracy: 0.5830\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4315 - accuracy: 0.5676 - val_loss: 1.2928 - val_accuracy: 0.6261\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3562 - accuracy: 0.5908 - val_loss: 1.2675 - val_accuracy: 0.6319\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.3007 - accuracy: 0.6097 - val_loss: 1.2127 - val_accuracy: 0.6540\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2576 - accuracy: 0.6232 - val_loss: 1.4015 - val_accuracy: 0.5767\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2150 - accuracy: 0.6378 - val_loss: 1.1613 - val_accuracy: 0.6629\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1788 - accuracy: 0.6495 - val_loss: 1.1270 - val_accuracy: 0.6709\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1523 - accuracy: 0.6591 - val_loss: 1.2792 - val_accuracy: 0.6171\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1247 - accuracy: 0.6644 - val_loss: 1.0372 - val_accuracy: 0.7025\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0950 - accuracy: 0.6762 - val_loss: 1.0523 - val_accuracy: 0.6950\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0742 - accuracy: 0.6832 - val_loss: 1.1283 - val_accuracy: 0.6650\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0531 - accuracy: 0.6881 - val_loss: 1.0029 - val_accuracy: 0.7075\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0294 - accuracy: 0.6942 - val_loss: 1.0068 - val_accuracy: 0.7019\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0104 - accuracy: 0.7008 - val_loss: 1.0086 - val_accuracy: 0.7054\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9945 - accuracy: 0.7060 - val_loss: 1.0627 - val_accuracy: 0.6861\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9715 - accuracy: 0.7134 - val_loss: 0.9532 - val_accuracy: 0.7231\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9586 - accuracy: 0.7152 - val_loss: 0.9766 - val_accuracy: 0.7161\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9408 - accuracy: 0.7210 - val_loss: 0.9802 - val_accuracy: 0.7125\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9274 - accuracy: 0.7245 - val_loss: 0.8653 - val_accuracy: 0.7502\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9136 - accuracy: 0.7294 - val_loss: 0.9513 - val_accuracy: 0.7229\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9046 - accuracy: 0.7293 - val_loss: 0.8894 - val_accuracy: 0.7421\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8845 - accuracy: 0.7380 - val_loss: 0.9148 - val_accuracy: 0.7354\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8710 - accuracy: 0.7401 - val_loss: 0.8617 - val_accuracy: 0.7510\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8573 - accuracy: 0.7466 - val_loss: 0.9117 - val_accuracy: 0.7357\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8503 - accuracy: 0.7474 - val_loss: 0.8893 - val_accuracy: 0.7364\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8382 - accuracy: 0.7519 - val_loss: 0.8223 - val_accuracy: 0.7655\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8340 - accuracy: 0.7518 - val_loss: 0.7918 - val_accuracy: 0.7761\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8220 - accuracy: 0.7577 - val_loss: 0.8520 - val_accuracy: 0.7476\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8078 - accuracy: 0.7613 - val_loss: 0.8231 - val_accuracy: 0.7668\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7949 - accuracy: 0.7653 - val_loss: 0.8263 - val_accuracy: 0.7671\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7896 - accuracy: 0.7668 - val_loss: 0.8041 - val_accuracy: 0.7711\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7810 - accuracy: 0.7682 - val_loss: 0.7992 - val_accuracy: 0.7736\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7734 - accuracy: 0.7722 - val_loss: 0.8232 - val_accuracy: 0.7642\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.7753 - val_loss: 0.7850 - val_accuracy: 0.7767\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7613 - accuracy: 0.7747 - val_loss: 0.7528 - val_accuracy: 0.7845\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7489 - accuracy: 0.7773 - val_loss: 0.8161 - val_accuracy: 0.7648\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7417 - accuracy: 0.7807 - val_loss: 0.7829 - val_accuracy: 0.7780\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7369 - accuracy: 0.7813 - val_loss: 0.8063 - val_accuracy: 0.7671\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7332 - accuracy: 0.7821 - val_loss: 0.7133 - val_accuracy: 0.8006\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7268 - accuracy: 0.7839 - val_loss: 0.7687 - val_accuracy: 0.7852\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7206 - accuracy: 0.7862 - val_loss: 0.7929 - val_accuracy: 0.7755\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7065 - accuracy: 0.7918 - val_loss: 0.7364 - val_accuracy: 0.7861\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7069 - accuracy: 0.7910 - val_loss: 0.6948 - val_accuracy: 0.8034\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6963 - accuracy: 0.7942 - val_loss: 0.7088 - val_accuracy: 0.8006\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6959 - accuracy: 0.7933 - val_loss: 0.7617 - val_accuracy: 0.7866\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6874 - accuracy: 0.7960 - val_loss: 0.7060 - val_accuracy: 0.8028\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6843 - accuracy: 0.7963 - val_loss: 0.7159 - val_accuracy: 0.7970\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6742 - accuracy: 0.8018 - val_loss: 0.7013 - val_accuracy: 0.8001\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6713 - accuracy: 0.8014 - val_loss: 0.7520 - val_accuracy: 0.7893\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6667 - accuracy: 0.8034 - val_loss: 0.7289 - val_accuracy: 0.7941\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6611 - accuracy: 0.8047 - val_loss: 0.7436 - val_accuracy: 0.7901\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6566 - accuracy: 0.8057 - val_loss: 0.6940 - val_accuracy: 0.8043\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6571 - accuracy: 0.8038 - val_loss: 0.7677 - val_accuracy: 0.7819\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6473 - accuracy: 0.8088 - val_loss: 0.7235 - val_accuracy: 0.7950\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6397 - accuracy: 0.8120 - val_loss: 0.7615 - val_accuracy: 0.7846\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6404 - accuracy: 0.8107 - val_loss: 0.6741 - val_accuracy: 0.8116\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6353 - accuracy: 0.8123 - val_loss: 0.6562 - val_accuracy: 0.8169\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6325 - accuracy: 0.8145 - val_loss: 0.6717 - val_accuracy: 0.8116\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6258 - accuracy: 0.8147 - val_loss: 0.7038 - val_accuracy: 0.8041\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6216 - accuracy: 0.8166 - val_loss: 0.6443 - val_accuracy: 0.8227\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6216 - accuracy: 0.8166 - val_loss: 0.6186 - val_accuracy: 0.8283\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6127 - accuracy: 0.8184 - val_loss: 0.6294 - val_accuracy: 0.8234\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6079 - accuracy: 0.8222 - val_loss: 0.7192 - val_accuracy: 0.7946\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6011 - accuracy: 0.8232 - val_loss: 0.6690 - val_accuracy: 0.8126\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6055 - accuracy: 0.8200 - val_loss: 0.6265 - val_accuracy: 0.8241\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6024 - accuracy: 0.8230 - val_loss: 0.6602 - val_accuracy: 0.8128\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5898 - accuracy: 0.8265 - val_loss: 0.5750 - val_accuracy: 0.8396\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5908 - accuracy: 0.8264 - val_loss: 0.6628 - val_accuracy: 0.8145\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5884 - accuracy: 0.8263 - val_loss: 0.5953 - val_accuracy: 0.8309\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5828 - accuracy: 0.8286 - val_loss: 0.6668 - val_accuracy: 0.8134\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5801 - accuracy: 0.8274 - val_loss: 0.5976 - val_accuracy: 0.8336\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5808 - accuracy: 0.8290 - val_loss: 0.5920 - val_accuracy: 0.8367\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5746 - accuracy: 0.8306 - val_loss: 0.5995 - val_accuracy: 0.8291\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5704 - accuracy: 0.8314 - val_loss: 0.6423 - val_accuracy: 0.8195\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5661 - accuracy: 0.8310 - val_loss: 0.6268 - val_accuracy: 0.8258\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5617 - accuracy: 0.8329 - val_loss: 0.6178 - val_accuracy: 0.8269\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5662 - accuracy: 0.8318 - val_loss: 0.6031 - val_accuracy: 0.8320\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5550 - accuracy: 0.8367 - val_loss: 0.6086 - val_accuracy: 0.8279\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5512 - accuracy: 0.8382 - val_loss: 0.6174 - val_accuracy: 0.8259\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5508 - accuracy: 0.8380 - val_loss: 0.5986 - val_accuracy: 0.8306\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5464 - accuracy: 0.8395 - val_loss: 0.6421 - val_accuracy: 0.8183\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5420 - accuracy: 0.8414 - val_loss: 0.6152 - val_accuracy: 0.8279\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5406 - accuracy: 0.8395 - val_loss: 0.6010 - val_accuracy: 0.8294\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5417 - accuracy: 0.8400 - val_loss: 0.5678 - val_accuracy: 0.8427\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5300 - accuracy: 0.8430 - val_loss: 0.5593 - val_accuracy: 0.8444\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5386 - accuracy: 0.8410 - val_loss: 0.5820 - val_accuracy: 0.8386\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5236 - accuracy: 0.8468 - val_loss: 0.5775 - val_accuracy: 0.8411\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5258 - accuracy: 0.8450 - val_loss: 0.5674 - val_accuracy: 0.8414\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5254 - accuracy: 0.8442 - val_loss: 0.6183 - val_accuracy: 0.8239\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5221 - accuracy: 0.8462 - val_loss: 0.6445 - val_accuracy: 0.8193\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5197 - accuracy: 0.8453 - val_loss: 0.5865 - val_accuracy: 0.8395\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5202 - accuracy: 0.8450 - val_loss: 0.6658 - val_accuracy: 0.8093\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5148 - accuracy: 0.8471 - val_loss: 0.6599 - val_accuracy: 0.8148\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5066 - accuracy: 0.8507 - val_loss: 0.6371 - val_accuracy: 0.8204\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5108 - accuracy: 0.8481 - val_loss: 0.5800 - val_accuracy: 0.8361\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5051 - accuracy: 0.8502 - val_loss: 0.5846 - val_accuracy: 0.8369\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5092 - accuracy: 0.8480 - val_loss: 0.5589 - val_accuracy: 0.8462\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4958 - accuracy: 0.8548 - val_loss: 0.5819 - val_accuracy: 0.8339\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4988 - accuracy: 0.8532 - val_loss: 0.5505 - val_accuracy: 0.8464\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4934 - accuracy: 0.8540 - val_loss: 0.5458 - val_accuracy: 0.8478\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4888 - accuracy: 0.8559 - val_loss: 0.5564 - val_accuracy: 0.8454\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4851 - accuracy: 0.8557 - val_loss: 0.5377 - val_accuracy: 0.8477\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4953 - accuracy: 0.8523 - val_loss: 0.5537 - val_accuracy: 0.8480\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4832 - accuracy: 0.8573 - val_loss: 0.5427 - val_accuracy: 0.8474\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4811 - accuracy: 0.8573 - val_loss: 0.5673 - val_accuracy: 0.8444\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4824 - accuracy: 0.8588 - val_loss: 0.5892 - val_accuracy: 0.8343\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4806 - accuracy: 0.8577 - val_loss: 0.5179 - val_accuracy: 0.8582\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4724 - accuracy: 0.8616 - val_loss: 0.5317 - val_accuracy: 0.8523\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4709 - accuracy: 0.8618 - val_loss: 0.4985 - val_accuracy: 0.8552\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4721 - accuracy: 0.8618 - val_loss: 0.5250 - val_accuracy: 0.8534\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4695 - accuracy: 0.8610 - val_loss: 0.5438 - val_accuracy: 0.8486\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4657 - accuracy: 0.8630 - val_loss: 0.5310 - val_accuracy: 0.8482\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4694 - accuracy: 0.8594 - val_loss: 0.5599 - val_accuracy: 0.8379\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4645 - accuracy: 0.8630 - val_loss: 0.5465 - val_accuracy: 0.8459\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4602 - accuracy: 0.8649 - val_loss: 0.5235 - val_accuracy: 0.8519\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4571 - accuracy: 0.8649 - val_loss: 0.5568 - val_accuracy: 0.8444\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4544 - accuracy: 0.8658 - val_loss: 0.5185 - val_accuracy: 0.8561\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4523 - accuracy: 0.8658 - val_loss: 0.6319 - val_accuracy: 0.8229\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4503 - accuracy: 0.8666 - val_loss: 0.5710 - val_accuracy: 0.8427\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4453 - accuracy: 0.8690 - val_loss: 0.5810 - val_accuracy: 0.8383\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4444 - accuracy: 0.8682 - val_loss: 0.5438 - val_accuracy: 0.8495\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4493 - accuracy: 0.8675 - val_loss: 0.5169 - val_accuracy: 0.8516\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4403 - accuracy: 0.8691 - val_loss: 0.5565 - val_accuracy: 0.8446\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4412 - accuracy: 0.8695 - val_loss: 0.5127 - val_accuracy: 0.8549\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4386 - accuracy: 0.8707 - val_loss: 0.5900 - val_accuracy: 0.8345\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4378 - accuracy: 0.8699 - val_loss: 0.4784 - val_accuracy: 0.8674\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4378 - accuracy: 0.8702 - val_loss: 0.6379 - val_accuracy: 0.8261\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4327 - accuracy: 0.8728 - val_loss: 0.5102 - val_accuracy: 0.8604\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4297 - accuracy: 0.8730 - val_loss: 0.5423 - val_accuracy: 0.8479\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4272 - accuracy: 0.8731 - val_loss: 0.4761 - val_accuracy: 0.8689\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4261 - accuracy: 0.8749 - val_loss: 0.5454 - val_accuracy: 0.8459\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4269 - accuracy: 0.8733 - val_loss: 0.5017 - val_accuracy: 0.8600\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4243 - accuracy: 0.8753 - val_loss: 0.5113 - val_accuracy: 0.8574\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4171 - accuracy: 0.8778 - val_loss: 0.5675 - val_accuracy: 0.8390\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4170 - accuracy: 0.8770 - val_loss: 0.4722 - val_accuracy: 0.8719\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4142 - accuracy: 0.8782 - val_loss: 0.5295 - val_accuracy: 0.8526\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4105 - accuracy: 0.8798 - val_loss: 0.5085 - val_accuracy: 0.8589\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4219 - accuracy: 0.8748 - val_loss: 0.5281 - val_accuracy: 0.8536\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4065 - accuracy: 0.8798 - val_loss: 0.5237 - val_accuracy: 0.8539\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4094 - accuracy: 0.8797 - val_loss: 0.6108 - val_accuracy: 0.8226\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4078 - accuracy: 0.8792 - val_loss: 0.4700 - val_accuracy: 0.8674\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4095 - accuracy: 0.8782 - val_loss: 0.4837 - val_accuracy: 0.8672\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4030 - accuracy: 0.8804 - val_loss: 0.4896 - val_accuracy: 0.8623\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4015 - accuracy: 0.8817 - val_loss: 0.5309 - val_accuracy: 0.8520\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4023 - accuracy: 0.8804 - val_loss: 0.5093 - val_accuracy: 0.8557\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3995 - accuracy: 0.8808 - val_loss: 0.5509 - val_accuracy: 0.8437\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3951 - accuracy: 0.8840 - val_loss: 0.4639 - val_accuracy: 0.8710\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3916 - accuracy: 0.8847 - val_loss: 0.5220 - val_accuracy: 0.8558\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3911 - accuracy: 0.8847 - val_loss: 0.4893 - val_accuracy: 0.8643\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3948 - accuracy: 0.8838 - val_loss: 0.5304 - val_accuracy: 0.8503\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3836 - accuracy: 0.8868 - val_loss: 0.4511 - val_accuracy: 0.8787\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3880 - accuracy: 0.8846 - val_loss: 0.4998 - val_accuracy: 0.8584\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3889 - accuracy: 0.8852 - val_loss: 0.4862 - val_accuracy: 0.8651\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3872 - accuracy: 0.8859 - val_loss: 0.5058 - val_accuracy: 0.8588\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3790 - accuracy: 0.8888 - val_loss: 0.5119 - val_accuracy: 0.8556\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3745 - accuracy: 0.8900 - val_loss: 0.4519 - val_accuracy: 0.8784\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3888 - accuracy: 0.8844 - val_loss: 0.4269 - val_accuracy: 0.8861\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3726 - accuracy: 0.8902 - val_loss: 0.4608 - val_accuracy: 0.8729\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3703 - accuracy: 0.8910 - val_loss: 0.5235 - val_accuracy: 0.8525\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3719 - accuracy: 0.8913 - val_loss: 0.4595 - val_accuracy: 0.8739\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3694 - accuracy: 0.8912 - val_loss: 0.4289 - val_accuracy: 0.8809\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3701 - accuracy: 0.8900 - val_loss: 0.5845 - val_accuracy: 0.8351\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3702 - accuracy: 0.8908 - val_loss: 0.4402 - val_accuracy: 0.8779\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3743 - accuracy: 0.8889 - val_loss: 0.5064 - val_accuracy: 0.8590\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3658 - accuracy: 0.8915 - val_loss: 0.4994 - val_accuracy: 0.8621\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3635 - accuracy: 0.8934 - val_loss: 0.5081 - val_accuracy: 0.8564\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3557 - accuracy: 0.8966 - val_loss: 0.5495 - val_accuracy: 0.8463\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3596 - accuracy: 0.8944 - val_loss: 0.4513 - val_accuracy: 0.8774\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3623 - accuracy: 0.8924 - val_loss: 0.5123 - val_accuracy: 0.8558\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3581 - accuracy: 0.8943 - val_loss: 0.4398 - val_accuracy: 0.8774\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3585 - accuracy: 0.8931 - val_loss: 0.3942 - val_accuracy: 0.8943\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3575 - accuracy: 0.8948 - val_loss: 0.5167 - val_accuracy: 0.8561\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3508 - accuracy: 0.8967 - val_loss: 0.5316 - val_accuracy: 0.8543\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3540 - accuracy: 0.8961 - val_loss: 0.4655 - val_accuracy: 0.8735\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3478 - accuracy: 0.8996 - val_loss: 0.4951 - val_accuracy: 0.8665\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3436 - accuracy: 0.8993 - val_loss: 0.5591 - val_accuracy: 0.8453\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3478 - accuracy: 0.8966 - val_loss: 0.4818 - val_accuracy: 0.8686\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3478 - accuracy: 0.8975 - val_loss: 0.4570 - val_accuracy: 0.8755\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3409 - accuracy: 0.9001 - val_loss: 0.4172 - val_accuracy: 0.8896\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3420 - accuracy: 0.8998 - val_loss: 0.4592 - val_accuracy: 0.8765\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3414 - accuracy: 0.9003 - val_loss: 0.5411 - val_accuracy: 0.8507\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3320 - accuracy: 0.9043 - val_loss: 0.4213 - val_accuracy: 0.8847\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3317 - accuracy: 0.9032 - val_loss: 0.4150 - val_accuracy: 0.8888\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3353 - accuracy: 0.9030 - val_loss: 0.4598 - val_accuracy: 0.8742\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3389 - accuracy: 0.8995 - val_loss: 0.5659 - val_accuracy: 0.8436\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3313 - accuracy: 0.9043 - val_loss: 0.4731 - val_accuracy: 0.8706\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3314 - accuracy: 0.9034 - val_loss: 0.4387 - val_accuracy: 0.8810\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3244 - accuracy: 0.9057 - val_loss: 0.4422 - val_accuracy: 0.8811\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3203 - accuracy: 0.9084 - val_loss: 0.4790 - val_accuracy: 0.8697\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3297 - accuracy: 0.9028 - val_loss: 0.5016 - val_accuracy: 0.8596\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3249 - accuracy: 0.9054 - val_loss: 0.4404 - val_accuracy: 0.8770\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3256 - accuracy: 0.9043 - val_loss: 0.4908 - val_accuracy: 0.8636\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3235 - accuracy: 0.9058 - val_loss: 0.4476 - val_accuracy: 0.8801\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3260 - accuracy: 0.9035 - val_loss: 0.4395 - val_accuracy: 0.8812\n",
      "Try 4/100: Best_val_acc: [0.6517172455787659, 0.8265555500984192], lr: 6.17543312193122e-05, Lambda: 5.99343930246932e-05\n",
      "\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2970 - accuracy: 0.1278 - val_loss: 2.2171 - val_accuracy: 0.2449\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2075 - accuracy: 0.2349 - val_loss: 2.2009 - val_accuracy: 0.2355\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0597 - accuracy: 0.3249 - val_loss: 2.0413 - val_accuracy: 0.3101\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8717 - accuracy: 0.4121 - val_loss: 1.8532 - val_accuracy: 0.4164\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7074 - accuracy: 0.4720 - val_loss: 1.7495 - val_accuracy: 0.4505\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5767 - accuracy: 0.5221 - val_loss: 1.6847 - val_accuracy: 0.4814\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4853 - accuracy: 0.5526 - val_loss: 1.4167 - val_accuracy: 0.6190\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4095 - accuracy: 0.5797 - val_loss: 1.4570 - val_accuracy: 0.5716\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3478 - accuracy: 0.6008 - val_loss: 1.3837 - val_accuracy: 0.6094\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2936 - accuracy: 0.6212 - val_loss: 1.3013 - val_accuracy: 0.6446\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2505 - accuracy: 0.6337 - val_loss: 1.2369 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2050 - accuracy: 0.6477 - val_loss: 1.3169 - val_accuracy: 0.6108\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1756 - accuracy: 0.6540 - val_loss: 1.1750 - val_accuracy: 0.6614\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1421 - accuracy: 0.6652 - val_loss: 1.2058 - val_accuracy: 0.6568\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1120 - accuracy: 0.6736 - val_loss: 1.1422 - val_accuracy: 0.6724\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0900 - accuracy: 0.6775 - val_loss: 1.1416 - val_accuracy: 0.6729\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0674 - accuracy: 0.6860 - val_loss: 1.1353 - val_accuracy: 0.6664\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0507 - accuracy: 0.6875 - val_loss: 1.0706 - val_accuracy: 0.6919\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0287 - accuracy: 0.6940 - val_loss: 1.1067 - val_accuracy: 0.6720\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0088 - accuracy: 0.7006 - val_loss: 0.9511 - val_accuracy: 0.7317\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9939 - accuracy: 0.7034 - val_loss: 1.0387 - val_accuracy: 0.7001\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9820 - accuracy: 0.7082 - val_loss: 1.0155 - val_accuracy: 0.7024\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9576 - accuracy: 0.7163 - val_loss: 1.0498 - val_accuracy: 0.6961\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9474 - accuracy: 0.7174 - val_loss: 0.9787 - val_accuracy: 0.7214\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9388 - accuracy: 0.7204 - val_loss: 1.0297 - val_accuracy: 0.7006\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9250 - accuracy: 0.7255 - val_loss: 0.9921 - val_accuracy: 0.7085\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9087 - accuracy: 0.7309 - val_loss: 0.9068 - val_accuracy: 0.7423\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8963 - accuracy: 0.7335 - val_loss: 0.9012 - val_accuracy: 0.7354\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8930 - accuracy: 0.7344 - val_loss: 0.8742 - val_accuracy: 0.7536\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8740 - accuracy: 0.7418 - val_loss: 0.9126 - val_accuracy: 0.7363\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8606 - accuracy: 0.7458 - val_loss: 0.9312 - val_accuracy: 0.7324\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8517 - accuracy: 0.7465 - val_loss: 0.8027 - val_accuracy: 0.7704\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8411 - accuracy: 0.7517 - val_loss: 0.8217 - val_accuracy: 0.7652\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8346 - accuracy: 0.7522 - val_loss: 0.8837 - val_accuracy: 0.7504\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8280 - accuracy: 0.7555 - val_loss: 0.8353 - val_accuracy: 0.7627\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8128 - accuracy: 0.7595 - val_loss: 0.8707 - val_accuracy: 0.7481\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8083 - accuracy: 0.7617 - val_loss: 0.8703 - val_accuracy: 0.7509\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8028 - accuracy: 0.7620 - val_loss: 0.8543 - val_accuracy: 0.7561\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7897 - accuracy: 0.7689 - val_loss: 0.8100 - val_accuracy: 0.7703\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7820 - accuracy: 0.7702 - val_loss: 0.8114 - val_accuracy: 0.7716\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7738 - accuracy: 0.7726 - val_loss: 0.8653 - val_accuracy: 0.7469\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7641 - accuracy: 0.7748 - val_loss: 0.8676 - val_accuracy: 0.7518\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7579 - accuracy: 0.7762 - val_loss: 0.7594 - val_accuracy: 0.7826\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7553 - accuracy: 0.7769 - val_loss: 0.7548 - val_accuracy: 0.7878\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7507 - accuracy: 0.7792 - val_loss: 0.7741 - val_accuracy: 0.7782\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7353 - accuracy: 0.7847 - val_loss: 0.8306 - val_accuracy: 0.7639\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7313 - accuracy: 0.7843 - val_loss: 0.8223 - val_accuracy: 0.7609\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7268 - accuracy: 0.7847 - val_loss: 0.8008 - val_accuracy: 0.7702\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7199 - accuracy: 0.7870 - val_loss: 0.8114 - val_accuracy: 0.7687\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7102 - accuracy: 0.7909 - val_loss: 0.6826 - val_accuracy: 0.8086\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7017 - accuracy: 0.7949 - val_loss: 0.7535 - val_accuracy: 0.7832\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6970 - accuracy: 0.7939 - val_loss: 0.7486 - val_accuracy: 0.7857\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6895 - accuracy: 0.7971 - val_loss: 0.7068 - val_accuracy: 0.8036\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6909 - accuracy: 0.7973 - val_loss: 0.7894 - val_accuracy: 0.7768\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6840 - accuracy: 0.7981 - val_loss: 0.7402 - val_accuracy: 0.7906\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6833 - accuracy: 0.7984 - val_loss: 0.8162 - val_accuracy: 0.7635\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6684 - accuracy: 0.8024 - val_loss: 0.7295 - val_accuracy: 0.7929\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6731 - accuracy: 0.8008 - val_loss: 0.7295 - val_accuracy: 0.7926\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6606 - accuracy: 0.8050 - val_loss: 0.6878 - val_accuracy: 0.8089\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6565 - accuracy: 0.8060 - val_loss: 0.6809 - val_accuracy: 0.8059\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6502 - accuracy: 0.8071 - val_loss: 0.7388 - val_accuracy: 0.7856\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6489 - accuracy: 0.8072 - val_loss: 0.6552 - val_accuracy: 0.8144\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6439 - accuracy: 0.8096 - val_loss: 0.7056 - val_accuracy: 0.7999\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6352 - accuracy: 0.8135 - val_loss: 0.7891 - val_accuracy: 0.7711\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6348 - accuracy: 0.8116 - val_loss: 0.6661 - val_accuracy: 0.8106\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6238 - accuracy: 0.8171 - val_loss: 0.6227 - val_accuracy: 0.8281\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6263 - accuracy: 0.8139 - val_loss: 0.5959 - val_accuracy: 0.8347\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6168 - accuracy: 0.8179 - val_loss: 0.6584 - val_accuracy: 0.8155\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6205 - accuracy: 0.8173 - val_loss: 0.7252 - val_accuracy: 0.7935\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6115 - accuracy: 0.8188 - val_loss: 0.7434 - val_accuracy: 0.7864\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6062 - accuracy: 0.8214 - val_loss: 0.6209 - val_accuracy: 0.8246\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6014 - accuracy: 0.8224 - val_loss: 0.6942 - val_accuracy: 0.8001\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6008 - accuracy: 0.8209 - val_loss: 0.5977 - val_accuracy: 0.8302\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5941 - accuracy: 0.8240 - val_loss: 0.6631 - val_accuracy: 0.8084\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5857 - accuracy: 0.8274 - val_loss: 0.5806 - val_accuracy: 0.8376\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5860 - accuracy: 0.8286 - val_loss: 0.6900 - val_accuracy: 0.8036\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5785 - accuracy: 0.8283 - val_loss: 0.6259 - val_accuracy: 0.8246\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5812 - accuracy: 0.8276 - val_loss: 0.6194 - val_accuracy: 0.8277\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5705 - accuracy: 0.8323 - val_loss: 0.6828 - val_accuracy: 0.8031\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5711 - accuracy: 0.8313 - val_loss: 0.7445 - val_accuracy: 0.7841\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5753 - accuracy: 0.8295 - val_loss: 0.6002 - val_accuracy: 0.8306\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5634 - accuracy: 0.8329 - val_loss: 0.6851 - val_accuracy: 0.8033\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5622 - accuracy: 0.8343 - val_loss: 0.6861 - val_accuracy: 0.8097\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5623 - accuracy: 0.8336 - val_loss: 0.5901 - val_accuracy: 0.8329\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5563 - accuracy: 0.8346 - val_loss: 0.6608 - val_accuracy: 0.8126\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5487 - accuracy: 0.8381 - val_loss: 0.6494 - val_accuracy: 0.8161\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5505 - accuracy: 0.8377 - val_loss: 0.7323 - val_accuracy: 0.7866\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5455 - accuracy: 0.8394 - val_loss: 0.6250 - val_accuracy: 0.8191\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5447 - accuracy: 0.8375 - val_loss: 0.6465 - val_accuracy: 0.8181\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5392 - accuracy: 0.8402 - val_loss: 0.5956 - val_accuracy: 0.8338\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5319 - accuracy: 0.8423 - val_loss: 0.6454 - val_accuracy: 0.8168\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5330 - accuracy: 0.8425 - val_loss: 0.6123 - val_accuracy: 0.8306\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5240 - accuracy: 0.8453 - val_loss: 0.5657 - val_accuracy: 0.8409\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5215 - accuracy: 0.8478 - val_loss: 0.5660 - val_accuracy: 0.8450\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5191 - accuracy: 0.8469 - val_loss: 0.5892 - val_accuracy: 0.8324\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5176 - accuracy: 0.8470 - val_loss: 0.6136 - val_accuracy: 0.8277\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5223 - accuracy: 0.8458 - val_loss: 0.5472 - val_accuracy: 0.8464\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5142 - accuracy: 0.8477 - val_loss: 0.6014 - val_accuracy: 0.8287\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5109 - accuracy: 0.8485 - val_loss: 0.5238 - val_accuracy: 0.8556\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5071 - accuracy: 0.8494 - val_loss: 0.5896 - val_accuracy: 0.8334\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5005 - accuracy: 0.8529 - val_loss: 0.6156 - val_accuracy: 0.8237\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5064 - accuracy: 0.8505 - val_loss: 0.5866 - val_accuracy: 0.8360\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5001 - accuracy: 0.8542 - val_loss: 0.5736 - val_accuracy: 0.8403\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4962 - accuracy: 0.8531 - val_loss: 0.6647 - val_accuracy: 0.8135\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4944 - accuracy: 0.8532 - val_loss: 0.5537 - val_accuracy: 0.8450\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4973 - accuracy: 0.8530 - val_loss: 0.5399 - val_accuracy: 0.8518\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4873 - accuracy: 0.8552 - val_loss: 0.6062 - val_accuracy: 0.8309\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4877 - accuracy: 0.8552 - val_loss: 0.4673 - val_accuracy: 0.8709\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4811 - accuracy: 0.8565 - val_loss: 0.6544 - val_accuracy: 0.8139\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4821 - accuracy: 0.8578 - val_loss: 0.5169 - val_accuracy: 0.8537\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4747 - accuracy: 0.8604 - val_loss: 0.5932 - val_accuracy: 0.8302\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4756 - accuracy: 0.8582 - val_loss: 0.5803 - val_accuracy: 0.8374\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4735 - accuracy: 0.8597 - val_loss: 0.5053 - val_accuracy: 0.8601\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4669 - accuracy: 0.8622 - val_loss: 0.5780 - val_accuracy: 0.8396\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4659 - accuracy: 0.8619 - val_loss: 0.6027 - val_accuracy: 0.8313\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4671 - accuracy: 0.8605 - val_loss: 0.5911 - val_accuracy: 0.8294\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4670 - accuracy: 0.8605 - val_loss: 0.6022 - val_accuracy: 0.8294\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4608 - accuracy: 0.8639 - val_loss: 0.5456 - val_accuracy: 0.8459\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4605 - accuracy: 0.8651 - val_loss: 0.5479 - val_accuracy: 0.8469\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4562 - accuracy: 0.8655 - val_loss: 0.5355 - val_accuracy: 0.8526\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4529 - accuracy: 0.8669 - val_loss: 0.5297 - val_accuracy: 0.8530\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4531 - accuracy: 0.8658 - val_loss: 0.5469 - val_accuracy: 0.8464\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4452 - accuracy: 0.8688 - val_loss: 0.4379 - val_accuracy: 0.8791\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4531 - accuracy: 0.8665 - val_loss: 0.4997 - val_accuracy: 0.8604\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4450 - accuracy: 0.8682 - val_loss: 0.5362 - val_accuracy: 0.8484\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4389 - accuracy: 0.8695 - val_loss: 0.5280 - val_accuracy: 0.8524\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4380 - accuracy: 0.8714 - val_loss: 0.5921 - val_accuracy: 0.8326\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4398 - accuracy: 0.8689 - val_loss: 0.5609 - val_accuracy: 0.8439\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4369 - accuracy: 0.8698 - val_loss: 0.5567 - val_accuracy: 0.8382\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4354 - accuracy: 0.8697 - val_loss: 0.5112 - val_accuracy: 0.8584\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4351 - accuracy: 0.8704 - val_loss: 0.5178 - val_accuracy: 0.8560\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4296 - accuracy: 0.8731 - val_loss: 0.4645 - val_accuracy: 0.8716\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4286 - accuracy: 0.8733 - val_loss: 0.5279 - val_accuracy: 0.8496\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4320 - accuracy: 0.8722 - val_loss: 0.5303 - val_accuracy: 0.8520\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4289 - accuracy: 0.8716 - val_loss: 0.5031 - val_accuracy: 0.8626\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4194 - accuracy: 0.8758 - val_loss: 0.4514 - val_accuracy: 0.8741\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4191 - accuracy: 0.8747 - val_loss: 0.4969 - val_accuracy: 0.8630\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4239 - accuracy: 0.8739 - val_loss: 0.4465 - val_accuracy: 0.8772\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4207 - accuracy: 0.8744 - val_loss: 0.5001 - val_accuracy: 0.8591\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4124 - accuracy: 0.8781 - val_loss: 0.4667 - val_accuracy: 0.8720\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4166 - accuracy: 0.8752 - val_loss: 0.4980 - val_accuracy: 0.8623\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4084 - accuracy: 0.8797 - val_loss: 0.4705 - val_accuracy: 0.8724\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4053 - accuracy: 0.8810 - val_loss: 0.4997 - val_accuracy: 0.8604\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4067 - accuracy: 0.8803 - val_loss: 0.4651 - val_accuracy: 0.8738\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4006 - accuracy: 0.8828 - val_loss: 0.4802 - val_accuracy: 0.8675\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4016 - accuracy: 0.8828 - val_loss: 0.5503 - val_accuracy: 0.8426\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3971 - accuracy: 0.8827 - val_loss: 0.4768 - val_accuracy: 0.8679\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3957 - accuracy: 0.8827 - val_loss: 0.4386 - val_accuracy: 0.8811\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3935 - accuracy: 0.8838 - val_loss: 0.4617 - val_accuracy: 0.8699\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3958 - accuracy: 0.8824 - val_loss: 0.4861 - val_accuracy: 0.8665\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3935 - accuracy: 0.8836 - val_loss: 0.5189 - val_accuracy: 0.8571\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3945 - accuracy: 0.8828 - val_loss: 0.5041 - val_accuracy: 0.8580\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3893 - accuracy: 0.8850 - val_loss: 0.4989 - val_accuracy: 0.8596\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3908 - accuracy: 0.8840 - val_loss: 0.4981 - val_accuracy: 0.8568\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3842 - accuracy: 0.8873 - val_loss: 0.5110 - val_accuracy: 0.8566\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3829 - accuracy: 0.8872 - val_loss: 0.5368 - val_accuracy: 0.8507\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3842 - accuracy: 0.8861 - val_loss: 0.4592 - val_accuracy: 0.8749\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3807 - accuracy: 0.8862 - val_loss: 0.5000 - val_accuracy: 0.8601\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3801 - accuracy: 0.8888 - val_loss: 0.5334 - val_accuracy: 0.8505\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3745 - accuracy: 0.8913 - val_loss: 0.4885 - val_accuracy: 0.8629\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3766 - accuracy: 0.8886 - val_loss: 0.4758 - val_accuracy: 0.8676\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3772 - accuracy: 0.8887 - val_loss: 0.4228 - val_accuracy: 0.8839\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3745 - accuracy: 0.8890 - val_loss: 0.5367 - val_accuracy: 0.8459\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3677 - accuracy: 0.8922 - val_loss: 0.4775 - val_accuracy: 0.8680\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3696 - accuracy: 0.8918 - val_loss: 0.5278 - val_accuracy: 0.8517\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3655 - accuracy: 0.8938 - val_loss: 0.5026 - val_accuracy: 0.8568\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3641 - accuracy: 0.8930 - val_loss: 0.4923 - val_accuracy: 0.8588\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3649 - accuracy: 0.8926 - val_loss: 0.4087 - val_accuracy: 0.8857\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3632 - accuracy: 0.8937 - val_loss: 0.4874 - val_accuracy: 0.8604\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3612 - accuracy: 0.8945 - val_loss: 0.4903 - val_accuracy: 0.8659\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3559 - accuracy: 0.8954 - val_loss: 0.4100 - val_accuracy: 0.8881\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3629 - accuracy: 0.8938 - val_loss: 0.4784 - val_accuracy: 0.8683\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3533 - accuracy: 0.8954 - val_loss: 0.4633 - val_accuracy: 0.8715\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3602 - accuracy: 0.8927 - val_loss: 0.4416 - val_accuracy: 0.8814\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3493 - accuracy: 0.8988 - val_loss: 0.4505 - val_accuracy: 0.8743\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3468 - accuracy: 0.8983 - val_loss: 0.4381 - val_accuracy: 0.8806\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3512 - accuracy: 0.8975 - val_loss: 0.5073 - val_accuracy: 0.8582\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3509 - accuracy: 0.8965 - val_loss: 0.4378 - val_accuracy: 0.8792\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3439 - accuracy: 0.8987 - val_loss: 0.4487 - val_accuracy: 0.8777\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3402 - accuracy: 0.9007 - val_loss: 0.4804 - val_accuracy: 0.8674\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3374 - accuracy: 0.9006 - val_loss: 0.4169 - val_accuracy: 0.8867\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3398 - accuracy: 0.9001 - val_loss: 0.4673 - val_accuracy: 0.8695\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3393 - accuracy: 0.9008 - val_loss: 0.4266 - val_accuracy: 0.8828\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3378 - accuracy: 0.9003 - val_loss: 0.4262 - val_accuracy: 0.8839\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3387 - accuracy: 0.9005 - val_loss: 0.4271 - val_accuracy: 0.8811\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3352 - accuracy: 0.9022 - val_loss: 0.4889 - val_accuracy: 0.8614\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3419 - accuracy: 0.8996 - val_loss: 0.5090 - val_accuracy: 0.8549\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3300 - accuracy: 0.9041 - val_loss: 0.4571 - val_accuracy: 0.8733\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3355 - accuracy: 0.9015 - val_loss: 0.4069 - val_accuracy: 0.8884\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3295 - accuracy: 0.9038 - val_loss: 0.4688 - val_accuracy: 0.8695\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3262 - accuracy: 0.9057 - val_loss: 0.4379 - val_accuracy: 0.8781\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3255 - accuracy: 0.9047 - val_loss: 0.3749 - val_accuracy: 0.8970\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3349 - accuracy: 0.9020 - val_loss: 0.4211 - val_accuracy: 0.8844\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3322 - accuracy: 0.9022 - val_loss: 0.4321 - val_accuracy: 0.8810\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3214 - accuracy: 0.9060 - val_loss: 0.5321 - val_accuracy: 0.8469\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3233 - accuracy: 0.9062 - val_loss: 0.4836 - val_accuracy: 0.8668\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3206 - accuracy: 0.9059 - val_loss: 0.4076 - val_accuracy: 0.8873\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3262 - accuracy: 0.9044 - val_loss: 0.4141 - val_accuracy: 0.8866\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3142 - accuracy: 0.9090 - val_loss: 0.4746 - val_accuracy: 0.8669\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3226 - accuracy: 0.9056 - val_loss: 0.4254 - val_accuracy: 0.8826\n",
      "Try 5/100: Best_val_acc: [0.6769930720329285, 0.8206666707992554], lr: 5.85937925569733e-05, Lambda: 5.8373651157384894e-05\n",
      "\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2913 - accuracy: 0.1399 - val_loss: 2.2062 - val_accuracy: 0.2771\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1719 - accuracy: 0.2464 - val_loss: 2.0179 - val_accuracy: 0.4270\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9910 - accuracy: 0.3596 - val_loss: 1.8881 - val_accuracy: 0.3993\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8025 - accuracy: 0.4365 - val_loss: 1.7392 - val_accuracy: 0.4939\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6443 - accuracy: 0.4950 - val_loss: 1.5813 - val_accuracy: 0.5909\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5186 - accuracy: 0.5402 - val_loss: 1.4738 - val_accuracy: 0.5783\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4277 - accuracy: 0.5662 - val_loss: 1.4490 - val_accuracy: 0.5349\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3520 - accuracy: 0.5908 - val_loss: 1.3101 - val_accuracy: 0.6084\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2954 - accuracy: 0.6069 - val_loss: 1.3060 - val_accuracy: 0.6161\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2468 - accuracy: 0.6232 - val_loss: 1.2375 - val_accuracy: 0.6272\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2025 - accuracy: 0.6377 - val_loss: 1.2160 - val_accuracy: 0.6543\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1761 - accuracy: 0.6420 - val_loss: 1.2019 - val_accuracy: 0.6347\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1406 - accuracy: 0.6559 - val_loss: 1.1648 - val_accuracy: 0.6464\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1172 - accuracy: 0.6616 - val_loss: 1.1512 - val_accuracy: 0.6511\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0940 - accuracy: 0.6670 - val_loss: 1.0565 - val_accuracy: 0.6984\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0764 - accuracy: 0.6736 - val_loss: 1.0485 - val_accuracy: 0.6967\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0501 - accuracy: 0.6828 - val_loss: 1.0316 - val_accuracy: 0.6990\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0372 - accuracy: 0.6874 - val_loss: 1.0411 - val_accuracy: 0.6874\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0161 - accuracy: 0.6943 - val_loss: 1.0527 - val_accuracy: 0.6793\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9934 - accuracy: 0.7029 - val_loss: 1.0418 - val_accuracy: 0.6886\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9850 - accuracy: 0.7035 - val_loss: 0.9751 - val_accuracy: 0.7233\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9659 - accuracy: 0.7109 - val_loss: 1.0213 - val_accuracy: 0.7093\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9563 - accuracy: 0.7128 - val_loss: 0.9245 - val_accuracy: 0.7336\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9407 - accuracy: 0.7170 - val_loss: 0.9600 - val_accuracy: 0.7164\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9221 - accuracy: 0.7242 - val_loss: 0.9022 - val_accuracy: 0.7346\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9077 - accuracy: 0.7286 - val_loss: 0.9202 - val_accuracy: 0.7303\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8959 - accuracy: 0.7322 - val_loss: 0.9437 - val_accuracy: 0.7297\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8796 - accuracy: 0.7390 - val_loss: 0.9115 - val_accuracy: 0.7406\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8713 - accuracy: 0.7415 - val_loss: 0.9029 - val_accuracy: 0.7416\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8615 - accuracy: 0.7441 - val_loss: 0.8848 - val_accuracy: 0.7455\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8440 - accuracy: 0.7490 - val_loss: 0.8482 - val_accuracy: 0.7650\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8295 - accuracy: 0.7546 - val_loss: 0.8242 - val_accuracy: 0.7640\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8237 - accuracy: 0.7559 - val_loss: 0.9046 - val_accuracy: 0.7421\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8126 - accuracy: 0.7605 - val_loss: 0.8177 - val_accuracy: 0.7677\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8008 - accuracy: 0.7639 - val_loss: 0.8592 - val_accuracy: 0.7467\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7927 - accuracy: 0.7657 - val_loss: 0.8225 - val_accuracy: 0.7650\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7843 - accuracy: 0.7684 - val_loss: 0.8425 - val_accuracy: 0.7607\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7817 - accuracy: 0.7675 - val_loss: 0.8764 - val_accuracy: 0.7515\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7697 - accuracy: 0.7707 - val_loss: 0.7826 - val_accuracy: 0.7736\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7561 - accuracy: 0.7765 - val_loss: 0.8063 - val_accuracy: 0.7694\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7520 - accuracy: 0.7776 - val_loss: 0.7811 - val_accuracy: 0.7726\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7412 - accuracy: 0.7810 - val_loss: 0.7620 - val_accuracy: 0.7846\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7341 - accuracy: 0.7822 - val_loss: 0.7861 - val_accuracy: 0.7743\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7282 - accuracy: 0.7856 - val_loss: 0.7720 - val_accuracy: 0.7828\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7195 - accuracy: 0.7882 - val_loss: 0.7442 - val_accuracy: 0.7895\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7184 - accuracy: 0.7876 - val_loss: 0.7237 - val_accuracy: 0.7934\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7098 - accuracy: 0.7899 - val_loss: 0.7378 - val_accuracy: 0.7895\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6991 - accuracy: 0.7928 - val_loss: 0.7624 - val_accuracy: 0.7798\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6909 - accuracy: 0.7953 - val_loss: 0.7502 - val_accuracy: 0.7829\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6879 - accuracy: 0.7964 - val_loss: 0.7308 - val_accuracy: 0.7864\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6760 - accuracy: 0.7993 - val_loss: 0.8010 - val_accuracy: 0.7688\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6767 - accuracy: 0.8000 - val_loss: 0.7537 - val_accuracy: 0.7851\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6660 - accuracy: 0.8031 - val_loss: 0.7127 - val_accuracy: 0.7991\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6627 - accuracy: 0.8035 - val_loss: 0.7138 - val_accuracy: 0.7919\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6565 - accuracy: 0.8048 - val_loss: 0.7127 - val_accuracy: 0.7961\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6527 - accuracy: 0.8066 - val_loss: 0.7286 - val_accuracy: 0.7844\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6421 - accuracy: 0.8084 - val_loss: 0.6703 - val_accuracy: 0.8085\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6423 - accuracy: 0.8089 - val_loss: 0.7144 - val_accuracy: 0.7951\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6418 - accuracy: 0.8098 - val_loss: 0.7117 - val_accuracy: 0.7962\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6305 - accuracy: 0.8119 - val_loss: 0.6845 - val_accuracy: 0.8023\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6345 - accuracy: 0.8103 - val_loss: 0.6971 - val_accuracy: 0.7892\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6229 - accuracy: 0.8142 - val_loss: 0.6836 - val_accuracy: 0.8045\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6149 - accuracy: 0.8168 - val_loss: 0.6543 - val_accuracy: 0.8159\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6135 - accuracy: 0.8177 - val_loss: 0.6796 - val_accuracy: 0.7966\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6093 - accuracy: 0.8203 - val_loss: 0.6499 - val_accuracy: 0.8139\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6008 - accuracy: 0.8210 - val_loss: 0.7012 - val_accuracy: 0.7928\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5976 - accuracy: 0.8227 - val_loss: 0.6290 - val_accuracy: 0.8187\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5926 - accuracy: 0.8245 - val_loss: 0.6407 - val_accuracy: 0.8187\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5878 - accuracy: 0.8254 - val_loss: 0.6703 - val_accuracy: 0.8089\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5839 - accuracy: 0.8268 - val_loss: 0.6909 - val_accuracy: 0.7980\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5756 - accuracy: 0.8294 - val_loss: 0.6278 - val_accuracy: 0.8169\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5788 - accuracy: 0.8287 - val_loss: 0.6377 - val_accuracy: 0.8143\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5739 - accuracy: 0.8305 - val_loss: 0.6386 - val_accuracy: 0.8162\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5706 - accuracy: 0.8296 - val_loss: 0.6608 - val_accuracy: 0.8054\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5622 - accuracy: 0.8318 - val_loss: 0.7221 - val_accuracy: 0.7857\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5637 - accuracy: 0.8317 - val_loss: 0.6384 - val_accuracy: 0.8186\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5543 - accuracy: 0.8346 - val_loss: 0.6262 - val_accuracy: 0.8200\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5516 - accuracy: 0.8353 - val_loss: 0.6320 - val_accuracy: 0.8184\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5480 - accuracy: 0.8369 - val_loss: 0.6278 - val_accuracy: 0.8180\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5446 - accuracy: 0.8381 - val_loss: 0.6162 - val_accuracy: 0.8208\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5442 - accuracy: 0.8381 - val_loss: 0.6261 - val_accuracy: 0.8181\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5403 - accuracy: 0.8392 - val_loss: 0.6217 - val_accuracy: 0.8227\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5324 - accuracy: 0.8422 - val_loss: 0.5686 - val_accuracy: 0.8389\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5322 - accuracy: 0.8387 - val_loss: 0.5854 - val_accuracy: 0.8318\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5285 - accuracy: 0.8425 - val_loss: 0.5714 - val_accuracy: 0.8392\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5280 - accuracy: 0.8430 - val_loss: 0.5973 - val_accuracy: 0.8276\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5213 - accuracy: 0.8460 - val_loss: 0.5981 - val_accuracy: 0.8318\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5176 - accuracy: 0.8455 - val_loss: 0.6021 - val_accuracy: 0.8218\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5128 - accuracy: 0.8475 - val_loss: 0.6160 - val_accuracy: 0.8261\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5095 - accuracy: 0.8492 - val_loss: 0.5908 - val_accuracy: 0.8317\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5116 - accuracy: 0.8484 - val_loss: 0.5937 - val_accuracy: 0.8266\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5076 - accuracy: 0.8489 - val_loss: 0.6317 - val_accuracy: 0.8124\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5068 - accuracy: 0.8491 - val_loss: 0.5667 - val_accuracy: 0.8376\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4981 - accuracy: 0.8528 - val_loss: 0.5700 - val_accuracy: 0.8321\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4947 - accuracy: 0.8540 - val_loss: 0.5695 - val_accuracy: 0.8393\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4941 - accuracy: 0.8527 - val_loss: 0.5948 - val_accuracy: 0.8237\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4897 - accuracy: 0.8549 - val_loss: 0.6089 - val_accuracy: 0.8181\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4868 - accuracy: 0.8551 - val_loss: 0.5734 - val_accuracy: 0.8360\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4863 - accuracy: 0.8552 - val_loss: 0.5818 - val_accuracy: 0.8351\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4847 - accuracy: 0.8551 - val_loss: 0.6031 - val_accuracy: 0.8251\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4800 - accuracy: 0.8576 - val_loss: 0.6001 - val_accuracy: 0.8315\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4774 - accuracy: 0.8580 - val_loss: 0.5568 - val_accuracy: 0.8364\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4741 - accuracy: 0.8599 - val_loss: 0.5311 - val_accuracy: 0.8471\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4759 - accuracy: 0.8590 - val_loss: 0.6007 - val_accuracy: 0.8290\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4731 - accuracy: 0.8590 - val_loss: 0.6154 - val_accuracy: 0.8217\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4723 - accuracy: 0.8583 - val_loss: 0.5769 - val_accuracy: 0.8379\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4616 - accuracy: 0.8624 - val_loss: 0.5639 - val_accuracy: 0.8391\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4606 - accuracy: 0.8640 - val_loss: 0.5549 - val_accuracy: 0.8394\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4589 - accuracy: 0.8640 - val_loss: 0.5611 - val_accuracy: 0.8387\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4553 - accuracy: 0.8654 - val_loss: 0.5894 - val_accuracy: 0.8316\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4566 - accuracy: 0.8635 - val_loss: 0.5572 - val_accuracy: 0.8435\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4514 - accuracy: 0.8664 - val_loss: 0.5362 - val_accuracy: 0.8456\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4459 - accuracy: 0.8676 - val_loss: 0.5036 - val_accuracy: 0.8545\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4467 - accuracy: 0.8655 - val_loss: 0.5916 - val_accuracy: 0.8311\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4414 - accuracy: 0.8690 - val_loss: 0.5143 - val_accuracy: 0.8509\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4364 - accuracy: 0.8699 - val_loss: 0.5225 - val_accuracy: 0.8495\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4405 - accuracy: 0.8692 - val_loss: 0.5147 - val_accuracy: 0.8566\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4368 - accuracy: 0.8701 - val_loss: 0.5064 - val_accuracy: 0.8549\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4315 - accuracy: 0.8724 - val_loss: 0.5670 - val_accuracy: 0.8381\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4308 - accuracy: 0.8726 - val_loss: 0.5466 - val_accuracy: 0.8426\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4283 - accuracy: 0.8725 - val_loss: 0.4945 - val_accuracy: 0.8582\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4296 - accuracy: 0.8725 - val_loss: 0.6366 - val_accuracy: 0.8191\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4260 - accuracy: 0.8738 - val_loss: 0.4988 - val_accuracy: 0.8567\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4271 - accuracy: 0.8723 - val_loss: 0.5533 - val_accuracy: 0.8418\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4213 - accuracy: 0.8737 - val_loss: 0.5253 - val_accuracy: 0.8495\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4221 - accuracy: 0.8739 - val_loss: 0.5125 - val_accuracy: 0.8532\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4175 - accuracy: 0.8753 - val_loss: 0.5589 - val_accuracy: 0.8355\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4141 - accuracy: 0.8774 - val_loss: 0.5484 - val_accuracy: 0.8440\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4126 - accuracy: 0.8775 - val_loss: 0.5492 - val_accuracy: 0.8407\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4102 - accuracy: 0.8772 - val_loss: 0.4998 - val_accuracy: 0.8554\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4090 - accuracy: 0.8770 - val_loss: 0.5553 - val_accuracy: 0.8419\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4054 - accuracy: 0.8798 - val_loss: 0.5142 - val_accuracy: 0.8501\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4042 - accuracy: 0.8788 - val_loss: 0.4855 - val_accuracy: 0.8617\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4015 - accuracy: 0.8805 - val_loss: 0.5170 - val_accuracy: 0.8498\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3968 - accuracy: 0.8817 - val_loss: 0.5190 - val_accuracy: 0.8436\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4007 - accuracy: 0.8796 - val_loss: 0.4977 - val_accuracy: 0.8607\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3907 - accuracy: 0.8845 - val_loss: 0.4620 - val_accuracy: 0.8681\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3927 - accuracy: 0.8835 - val_loss: 0.5008 - val_accuracy: 0.8564\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3901 - accuracy: 0.8833 - val_loss: 0.5019 - val_accuracy: 0.8571\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3884 - accuracy: 0.8846 - val_loss: 0.4777 - val_accuracy: 0.8656\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3902 - accuracy: 0.8825 - val_loss: 0.5420 - val_accuracy: 0.8460\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3858 - accuracy: 0.8845 - val_loss: 0.4619 - val_accuracy: 0.8683\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3819 - accuracy: 0.8878 - val_loss: 0.4841 - val_accuracy: 0.8646\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3868 - accuracy: 0.8842 - val_loss: 0.5100 - val_accuracy: 0.8563\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3788 - accuracy: 0.8870 - val_loss: 0.4710 - val_accuracy: 0.8659\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3758 - accuracy: 0.8891 - val_loss: 0.5497 - val_accuracy: 0.8390\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3731 - accuracy: 0.8899 - val_loss: 0.4909 - val_accuracy: 0.8598\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3731 - accuracy: 0.8894 - val_loss: 0.4776 - val_accuracy: 0.8639\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3715 - accuracy: 0.8912 - val_loss: 0.4871 - val_accuracy: 0.8616\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3723 - accuracy: 0.8892 - val_loss: 0.4671 - val_accuracy: 0.8658\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3680 - accuracy: 0.8895 - val_loss: 0.5024 - val_accuracy: 0.8556\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3672 - accuracy: 0.8913 - val_loss: 0.5386 - val_accuracy: 0.8500\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3657 - accuracy: 0.8908 - val_loss: 0.4783 - val_accuracy: 0.8636\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3611 - accuracy: 0.8938 - val_loss: 0.4685 - val_accuracy: 0.8694\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3615 - accuracy: 0.8939 - val_loss: 0.5085 - val_accuracy: 0.8523\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.8938 - val_loss: 0.4649 - val_accuracy: 0.8696\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3562 - accuracy: 0.8941 - val_loss: 0.5002 - val_accuracy: 0.8561\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3517 - accuracy: 0.8972 - val_loss: 0.4764 - val_accuracy: 0.8639\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3524 - accuracy: 0.8952 - val_loss: 0.4690 - val_accuracy: 0.8651\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3538 - accuracy: 0.8948 - val_loss: 0.4472 - val_accuracy: 0.8750\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3484 - accuracy: 0.8970 - val_loss: 0.4434 - val_accuracy: 0.8766\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3511 - accuracy: 0.8965 - val_loss: 0.4737 - val_accuracy: 0.8650\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3513 - accuracy: 0.8951 - val_loss: 0.4999 - val_accuracy: 0.8597\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3464 - accuracy: 0.8982 - val_loss: 0.4304 - val_accuracy: 0.8794\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3456 - accuracy: 0.8977 - val_loss: 0.5007 - val_accuracy: 0.8575\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3443 - accuracy: 0.8984 - val_loss: 0.4789 - val_accuracy: 0.8617\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3474 - accuracy: 0.8957 - val_loss: 0.4495 - val_accuracy: 0.8749\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3424 - accuracy: 0.8989 - val_loss: 0.4847 - val_accuracy: 0.8650\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3372 - accuracy: 0.9005 - val_loss: 0.4516 - val_accuracy: 0.8756\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3322 - accuracy: 0.9019 - val_loss: 0.4843 - val_accuracy: 0.8661\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3360 - accuracy: 0.9007 - val_loss: 0.4709 - val_accuracy: 0.8654\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3315 - accuracy: 0.9015 - val_loss: 0.4329 - val_accuracy: 0.8781\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3388 - accuracy: 0.8993 - val_loss: 0.4555 - val_accuracy: 0.8699\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3272 - accuracy: 0.9032 - val_loss: 0.5056 - val_accuracy: 0.8595\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3296 - accuracy: 0.9033 - val_loss: 0.4084 - val_accuracy: 0.8867\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3199 - accuracy: 0.9065 - val_loss: 0.4290 - val_accuracy: 0.8843\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3214 - accuracy: 0.9061 - val_loss: 0.4179 - val_accuracy: 0.8866\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3164 - accuracy: 0.9071 - val_loss: 0.4174 - val_accuracy: 0.8859\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3254 - accuracy: 0.9031 - val_loss: 0.4425 - val_accuracy: 0.8764\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3221 - accuracy: 0.9061 - val_loss: 0.4636 - val_accuracy: 0.8724\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3142 - accuracy: 0.9074 - val_loss: 0.4124 - val_accuracy: 0.8863\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3199 - accuracy: 0.9060 - val_loss: 0.4312 - val_accuracy: 0.8829\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3159 - accuracy: 0.9070 - val_loss: 0.4389 - val_accuracy: 0.8778\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3140 - accuracy: 0.9076 - val_loss: 0.4214 - val_accuracy: 0.8837\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3152 - accuracy: 0.9064 - val_loss: 0.4535 - val_accuracy: 0.8733\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3116 - accuracy: 0.9087 - val_loss: 0.4320 - val_accuracy: 0.8827\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3038 - accuracy: 0.9120 - val_loss: 0.4216 - val_accuracy: 0.8814\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3050 - accuracy: 0.9098 - val_loss: 0.4591 - val_accuracy: 0.8666\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2998 - accuracy: 0.9117 - val_loss: 0.4586 - val_accuracy: 0.8717\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3031 - accuracy: 0.9108 - val_loss: 0.4506 - val_accuracy: 0.8748\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3035 - accuracy: 0.9105 - val_loss: 0.4517 - val_accuracy: 0.8717\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3020 - accuracy: 0.9115 - val_loss: 0.4577 - val_accuracy: 0.8721\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3007 - accuracy: 0.9113 - val_loss: 0.4206 - val_accuracy: 0.8859\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2959 - accuracy: 0.9139 - val_loss: 0.3895 - val_accuracy: 0.8971\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2955 - accuracy: 0.9138 - val_loss: 0.3910 - val_accuracy: 0.8955\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2970 - accuracy: 0.9137 - val_loss: 0.4036 - val_accuracy: 0.8894\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2954 - accuracy: 0.9151 - val_loss: 0.4683 - val_accuracy: 0.8711\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2960 - accuracy: 0.9147 - val_loss: 0.5033 - val_accuracy: 0.8582\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2931 - accuracy: 0.9136 - val_loss: 0.4472 - val_accuracy: 0.8730\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2958 - accuracy: 0.9130 - val_loss: 0.4631 - val_accuracy: 0.8696\n",
      "Try 6/100: Best_val_acc: [0.6597148180007935, 0.8263888955116272], lr: 6.052732596550928e-05, Lambda: 6.25286653666543e-05\n",
      "\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.2809 - accuracy: 0.1475 - val_loss: 2.1854 - val_accuracy: 0.3801\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1649 - accuracy: 0.2650 - val_loss: 2.0190 - val_accuracy: 0.4470\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0011 - accuracy: 0.3446 - val_loss: 1.8171 - val_accuracy: 0.5176\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8139 - accuracy: 0.4211 - val_loss: 1.6513 - val_accuracy: 0.5465\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6429 - accuracy: 0.4905 - val_loss: 1.4846 - val_accuracy: 0.6001\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5055 - accuracy: 0.5428 - val_loss: 1.3876 - val_accuracy: 0.6156\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4081 - accuracy: 0.5755 - val_loss: 1.2574 - val_accuracy: 0.6564\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3323 - accuracy: 0.6020 - val_loss: 1.2465 - val_accuracy: 0.6574\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2739 - accuracy: 0.6198 - val_loss: 1.1911 - val_accuracy: 0.6609\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.2276 - accuracy: 0.6331 - val_loss: 1.2342 - val_accuracy: 0.6323\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1847 - accuracy: 0.6482 - val_loss: 1.1014 - val_accuracy: 0.6936\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1488 - accuracy: 0.6586 - val_loss: 1.1416 - val_accuracy: 0.6714\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.1082 - accuracy: 0.6709 - val_loss: 1.0835 - val_accuracy: 0.6898\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0897 - accuracy: 0.6735 - val_loss: 1.0872 - val_accuracy: 0.6920\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0591 - accuracy: 0.6859 - val_loss: 1.0052 - val_accuracy: 0.7128\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0352 - accuracy: 0.6927 - val_loss: 0.9968 - val_accuracy: 0.7142\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0103 - accuracy: 0.7002 - val_loss: 0.9770 - val_accuracy: 0.7266\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9915 - accuracy: 0.7064 - val_loss: 0.9865 - val_accuracy: 0.7169\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9746 - accuracy: 0.7114 - val_loss: 0.9760 - val_accuracy: 0.7192\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9561 - accuracy: 0.7152 - val_loss: 0.9589 - val_accuracy: 0.7309\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9389 - accuracy: 0.7228 - val_loss: 0.9347 - val_accuracy: 0.7290\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9219 - accuracy: 0.7266 - val_loss: 0.9072 - val_accuracy: 0.7428\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9086 - accuracy: 0.7301 - val_loss: 0.9917 - val_accuracy: 0.7115\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9015 - accuracy: 0.7315 - val_loss: 0.8857 - val_accuracy: 0.7521\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8803 - accuracy: 0.7390 - val_loss: 0.9002 - val_accuracy: 0.7472\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8665 - accuracy: 0.7444 - val_loss: 0.9003 - val_accuracy: 0.7400\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8596 - accuracy: 0.7442 - val_loss: 0.8790 - val_accuracy: 0.7504\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8493 - accuracy: 0.7457 - val_loss: 0.8552 - val_accuracy: 0.7520\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8359 - accuracy: 0.7507 - val_loss: 0.9020 - val_accuracy: 0.7309\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8273 - accuracy: 0.7532 - val_loss: 0.8239 - val_accuracy: 0.7667\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8089 - accuracy: 0.7599 - val_loss: 0.8329 - val_accuracy: 0.7601\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8030 - accuracy: 0.7620 - val_loss: 0.7658 - val_accuracy: 0.7862\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7950 - accuracy: 0.7635 - val_loss: 0.7849 - val_accuracy: 0.7806\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7819 - accuracy: 0.7655 - val_loss: 0.8403 - val_accuracy: 0.7615\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7866 - accuracy: 0.7645 - val_loss: 0.8979 - val_accuracy: 0.7442\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7691 - accuracy: 0.7710 - val_loss: 0.8010 - val_accuracy: 0.7711\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7593 - accuracy: 0.7742 - val_loss: 0.7852 - val_accuracy: 0.7786\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7503 - accuracy: 0.7770 - val_loss: 0.7667 - val_accuracy: 0.7798\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7444 - accuracy: 0.7784 - val_loss: 0.7495 - val_accuracy: 0.7894\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7367 - accuracy: 0.7821 - val_loss: 0.7877 - val_accuracy: 0.7764\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7292 - accuracy: 0.7835 - val_loss: 0.7975 - val_accuracy: 0.7746\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7246 - accuracy: 0.7839 - val_loss: 0.7679 - val_accuracy: 0.7826\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7159 - accuracy: 0.7849 - val_loss: 0.7631 - val_accuracy: 0.7815\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7041 - accuracy: 0.7910 - val_loss: 0.7159 - val_accuracy: 0.7982\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7027 - accuracy: 0.7918 - val_loss: 0.7935 - val_accuracy: 0.7693\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7025 - accuracy: 0.7904 - val_loss: 0.7413 - val_accuracy: 0.7927\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6893 - accuracy: 0.7938 - val_loss: 0.6847 - val_accuracy: 0.8101\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6815 - accuracy: 0.7977 - val_loss: 0.7700 - val_accuracy: 0.7793\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6783 - accuracy: 0.7975 - val_loss: 0.7283 - val_accuracy: 0.7931\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6705 - accuracy: 0.7990 - val_loss: 0.7127 - val_accuracy: 0.7975\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6693 - accuracy: 0.8003 - val_loss: 0.7169 - val_accuracy: 0.7989\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6621 - accuracy: 0.8027 - val_loss: 0.7188 - val_accuracy: 0.7953\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6534 - accuracy: 0.8051 - val_loss: 0.7037 - val_accuracy: 0.7979\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6506 - accuracy: 0.8047 - val_loss: 0.6802 - val_accuracy: 0.8062\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6464 - accuracy: 0.8079 - val_loss: 0.6801 - val_accuracy: 0.8091\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6390 - accuracy: 0.8091 - val_loss: 0.7077 - val_accuracy: 0.7968\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6373 - accuracy: 0.8096 - val_loss: 0.7406 - val_accuracy: 0.7838\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6312 - accuracy: 0.8117 - val_loss: 0.6659 - val_accuracy: 0.8116\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6281 - accuracy: 0.8117 - val_loss: 0.6498 - val_accuracy: 0.8185\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6216 - accuracy: 0.8146 - val_loss: 0.7330 - val_accuracy: 0.7866\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6131 - accuracy: 0.8177 - val_loss: 0.6281 - val_accuracy: 0.8237\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6107 - accuracy: 0.8187 - val_loss: 0.6869 - val_accuracy: 0.8012\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6113 - accuracy: 0.8168 - val_loss: 0.6333 - val_accuracy: 0.8212\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6059 - accuracy: 0.8195 - val_loss: 0.6360 - val_accuracy: 0.8241\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6044 - accuracy: 0.8199 - val_loss: 0.6899 - val_accuracy: 0.8026\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5914 - accuracy: 0.8256 - val_loss: 0.6273 - val_accuracy: 0.8241\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5902 - accuracy: 0.8251 - val_loss: 0.6117 - val_accuracy: 0.8319\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5843 - accuracy: 0.8263 - val_loss: 0.7078 - val_accuracy: 0.7938\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5819 - accuracy: 0.8267 - val_loss: 0.6225 - val_accuracy: 0.8226\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5841 - accuracy: 0.8244 - val_loss: 0.6444 - val_accuracy: 0.8182\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5716 - accuracy: 0.8293 - val_loss: 0.6391 - val_accuracy: 0.8166\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5702 - accuracy: 0.8300 - val_loss: 0.6704 - val_accuracy: 0.8068\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5747 - accuracy: 0.8278 - val_loss: 0.7066 - val_accuracy: 0.7917\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5638 - accuracy: 0.8327 - val_loss: 0.6323 - val_accuracy: 0.8227\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5567 - accuracy: 0.8338 - val_loss: 0.5903 - val_accuracy: 0.8314\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5566 - accuracy: 0.8343 - val_loss: 0.5906 - val_accuracy: 0.8326\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5526 - accuracy: 0.8365 - val_loss: 0.5852 - val_accuracy: 0.8360\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5423 - accuracy: 0.8389 - val_loss: 0.6055 - val_accuracy: 0.8289\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5464 - accuracy: 0.8357 - val_loss: 0.6587 - val_accuracy: 0.8030\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5432 - accuracy: 0.8375 - val_loss: 0.5661 - val_accuracy: 0.8394\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5387 - accuracy: 0.8394 - val_loss: 0.5620 - val_accuracy: 0.8389\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5347 - accuracy: 0.8405 - val_loss: 0.5530 - val_accuracy: 0.8469\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5311 - accuracy: 0.8415 - val_loss: 0.5534 - val_accuracy: 0.8464\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5294 - accuracy: 0.8425 - val_loss: 0.6046 - val_accuracy: 0.8291\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5207 - accuracy: 0.8469 - val_loss: 0.5792 - val_accuracy: 0.8369\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5305 - accuracy: 0.8410 - val_loss: 0.5643 - val_accuracy: 0.8384\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5167 - accuracy: 0.8462 - val_loss: 0.5525 - val_accuracy: 0.8404\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5132 - accuracy: 0.8477 - val_loss: 0.5492 - val_accuracy: 0.8446\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5160 - accuracy: 0.8447 - val_loss: 0.5825 - val_accuracy: 0.8333\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5082 - accuracy: 0.8483 - val_loss: 0.6099 - val_accuracy: 0.8255\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5081 - accuracy: 0.8488 - val_loss: 0.6796 - val_accuracy: 0.8034\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5066 - accuracy: 0.8500 - val_loss: 0.5632 - val_accuracy: 0.8417\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5035 - accuracy: 0.8496 - val_loss: 0.6526 - val_accuracy: 0.8139\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4996 - accuracy: 0.8520 - val_loss: 0.5225 - val_accuracy: 0.8530\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4924 - accuracy: 0.8543 - val_loss: 0.5353 - val_accuracy: 0.8484\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4903 - accuracy: 0.8537 - val_loss: 0.5975 - val_accuracy: 0.8301\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4864 - accuracy: 0.8562 - val_loss: 0.5456 - val_accuracy: 0.8464\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4847 - accuracy: 0.8575 - val_loss: 0.5217 - val_accuracy: 0.8536\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4857 - accuracy: 0.8562 - val_loss: 0.5925 - val_accuracy: 0.8256\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4833 - accuracy: 0.8567 - val_loss: 0.5966 - val_accuracy: 0.8301\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4797 - accuracy: 0.8579 - val_loss: 0.5124 - val_accuracy: 0.8557\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4767 - accuracy: 0.8582 - val_loss: 0.5608 - val_accuracy: 0.8426\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4691 - accuracy: 0.8613 - val_loss: 0.5286 - val_accuracy: 0.8523\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4664 - accuracy: 0.8623 - val_loss: 0.5090 - val_accuracy: 0.8554\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4677 - accuracy: 0.8610 - val_loss: 0.4995 - val_accuracy: 0.8564\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4690 - accuracy: 0.8593 - val_loss: 0.6561 - val_accuracy: 0.8103\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4728 - accuracy: 0.8595 - val_loss: 0.5735 - val_accuracy: 0.8357\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4669 - accuracy: 0.8604 - val_loss: 0.5070 - val_accuracy: 0.8555\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4576 - accuracy: 0.8641 - val_loss: 0.5760 - val_accuracy: 0.8364\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4495 - accuracy: 0.8678 - val_loss: 0.4996 - val_accuracy: 0.8602\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4523 - accuracy: 0.8646 - val_loss: 0.5053 - val_accuracy: 0.8569\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4505 - accuracy: 0.8666 - val_loss: 0.5533 - val_accuracy: 0.8441\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4488 - accuracy: 0.8670 - val_loss: 0.4982 - val_accuracy: 0.8596\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4482 - accuracy: 0.8674 - val_loss: 0.5450 - val_accuracy: 0.8453\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4423 - accuracy: 0.8691 - val_loss: 0.5097 - val_accuracy: 0.8549\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4409 - accuracy: 0.8692 - val_loss: 0.5455 - val_accuracy: 0.8444\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4379 - accuracy: 0.8694 - val_loss: 0.5219 - val_accuracy: 0.8520\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4314 - accuracy: 0.8725 - val_loss: 0.4807 - val_accuracy: 0.8624\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4432 - accuracy: 0.8680 - val_loss: 0.5678 - val_accuracy: 0.8371\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4353 - accuracy: 0.8709 - val_loss: 0.4982 - val_accuracy: 0.8601\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4266 - accuracy: 0.8732 - val_loss: 0.5201 - val_accuracy: 0.8517\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4342 - accuracy: 0.8702 - val_loss: 0.5723 - val_accuracy: 0.8347\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4269 - accuracy: 0.8726 - val_loss: 0.5258 - val_accuracy: 0.8486\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4243 - accuracy: 0.8742 - val_loss: 0.5244 - val_accuracy: 0.8544\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4257 - accuracy: 0.8739 - val_loss: 0.5272 - val_accuracy: 0.8480\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4164 - accuracy: 0.8780 - val_loss: 0.5178 - val_accuracy: 0.8536\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4180 - accuracy: 0.8763 - val_loss: 0.4673 - val_accuracy: 0.8701\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4113 - accuracy: 0.8778 - val_loss: 0.5088 - val_accuracy: 0.8549\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4209 - accuracy: 0.8753 - val_loss: 0.5811 - val_accuracy: 0.8271\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4120 - accuracy: 0.8787 - val_loss: 0.5236 - val_accuracy: 0.8513\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4114 - accuracy: 0.8776 - val_loss: 0.4603 - val_accuracy: 0.8698\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4077 - accuracy: 0.8802 - val_loss: 0.4998 - val_accuracy: 0.8603\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4010 - accuracy: 0.8809 - val_loss: 0.5533 - val_accuracy: 0.8401\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3989 - accuracy: 0.8829 - val_loss: 0.5853 - val_accuracy: 0.8260\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4040 - accuracy: 0.8800 - val_loss: 0.4595 - val_accuracy: 0.8699\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4022 - accuracy: 0.8808 - val_loss: 0.4988 - val_accuracy: 0.8580\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3950 - accuracy: 0.8845 - val_loss: 0.4891 - val_accuracy: 0.8612\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3955 - accuracy: 0.8842 - val_loss: 0.5092 - val_accuracy: 0.8555\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3930 - accuracy: 0.8827 - val_loss: 0.4796 - val_accuracy: 0.8630\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3939 - accuracy: 0.8840 - val_loss: 0.4852 - val_accuracy: 0.8604\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3883 - accuracy: 0.8862 - val_loss: 0.5079 - val_accuracy: 0.8558\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3839 - accuracy: 0.8874 - val_loss: 0.5053 - val_accuracy: 0.8562\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3808 - accuracy: 0.8891 - val_loss: 0.4930 - val_accuracy: 0.8600\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3883 - accuracy: 0.8844 - val_loss: 0.4690 - val_accuracy: 0.8689\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3848 - accuracy: 0.8864 - val_loss: 0.4936 - val_accuracy: 0.8599\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3810 - accuracy: 0.8867 - val_loss: 0.4367 - val_accuracy: 0.8776\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3779 - accuracy: 0.8893 - val_loss: 0.4766 - val_accuracy: 0.8656\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3783 - accuracy: 0.8874 - val_loss: 0.4715 - val_accuracy: 0.8663\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3698 - accuracy: 0.8907 - val_loss: 0.4571 - val_accuracy: 0.8734\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3720 - accuracy: 0.8913 - val_loss: 0.4507 - val_accuracy: 0.8743\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3722 - accuracy: 0.8890 - val_loss: 0.4739 - val_accuracy: 0.8644\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3625 - accuracy: 0.8932 - val_loss: 0.4756 - val_accuracy: 0.8644\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3652 - accuracy: 0.8932 - val_loss: 0.4414 - val_accuracy: 0.8779\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3633 - accuracy: 0.8923 - val_loss: 0.4501 - val_accuracy: 0.8746\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3623 - accuracy: 0.8939 - val_loss: 0.4694 - val_accuracy: 0.8692\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3627 - accuracy: 0.8934 - val_loss: 0.4529 - val_accuracy: 0.8719\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3590 - accuracy: 0.8936 - val_loss: 0.4654 - val_accuracy: 0.8691\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3570 - accuracy: 0.8959 - val_loss: 0.4734 - val_accuracy: 0.8637\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3568 - accuracy: 0.8942 - val_loss: 0.4496 - val_accuracy: 0.8743\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3592 - accuracy: 0.8933 - val_loss: 0.4892 - val_accuracy: 0.8574\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3516 - accuracy: 0.8969 - val_loss: 0.4992 - val_accuracy: 0.8542\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3515 - accuracy: 0.8970 - val_loss: 0.4950 - val_accuracy: 0.8605\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3500 - accuracy: 0.8973 - val_loss: 0.4247 - val_accuracy: 0.8821\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3517 - accuracy: 0.8956 - val_loss: 0.4532 - val_accuracy: 0.8751\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3453 - accuracy: 0.8982 - val_loss: 0.4591 - val_accuracy: 0.8721\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3464 - accuracy: 0.8970 - val_loss: 0.4919 - val_accuracy: 0.8612\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3453 - accuracy: 0.8981 - val_loss: 0.4297 - val_accuracy: 0.8813\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3453 - accuracy: 0.8963 - val_loss: 0.4875 - val_accuracy: 0.8644\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3446 - accuracy: 0.8990 - val_loss: 0.5010 - val_accuracy: 0.8590\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3447 - accuracy: 0.8978 - val_loss: 0.4396 - val_accuracy: 0.8779\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3374 - accuracy: 0.9002 - val_loss: 0.4457 - val_accuracy: 0.8764\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3306 - accuracy: 0.9032 - val_loss: 0.4587 - val_accuracy: 0.8720\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3314 - accuracy: 0.9019 - val_loss: 0.4539 - val_accuracy: 0.8720\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3304 - accuracy: 0.9040 - val_loss: 0.4450 - val_accuracy: 0.8764\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3372 - accuracy: 0.8994 - val_loss: 0.4334 - val_accuracy: 0.8799\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3334 - accuracy: 0.9005 - val_loss: 0.4159 - val_accuracy: 0.8854\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3240 - accuracy: 0.9046 - val_loss: 0.4618 - val_accuracy: 0.8674\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3238 - accuracy: 0.9035 - val_loss: 0.4055 - val_accuracy: 0.8884\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3263 - accuracy: 0.9043 - val_loss: 0.4102 - val_accuracy: 0.8862\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3238 - accuracy: 0.9044 - val_loss: 0.4342 - val_accuracy: 0.8787\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3191 - accuracy: 0.9054 - val_loss: 0.4007 - val_accuracy: 0.8879\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3269 - accuracy: 0.9032 - val_loss: 0.4255 - val_accuracy: 0.8782\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3211 - accuracy: 0.9062 - val_loss: 0.4201 - val_accuracy: 0.8845\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3156 - accuracy: 0.9072 - val_loss: 0.4746 - val_accuracy: 0.8674\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3133 - accuracy: 0.9078 - val_loss: 0.4548 - val_accuracy: 0.8744\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3107 - accuracy: 0.9092 - val_loss: 0.4162 - val_accuracy: 0.8869\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3145 - accuracy: 0.9083 - val_loss: 0.4594 - val_accuracy: 0.8754\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3238 - accuracy: 0.9020 - val_loss: 0.4666 - val_accuracy: 0.8689\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3154 - accuracy: 0.9060 - val_loss: 0.4345 - val_accuracy: 0.8820\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3081 - accuracy: 0.9094 - val_loss: 0.4208 - val_accuracy: 0.8842\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3046 - accuracy: 0.9109 - val_loss: 0.4621 - val_accuracy: 0.8723\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3113 - accuracy: 0.9088 - val_loss: 0.3996 - val_accuracy: 0.8911\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3002 - accuracy: 0.9107 - val_loss: 0.4331 - val_accuracy: 0.8814\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3052 - accuracy: 0.9087 - val_loss: 0.4527 - val_accuracy: 0.8727\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3029 - accuracy: 0.9115 - val_loss: 0.4646 - val_accuracy: 0.8712\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3054 - accuracy: 0.9093 - val_loss: 0.4370 - val_accuracy: 0.8737\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3013 - accuracy: 0.9112 - val_loss: 0.4729 - val_accuracy: 0.8696\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2983 - accuracy: 0.9116 - val_loss: 0.4409 - val_accuracy: 0.8754\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3014 - accuracy: 0.9113 - val_loss: 0.4191 - val_accuracy: 0.8840\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2928 - accuracy: 0.9132 - val_loss: 0.3805 - val_accuracy: 0.8940\n",
      "Try 7/100: Best_val_acc: [0.6459978818893433, 0.8311111330986023], lr: 6.0549510765612984e-05, Lambda: 5.672850268173574e-05\n",
      "\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.3013 - accuracy: 0.1286 - val_loss: 2.2337 - val_accuracy: 0.1978\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1970 - accuracy: 0.2249 - val_loss: 2.1562 - val_accuracy: 0.2516\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0282 - accuracy: 0.3299 - val_loss: 1.8785 - val_accuracy: 0.4044\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.8443 - accuracy: 0.4026 - val_loss: 1.7393 - val_accuracy: 0.4772\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.6647 - accuracy: 0.4903 - val_loss: 1.6566 - val_accuracy: 0.5098\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5179 - accuracy: 0.5479 - val_loss: 1.4345 - val_accuracy: 0.5849\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4052 - accuracy: 0.5828 - val_loss: 1.3740 - val_accuracy: 0.6101\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3233 - accuracy: 0.6047 - val_loss: 1.2844 - val_accuracy: 0.6337\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2653 - accuracy: 0.6210 - val_loss: 1.2473 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2192 - accuracy: 0.6342 - val_loss: 1.1729 - val_accuracy: 0.6658\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1769 - accuracy: 0.6468 - val_loss: 1.2579 - val_accuracy: 0.6220\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1466 - accuracy: 0.6550 - val_loss: 1.1609 - val_accuracy: 0.6694\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1167 - accuracy: 0.6641 - val_loss: 1.2316 - val_accuracy: 0.6280\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0875 - accuracy: 0.6752 - val_loss: 1.1537 - val_accuracy: 0.6564\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0627 - accuracy: 0.6812 - val_loss: 1.0706 - val_accuracy: 0.6927\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0453 - accuracy: 0.6872 - val_loss: 1.0935 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0219 - accuracy: 0.6945 - val_loss: 1.0113 - val_accuracy: 0.6974\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0031 - accuracy: 0.7003 - val_loss: 1.0387 - val_accuracy: 0.6975\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9860 - accuracy: 0.7051 - val_loss: 1.0223 - val_accuracy: 0.7094\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9649 - accuracy: 0.7116 - val_loss: 1.0434 - val_accuracy: 0.6930\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9493 - accuracy: 0.7187 - val_loss: 0.9775 - val_accuracy: 0.7179\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9368 - accuracy: 0.7207 - val_loss: 0.9517 - val_accuracy: 0.7244\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9231 - accuracy: 0.7253 - val_loss: 0.9871 - val_accuracy: 0.7051\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9040 - accuracy: 0.7304 - val_loss: 0.9483 - val_accuracy: 0.7212\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8884 - accuracy: 0.7355 - val_loss: 0.9753 - val_accuracy: 0.7106\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8751 - accuracy: 0.7404 - val_loss: 0.9087 - val_accuracy: 0.7420\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8641 - accuracy: 0.7436 - val_loss: 0.8389 - val_accuracy: 0.7576\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8525 - accuracy: 0.7462 - val_loss: 0.8847 - val_accuracy: 0.7439\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8425 - accuracy: 0.7497 - val_loss: 0.9443 - val_accuracy: 0.7227\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8277 - accuracy: 0.7531 - val_loss: 0.8885 - val_accuracy: 0.7399\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8190 - accuracy: 0.7548 - val_loss: 0.8982 - val_accuracy: 0.7372\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8082 - accuracy: 0.7579 - val_loss: 0.8486 - val_accuracy: 0.7522\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7969 - accuracy: 0.7623 - val_loss: 0.8364 - val_accuracy: 0.7577\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7848 - accuracy: 0.7671 - val_loss: 0.8455 - val_accuracy: 0.7552\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7807 - accuracy: 0.7657 - val_loss: 0.8766 - val_accuracy: 0.7361\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7700 - accuracy: 0.7698 - val_loss: 0.8573 - val_accuracy: 0.7459\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7622 - accuracy: 0.7733 - val_loss: 0.7968 - val_accuracy: 0.7694\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7485 - accuracy: 0.7766 - val_loss: 0.7621 - val_accuracy: 0.7805\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7481 - accuracy: 0.7765 - val_loss: 0.7636 - val_accuracy: 0.7798\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7359 - accuracy: 0.7818 - val_loss: 0.7775 - val_accuracy: 0.7779\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7304 - accuracy: 0.7822 - val_loss: 0.7331 - val_accuracy: 0.7893\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7215 - accuracy: 0.7845 - val_loss: 0.7258 - val_accuracy: 0.7923\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7142 - accuracy: 0.7865 - val_loss: 0.8364 - val_accuracy: 0.7549\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7087 - accuracy: 0.7882 - val_loss: 0.7680 - val_accuracy: 0.7749\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7003 - accuracy: 0.7907 - val_loss: 0.7853 - val_accuracy: 0.7649\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6959 - accuracy: 0.7915 - val_loss: 0.7282 - val_accuracy: 0.7853\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6895 - accuracy: 0.7951 - val_loss: 0.7514 - val_accuracy: 0.7796\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6817 - accuracy: 0.7963 - val_loss: 0.7886 - val_accuracy: 0.7651\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6799 - accuracy: 0.7981 - val_loss: 0.6823 - val_accuracy: 0.8010\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6712 - accuracy: 0.7995 - val_loss: 0.8061 - val_accuracy: 0.7614\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6642 - accuracy: 0.8016 - val_loss: 0.6450 - val_accuracy: 0.8150\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6589 - accuracy: 0.8039 - val_loss: 0.7355 - val_accuracy: 0.7831\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6494 - accuracy: 0.8050 - val_loss: 0.6774 - val_accuracy: 0.8049\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6473 - accuracy: 0.8059 - val_loss: 0.8192 - val_accuracy: 0.7575\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6433 - accuracy: 0.8080 - val_loss: 0.7028 - val_accuracy: 0.7926\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6346 - accuracy: 0.8099 - val_loss: 0.6853 - val_accuracy: 0.8006\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6324 - accuracy: 0.8115 - val_loss: 0.6972 - val_accuracy: 0.7962\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6267 - accuracy: 0.8125 - val_loss: 0.6279 - val_accuracy: 0.8189\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6190 - accuracy: 0.8159 - val_loss: 0.7119 - val_accuracy: 0.7939\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6203 - accuracy: 0.8148 - val_loss: 0.7177 - val_accuracy: 0.7896\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6142 - accuracy: 0.8154 - val_loss: 0.6739 - val_accuracy: 0.8046\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6058 - accuracy: 0.8205 - val_loss: 0.7070 - val_accuracy: 0.7888\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6018 - accuracy: 0.8197 - val_loss: 0.6254 - val_accuracy: 0.8171\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5951 - accuracy: 0.8234 - val_loss: 0.6603 - val_accuracy: 0.8069\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5888 - accuracy: 0.8250 - val_loss: 0.6956 - val_accuracy: 0.7974\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5904 - accuracy: 0.8228 - val_loss: 0.6960 - val_accuracy: 0.7941\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5861 - accuracy: 0.8265 - val_loss: 0.6963 - val_accuracy: 0.7946\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5850 - accuracy: 0.8232 - val_loss: 0.7313 - val_accuracy: 0.7819\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5767 - accuracy: 0.8277 - val_loss: 0.6370 - val_accuracy: 0.8159\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5699 - accuracy: 0.8302 - val_loss: 0.6240 - val_accuracy: 0.8161\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5687 - accuracy: 0.8295 - val_loss: 0.6600 - val_accuracy: 0.8069\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5669 - accuracy: 0.8312 - val_loss: 0.6672 - val_accuracy: 0.8016\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5620 - accuracy: 0.8333 - val_loss: 0.6013 - val_accuracy: 0.8266\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5548 - accuracy: 0.8325 - val_loss: 0.6171 - val_accuracy: 0.8234\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5513 - accuracy: 0.8360 - val_loss: 0.6948 - val_accuracy: 0.7975\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5501 - accuracy: 0.8358 - val_loss: 0.5863 - val_accuracy: 0.8300\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5478 - accuracy: 0.8355 - val_loss: 0.6484 - val_accuracy: 0.8109\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5416 - accuracy: 0.8384 - val_loss: 0.6439 - val_accuracy: 0.8133\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5365 - accuracy: 0.8392 - val_loss: 0.6363 - val_accuracy: 0.8138\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5312 - accuracy: 0.8420 - val_loss: 0.6457 - val_accuracy: 0.8101\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5318 - accuracy: 0.8413 - val_loss: 0.6442 - val_accuracy: 0.8084\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5378 - accuracy: 0.8385 - val_loss: 0.6738 - val_accuracy: 0.8019\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5261 - accuracy: 0.8444 - val_loss: 0.5961 - val_accuracy: 0.8245\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5275 - accuracy: 0.8410 - val_loss: 0.6354 - val_accuracy: 0.8175\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5192 - accuracy: 0.8439 - val_loss: 0.5937 - val_accuracy: 0.8274\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5166 - accuracy: 0.8443 - val_loss: 0.5883 - val_accuracy: 0.8273\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5113 - accuracy: 0.8463 - val_loss: 0.5931 - val_accuracy: 0.8281\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5082 - accuracy: 0.8480 - val_loss: 0.5768 - val_accuracy: 0.8354\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5026 - accuracy: 0.8500 - val_loss: 0.5609 - val_accuracy: 0.8362\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5062 - accuracy: 0.8494 - val_loss: 0.5380 - val_accuracy: 0.8477\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5002 - accuracy: 0.8520 - val_loss: 0.5936 - val_accuracy: 0.8281\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4980 - accuracy: 0.8505 - val_loss: 0.5846 - val_accuracy: 0.8281\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4989 - accuracy: 0.8508 - val_loss: 0.6501 - val_accuracy: 0.8079\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4912 - accuracy: 0.8548 - val_loss: 0.6038 - val_accuracy: 0.8295\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4865 - accuracy: 0.8557 - val_loss: 0.6226 - val_accuracy: 0.8199\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4859 - accuracy: 0.8547 - val_loss: 0.5466 - val_accuracy: 0.8452\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4870 - accuracy: 0.8541 - val_loss: 0.5393 - val_accuracy: 0.8463\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4824 - accuracy: 0.8545 - val_loss: 0.5824 - val_accuracy: 0.8301\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4742 - accuracy: 0.8585 - val_loss: 0.5719 - val_accuracy: 0.8358\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4745 - accuracy: 0.8576 - val_loss: 0.6234 - val_accuracy: 0.8164\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.8605 - val_loss: 0.5797 - val_accuracy: 0.8306\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4669 - accuracy: 0.8617 - val_loss: 0.5969 - val_accuracy: 0.8274\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4672 - accuracy: 0.8613 - val_loss: 0.5135 - val_accuracy: 0.8549\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4661 - accuracy: 0.8621 - val_loss: 0.5869 - val_accuracy: 0.8267\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4624 - accuracy: 0.8622 - val_loss: 0.6084 - val_accuracy: 0.8229\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4564 - accuracy: 0.8643 - val_loss: 0.6366 - val_accuracy: 0.8161\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4539 - accuracy: 0.8649 - val_loss: 0.5276 - val_accuracy: 0.8465\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4517 - accuracy: 0.8647 - val_loss: 0.5805 - val_accuracy: 0.8315\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4497 - accuracy: 0.8654 - val_loss: 0.5440 - val_accuracy: 0.8443\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4458 - accuracy: 0.8679 - val_loss: 0.5541 - val_accuracy: 0.8355\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4451 - accuracy: 0.8678 - val_loss: 0.5395 - val_accuracy: 0.8433\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4481 - accuracy: 0.8666 - val_loss: 0.5264 - val_accuracy: 0.8482\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4383 - accuracy: 0.8694 - val_loss: 0.5267 - val_accuracy: 0.8506\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4379 - accuracy: 0.8692 - val_loss: 0.5390 - val_accuracy: 0.8438\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4374 - accuracy: 0.8697 - val_loss: 0.5081 - val_accuracy: 0.8546\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4328 - accuracy: 0.8724 - val_loss: 0.5055 - val_accuracy: 0.8560\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4282 - accuracy: 0.8739 - val_loss: 0.4956 - val_accuracy: 0.8589\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4361 - accuracy: 0.8709 - val_loss: 0.6248 - val_accuracy: 0.8112\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4277 - accuracy: 0.8730 - val_loss: 0.5489 - val_accuracy: 0.8409\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4281 - accuracy: 0.8710 - val_loss: 0.5120 - val_accuracy: 0.8524\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4227 - accuracy: 0.8746 - val_loss: 0.4686 - val_accuracy: 0.8677\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4250 - accuracy: 0.8725 - val_loss: 0.5025 - val_accuracy: 0.8579\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4192 - accuracy: 0.8743 - val_loss: 0.5139 - val_accuracy: 0.8527\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4148 - accuracy: 0.8761 - val_loss: 0.5136 - val_accuracy: 0.8533\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4122 - accuracy: 0.8780 - val_loss: 0.4797 - val_accuracy: 0.8660\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4163 - accuracy: 0.8748 - val_loss: 0.5349 - val_accuracy: 0.8451\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4083 - accuracy: 0.8793 - val_loss: 0.5357 - val_accuracy: 0.8473\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4077 - accuracy: 0.8795 - val_loss: 0.5575 - val_accuracy: 0.8405\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4056 - accuracy: 0.8802 - val_loss: 0.5458 - val_accuracy: 0.8415\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4005 - accuracy: 0.8809 - val_loss: 0.5117 - val_accuracy: 0.8544\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4055 - accuracy: 0.8792 - val_loss: 0.5086 - val_accuracy: 0.8546\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4009 - accuracy: 0.8807 - val_loss: 0.4602 - val_accuracy: 0.8689\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3957 - accuracy: 0.8824 - val_loss: 0.4994 - val_accuracy: 0.8579\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3921 - accuracy: 0.8842 - val_loss: 0.4553 - val_accuracy: 0.8723\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3920 - accuracy: 0.8833 - val_loss: 0.4645 - val_accuracy: 0.8674\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3882 - accuracy: 0.8852 - val_loss: 0.5243 - val_accuracy: 0.8504\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3910 - accuracy: 0.8843 - val_loss: 0.5273 - val_accuracy: 0.8438\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3884 - accuracy: 0.8852 - val_loss: 0.4793 - val_accuracy: 0.8639\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3859 - accuracy: 0.8845 - val_loss: 0.4672 - val_accuracy: 0.8659\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3781 - accuracy: 0.8897 - val_loss: 0.5185 - val_accuracy: 0.8491\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3812 - accuracy: 0.8874 - val_loss: 0.5626 - val_accuracy: 0.8381\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3811 - accuracy: 0.8866 - val_loss: 0.4890 - val_accuracy: 0.8649\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3793 - accuracy: 0.8882 - val_loss: 0.4878 - val_accuracy: 0.8627\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3721 - accuracy: 0.8904 - val_loss: 0.5066 - val_accuracy: 0.8529\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3726 - accuracy: 0.8898 - val_loss: 0.5183 - val_accuracy: 0.8522\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3704 - accuracy: 0.8902 - val_loss: 0.4507 - val_accuracy: 0.8741\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3681 - accuracy: 0.8918 - val_loss: 0.4565 - val_accuracy: 0.8722\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3686 - accuracy: 0.8917 - val_loss: 0.4260 - val_accuracy: 0.8801\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3642 - accuracy: 0.8935 - val_loss: 0.4837 - val_accuracy: 0.8608\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3626 - accuracy: 0.8945 - val_loss: 0.4894 - val_accuracy: 0.8611\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3626 - accuracy: 0.8923 - val_loss: 0.4540 - val_accuracy: 0.8714\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3626 - accuracy: 0.8930 - val_loss: 0.4623 - val_accuracy: 0.8728\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3571 - accuracy: 0.8951 - val_loss: 0.4221 - val_accuracy: 0.8842\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3551 - accuracy: 0.8958 - val_loss: 0.4473 - val_accuracy: 0.8746\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3555 - accuracy: 0.8955 - val_loss: 0.4567 - val_accuracy: 0.8724\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3565 - accuracy: 0.8939 - val_loss: 0.4666 - val_accuracy: 0.8651\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3516 - accuracy: 0.8959 - val_loss: 0.4585 - val_accuracy: 0.8709\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3500 - accuracy: 0.8978 - val_loss: 0.4467 - val_accuracy: 0.8759\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3474 - accuracy: 0.8972 - val_loss: 0.4335 - val_accuracy: 0.8800\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3446 - accuracy: 0.8989 - val_loss: 0.4446 - val_accuracy: 0.8733\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3403 - accuracy: 0.9002 - val_loss: 0.4651 - val_accuracy: 0.8708\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3429 - accuracy: 0.8977 - val_loss: 0.5142 - val_accuracy: 0.8534\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3421 - accuracy: 0.8982 - val_loss: 0.4913 - val_accuracy: 0.8634\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3389 - accuracy: 0.9020 - val_loss: 0.4742 - val_accuracy: 0.8646\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3380 - accuracy: 0.9003 - val_loss: 0.3984 - val_accuracy: 0.8911\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3373 - accuracy: 0.9005 - val_loss: 0.4403 - val_accuracy: 0.8787\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3338 - accuracy: 0.9025 - val_loss: 0.4336 - val_accuracy: 0.8796\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3355 - accuracy: 0.9009 - val_loss: 0.4156 - val_accuracy: 0.8876\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3308 - accuracy: 0.9032 - val_loss: 0.4213 - val_accuracy: 0.8845\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3250 - accuracy: 0.9054 - val_loss: 0.4514 - val_accuracy: 0.8758\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3294 - accuracy: 0.9025 - val_loss: 0.4342 - val_accuracy: 0.8806\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3252 - accuracy: 0.9047 - val_loss: 0.4487 - val_accuracy: 0.8751\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3262 - accuracy: 0.9042 - val_loss: 0.4396 - val_accuracy: 0.8777\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3224 - accuracy: 0.9050 - val_loss: 0.4197 - val_accuracy: 0.8848\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3231 - accuracy: 0.9047 - val_loss: 0.4430 - val_accuracy: 0.8743\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3151 - accuracy: 0.9088 - val_loss: 0.4729 - val_accuracy: 0.8659\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3201 - accuracy: 0.9057 - val_loss: 0.4496 - val_accuracy: 0.8739\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3191 - accuracy: 0.9073 - val_loss: 0.4344 - val_accuracy: 0.8796\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3102 - accuracy: 0.9101 - val_loss: 0.4795 - val_accuracy: 0.8642\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3190 - accuracy: 0.9062 - val_loss: 0.4720 - val_accuracy: 0.8646\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3115 - accuracy: 0.9093 - val_loss: 0.5142 - val_accuracy: 0.8566\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3123 - accuracy: 0.9085 - val_loss: 0.4433 - val_accuracy: 0.8783\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3112 - accuracy: 0.9083 - val_loss: 0.4479 - val_accuracy: 0.8724\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3052 - accuracy: 0.9120 - val_loss: 0.4671 - val_accuracy: 0.8699\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3040 - accuracy: 0.9110 - val_loss: 0.4728 - val_accuracy: 0.8669\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3009 - accuracy: 0.9114 - val_loss: 0.3794 - val_accuracy: 0.8999\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3036 - accuracy: 0.9126 - val_loss: 0.4533 - val_accuracy: 0.8721\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3027 - accuracy: 0.9111 - val_loss: 0.4538 - val_accuracy: 0.8746\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3013 - accuracy: 0.9115 - val_loss: 0.4033 - val_accuracy: 0.8857\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2988 - accuracy: 0.9130 - val_loss: 0.4476 - val_accuracy: 0.8746\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3006 - accuracy: 0.9125 - val_loss: 0.4050 - val_accuracy: 0.8894\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2930 - accuracy: 0.9152 - val_loss: 0.4028 - val_accuracy: 0.8891\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2961 - accuracy: 0.9136 - val_loss: 0.4392 - val_accuracy: 0.8781\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2961 - accuracy: 0.9125 - val_loss: 0.4192 - val_accuracy: 0.8873\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2920 - accuracy: 0.9157 - val_loss: 0.3909 - val_accuracy: 0.8959\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2896 - accuracy: 0.9151 - val_loss: 0.3767 - val_accuracy: 0.8984\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2879 - accuracy: 0.9151 - val_loss: 0.4599 - val_accuracy: 0.8721\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2855 - accuracy: 0.9173 - val_loss: 0.4465 - val_accuracy: 0.8782\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2873 - accuracy: 0.9167 - val_loss: 0.4529 - val_accuracy: 0.8742\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2913 - accuracy: 0.9126 - val_loss: 0.3847 - val_accuracy: 0.8949\n",
      "Try 8/100: Best_val_acc: [0.6680024266242981, 0.829277753829956], lr: 5.8583004298583893e-05, Lambda: 6.189601897580547e-05\n",
      "\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 699,690\n",
      "Trainable params: 699,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 2.2875 - accuracy: 0.1438 - val_loss: 2.3000 - val_accuracy: 0.1020\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1913 - accuracy: 0.2590 - val_loss: 2.2469 - val_accuracy: 0.1655\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.0429 - accuracy: 0.3538 - val_loss: 2.0435 - val_accuracy: 0.3159\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.8621 - accuracy: 0.4303 - val_loss: 1.8767 - val_accuracy: 0.4132\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.6912 - accuracy: 0.4960 - val_loss: 1.7318 - val_accuracy: 0.4647\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.5545 - accuracy: 0.5401 - val_loss: 1.5259 - val_accuracy: 0.5696\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4527 - accuracy: 0.5702 - val_loss: 1.4590 - val_accuracy: 0.5844\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3754 - accuracy: 0.5917 - val_loss: 1.3623 - val_accuracy: 0.6211\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.3162 - accuracy: 0.6088 - val_loss: 1.3272 - val_accuracy: 0.6192\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2615 - accuracy: 0.6259 - val_loss: 1.2518 - val_accuracy: 0.6371\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2206 - accuracy: 0.6391 - val_loss: 1.2584 - val_accuracy: 0.6380\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1877 - accuracy: 0.6458 - val_loss: 1.2378 - val_accuracy: 0.6463\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1568 - accuracy: 0.6558 - val_loss: 1.1814 - val_accuracy: 0.6511\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1302 - accuracy: 0.6635 - val_loss: 1.1597 - val_accuracy: 0.6622\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1077 - accuracy: 0.6682 - val_loss: 1.1263 - val_accuracy: 0.6796\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0816 - accuracy: 0.6770 - val_loss: 1.1336 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0585 - accuracy: 0.6825 - val_loss: 1.0403 - val_accuracy: 0.7041\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0411 - accuracy: 0.6886 - val_loss: 1.0826 - val_accuracy: 0.6866\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0230 - accuracy: 0.6937 - val_loss: 1.0380 - val_accuracy: 0.6959\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0028 - accuracy: 0.7001 - val_loss: 1.0589 - val_accuracy: 0.6961\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9867 - accuracy: 0.7043 - val_loss: 1.0642 - val_accuracy: 0.6826\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9751 - accuracy: 0.7085 - val_loss: 1.0305 - val_accuracy: 0.7007\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9590 - accuracy: 0.7124 - val_loss: 0.9564 - val_accuracy: 0.7294\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9457 - accuracy: 0.7175 - val_loss: 1.0030 - val_accuracy: 0.7113\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9309 - accuracy: 0.7216 - val_loss: 0.9688 - val_accuracy: 0.7159\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9217 - accuracy: 0.7235 - val_loss: 0.9653 - val_accuracy: 0.7229\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9094 - accuracy: 0.7268 - val_loss: 0.9347 - val_accuracy: 0.7278\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8977 - accuracy: 0.7290 - val_loss: 0.9172 - val_accuracy: 0.7271\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8809 - accuracy: 0.7365 - val_loss: 0.9779 - val_accuracy: 0.7161\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8756 - accuracy: 0.7377 - val_loss: 0.9854 - val_accuracy: 0.7127\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8694 - accuracy: 0.7397 - val_loss: 0.8760 - val_accuracy: 0.7454\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8547 - accuracy: 0.7442 - val_loss: 0.8138 - val_accuracy: 0.7656\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8455 - accuracy: 0.7481 - val_loss: 0.9339 - val_accuracy: 0.7251\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8381 - accuracy: 0.7470 - val_loss: 0.8732 - val_accuracy: 0.7456\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8270 - accuracy: 0.7527 - val_loss: 0.8502 - val_accuracy: 0.7579\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8221 - accuracy: 0.7529 - val_loss: 0.9263 - val_accuracy: 0.7236\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8142 - accuracy: 0.7564 - val_loss: 0.8367 - val_accuracy: 0.7541\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8056 - accuracy: 0.7584 - val_loss: 0.8787 - val_accuracy: 0.7395\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8022 - accuracy: 0.7586 - val_loss: 0.8283 - val_accuracy: 0.7553\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7883 - accuracy: 0.7641 - val_loss: 0.8201 - val_accuracy: 0.7611\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7870 - accuracy: 0.7644 - val_loss: 0.8338 - val_accuracy: 0.7574\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7705 - accuracy: 0.7700 - val_loss: 0.7894 - val_accuracy: 0.7725\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7730 - accuracy: 0.7674 - val_loss: 0.8101 - val_accuracy: 0.7522\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7656 - accuracy: 0.7705 - val_loss: 0.7340 - val_accuracy: 0.7896\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7526 - accuracy: 0.7742 - val_loss: 0.7786 - val_accuracy: 0.7801\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7505 - accuracy: 0.7752 - val_loss: 0.8024 - val_accuracy: 0.7644\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7451 - accuracy: 0.7770 - val_loss: 0.7745 - val_accuracy: 0.7688\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7355 - accuracy: 0.7805 - val_loss: 0.7966 - val_accuracy: 0.7703\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7296 - accuracy: 0.7822 - val_loss: 0.7683 - val_accuracy: 0.7761\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7259 - accuracy: 0.7827 - val_loss: 0.7280 - val_accuracy: 0.7902\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7219 - accuracy: 0.7843 - val_loss: 0.8182 - val_accuracy: 0.7557\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7155 - accuracy: 0.7872 - val_loss: 0.7738 - val_accuracy: 0.7795\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7070 - accuracy: 0.7894 - val_loss: 0.7489 - val_accuracy: 0.7811\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7040 - accuracy: 0.7896 - val_loss: 0.7240 - val_accuracy: 0.7925\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6994 - accuracy: 0.7905 - val_loss: 0.6805 - val_accuracy: 0.8039\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6947 - accuracy: 0.7926 - val_loss: 0.7638 - val_accuracy: 0.7724\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6913 - accuracy: 0.7940 - val_loss: 0.7542 - val_accuracy: 0.7864\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6832 - accuracy: 0.7960 - val_loss: 0.7353 - val_accuracy: 0.7874\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6789 - accuracy: 0.7962 - val_loss: 0.7438 - val_accuracy: 0.7854\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6757 - accuracy: 0.7981 - val_loss: 0.7456 - val_accuracy: 0.7772\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6664 - accuracy: 0.8014 - val_loss: 0.7125 - val_accuracy: 0.7959\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6625 - accuracy: 0.8035 - val_loss: 0.7414 - val_accuracy: 0.7842\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6618 - accuracy: 0.8018 - val_loss: 0.6580 - val_accuracy: 0.8128\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6530 - accuracy: 0.8050 - val_loss: 0.7094 - val_accuracy: 0.7975\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6514 - accuracy: 0.8060 - val_loss: 0.7528 - val_accuracy: 0.7818\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6508 - accuracy: 0.8061 - val_loss: 0.6887 - val_accuracy: 0.8029\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6419 - accuracy: 0.8087 - val_loss: 0.6816 - val_accuracy: 0.8033\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6390 - accuracy: 0.8085 - val_loss: 0.6634 - val_accuracy: 0.8115\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6396 - accuracy: 0.8067 - val_loss: 0.7464 - val_accuracy: 0.7836\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6309 - accuracy: 0.8124 - val_loss: 0.6947 - val_accuracy: 0.8009\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6257 - accuracy: 0.8139 - val_loss: 0.6400 - val_accuracy: 0.8139\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6198 - accuracy: 0.8163 - val_loss: 0.6870 - val_accuracy: 0.7990\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6313 - accuracy: 0.8114 - val_loss: 0.6837 - val_accuracy: 0.7967\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6141 - accuracy: 0.8170 - val_loss: 0.6844 - val_accuracy: 0.8011\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6078 - accuracy: 0.8173 - val_loss: 0.6781 - val_accuracy: 0.7997\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6096 - accuracy: 0.8182 - val_loss: 0.6620 - val_accuracy: 0.8050\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6049 - accuracy: 0.8208 - val_loss: 0.6746 - val_accuracy: 0.8050\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6005 - accuracy: 0.8206 - val_loss: 0.6580 - val_accuracy: 0.8116\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5957 - accuracy: 0.8221 - val_loss: 0.6158 - val_accuracy: 0.8230\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5979 - accuracy: 0.8218 - val_loss: 0.6672 - val_accuracy: 0.8066\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5896 - accuracy: 0.8234 - val_loss: 0.6589 - val_accuracy: 0.8104\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5866 - accuracy: 0.8242 - val_loss: 0.6136 - val_accuracy: 0.8259\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5895 - accuracy: 0.8226 - val_loss: 0.6710 - val_accuracy: 0.8039\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5812 - accuracy: 0.8264 - val_loss: 0.6047 - val_accuracy: 0.8251\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5785 - accuracy: 0.8275 - val_loss: 0.6478 - val_accuracy: 0.8012\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5724 - accuracy: 0.8286 - val_loss: 0.5958 - val_accuracy: 0.8306\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5685 - accuracy: 0.8302 - val_loss: 0.6209 - val_accuracy: 0.8190\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5706 - accuracy: 0.8290 - val_loss: 0.6288 - val_accuracy: 0.8228\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5659 - accuracy: 0.8299 - val_loss: 0.6348 - val_accuracy: 0.8185\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5610 - accuracy: 0.8326 - val_loss: 0.6353 - val_accuracy: 0.8171\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5582 - accuracy: 0.8335 - val_loss: 0.6082 - val_accuracy: 0.8179\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5572 - accuracy: 0.8325 - val_loss: 0.5997 - val_accuracy: 0.8286\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5493 - accuracy: 0.8365 - val_loss: 0.6144 - val_accuracy: 0.8229\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5485 - accuracy: 0.8352 - val_loss: 0.5876 - val_accuracy: 0.8328\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5506 - accuracy: 0.8355 - val_loss: 0.5860 - val_accuracy: 0.8336\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5441 - accuracy: 0.8367 - val_loss: 0.6431 - val_accuracy: 0.8156\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5486 - accuracy: 0.8364 - val_loss: 0.5859 - val_accuracy: 0.8321\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5376 - accuracy: 0.8397 - val_loss: 0.6258 - val_accuracy: 0.8174\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5339 - accuracy: 0.8393 - val_loss: 0.5764 - val_accuracy: 0.8314\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5314 - accuracy: 0.8395 - val_loss: 0.6311 - val_accuracy: 0.8104\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5275 - accuracy: 0.8430 - val_loss: 0.6227 - val_accuracy: 0.8257\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5236 - accuracy: 0.8425 - val_loss: 0.5808 - val_accuracy: 0.8316\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5249 - accuracy: 0.8418 - val_loss: 0.6106 - val_accuracy: 0.8245\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5223 - accuracy: 0.8438 - val_loss: 0.6015 - val_accuracy: 0.8285\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5178 - accuracy: 0.8449 - val_loss: 0.6269 - val_accuracy: 0.8171\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5188 - accuracy: 0.8446 - val_loss: 0.6575 - val_accuracy: 0.8076\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5195 - accuracy: 0.8440 - val_loss: 0.5929 - val_accuracy: 0.8310\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5107 - accuracy: 0.8469 - val_loss: 0.5900 - val_accuracy: 0.8306\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5149 - accuracy: 0.8460 - val_loss: 0.5896 - val_accuracy: 0.8299\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5060 - accuracy: 0.8469 - val_loss: 0.5827 - val_accuracy: 0.8346\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5008 - accuracy: 0.8497 - val_loss: 0.5567 - val_accuracy: 0.8376\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5025 - accuracy: 0.8484 - val_loss: 0.5663 - val_accuracy: 0.8344\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4958 - accuracy: 0.8504 - val_loss: 0.5484 - val_accuracy: 0.8409\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4969 - accuracy: 0.8497 - val_loss: 0.5693 - val_accuracy: 0.8367\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4962 - accuracy: 0.8500 - val_loss: 0.5815 - val_accuracy: 0.8311\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4929 - accuracy: 0.8524 - val_loss: 0.5683 - val_accuracy: 0.8360\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4901 - accuracy: 0.8531 - val_loss: 0.5631 - val_accuracy: 0.8379\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4869 - accuracy: 0.8539 - val_loss: 0.5527 - val_accuracy: 0.8401\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4900 - accuracy: 0.8534 - val_loss: 0.5867 - val_accuracy: 0.8324\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4868 - accuracy: 0.8540 - val_loss: 0.5609 - val_accuracy: 0.8405\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4806 - accuracy: 0.8557 - val_loss: 0.5304 - val_accuracy: 0.8487\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4775 - accuracy: 0.8569 - val_loss: 0.5396 - val_accuracy: 0.8451\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4738 - accuracy: 0.8584 - val_loss: 0.5453 - val_accuracy: 0.8399\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4710 - accuracy: 0.8594 - val_loss: 0.5306 - val_accuracy: 0.8426\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4689 - accuracy: 0.8588 - val_loss: 0.5506 - val_accuracy: 0.8409\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4660 - accuracy: 0.8604 - val_loss: 0.5522 - val_accuracy: 0.8388\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4688 - accuracy: 0.8585 - val_loss: 0.5501 - val_accuracy: 0.8426\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4598 - accuracy: 0.8637 - val_loss: 0.5429 - val_accuracy: 0.8441\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4621 - accuracy: 0.8612 - val_loss: 0.5263 - val_accuracy: 0.8512\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4633 - accuracy: 0.8602 - val_loss: 0.5091 - val_accuracy: 0.8536\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4551 - accuracy: 0.8640 - val_loss: 0.5311 - val_accuracy: 0.8503\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4569 - accuracy: 0.8623 - val_loss: 0.5472 - val_accuracy: 0.8371\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4553 - accuracy: 0.8630 - val_loss: 0.5185 - val_accuracy: 0.8525\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4583 - accuracy: 0.8618 - val_loss: 0.5106 - val_accuracy: 0.8533\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4512 - accuracy: 0.8658 - val_loss: 0.5790 - val_accuracy: 0.8314\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4495 - accuracy: 0.8652 - val_loss: 0.5079 - val_accuracy: 0.8509\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4526 - accuracy: 0.8625 - val_loss: 0.6044 - val_accuracy: 0.8269\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4510 - accuracy: 0.8647 - val_loss: 0.5214 - val_accuracy: 0.8494\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4403 - accuracy: 0.8674 - val_loss: 0.5356 - val_accuracy: 0.8469\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4417 - accuracy: 0.8675 - val_loss: 0.5024 - val_accuracy: 0.8576\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4419 - accuracy: 0.8675 - val_loss: 0.5613 - val_accuracy: 0.8426\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4382 - accuracy: 0.8687 - val_loss: 0.5549 - val_accuracy: 0.8386\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4359 - accuracy: 0.8699 - val_loss: 0.5068 - val_accuracy: 0.8542\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4291 - accuracy: 0.8710 - val_loss: 0.4988 - val_accuracy: 0.8589\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4280 - accuracy: 0.8731 - val_loss: 0.5798 - val_accuracy: 0.8289\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4309 - accuracy: 0.8709 - val_loss: 0.4878 - val_accuracy: 0.8596\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4239 - accuracy: 0.8727 - val_loss: 0.4972 - val_accuracy: 0.8593\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4236 - accuracy: 0.8731 - val_loss: 0.5348 - val_accuracy: 0.8454\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4270 - accuracy: 0.8726 - val_loss: 0.5007 - val_accuracy: 0.8568\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4204 - accuracy: 0.8745 - val_loss: 0.4670 - val_accuracy: 0.8668\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4192 - accuracy: 0.8734 - val_loss: 0.4945 - val_accuracy: 0.8615\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4199 - accuracy: 0.8744 - val_loss: 0.5650 - val_accuracy: 0.8365\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4143 - accuracy: 0.8766 - val_loss: 0.5461 - val_accuracy: 0.8448\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4169 - accuracy: 0.8762 - val_loss: 0.5271 - val_accuracy: 0.8485\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4100 - accuracy: 0.8775 - val_loss: 0.5178 - val_accuracy: 0.8523\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4147 - accuracy: 0.8765 - val_loss: 0.5037 - val_accuracy: 0.8563\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4145 - accuracy: 0.8750 - val_loss: 0.5095 - val_accuracy: 0.8544\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4068 - accuracy: 0.8786 - val_loss: 0.4636 - val_accuracy: 0.8693\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4029 - accuracy: 0.8799 - val_loss: 0.4987 - val_accuracy: 0.8554\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4025 - accuracy: 0.8797 - val_loss: 0.4925 - val_accuracy: 0.8599\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3972 - accuracy: 0.8813 - val_loss: 0.5177 - val_accuracy: 0.8531\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3982 - accuracy: 0.8802 - val_loss: 0.5317 - val_accuracy: 0.8458\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4049 - accuracy: 0.8783 - val_loss: 0.5144 - val_accuracy: 0.8545\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3941 - accuracy: 0.8837 - val_loss: 0.5109 - val_accuracy: 0.8543\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3982 - accuracy: 0.8799 - val_loss: 0.5127 - val_accuracy: 0.8554\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3905 - accuracy: 0.8833 - val_loss: 0.4894 - val_accuracy: 0.8586\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3948 - accuracy: 0.8833 - val_loss: 0.5267 - val_accuracy: 0.8506\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3904 - accuracy: 0.8824 - val_loss: 0.5161 - val_accuracy: 0.8566\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3822 - accuracy: 0.8861 - val_loss: 0.4991 - val_accuracy: 0.8606\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3894 - accuracy: 0.8836 - val_loss: 0.5027 - val_accuracy: 0.8571\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3898 - accuracy: 0.8837 - val_loss: 0.4620 - val_accuracy: 0.8704\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3869 - accuracy: 0.8834 - val_loss: 0.4674 - val_accuracy: 0.8691\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3846 - accuracy: 0.8857 - val_loss: 0.4962 - val_accuracy: 0.8587\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3777 - accuracy: 0.8874 - val_loss: 0.4388 - val_accuracy: 0.8763\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3774 - accuracy: 0.8869 - val_loss: 0.4809 - val_accuracy: 0.8641\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3796 - accuracy: 0.8865 - val_loss: 0.4633 - val_accuracy: 0.8641\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3717 - accuracy: 0.8896 - val_loss: 0.4542 - val_accuracy: 0.8693\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3819 - accuracy: 0.8861 - val_loss: 0.4631 - val_accuracy: 0.8702\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3804 - accuracy: 0.8856 - val_loss: 0.4521 - val_accuracy: 0.8737\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3690 - accuracy: 0.8920 - val_loss: 0.4902 - val_accuracy: 0.8621\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3645 - accuracy: 0.8917 - val_loss: 0.4688 - val_accuracy: 0.8664\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3657 - accuracy: 0.8906 - val_loss: 0.4809 - val_accuracy: 0.8650\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3663 - accuracy: 0.8922 - val_loss: 0.4936 - val_accuracy: 0.8624\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3618 - accuracy: 0.8930 - val_loss: 0.4512 - val_accuracy: 0.8754\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3642 - accuracy: 0.8913 - val_loss: 0.4566 - val_accuracy: 0.8716\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3656 - accuracy: 0.8912 - val_loss: 0.4871 - val_accuracy: 0.8633\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3598 - accuracy: 0.8922 - val_loss: 0.4691 - val_accuracy: 0.8704\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3652 - accuracy: 0.8915 - val_loss: 0.4887 - val_accuracy: 0.8551\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3521 - accuracy: 0.8952 - val_loss: 0.4872 - val_accuracy: 0.8659\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3552 - accuracy: 0.8951 - val_loss: 0.4680 - val_accuracy: 0.8668\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3526 - accuracy: 0.8935 - val_loss: 0.4642 - val_accuracy: 0.8687\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3513 - accuracy: 0.8940 - val_loss: 0.4288 - val_accuracy: 0.8829\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3453 - accuracy: 0.8984 - val_loss: 0.4703 - val_accuracy: 0.8691\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3482 - accuracy: 0.8973 - val_loss: 0.4565 - val_accuracy: 0.8718\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3434 - accuracy: 0.8986 - val_loss: 0.4624 - val_accuracy: 0.8714\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3494 - accuracy: 0.8963 - val_loss: 0.4638 - val_accuracy: 0.8707\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3442 - accuracy: 0.8988 - val_loss: 0.4607 - val_accuracy: 0.8724\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3514 - accuracy: 0.8953 - val_loss: 0.4571 - val_accuracy: 0.8712\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3406 - accuracy: 0.8983 - val_loss: 0.4561 - val_accuracy: 0.8718\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3428 - accuracy: 0.8990 - val_loss: 0.4561 - val_accuracy: 0.8716\n",
      "Try 9/100: Best_val_acc: [0.6446195840835571, 0.8258333206176758], lr: 5.6686098638600244e-05, Lambda: 5.7145028093666285e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-4.25, -4.2))\n",
    "    Lambda = math.pow(10, np.random.uniform(-4.25,-4.2))\n",
    "    best_acc = basicHPCheckFCNN(200, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIsn9PGd9IsO"
   },
   "source": [
    "#### After some lot of testing, i found the learning rate around 6.14e-5 is good and a lambda of around 5.843-5 is good. For me optimal learning rate i found is 6.137809361954345e-05 and optimal Lambda is 5.8364246649734e-05 with a model which generalizes very well , where training and test accuracy is almost similar and should not be more than 7% apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxXZ-Mr-IEel"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSvNVlEVIEcM"
   },
   "source": [
    "## Adding Batch Normalization & Dropouts now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8He23ZAJZ70a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def basicDeepNN1(iterations, lr, Lambda, activation, k_initial, verb=True):\n",
    "    ## hyperparameters\n",
    "    epochs = iterations\n",
    "    learning_rate = lr\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape = (X_train.shape[1], ), kernel_initializer=k_initial, name='Input'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, kernel_initializer=k_initial, name='Hidden_Layer_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, kernel_initializer=k_initial, name='Hidden_Layer_2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, kernel_initializer=k_initial, name='Hidden_Layer_3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, kernel_initializer=k_initial, name='Hidden_Layer_4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer=k_initial ,kernel_regularizer=regularizers.l2(Lambda), name='Output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #opt = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9)\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]),\n",
    "              epochs=iterations, batch_size=500, verbose= 1)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1886075,
     "status": "ok",
     "timestamp": 1594541815590,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "GTP0D-9gw1pD",
    "outputId": "ee2b5ed1-381a-4631-ee44-efc02369de40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6703 - accuracy: 0.1085 - val_loss: 2.3351 - val_accuracy: 0.0513\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5402 - accuracy: 0.1279 - val_loss: 2.2964 - val_accuracy: 0.1093\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4409 - accuracy: 0.1486 - val_loss: 2.2160 - val_accuracy: 0.1709\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3529 - accuracy: 0.1717 - val_loss: 2.1688 - val_accuracy: 0.2225\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2660 - accuracy: 0.2007 - val_loss: 2.2306 - val_accuracy: 0.1530\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1770 - accuracy: 0.2295 - val_loss: 2.0919 - val_accuracy: 0.2346\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0986 - accuracy: 0.2579 - val_loss: 2.0399 - val_accuracy: 0.2601\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0165 - accuracy: 0.2875 - val_loss: 1.9535 - val_accuracy: 0.3126\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9411 - accuracy: 0.3217 - val_loss: 1.8293 - val_accuracy: 0.3986\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8726 - accuracy: 0.3470 - val_loss: 1.7967 - val_accuracy: 0.3643\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8033 - accuracy: 0.3797 - val_loss: 1.7038 - val_accuracy: 0.4365\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7462 - accuracy: 0.4041 - val_loss: 1.5733 - val_accuracy: 0.5134\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6876 - accuracy: 0.4268 - val_loss: 1.5269 - val_accuracy: 0.5565\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6341 - accuracy: 0.4555 - val_loss: 1.4815 - val_accuracy: 0.5571\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5774 - accuracy: 0.4747 - val_loss: 1.4389 - val_accuracy: 0.5756\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5337 - accuracy: 0.4931 - val_loss: 1.3211 - val_accuracy: 0.6264\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4887 - accuracy: 0.5126 - val_loss: 1.3097 - val_accuracy: 0.6169\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4476 - accuracy: 0.5311 - val_loss: 1.2861 - val_accuracy: 0.6325\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4086 - accuracy: 0.5434 - val_loss: 1.1966 - val_accuracy: 0.6475\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3645 - accuracy: 0.5652 - val_loss: 1.2034 - val_accuracy: 0.6466\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3298 - accuracy: 0.5735 - val_loss: 1.1732 - val_accuracy: 0.6659\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2959 - accuracy: 0.5901 - val_loss: 1.0887 - val_accuracy: 0.6914\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2694 - accuracy: 0.6008 - val_loss: 1.1217 - val_accuracy: 0.6693\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2356 - accuracy: 0.6136 - val_loss: 1.0369 - val_accuracy: 0.6916\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1985 - accuracy: 0.6276 - val_loss: 1.0269 - val_accuracy: 0.7079\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1800 - accuracy: 0.6340 - val_loss: 1.0060 - val_accuracy: 0.7025\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1486 - accuracy: 0.6456 - val_loss: 0.9772 - val_accuracy: 0.7210\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1265 - accuracy: 0.6524 - val_loss: 0.9084 - val_accuracy: 0.7432\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1008 - accuracy: 0.6626 - val_loss: 0.8695 - val_accuracy: 0.7463\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0800 - accuracy: 0.6672 - val_loss: 0.8886 - val_accuracy: 0.7443\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0584 - accuracy: 0.6748 - val_loss: 0.8656 - val_accuracy: 0.7582\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0376 - accuracy: 0.6839 - val_loss: 0.8145 - val_accuracy: 0.7742\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0238 - accuracy: 0.6854 - val_loss: 0.7796 - val_accuracy: 0.7759\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0035 - accuracy: 0.6961 - val_loss: 0.8487 - val_accuracy: 0.7456\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9890 - accuracy: 0.6968 - val_loss: 0.8341 - val_accuracy: 0.7488\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9692 - accuracy: 0.7047 - val_loss: 0.7750 - val_accuracy: 0.7784\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9508 - accuracy: 0.7118 - val_loss: 0.7841 - val_accuracy: 0.7576\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9416 - accuracy: 0.7148 - val_loss: 0.8961 - val_accuracy: 0.7273\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9194 - accuracy: 0.7218 - val_loss: 0.7623 - val_accuracy: 0.7740\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9090 - accuracy: 0.7256 - val_loss: 0.7159 - val_accuracy: 0.7901\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8951 - accuracy: 0.7305 - val_loss: 0.7158 - val_accuracy: 0.7904\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8905 - accuracy: 0.7298 - val_loss: 0.6867 - val_accuracy: 0.7994\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8761 - accuracy: 0.7344 - val_loss: 0.7984 - val_accuracy: 0.7489\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8667 - accuracy: 0.7387 - val_loss: 0.7087 - val_accuracy: 0.7907\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8519 - accuracy: 0.7443 - val_loss: 0.7082 - val_accuracy: 0.7861\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8402 - accuracy: 0.7460 - val_loss: 0.8124 - val_accuracy: 0.7486\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8323 - accuracy: 0.7489 - val_loss: 0.6838 - val_accuracy: 0.7947\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8202 - accuracy: 0.7520 - val_loss: 0.7662 - val_accuracy: 0.7617\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8087 - accuracy: 0.7549 - val_loss: 0.6735 - val_accuracy: 0.8061\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8015 - accuracy: 0.7601 - val_loss: 0.8082 - val_accuracy: 0.7444\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7957 - accuracy: 0.7620 - val_loss: 0.6420 - val_accuracy: 0.8018\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7881 - accuracy: 0.7630 - val_loss: 0.6774 - val_accuracy: 0.7926\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7742 - accuracy: 0.7680 - val_loss: 0.6448 - val_accuracy: 0.8042\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7683 - accuracy: 0.7694 - val_loss: 0.5818 - val_accuracy: 0.8296\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7587 - accuracy: 0.7720 - val_loss: 0.5705 - val_accuracy: 0.8322\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7577 - accuracy: 0.7732 - val_loss: 0.6409 - val_accuracy: 0.8065\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7487 - accuracy: 0.7744 - val_loss: 0.5998 - val_accuracy: 0.8189\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7408 - accuracy: 0.7778 - val_loss: 0.5953 - val_accuracy: 0.8256\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7340 - accuracy: 0.7804 - val_loss: 0.6037 - val_accuracy: 0.8195\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7280 - accuracy: 0.7829 - val_loss: 0.5811 - val_accuracy: 0.8201\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7230 - accuracy: 0.7829 - val_loss: 0.5252 - val_accuracy: 0.8438\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7100 - accuracy: 0.7873 - val_loss: 0.6418 - val_accuracy: 0.7962\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7109 - accuracy: 0.7884 - val_loss: 0.6074 - val_accuracy: 0.8191\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7067 - accuracy: 0.7895 - val_loss: 0.5711 - val_accuracy: 0.8270\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6978 - accuracy: 0.7907 - val_loss: 0.5267 - val_accuracy: 0.8449\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6966 - accuracy: 0.7956 - val_loss: 0.6385 - val_accuracy: 0.8037\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6847 - accuracy: 0.7952 - val_loss: 0.5460 - val_accuracy: 0.8365\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6821 - accuracy: 0.7974 - val_loss: 0.6368 - val_accuracy: 0.8042\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6770 - accuracy: 0.7995 - val_loss: 0.5668 - val_accuracy: 0.8267\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6745 - accuracy: 0.7999 - val_loss: 0.5454 - val_accuracy: 0.8324\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6638 - accuracy: 0.8044 - val_loss: 0.5869 - val_accuracy: 0.8221\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6542 - accuracy: 0.8065 - val_loss: 0.5792 - val_accuracy: 0.8217\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6641 - accuracy: 0.8028 - val_loss: 0.5374 - val_accuracy: 0.8401\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6509 - accuracy: 0.8056 - val_loss: 0.5672 - val_accuracy: 0.8290\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6393 - accuracy: 0.8111 - val_loss: 0.6530 - val_accuracy: 0.7994\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6443 - accuracy: 0.8105 - val_loss: 0.6209 - val_accuracy: 0.8107\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6396 - accuracy: 0.8110 - val_loss: 0.5337 - val_accuracy: 0.8417\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6333 - accuracy: 0.8112 - val_loss: 0.5705 - val_accuracy: 0.8266\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6267 - accuracy: 0.8148 - val_loss: 0.5232 - val_accuracy: 0.8432\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6284 - accuracy: 0.8126 - val_loss: 0.5234 - val_accuracy: 0.8449\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6189 - accuracy: 0.8169 - val_loss: 0.5145 - val_accuracy: 0.8489\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6211 - accuracy: 0.8163 - val_loss: 0.5365 - val_accuracy: 0.8408\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6174 - accuracy: 0.8180 - val_loss: 0.6458 - val_accuracy: 0.8026\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6094 - accuracy: 0.8188 - val_loss: 0.4848 - val_accuracy: 0.8581\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6063 - accuracy: 0.8198 - val_loss: 0.5291 - val_accuracy: 0.8419\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6083 - accuracy: 0.8227 - val_loss: 0.5270 - val_accuracy: 0.8423\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6017 - accuracy: 0.8238 - val_loss: 0.5073 - val_accuracy: 0.8417\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5916 - accuracy: 0.8245 - val_loss: 0.4666 - val_accuracy: 0.8592\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5890 - accuracy: 0.8270 - val_loss: 0.6124 - val_accuracy: 0.8127\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5919 - accuracy: 0.8256 - val_loss: 0.4433 - val_accuracy: 0.8696\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5789 - accuracy: 0.8289 - val_loss: 0.4705 - val_accuracy: 0.8623\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5791 - accuracy: 0.8297 - val_loss: 0.4346 - val_accuracy: 0.8746\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5798 - accuracy: 0.8295 - val_loss: 0.4717 - val_accuracy: 0.8522\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5818 - accuracy: 0.8285 - val_loss: 0.4415 - val_accuracy: 0.8706\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5731 - accuracy: 0.8300 - val_loss: 0.5065 - val_accuracy: 0.8419\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5674 - accuracy: 0.8319 - val_loss: 0.5140 - val_accuracy: 0.8469\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5727 - accuracy: 0.8316 - val_loss: 0.5431 - val_accuracy: 0.8231\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5660 - accuracy: 0.8341 - val_loss: 0.5017 - val_accuracy: 0.8488\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5652 - accuracy: 0.8353 - val_loss: 0.4852 - val_accuracy: 0.8541\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5524 - accuracy: 0.8383 - val_loss: 0.4520 - val_accuracy: 0.8692\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5541 - accuracy: 0.8391 - val_loss: 0.4555 - val_accuracy: 0.8652\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5490 - accuracy: 0.8395 - val_loss: 0.4513 - val_accuracy: 0.8602\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5415 - accuracy: 0.8414 - val_loss: 0.4671 - val_accuracy: 0.8581\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5486 - accuracy: 0.8381 - val_loss: 0.4190 - val_accuracy: 0.8774\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5400 - accuracy: 0.8416 - val_loss: 0.5196 - val_accuracy: 0.8443\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5391 - accuracy: 0.8400 - val_loss: 0.4481 - val_accuracy: 0.8624\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5415 - accuracy: 0.8420 - val_loss: 0.4823 - val_accuracy: 0.8594\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5357 - accuracy: 0.8434 - val_loss: 0.4414 - val_accuracy: 0.8734\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5256 - accuracy: 0.8448 - val_loss: 0.5173 - val_accuracy: 0.8419\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5267 - accuracy: 0.8449 - val_loss: 0.5511 - val_accuracy: 0.8323\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5235 - accuracy: 0.8454 - val_loss: 0.4108 - val_accuracy: 0.8809\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5237 - accuracy: 0.8461 - val_loss: 0.4414 - val_accuracy: 0.8697\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5223 - accuracy: 0.8458 - val_loss: 0.4473 - val_accuracy: 0.8670\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5242 - accuracy: 0.8446 - val_loss: 0.5162 - val_accuracy: 0.8359\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5178 - accuracy: 0.8478 - val_loss: 0.3851 - val_accuracy: 0.8866\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5177 - accuracy: 0.8485 - val_loss: 0.4494 - val_accuracy: 0.8646\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5184 - accuracy: 0.8480 - val_loss: 0.4569 - val_accuracy: 0.8604\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5117 - accuracy: 0.8499 - val_loss: 0.4168 - val_accuracy: 0.8809\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8537 - val_loss: 0.5025 - val_accuracy: 0.8514\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5085 - accuracy: 0.8515 - val_loss: 0.4928 - val_accuracy: 0.8558\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5075 - accuracy: 0.8514 - val_loss: 0.4266 - val_accuracy: 0.8751\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5023 - accuracy: 0.8534 - val_loss: 0.4914 - val_accuracy: 0.8522\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5002 - accuracy: 0.8531 - val_loss: 0.3523 - val_accuracy: 0.8971\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4974 - accuracy: 0.8556 - val_loss: 0.3942 - val_accuracy: 0.8842\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4954 - accuracy: 0.8529 - val_loss: 0.3737 - val_accuracy: 0.8918\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4922 - accuracy: 0.8568 - val_loss: 0.5421 - val_accuracy: 0.8379\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4893 - accuracy: 0.8578 - val_loss: 0.4555 - val_accuracy: 0.8686\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4886 - accuracy: 0.8560 - val_loss: 0.5991 - val_accuracy: 0.8204\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4859 - accuracy: 0.8562 - val_loss: 0.4481 - val_accuracy: 0.8657\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4867 - accuracy: 0.8587 - val_loss: 0.4881 - val_accuracy: 0.8544\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4864 - accuracy: 0.8581 - val_loss: 0.3963 - val_accuracy: 0.8882\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4892 - accuracy: 0.8565 - val_loss: 0.4426 - val_accuracy: 0.8705\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.8583 - val_loss: 0.3828 - val_accuracy: 0.8926\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4727 - accuracy: 0.8632 - val_loss: 0.4272 - val_accuracy: 0.8744\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4792 - accuracy: 0.8613 - val_loss: 0.5131 - val_accuracy: 0.8459\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4762 - accuracy: 0.8620 - val_loss: 0.4308 - val_accuracy: 0.8736\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4663 - accuracy: 0.8651 - val_loss: 0.4696 - val_accuracy: 0.8611\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4631 - accuracy: 0.8640 - val_loss: 0.3852 - val_accuracy: 0.8868\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4752 - accuracy: 0.8599 - val_loss: 0.3627 - val_accuracy: 0.8938\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4696 - accuracy: 0.8621 - val_loss: 0.3953 - val_accuracy: 0.8826\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4582 - accuracy: 0.8658 - val_loss: 0.3654 - val_accuracy: 0.8952\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4631 - accuracy: 0.8652 - val_loss: 0.4912 - val_accuracy: 0.8511\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4636 - accuracy: 0.8647 - val_loss: 0.4385 - val_accuracy: 0.8710\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4551 - accuracy: 0.8668 - val_loss: 0.4318 - val_accuracy: 0.8699\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4616 - accuracy: 0.8654 - val_loss: 0.4278 - val_accuracy: 0.8734\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4506 - accuracy: 0.8666 - val_loss: 0.4636 - val_accuracy: 0.8652\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4557 - accuracy: 0.8680 - val_loss: 0.3773 - val_accuracy: 0.8926\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4509 - accuracy: 0.8681 - val_loss: 0.4348 - val_accuracy: 0.8748\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4481 - accuracy: 0.8679 - val_loss: 0.4075 - val_accuracy: 0.8789\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4464 - accuracy: 0.8705 - val_loss: 0.4146 - val_accuracy: 0.8771\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4514 - accuracy: 0.8673 - val_loss: 0.3771 - val_accuracy: 0.8915\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4499 - accuracy: 0.8671 - val_loss: 0.3930 - val_accuracy: 0.8827\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4416 - accuracy: 0.8713 - val_loss: 0.3584 - val_accuracy: 0.8963\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4414 - accuracy: 0.8723 - val_loss: 0.3917 - val_accuracy: 0.8860\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4459 - accuracy: 0.8712 - val_loss: 0.5993 - val_accuracy: 0.8198\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4430 - accuracy: 0.8699 - val_loss: 0.4458 - val_accuracy: 0.8675\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4407 - accuracy: 0.8725 - val_loss: 0.4537 - val_accuracy: 0.8678\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4421 - accuracy: 0.8726 - val_loss: 0.4445 - val_accuracy: 0.8641\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8703 - val_loss: 0.4344 - val_accuracy: 0.8760\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4337 - accuracy: 0.8734 - val_loss: 0.4455 - val_accuracy: 0.8687\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4340 - accuracy: 0.8730 - val_loss: 0.3985 - val_accuracy: 0.8841\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4301 - accuracy: 0.8745 - val_loss: 0.4263 - val_accuracy: 0.8722\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4284 - accuracy: 0.8750 - val_loss: 0.3490 - val_accuracy: 0.9012\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4270 - accuracy: 0.8757 - val_loss: 0.4160 - val_accuracy: 0.8785\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4227 - accuracy: 0.8775 - val_loss: 0.3777 - val_accuracy: 0.8909\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4212 - accuracy: 0.8768 - val_loss: 0.4588 - val_accuracy: 0.8651\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4252 - accuracy: 0.8775 - val_loss: 0.3917 - val_accuracy: 0.8854\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4233 - accuracy: 0.8776 - val_loss: 0.4016 - val_accuracy: 0.8846\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4136 - accuracy: 0.8807 - val_loss: 0.3734 - val_accuracy: 0.8904\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4222 - accuracy: 0.8765 - val_loss: 0.4525 - val_accuracy: 0.8701\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4180 - accuracy: 0.8775 - val_loss: 0.4106 - val_accuracy: 0.8729\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4150 - accuracy: 0.8789 - val_loss: 0.4897 - val_accuracy: 0.8548\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8783 - val_loss: 0.3360 - val_accuracy: 0.9047\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4147 - accuracy: 0.8780 - val_loss: 0.4363 - val_accuracy: 0.8699\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4195 - accuracy: 0.8780 - val_loss: 0.4184 - val_accuracy: 0.8739\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4090 - accuracy: 0.8818 - val_loss: 0.4051 - val_accuracy: 0.8821\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4044 - accuracy: 0.8828 - val_loss: 0.5077 - val_accuracy: 0.8481\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4068 - accuracy: 0.8824 - val_loss: 0.4345 - val_accuracy: 0.8755\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4093 - accuracy: 0.8807 - val_loss: 0.4345 - val_accuracy: 0.8746\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4027 - accuracy: 0.8832 - val_loss: 0.3089 - val_accuracy: 0.9133\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4040 - accuracy: 0.8824 - val_loss: 0.4757 - val_accuracy: 0.8622\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4021 - accuracy: 0.8831 - val_loss: 0.4265 - val_accuracy: 0.8729\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3992 - accuracy: 0.8836 - val_loss: 0.3520 - val_accuracy: 0.8999\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4024 - accuracy: 0.8831 - val_loss: 0.4096 - val_accuracy: 0.8805\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4015 - accuracy: 0.8838 - val_loss: 0.4851 - val_accuracy: 0.8620\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3997 - accuracy: 0.8827 - val_loss: 0.3483 - val_accuracy: 0.8987\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3987 - accuracy: 0.8850 - val_loss: 0.3557 - val_accuracy: 0.8987\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3957 - accuracy: 0.8860 - val_loss: 0.4514 - val_accuracy: 0.8661\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8869 - val_loss: 0.3295 - val_accuracy: 0.9073\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3911 - accuracy: 0.8872 - val_loss: 0.4488 - val_accuracy: 0.8652\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3947 - accuracy: 0.8844 - val_loss: 0.3789 - val_accuracy: 0.8894\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8870 - val_loss: 0.3340 - val_accuracy: 0.9041\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3861 - accuracy: 0.8874 - val_loss: 0.4041 - val_accuracy: 0.8809\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3858 - accuracy: 0.8877 - val_loss: 0.4105 - val_accuracy: 0.8819\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3866 - accuracy: 0.8883 - val_loss: 0.5457 - val_accuracy: 0.8406\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8859 - val_loss: 0.4221 - val_accuracy: 0.8829\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3832 - accuracy: 0.8886 - val_loss: 0.3620 - val_accuracy: 0.8960\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3818 - accuracy: 0.8894 - val_loss: 0.3964 - val_accuracy: 0.8860\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8871 - val_loss: 0.3584 - val_accuracy: 0.8989\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8884 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
      "Try 1/100: Best_val_acc: [0.5645294785499573, 0.8372777700424194], lr: 6.125812180195474e-05, Lambda: 5.8056736752022896e-05\n",
      "\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6356 - accuracy: 0.1078 - val_loss: 2.2733 - val_accuracy: 0.1156\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5182 - accuracy: 0.1257 - val_loss: 2.2704 - val_accuracy: 0.1490\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4137 - accuracy: 0.1479 - val_loss: 2.2622 - val_accuracy: 0.1941\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3253 - accuracy: 0.1675 - val_loss: 2.1895 - val_accuracy: 0.2404\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2494 - accuracy: 0.1969 - val_loss: 2.1096 - val_accuracy: 0.2806\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1619 - accuracy: 0.2285 - val_loss: 2.0232 - val_accuracy: 0.2970\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0948 - accuracy: 0.2461 - val_loss: 1.9232 - val_accuracy: 0.3578\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0264 - accuracy: 0.2732 - val_loss: 1.8412 - val_accuracy: 0.4256\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9632 - accuracy: 0.3031 - val_loss: 1.7555 - val_accuracy: 0.4816\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9116 - accuracy: 0.3241 - val_loss: 1.6750 - val_accuracy: 0.5354\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8538 - accuracy: 0.3490 - val_loss: 1.6840 - val_accuracy: 0.5270\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8061 - accuracy: 0.3732 - val_loss: 1.6201 - val_accuracy: 0.5522\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7529 - accuracy: 0.3994 - val_loss: 1.5575 - val_accuracy: 0.5829\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7092 - accuracy: 0.4211 - val_loss: 1.4734 - val_accuracy: 0.6000\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6662 - accuracy: 0.4370 - val_loss: 1.4518 - val_accuracy: 0.6042\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6248 - accuracy: 0.4561 - val_loss: 1.4099 - val_accuracy: 0.6275\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5807 - accuracy: 0.4777 - val_loss: 1.2946 - val_accuracy: 0.6804\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5348 - accuracy: 0.4972 - val_loss: 1.2828 - val_accuracy: 0.6754\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4910 - accuracy: 0.5162 - val_loss: 1.2575 - val_accuracy: 0.6687\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4548 - accuracy: 0.5309 - val_loss: 1.1972 - val_accuracy: 0.7011\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4282 - accuracy: 0.5412 - val_loss: 1.1758 - val_accuracy: 0.7067\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3877 - accuracy: 0.5583 - val_loss: 1.1803 - val_accuracy: 0.6973\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3559 - accuracy: 0.5721 - val_loss: 1.1164 - val_accuracy: 0.6942\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3291 - accuracy: 0.5801 - val_loss: 1.0399 - val_accuracy: 0.7264\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2902 - accuracy: 0.5953 - val_loss: 1.0135 - val_accuracy: 0.7345\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2629 - accuracy: 0.6051 - val_loss: 1.0394 - val_accuracy: 0.7216\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2369 - accuracy: 0.6178 - val_loss: 0.9921 - val_accuracy: 0.7434\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2122 - accuracy: 0.6286 - val_loss: 0.9349 - val_accuracy: 0.7618\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1870 - accuracy: 0.6357 - val_loss: 0.9992 - val_accuracy: 0.7139\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1609 - accuracy: 0.6435 - val_loss: 0.9358 - val_accuracy: 0.7485\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1432 - accuracy: 0.6504 - val_loss: 0.9212 - val_accuracy: 0.7556\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1180 - accuracy: 0.6599 - val_loss: 0.8624 - val_accuracy: 0.7696\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0946 - accuracy: 0.6689 - val_loss: 0.9194 - val_accuracy: 0.7394\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0758 - accuracy: 0.6719 - val_loss: 0.8219 - val_accuracy: 0.7825\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0569 - accuracy: 0.6802 - val_loss: 0.9338 - val_accuracy: 0.7431\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0440 - accuracy: 0.6824 - val_loss: 0.8265 - val_accuracy: 0.7704\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0208 - accuracy: 0.6920 - val_loss: 0.7677 - val_accuracy: 0.7961\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0080 - accuracy: 0.6956 - val_loss: 0.8161 - val_accuracy: 0.7646\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9946 - accuracy: 0.6981 - val_loss: 0.7265 - val_accuracy: 0.7923\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9789 - accuracy: 0.7047 - val_loss: 0.8170 - val_accuracy: 0.7596\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9642 - accuracy: 0.7103 - val_loss: 0.7545 - val_accuracy: 0.7911\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9466 - accuracy: 0.7162 - val_loss: 0.7816 - val_accuracy: 0.7829\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9357 - accuracy: 0.7205 - val_loss: 0.7060 - val_accuracy: 0.8076\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9195 - accuracy: 0.7217 - val_loss: 0.7735 - val_accuracy: 0.7746\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9091 - accuracy: 0.7233 - val_loss: 0.8258 - val_accuracy: 0.7589\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8948 - accuracy: 0.7294 - val_loss: 0.7545 - val_accuracy: 0.7860\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8862 - accuracy: 0.7335 - val_loss: 0.8388 - val_accuracy: 0.7455\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8724 - accuracy: 0.7366 - val_loss: 0.7074 - val_accuracy: 0.7981\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8604 - accuracy: 0.7405 - val_loss: 0.7007 - val_accuracy: 0.7944\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8475 - accuracy: 0.7467 - val_loss: 0.7688 - val_accuracy: 0.7675\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8444 - accuracy: 0.7455 - val_loss: 0.6846 - val_accuracy: 0.8035\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8309 - accuracy: 0.7487 - val_loss: 0.6326 - val_accuracy: 0.8250\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8206 - accuracy: 0.7552 - val_loss: 0.7351 - val_accuracy: 0.7889\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8111 - accuracy: 0.7577 - val_loss: 0.6172 - val_accuracy: 0.8223\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8024 - accuracy: 0.7599 - val_loss: 0.6965 - val_accuracy: 0.7914\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7957 - accuracy: 0.7593 - val_loss: 0.6373 - val_accuracy: 0.8002\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7855 - accuracy: 0.7637 - val_loss: 0.6534 - val_accuracy: 0.8104\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7730 - accuracy: 0.7702 - val_loss: 0.5596 - val_accuracy: 0.8429\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7653 - accuracy: 0.7719 - val_loss: 0.6412 - val_accuracy: 0.8170\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7592 - accuracy: 0.7728 - val_loss: 0.5895 - val_accuracy: 0.8279\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7593 - accuracy: 0.7728 - val_loss: 0.5936 - val_accuracy: 0.8290\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7414 - accuracy: 0.7813 - val_loss: 0.5536 - val_accuracy: 0.8443\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7336 - accuracy: 0.7808 - val_loss: 0.5949 - val_accuracy: 0.8294\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7364 - accuracy: 0.7805 - val_loss: 0.5397 - val_accuracy: 0.8424\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7264 - accuracy: 0.7848 - val_loss: 0.5414 - val_accuracy: 0.8464\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7193 - accuracy: 0.7851 - val_loss: 0.6424 - val_accuracy: 0.8096\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7120 - accuracy: 0.7855 - val_loss: 0.5475 - val_accuracy: 0.8367\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7039 - accuracy: 0.7921 - val_loss: 0.5727 - val_accuracy: 0.8325\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6971 - accuracy: 0.7910 - val_loss: 0.4969 - val_accuracy: 0.8564\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6917 - accuracy: 0.7950 - val_loss: 0.5247 - val_accuracy: 0.8423\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6850 - accuracy: 0.7958 - val_loss: 0.5936 - val_accuracy: 0.8245\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6849 - accuracy: 0.7980 - val_loss: 0.5344 - val_accuracy: 0.8399\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6769 - accuracy: 0.7989 - val_loss: 0.5615 - val_accuracy: 0.8315\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6717 - accuracy: 0.8017 - val_loss: 0.5903 - val_accuracy: 0.8278\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6672 - accuracy: 0.8024 - val_loss: 0.5728 - val_accuracy: 0.8314\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6610 - accuracy: 0.8063 - val_loss: 0.5536 - val_accuracy: 0.8284\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6594 - accuracy: 0.8065 - val_loss: 0.5070 - val_accuracy: 0.8509\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6488 - accuracy: 0.8072 - val_loss: 0.5507 - val_accuracy: 0.8364\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6427 - accuracy: 0.8107 - val_loss: 0.5930 - val_accuracy: 0.8277\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6449 - accuracy: 0.8093 - val_loss: 0.4863 - val_accuracy: 0.8539\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6402 - accuracy: 0.8095 - val_loss: 0.4575 - val_accuracy: 0.8679\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6335 - accuracy: 0.8129 - val_loss: 0.5465 - val_accuracy: 0.8405\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6255 - accuracy: 0.8150 - val_loss: 0.6966 - val_accuracy: 0.7881\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6268 - accuracy: 0.8161 - val_loss: 0.5082 - val_accuracy: 0.8542\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6188 - accuracy: 0.8168 - val_loss: 0.4882 - val_accuracy: 0.8584\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6153 - accuracy: 0.8180 - val_loss: 0.5304 - val_accuracy: 0.8394\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6128 - accuracy: 0.8181 - val_loss: 0.5560 - val_accuracy: 0.8396\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6121 - accuracy: 0.8200 - val_loss: 0.5667 - val_accuracy: 0.8284\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6010 - accuracy: 0.8237 - val_loss: 0.5383 - val_accuracy: 0.8379\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6014 - accuracy: 0.8215 - val_loss: 0.4467 - val_accuracy: 0.8722\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6012 - accuracy: 0.8218 - val_loss: 0.4799 - val_accuracy: 0.8576\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5938 - accuracy: 0.8245 - val_loss: 0.5037 - val_accuracy: 0.8545\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5898 - accuracy: 0.8259 - val_loss: 0.5167 - val_accuracy: 0.8475\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5912 - accuracy: 0.8250 - val_loss: 0.4200 - val_accuracy: 0.8799\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5792 - accuracy: 0.8283 - val_loss: 0.5169 - val_accuracy: 0.8401\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5795 - accuracy: 0.8301 - val_loss: 0.5295 - val_accuracy: 0.8410\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5760 - accuracy: 0.8310 - val_loss: 0.4947 - val_accuracy: 0.8497\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5753 - accuracy: 0.8314 - val_loss: 0.4807 - val_accuracy: 0.8597\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5712 - accuracy: 0.8304 - val_loss: 0.4568 - val_accuracy: 0.8663\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5641 - accuracy: 0.8335 - val_loss: 0.4790 - val_accuracy: 0.8599\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5602 - accuracy: 0.8353 - val_loss: 0.5294 - val_accuracy: 0.8418\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5615 - accuracy: 0.8349 - val_loss: 0.5382 - val_accuracy: 0.8381\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5580 - accuracy: 0.8355 - val_loss: 0.6113 - val_accuracy: 0.8142\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5494 - accuracy: 0.8384 - val_loss: 0.4719 - val_accuracy: 0.8618\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5512 - accuracy: 0.8374 - val_loss: 0.5637 - val_accuracy: 0.8301\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5458 - accuracy: 0.8394 - val_loss: 0.5027 - val_accuracy: 0.8551\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5449 - accuracy: 0.8416 - val_loss: 0.4509 - val_accuracy: 0.8705\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5336 - accuracy: 0.8442 - val_loss: 0.4818 - val_accuracy: 0.8559\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5449 - accuracy: 0.8407 - val_loss: 0.4826 - val_accuracy: 0.8589\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5370 - accuracy: 0.8437 - val_loss: 0.4797 - val_accuracy: 0.8546\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5319 - accuracy: 0.8435 - val_loss: 0.4261 - val_accuracy: 0.8764\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5333 - accuracy: 0.8449 - val_loss: 0.4997 - val_accuracy: 0.8369\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5236 - accuracy: 0.8467 - val_loss: 0.5228 - val_accuracy: 0.8458\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5231 - accuracy: 0.8459 - val_loss: 0.3775 - val_accuracy: 0.8946\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5290 - accuracy: 0.8459 - val_loss: 0.4525 - val_accuracy: 0.8670\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5211 - accuracy: 0.8472 - val_loss: 0.5908 - val_accuracy: 0.8281\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5180 - accuracy: 0.8471 - val_loss: 0.4183 - val_accuracy: 0.8805\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5171 - accuracy: 0.8477 - val_loss: 0.4383 - val_accuracy: 0.8688\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5177 - accuracy: 0.8491 - val_loss: 0.5342 - val_accuracy: 0.8380\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5144 - accuracy: 0.8496 - val_loss: 0.4462 - val_accuracy: 0.8676\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5067 - accuracy: 0.8513 - val_loss: 0.4632 - val_accuracy: 0.8654\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5042 - accuracy: 0.8540 - val_loss: 0.4704 - val_accuracy: 0.8588\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5068 - accuracy: 0.8523 - val_loss: 0.4772 - val_accuracy: 0.8628\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5011 - accuracy: 0.8534 - val_loss: 0.5227 - val_accuracy: 0.8413\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4964 - accuracy: 0.8558 - val_loss: 0.4385 - val_accuracy: 0.8770\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4941 - accuracy: 0.8566 - val_loss: 0.4340 - val_accuracy: 0.8751\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4877 - accuracy: 0.8573 - val_loss: 0.3304 - val_accuracy: 0.9059\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4884 - accuracy: 0.8564 - val_loss: 0.3581 - val_accuracy: 0.8978\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4894 - accuracy: 0.8567 - val_loss: 0.4101 - val_accuracy: 0.8774\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4885 - accuracy: 0.8574 - val_loss: 0.3819 - val_accuracy: 0.8926\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4869 - accuracy: 0.8576 - val_loss: 0.4245 - val_accuracy: 0.8761\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4855 - accuracy: 0.8601 - val_loss: 0.6245 - val_accuracy: 0.8176\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4840 - accuracy: 0.8602 - val_loss: 0.5477 - val_accuracy: 0.8319\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4758 - accuracy: 0.8621 - val_loss: 0.5529 - val_accuracy: 0.8252\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4699 - accuracy: 0.8634 - val_loss: 0.4108 - val_accuracy: 0.8811\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4809 - accuracy: 0.8599 - val_loss: 0.4636 - val_accuracy: 0.8655\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4799 - accuracy: 0.8591 - val_loss: 0.4053 - val_accuracy: 0.8856\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4736 - accuracy: 0.8614 - val_loss: 0.4220 - val_accuracy: 0.8766\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4709 - accuracy: 0.8624 - val_loss: 0.3661 - val_accuracy: 0.8972\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4701 - accuracy: 0.8631 - val_loss: 0.3506 - val_accuracy: 0.9014\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4697 - accuracy: 0.8627 - val_loss: 0.4043 - val_accuracy: 0.8824\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4662 - accuracy: 0.8658 - val_loss: 0.3630 - val_accuracy: 0.8925\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4669 - accuracy: 0.8646 - val_loss: 0.4471 - val_accuracy: 0.8675\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4651 - accuracy: 0.8649 - val_loss: 0.4079 - val_accuracy: 0.8794\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4544 - accuracy: 0.8683 - val_loss: 0.4366 - val_accuracy: 0.8696\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4607 - accuracy: 0.8647 - val_loss: 0.3701 - val_accuracy: 0.8950\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4540 - accuracy: 0.8678 - val_loss: 0.3869 - val_accuracy: 0.8847\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4558 - accuracy: 0.8686 - val_loss: 0.4192 - val_accuracy: 0.8783\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4532 - accuracy: 0.8660 - val_loss: 0.4775 - val_accuracy: 0.8638\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4532 - accuracy: 0.8659 - val_loss: 0.5021 - val_accuracy: 0.8485\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4528 - accuracy: 0.8676 - val_loss: 0.5321 - val_accuracy: 0.8421\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4475 - accuracy: 0.8698 - val_loss: 0.3879 - val_accuracy: 0.8864\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4479 - accuracy: 0.8704 - val_loss: 0.3824 - val_accuracy: 0.8906\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4489 - accuracy: 0.8695 - val_loss: 0.4190 - val_accuracy: 0.8699\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4460 - accuracy: 0.8702 - val_loss: 0.4755 - val_accuracy: 0.8575\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4382 - accuracy: 0.8724 - val_loss: 0.3907 - val_accuracy: 0.8883\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4317 - accuracy: 0.8745 - val_loss: 0.4534 - val_accuracy: 0.8631\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4434 - accuracy: 0.8706 - val_loss: 0.3918 - val_accuracy: 0.8866\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4344 - accuracy: 0.8740 - val_loss: 0.3839 - val_accuracy: 0.8907\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4387 - accuracy: 0.8728 - val_loss: 0.4244 - val_accuracy: 0.8767\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4353 - accuracy: 0.8739 - val_loss: 0.4182 - val_accuracy: 0.8709\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4332 - accuracy: 0.8735 - val_loss: 0.4323 - val_accuracy: 0.8753\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4324 - accuracy: 0.8744 - val_loss: 0.5204 - val_accuracy: 0.8454\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4307 - accuracy: 0.8748 - val_loss: 0.5460 - val_accuracy: 0.8418\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4247 - accuracy: 0.8765 - val_loss: 0.4390 - val_accuracy: 0.8706\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4218 - accuracy: 0.8778 - val_loss: 0.4115 - val_accuracy: 0.8814\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4229 - accuracy: 0.8766 - val_loss: 0.5385 - val_accuracy: 0.8466\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4251 - accuracy: 0.8765 - val_loss: 0.3707 - val_accuracy: 0.8941\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4259 - accuracy: 0.8759 - val_loss: 0.4697 - val_accuracy: 0.8633\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4190 - accuracy: 0.8771 - val_loss: 0.4042 - val_accuracy: 0.8814\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4179 - accuracy: 0.8807 - val_loss: 0.3131 - val_accuracy: 0.9120\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4141 - accuracy: 0.8813 - val_loss: 0.3978 - val_accuracy: 0.8850\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4151 - accuracy: 0.8786 - val_loss: 0.3706 - val_accuracy: 0.8911\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4116 - accuracy: 0.8813 - val_loss: 0.4316 - val_accuracy: 0.8744\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4077 - accuracy: 0.8821 - val_loss: 0.3969 - val_accuracy: 0.8812\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4098 - accuracy: 0.8803 - val_loss: 0.5089 - val_accuracy: 0.8565\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4081 - accuracy: 0.8819 - val_loss: 0.5764 - val_accuracy: 0.8286\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4131 - accuracy: 0.8793 - val_loss: 0.2805 - val_accuracy: 0.9219\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4048 - accuracy: 0.8815 - val_loss: 0.3794 - val_accuracy: 0.8900\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4121 - accuracy: 0.8800 - val_loss: 0.3272 - val_accuracy: 0.9084\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4041 - accuracy: 0.8822 - val_loss: 0.3127 - val_accuracy: 0.9137\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4074 - accuracy: 0.8807 - val_loss: 0.4644 - val_accuracy: 0.8651\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4024 - accuracy: 0.8820 - val_loss: 0.3736 - val_accuracy: 0.8953\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3994 - accuracy: 0.8852 - val_loss: 0.3554 - val_accuracy: 0.8996\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4048 - accuracy: 0.8825 - val_loss: 0.3739 - val_accuracy: 0.8914\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3980 - accuracy: 0.8833 - val_loss: 0.3431 - val_accuracy: 0.9030\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8838 - val_loss: 0.4370 - val_accuracy: 0.8740\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3973 - accuracy: 0.8848 - val_loss: 0.3819 - val_accuracy: 0.8887\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8848 - val_loss: 0.3620 - val_accuracy: 0.8937\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8864 - val_loss: 0.2800 - val_accuracy: 0.9230\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3912 - accuracy: 0.8874 - val_loss: 0.5574 - val_accuracy: 0.8347\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3900 - accuracy: 0.8874 - val_loss: 0.4697 - val_accuracy: 0.8642\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3892 - accuracy: 0.8863 - val_loss: 0.4755 - val_accuracy: 0.8587\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3907 - accuracy: 0.8865 - val_loss: 0.4042 - val_accuracy: 0.8825\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8866 - val_loss: 0.3490 - val_accuracy: 0.8996\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3842 - accuracy: 0.8890 - val_loss: 0.3548 - val_accuracy: 0.9004\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3822 - accuracy: 0.8889 - val_loss: 0.4638 - val_accuracy: 0.8614\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8895 - val_loss: 0.4659 - val_accuracy: 0.8666\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3822 - accuracy: 0.8879 - val_loss: 0.3706 - val_accuracy: 0.8905\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3826 - accuracy: 0.8894 - val_loss: 0.4008 - val_accuracy: 0.8871\n",
      "Try 2/100: Best_val_acc: [0.5445862412452698, 0.8423333168029785], lr: 6.130617570455691e-05, Lambda: 5.785242644107573e-05\n",
      "\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7225 - accuracy: 0.1093 - val_loss: 2.2496 - val_accuracy: 0.1571\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5766 - accuracy: 0.1319 - val_loss: 2.3139 - val_accuracy: 0.1129\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4499 - accuracy: 0.1513 - val_loss: 2.2096 - val_accuracy: 0.2036\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3428 - accuracy: 0.1775 - val_loss: 2.0565 - val_accuracy: 0.2588\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2523 - accuracy: 0.2041 - val_loss: 1.9998 - val_accuracy: 0.2565\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1520 - accuracy: 0.2347 - val_loss: 1.8616 - val_accuracy: 0.3366\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0788 - accuracy: 0.2614 - val_loss: 1.7656 - val_accuracy: 0.4043\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0058 - accuracy: 0.2873 - val_loss: 1.7904 - val_accuracy: 0.3784\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9455 - accuracy: 0.3105 - val_loss: 1.6795 - val_accuracy: 0.4339\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8841 - accuracy: 0.3363 - val_loss: 1.5811 - val_accuracy: 0.4853\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8239 - accuracy: 0.3619 - val_loss: 1.5112 - val_accuracy: 0.5355\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7814 - accuracy: 0.3794 - val_loss: 1.5079 - val_accuracy: 0.5257\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7268 - accuracy: 0.4035 - val_loss: 1.4325 - val_accuracy: 0.5644\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6815 - accuracy: 0.4208 - val_loss: 1.4028 - val_accuracy: 0.5875\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.6371 - accuracy: 0.4422 - val_loss: 1.3550 - val_accuracy: 0.6071\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.5947 - accuracy: 0.4631 - val_loss: 1.3115 - val_accuracy: 0.6396\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5483 - accuracy: 0.4809 - val_loss: 1.3238 - val_accuracy: 0.6011\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5127 - accuracy: 0.4972 - val_loss: 1.2543 - val_accuracy: 0.6662\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4750 - accuracy: 0.5135 - val_loss: 1.2934 - val_accuracy: 0.6390\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4317 - accuracy: 0.5290 - val_loss: 1.1956 - val_accuracy: 0.6640\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4044 - accuracy: 0.5450 - val_loss: 1.1722 - val_accuracy: 0.6825\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3718 - accuracy: 0.5588 - val_loss: 1.1449 - val_accuracy: 0.6886\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3358 - accuracy: 0.5700 - val_loss: 1.1097 - val_accuracy: 0.6777\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3008 - accuracy: 0.5845 - val_loss: 1.1060 - val_accuracy: 0.6863\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2697 - accuracy: 0.5974 - val_loss: 1.1187 - val_accuracy: 0.6751\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2509 - accuracy: 0.6023 - val_loss: 1.0448 - val_accuracy: 0.7140\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2163 - accuracy: 0.6179 - val_loss: 0.9797 - val_accuracy: 0.7395\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1982 - accuracy: 0.6236 - val_loss: 1.0286 - val_accuracy: 0.7051\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1729 - accuracy: 0.6341 - val_loss: 0.9658 - val_accuracy: 0.7344\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1478 - accuracy: 0.6421 - val_loss: 0.9410 - val_accuracy: 0.7353\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1290 - accuracy: 0.6463 - val_loss: 0.9738 - val_accuracy: 0.7176\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1100 - accuracy: 0.6553 - val_loss: 0.9254 - val_accuracy: 0.7332\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0911 - accuracy: 0.6618 - val_loss: 0.9480 - val_accuracy: 0.7282\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0679 - accuracy: 0.6693 - val_loss: 0.8683 - val_accuracy: 0.7533\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0509 - accuracy: 0.6772 - val_loss: 0.8843 - val_accuracy: 0.7454\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0305 - accuracy: 0.6841 - val_loss: 0.8472 - val_accuracy: 0.7609\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0194 - accuracy: 0.6869 - val_loss: 0.8367 - val_accuracy: 0.7671\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9988 - accuracy: 0.6941 - val_loss: 0.8863 - val_accuracy: 0.7370\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9854 - accuracy: 0.6989 - val_loss: 0.8722 - val_accuracy: 0.7414\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9733 - accuracy: 0.7029 - val_loss: 0.8014 - val_accuracy: 0.7688\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9554 - accuracy: 0.7089 - val_loss: 0.7915 - val_accuracy: 0.7770\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9413 - accuracy: 0.7140 - val_loss: 0.6782 - val_accuracy: 0.8104\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9223 - accuracy: 0.7190 - val_loss: 0.8136 - val_accuracy: 0.7529\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9166 - accuracy: 0.7215 - val_loss: 0.8752 - val_accuracy: 0.7284\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8991 - accuracy: 0.7264 - val_loss: 0.7564 - val_accuracy: 0.7725\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8922 - accuracy: 0.7303 - val_loss: 0.7900 - val_accuracy: 0.7646\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8779 - accuracy: 0.7348 - val_loss: 0.7698 - val_accuracy: 0.7672\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8703 - accuracy: 0.7376 - val_loss: 0.7076 - val_accuracy: 0.7903\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8573 - accuracy: 0.7402 - val_loss: 0.7350 - val_accuracy: 0.7741\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8457 - accuracy: 0.7454 - val_loss: 0.6965 - val_accuracy: 0.7863\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8329 - accuracy: 0.7496 - val_loss: 0.6280 - val_accuracy: 0.8246\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8294 - accuracy: 0.7504 - val_loss: 0.6834 - val_accuracy: 0.7924\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8223 - accuracy: 0.7530 - val_loss: 0.7553 - val_accuracy: 0.7734\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8081 - accuracy: 0.7567 - val_loss: 0.7058 - val_accuracy: 0.7844\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7961 - accuracy: 0.7627 - val_loss: 0.6682 - val_accuracy: 0.8010\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7932 - accuracy: 0.7623 - val_loss: 0.6722 - val_accuracy: 0.7974\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7750 - accuracy: 0.7685 - val_loss: 0.7361 - val_accuracy: 0.7810\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7724 - accuracy: 0.7691 - val_loss: 0.6766 - val_accuracy: 0.8001\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7653 - accuracy: 0.7722 - val_loss: 0.6745 - val_accuracy: 0.7997\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7575 - accuracy: 0.7730 - val_loss: 0.5956 - val_accuracy: 0.8216\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7516 - accuracy: 0.7766 - val_loss: 0.6472 - val_accuracy: 0.7980\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7461 - accuracy: 0.7789 - val_loss: 0.6533 - val_accuracy: 0.8039\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7371 - accuracy: 0.7814 - val_loss: 0.6123 - val_accuracy: 0.8119\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7305 - accuracy: 0.7818 - val_loss: 0.5875 - val_accuracy: 0.8261\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7274 - accuracy: 0.7821 - val_loss: 0.6896 - val_accuracy: 0.7846\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7186 - accuracy: 0.7872 - val_loss: 0.7412 - val_accuracy: 0.7742\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7096 - accuracy: 0.7901 - val_loss: 0.6220 - val_accuracy: 0.8170\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7035 - accuracy: 0.7896 - val_loss: 0.6536 - val_accuracy: 0.8064\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6968 - accuracy: 0.7938 - val_loss: 0.5375 - val_accuracy: 0.8423\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6899 - accuracy: 0.7957 - val_loss: 0.7041 - val_accuracy: 0.7862\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6865 - accuracy: 0.7970 - val_loss: 0.5912 - val_accuracy: 0.8241\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6885 - accuracy: 0.7968 - val_loss: 0.5724 - val_accuracy: 0.8248\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6728 - accuracy: 0.8011 - val_loss: 0.5489 - val_accuracy: 0.8388\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6703 - accuracy: 0.8014 - val_loss: 0.5217 - val_accuracy: 0.8472\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6664 - accuracy: 0.8031 - val_loss: 0.6009 - val_accuracy: 0.8202\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6663 - accuracy: 0.8032 - val_loss: 0.7094 - val_accuracy: 0.7999\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6571 - accuracy: 0.8044 - val_loss: 0.5949 - val_accuracy: 0.8234\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6542 - accuracy: 0.8078 - val_loss: 0.5749 - val_accuracy: 0.8253\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6416 - accuracy: 0.8085 - val_loss: 0.5537 - val_accuracy: 0.8343\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6481 - accuracy: 0.8081 - val_loss: 0.5397 - val_accuracy: 0.8371\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6396 - accuracy: 0.8104 - val_loss: 0.5539 - val_accuracy: 0.8289\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6300 - accuracy: 0.8148 - val_loss: 0.4999 - val_accuracy: 0.8537\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6271 - accuracy: 0.8144 - val_loss: 0.6812 - val_accuracy: 0.7917\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6239 - accuracy: 0.8164 - val_loss: 0.6546 - val_accuracy: 0.7986\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6166 - accuracy: 0.8200 - val_loss: 0.5810 - val_accuracy: 0.8306\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6097 - accuracy: 0.8217 - val_loss: 0.6735 - val_accuracy: 0.7991\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6122 - accuracy: 0.8202 - val_loss: 0.5882 - val_accuracy: 0.8267\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6043 - accuracy: 0.8218 - val_loss: 0.5395 - val_accuracy: 0.8364\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6014 - accuracy: 0.8250 - val_loss: 0.7218 - val_accuracy: 0.7844\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5967 - accuracy: 0.8231 - val_loss: 0.5098 - val_accuracy: 0.8499\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5946 - accuracy: 0.8254 - val_loss: 0.5369 - val_accuracy: 0.8426\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5899 - accuracy: 0.8261 - val_loss: 0.4637 - val_accuracy: 0.8661\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5910 - accuracy: 0.8270 - val_loss: 0.4510 - val_accuracy: 0.8659\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5850 - accuracy: 0.8288 - val_loss: 0.4163 - val_accuracy: 0.8772\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5811 - accuracy: 0.8303 - val_loss: 0.5361 - val_accuracy: 0.8385\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5773 - accuracy: 0.8318 - val_loss: 0.5008 - val_accuracy: 0.8506\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5789 - accuracy: 0.8314 - val_loss: 0.5582 - val_accuracy: 0.8354\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5662 - accuracy: 0.8329 - val_loss: 0.4887 - val_accuracy: 0.8531\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5686 - accuracy: 0.8321 - val_loss: 0.4582 - val_accuracy: 0.8679\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5648 - accuracy: 0.8328 - val_loss: 0.6045 - val_accuracy: 0.8211\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5679 - accuracy: 0.8340 - val_loss: 0.4226 - val_accuracy: 0.8751\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5569 - accuracy: 0.8370 - val_loss: 0.4122 - val_accuracy: 0.8826\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5603 - accuracy: 0.8357 - val_loss: 0.5902 - val_accuracy: 0.8233\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5524 - accuracy: 0.8385 - val_loss: 0.5013 - val_accuracy: 0.8522\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5469 - accuracy: 0.8418 - val_loss: 0.5851 - val_accuracy: 0.8239\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5457 - accuracy: 0.8404 - val_loss: 0.4448 - val_accuracy: 0.8679\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5466 - accuracy: 0.8391 - val_loss: 0.3564 - val_accuracy: 0.9008\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5372 - accuracy: 0.8427 - val_loss: 0.4554 - val_accuracy: 0.8644\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5401 - accuracy: 0.8428 - val_loss: 0.3651 - val_accuracy: 0.8984\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5352 - accuracy: 0.8431 - val_loss: 0.4915 - val_accuracy: 0.8536\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5329 - accuracy: 0.8448 - val_loss: 0.4642 - val_accuracy: 0.8624\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5365 - accuracy: 0.8434 - val_loss: 0.3901 - val_accuracy: 0.8864\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5326 - accuracy: 0.8443 - val_loss: 0.4728 - val_accuracy: 0.8584\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5323 - accuracy: 0.8445 - val_loss: 0.3826 - val_accuracy: 0.8936\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5223 - accuracy: 0.8467 - val_loss: 0.3761 - val_accuracy: 0.8935\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5224 - accuracy: 0.8480 - val_loss: 0.4712 - val_accuracy: 0.8636\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5210 - accuracy: 0.8471 - val_loss: 0.5177 - val_accuracy: 0.8458\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5175 - accuracy: 0.8515 - val_loss: 0.4579 - val_accuracy: 0.8657\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5139 - accuracy: 0.8517 - val_loss: 0.5042 - val_accuracy: 0.8524\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5079 - accuracy: 0.8506 - val_loss: 0.3794 - val_accuracy: 0.8901\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5198 - accuracy: 0.8478 - val_loss: 0.3836 - val_accuracy: 0.8899\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5091 - accuracy: 0.8515 - val_loss: 0.6276 - val_accuracy: 0.8114\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5070 - accuracy: 0.8534 - val_loss: 0.5902 - val_accuracy: 0.8278\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4996 - accuracy: 0.8534 - val_loss: 0.3837 - val_accuracy: 0.8916\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4938 - accuracy: 0.8564 - val_loss: 0.4184 - val_accuracy: 0.8732\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4967 - accuracy: 0.8539 - val_loss: 0.4283 - val_accuracy: 0.8755\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4935 - accuracy: 0.8570 - val_loss: 0.4582 - val_accuracy: 0.8661\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4946 - accuracy: 0.8578 - val_loss: 0.4364 - val_accuracy: 0.8746\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4880 - accuracy: 0.8560 - val_loss: 0.4160 - val_accuracy: 0.8769\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4903 - accuracy: 0.8565 - val_loss: 0.5474 - val_accuracy: 0.8398\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4875 - accuracy: 0.8579 - val_loss: 0.3470 - val_accuracy: 0.9030\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4798 - accuracy: 0.8617 - val_loss: 0.3905 - val_accuracy: 0.8876\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4851 - accuracy: 0.8589 - val_loss: 0.4445 - val_accuracy: 0.8728\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4857 - accuracy: 0.8606 - val_loss: 0.3984 - val_accuracy: 0.8874\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4815 - accuracy: 0.8604 - val_loss: 0.4248 - val_accuracy: 0.8756\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4769 - accuracy: 0.8620 - val_loss: 0.5173 - val_accuracy: 0.8406\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4743 - accuracy: 0.8618 - val_loss: 0.4680 - val_accuracy: 0.8640\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4743 - accuracy: 0.8610 - val_loss: 0.4234 - val_accuracy: 0.8759\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4647 - accuracy: 0.8664 - val_loss: 0.4638 - val_accuracy: 0.8605\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4688 - accuracy: 0.8649 - val_loss: 0.4581 - val_accuracy: 0.8674\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4636 - accuracy: 0.8650 - val_loss: 0.4225 - val_accuracy: 0.8774\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4670 - accuracy: 0.8650 - val_loss: 0.3834 - val_accuracy: 0.8897\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4617 - accuracy: 0.8659 - val_loss: 0.3694 - val_accuracy: 0.8897\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8656 - val_loss: 0.3823 - val_accuracy: 0.8928\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4590 - accuracy: 0.8663 - val_loss: 0.4231 - val_accuracy: 0.8737\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4594 - accuracy: 0.8651 - val_loss: 0.3230 - val_accuracy: 0.9104\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4499 - accuracy: 0.8705 - val_loss: 0.3537 - val_accuracy: 0.9016\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4565 - accuracy: 0.8653 - val_loss: 0.5635 - val_accuracy: 0.8357\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4562 - accuracy: 0.8699 - val_loss: 0.3484 - val_accuracy: 0.9017\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4550 - accuracy: 0.8677 - val_loss: 0.3476 - val_accuracy: 0.9004\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4520 - accuracy: 0.8691 - val_loss: 0.3456 - val_accuracy: 0.9035\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4484 - accuracy: 0.8702 - val_loss: 0.4604 - val_accuracy: 0.8649\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4469 - accuracy: 0.8709 - val_loss: 0.4059 - val_accuracy: 0.8824\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4510 - accuracy: 0.8666 - val_loss: 0.3510 - val_accuracy: 0.9000\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4418 - accuracy: 0.8754 - val_loss: 0.4257 - val_accuracy: 0.8759\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4370 - accuracy: 0.8744 - val_loss: 0.3839 - val_accuracy: 0.8921\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4364 - accuracy: 0.8729 - val_loss: 0.3645 - val_accuracy: 0.8971\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8753 - val_loss: 0.4517 - val_accuracy: 0.8694\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4367 - accuracy: 0.8746 - val_loss: 0.4749 - val_accuracy: 0.8616\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4387 - accuracy: 0.8716 - val_loss: 0.3653 - val_accuracy: 0.8944\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8765 - val_loss: 0.4556 - val_accuracy: 0.8687\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4343 - accuracy: 0.8748 - val_loss: 0.3841 - val_accuracy: 0.8891\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4301 - accuracy: 0.8756 - val_loss: 0.3739 - val_accuracy: 0.8938\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4252 - accuracy: 0.8771 - val_loss: 0.3067 - val_accuracy: 0.9129\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4306 - accuracy: 0.8739 - val_loss: 0.5626 - val_accuracy: 0.8347\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4306 - accuracy: 0.8759 - val_loss: 0.4581 - val_accuracy: 0.8621\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4235 - accuracy: 0.8769 - val_loss: 0.3376 - val_accuracy: 0.9037\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4226 - accuracy: 0.8785 - val_loss: 0.4263 - val_accuracy: 0.8783\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4207 - accuracy: 0.8780 - val_loss: 0.3738 - val_accuracy: 0.8906\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8775 - val_loss: 0.3646 - val_accuracy: 0.8963\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4209 - accuracy: 0.8788 - val_loss: 0.5108 - val_accuracy: 0.8474\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4197 - accuracy: 0.8792 - val_loss: 0.3291 - val_accuracy: 0.9083\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4141 - accuracy: 0.8805 - val_loss: 0.3185 - val_accuracy: 0.9103\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4163 - accuracy: 0.8795 - val_loss: 0.4549 - val_accuracy: 0.8682\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4126 - accuracy: 0.8819 - val_loss: 0.4620 - val_accuracy: 0.8625\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4086 - accuracy: 0.8826 - val_loss: 0.4374 - val_accuracy: 0.8716\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8821 - val_loss: 0.5304 - val_accuracy: 0.8426\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4131 - accuracy: 0.8802 - val_loss: 0.4742 - val_accuracy: 0.8635\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4079 - accuracy: 0.8826 - val_loss: 0.3787 - val_accuracy: 0.8902\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4062 - accuracy: 0.8840 - val_loss: 0.4160 - val_accuracy: 0.8839\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4027 - accuracy: 0.8833 - val_loss: 0.5143 - val_accuracy: 0.8494\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4010 - accuracy: 0.8836 - val_loss: 0.3158 - val_accuracy: 0.9113\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4009 - accuracy: 0.8844 - val_loss: 0.3696 - val_accuracy: 0.8951\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4017 - accuracy: 0.8844 - val_loss: 0.4670 - val_accuracy: 0.8631\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3987 - accuracy: 0.8848 - val_loss: 0.3151 - val_accuracy: 0.9113\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3948 - accuracy: 0.8863 - val_loss: 0.4141 - val_accuracy: 0.8801\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3984 - accuracy: 0.8857 - val_loss: 0.4705 - val_accuracy: 0.8661\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8847 - val_loss: 0.3723 - val_accuracy: 0.8874\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8873 - val_loss: 0.4694 - val_accuracy: 0.8609\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3934 - accuracy: 0.8865 - val_loss: 0.3603 - val_accuracy: 0.8958\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8832 - val_loss: 0.3707 - val_accuracy: 0.8946\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3904 - accuracy: 0.8853 - val_loss: 0.3716 - val_accuracy: 0.8916\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3870 - accuracy: 0.8900 - val_loss: 0.4200 - val_accuracy: 0.8807\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3937 - accuracy: 0.8865 - val_loss: 0.3971 - val_accuracy: 0.8887\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3921 - accuracy: 0.8870 - val_loss: 0.4145 - val_accuracy: 0.8778\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8898 - val_loss: 0.4476 - val_accuracy: 0.8646\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3938 - accuracy: 0.8867 - val_loss: 0.4152 - val_accuracy: 0.8814\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3823 - accuracy: 0.8900 - val_loss: 0.3709 - val_accuracy: 0.8946\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3844 - accuracy: 0.8898 - val_loss: 0.3679 - val_accuracy: 0.8874\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3843 - accuracy: 0.8905 - val_loss: 0.4379 - val_accuracy: 0.8757\n",
      "Try 3/100: Best_val_acc: [0.5264115929603577, 0.8481666445732117], lr: 6.130761615173424e-05, Lambda: 5.865931915136193e-05\n",
      "\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6840 - accuracy: 0.1087 - val_loss: 2.2320 - val_accuracy: 0.1785\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5605 - accuracy: 0.1227 - val_loss: 2.2062 - val_accuracy: 0.2144\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4557 - accuracy: 0.1449 - val_loss: 2.1348 - val_accuracy: 0.2376\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3635 - accuracy: 0.1662 - val_loss: 2.0186 - val_accuracy: 0.3085\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2764 - accuracy: 0.1882 - val_loss: 1.9294 - val_accuracy: 0.3161\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1907 - accuracy: 0.2172 - val_loss: 1.9053 - val_accuracy: 0.3289\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1137 - accuracy: 0.2443 - val_loss: 1.8155 - val_accuracy: 0.3925\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0327 - accuracy: 0.2767 - val_loss: 1.7531 - val_accuracy: 0.4559\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9549 - accuracy: 0.3094 - val_loss: 1.6965 - val_accuracy: 0.4684\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8937 - accuracy: 0.3328 - val_loss: 1.5495 - val_accuracy: 0.5421\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8341 - accuracy: 0.3613 - val_loss: 1.5561 - val_accuracy: 0.5403\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7677 - accuracy: 0.3898 - val_loss: 1.4519 - val_accuracy: 0.5636\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7172 - accuracy: 0.4115 - val_loss: 1.3992 - val_accuracy: 0.6136\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6616 - accuracy: 0.4350 - val_loss: 1.3702 - val_accuracy: 0.6094\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6080 - accuracy: 0.4608 - val_loss: 1.3027 - val_accuracy: 0.6305\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5645 - accuracy: 0.4750 - val_loss: 1.2839 - val_accuracy: 0.6341\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5226 - accuracy: 0.4972 - val_loss: 1.2133 - val_accuracy: 0.6568\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4869 - accuracy: 0.5077 - val_loss: 1.2227 - val_accuracy: 0.6308\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4496 - accuracy: 0.5235 - val_loss: 1.1706 - val_accuracy: 0.6594\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4138 - accuracy: 0.5380 - val_loss: 1.1524 - val_accuracy: 0.6670\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3749 - accuracy: 0.5590 - val_loss: 1.1094 - val_accuracy: 0.6880\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3414 - accuracy: 0.5702 - val_loss: 1.0921 - val_accuracy: 0.6769\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3088 - accuracy: 0.5830 - val_loss: 1.0876 - val_accuracy: 0.6678\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2790 - accuracy: 0.5935 - val_loss: 1.0446 - val_accuracy: 0.6939\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2475 - accuracy: 0.6077 - val_loss: 0.9846 - val_accuracy: 0.7328\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2236 - accuracy: 0.6141 - val_loss: 1.0524 - val_accuracy: 0.6774\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1948 - accuracy: 0.6266 - val_loss: 0.9711 - val_accuracy: 0.7100\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1702 - accuracy: 0.6365 - val_loss: 0.9038 - val_accuracy: 0.7458\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1454 - accuracy: 0.6426 - val_loss: 0.9034 - val_accuracy: 0.7389\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1189 - accuracy: 0.6514 - val_loss: 0.8894 - val_accuracy: 0.7394\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0960 - accuracy: 0.6591 - val_loss: 0.8955 - val_accuracy: 0.7377\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0733 - accuracy: 0.6699 - val_loss: 0.8826 - val_accuracy: 0.7359\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0620 - accuracy: 0.6712 - val_loss: 0.8498 - val_accuracy: 0.7494\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0409 - accuracy: 0.6808 - val_loss: 0.8202 - val_accuracy: 0.7639\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0142 - accuracy: 0.6878 - val_loss: 0.9006 - val_accuracy: 0.7246\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0068 - accuracy: 0.6900 - val_loss: 0.8949 - val_accuracy: 0.7284\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9832 - accuracy: 0.6993 - val_loss: 0.8526 - val_accuracy: 0.7397\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9701 - accuracy: 0.7035 - val_loss: 0.8076 - val_accuracy: 0.7600\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9575 - accuracy: 0.7075 - val_loss: 0.8259 - val_accuracy: 0.7495\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9372 - accuracy: 0.7144 - val_loss: 0.8288 - val_accuracy: 0.7447\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9224 - accuracy: 0.7185 - val_loss: 0.8786 - val_accuracy: 0.7256\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9077 - accuracy: 0.7232 - val_loss: 0.7640 - val_accuracy: 0.7769\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8959 - accuracy: 0.7294 - val_loss: 0.6761 - val_accuracy: 0.8030\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8890 - accuracy: 0.7293 - val_loss: 0.7733 - val_accuracy: 0.7631\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8731 - accuracy: 0.7348 - val_loss: 0.7478 - val_accuracy: 0.7799\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8612 - accuracy: 0.7423 - val_loss: 0.7233 - val_accuracy: 0.7844\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8485 - accuracy: 0.7429 - val_loss: 0.6529 - val_accuracy: 0.8059\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8364 - accuracy: 0.7481 - val_loss: 0.6675 - val_accuracy: 0.7994\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8343 - accuracy: 0.7476 - val_loss: 0.6980 - val_accuracy: 0.7866\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8186 - accuracy: 0.7543 - val_loss: 0.6484 - val_accuracy: 0.8093\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8062 - accuracy: 0.7560 - val_loss: 0.5904 - val_accuracy: 0.8239\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7944 - accuracy: 0.7602 - val_loss: 0.5720 - val_accuracy: 0.8316\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7859 - accuracy: 0.7658 - val_loss: 0.6471 - val_accuracy: 0.8059\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7843 - accuracy: 0.7640 - val_loss: 0.6031 - val_accuracy: 0.8152\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7760 - accuracy: 0.7675 - val_loss: 0.6727 - val_accuracy: 0.7894\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7645 - accuracy: 0.7694 - val_loss: 0.6200 - val_accuracy: 0.8166\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7669 - accuracy: 0.7707 - val_loss: 0.5642 - val_accuracy: 0.8301\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7509 - accuracy: 0.7758 - val_loss: 0.5475 - val_accuracy: 0.8357\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7459 - accuracy: 0.7770 - val_loss: 0.6101 - val_accuracy: 0.8156\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7351 - accuracy: 0.7819 - val_loss: 0.6970 - val_accuracy: 0.7846\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7299 - accuracy: 0.7816 - val_loss: 0.5800 - val_accuracy: 0.8280\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7252 - accuracy: 0.7832 - val_loss: 0.5671 - val_accuracy: 0.8329\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7156 - accuracy: 0.7849 - val_loss: 0.6665 - val_accuracy: 0.8033\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7067 - accuracy: 0.7893 - val_loss: 0.7160 - val_accuracy: 0.7794\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7019 - accuracy: 0.7906 - val_loss: 0.6131 - val_accuracy: 0.8091\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6972 - accuracy: 0.7925 - val_loss: 0.6786 - val_accuracy: 0.7895\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6916 - accuracy: 0.7929 - val_loss: 0.5706 - val_accuracy: 0.8258\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6865 - accuracy: 0.7952 - val_loss: 0.6456 - val_accuracy: 0.7981\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6822 - accuracy: 0.7949 - val_loss: 0.6328 - val_accuracy: 0.8085\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6728 - accuracy: 0.7999 - val_loss: 0.5378 - val_accuracy: 0.8351\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6778 - accuracy: 0.7982 - val_loss: 0.6373 - val_accuracy: 0.8019\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6600 - accuracy: 0.8038 - val_loss: 0.5681 - val_accuracy: 0.8262\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6560 - accuracy: 0.8036 - val_loss: 0.4555 - val_accuracy: 0.8666\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6567 - accuracy: 0.8053 - val_loss: 0.5573 - val_accuracy: 0.8298\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6480 - accuracy: 0.8070 - val_loss: 0.4820 - val_accuracy: 0.8591\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6474 - accuracy: 0.8070 - val_loss: 0.5001 - val_accuracy: 0.8529\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6378 - accuracy: 0.8102 - val_loss: 0.4996 - val_accuracy: 0.8511\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6331 - accuracy: 0.8126 - val_loss: 0.4972 - val_accuracy: 0.8549\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6321 - accuracy: 0.8103 - val_loss: 0.6197 - val_accuracy: 0.8153\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6256 - accuracy: 0.8144 - val_loss: 0.5030 - val_accuracy: 0.8474\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6192 - accuracy: 0.8177 - val_loss: 0.4958 - val_accuracy: 0.8511\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6153 - accuracy: 0.8171 - val_loss: 0.5568 - val_accuracy: 0.8281\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6172 - accuracy: 0.8170 - val_loss: 0.4612 - val_accuracy: 0.8653\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6140 - accuracy: 0.8192 - val_loss: 0.4572 - val_accuracy: 0.8676\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6040 - accuracy: 0.8225 - val_loss: 0.5730 - val_accuracy: 0.8271\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6059 - accuracy: 0.8212 - val_loss: 0.5669 - val_accuracy: 0.8276\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5977 - accuracy: 0.8225 - val_loss: 0.5092 - val_accuracy: 0.8500\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5959 - accuracy: 0.8234 - val_loss: 0.5948 - val_accuracy: 0.8122\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5907 - accuracy: 0.8262 - val_loss: 0.6136 - val_accuracy: 0.8142\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5841 - accuracy: 0.8269 - val_loss: 0.4230 - val_accuracy: 0.8762\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5800 - accuracy: 0.8292 - val_loss: 0.5746 - val_accuracy: 0.8227\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5830 - accuracy: 0.8271 - val_loss: 0.4556 - val_accuracy: 0.8621\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5738 - accuracy: 0.8332 - val_loss: 0.4828 - val_accuracy: 0.8536\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5750 - accuracy: 0.8313 - val_loss: 0.6191 - val_accuracy: 0.8104\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5699 - accuracy: 0.8316 - val_loss: 0.4274 - val_accuracy: 0.8740\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5698 - accuracy: 0.8328 - val_loss: 0.4314 - val_accuracy: 0.8693\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5603 - accuracy: 0.8338 - val_loss: 0.4194 - val_accuracy: 0.8790\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5603 - accuracy: 0.8354 - val_loss: 0.4767 - val_accuracy: 0.8552\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5580 - accuracy: 0.8335 - val_loss: 0.5064 - val_accuracy: 0.8493\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5558 - accuracy: 0.8366 - val_loss: 0.6270 - val_accuracy: 0.8155\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5555 - accuracy: 0.8370 - val_loss: 0.4926 - val_accuracy: 0.8530\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5496 - accuracy: 0.8395 - val_loss: 0.5641 - val_accuracy: 0.8317\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5476 - accuracy: 0.8390 - val_loss: 0.4174 - val_accuracy: 0.8774\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5456 - accuracy: 0.8398 - val_loss: 0.4060 - val_accuracy: 0.8818\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5415 - accuracy: 0.8419 - val_loss: 0.5736 - val_accuracy: 0.8281\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5391 - accuracy: 0.8420 - val_loss: 0.4781 - val_accuracy: 0.8545\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5296 - accuracy: 0.8450 - val_loss: 0.5585 - val_accuracy: 0.8346\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5316 - accuracy: 0.8434 - val_loss: 0.5184 - val_accuracy: 0.8394\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5294 - accuracy: 0.8456 - val_loss: 0.5189 - val_accuracy: 0.8459\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5297 - accuracy: 0.8453 - val_loss: 0.4829 - val_accuracy: 0.8566\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5221 - accuracy: 0.8467 - val_loss: 0.4832 - val_accuracy: 0.8555\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5175 - accuracy: 0.8479 - val_loss: 0.4073 - val_accuracy: 0.8754\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5196 - accuracy: 0.8472 - val_loss: 0.3991 - val_accuracy: 0.8819\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5207 - accuracy: 0.8458 - val_loss: 0.4934 - val_accuracy: 0.8496\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5133 - accuracy: 0.8490 - val_loss: 0.4552 - val_accuracy: 0.8679\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5158 - accuracy: 0.8488 - val_loss: 0.4812 - val_accuracy: 0.8576\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5137 - accuracy: 0.8501 - val_loss: 0.4929 - val_accuracy: 0.8524\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5094 - accuracy: 0.8493 - val_loss: 0.6177 - val_accuracy: 0.8138\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5087 - accuracy: 0.8505 - val_loss: 0.4359 - val_accuracy: 0.8724\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5011 - accuracy: 0.8532 - val_loss: 0.3983 - val_accuracy: 0.8839\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5002 - accuracy: 0.8523 - val_loss: 0.3910 - val_accuracy: 0.8845\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5003 - accuracy: 0.8542 - val_loss: 0.4003 - val_accuracy: 0.8826\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5007 - accuracy: 0.8541 - val_loss: 0.4754 - val_accuracy: 0.8542\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4970 - accuracy: 0.8549 - val_loss: 0.4804 - val_accuracy: 0.8521\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4945 - accuracy: 0.8566 - val_loss: 0.6520 - val_accuracy: 0.8044\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4906 - accuracy: 0.8570 - val_loss: 0.5332 - val_accuracy: 0.8431\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8580 - val_loss: 0.4042 - val_accuracy: 0.8816\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4858 - accuracy: 0.8581 - val_loss: 0.4227 - val_accuracy: 0.8719\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4871 - accuracy: 0.8569 - val_loss: 0.4657 - val_accuracy: 0.8631\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.8586 - val_loss: 0.5554 - val_accuracy: 0.8332\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4819 - accuracy: 0.8590 - val_loss: 0.4176 - val_accuracy: 0.8805\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4754 - accuracy: 0.8616 - val_loss: 0.3823 - val_accuracy: 0.8872\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4820 - accuracy: 0.8585 - val_loss: 0.3915 - val_accuracy: 0.8844\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4730 - accuracy: 0.8631 - val_loss: 0.4998 - val_accuracy: 0.8457\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4644 - accuracy: 0.8643 - val_loss: 0.3952 - val_accuracy: 0.8845\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4705 - accuracy: 0.8625 - val_loss: 0.4126 - val_accuracy: 0.8816\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4742 - accuracy: 0.8632 - val_loss: 0.4271 - val_accuracy: 0.8729\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4638 - accuracy: 0.8645 - val_loss: 0.5976 - val_accuracy: 0.8164\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4669 - accuracy: 0.8636 - val_loss: 0.5262 - val_accuracy: 0.8354\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4585 - accuracy: 0.8656 - val_loss: 0.4133 - val_accuracy: 0.8801\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4603 - accuracy: 0.8656 - val_loss: 0.3725 - val_accuracy: 0.8915\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4590 - accuracy: 0.8657 - val_loss: 0.3956 - val_accuracy: 0.8852\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4605 - accuracy: 0.8653 - val_loss: 0.9567 - val_accuracy: 0.7304\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4554 - accuracy: 0.8665 - val_loss: 0.4909 - val_accuracy: 0.8564\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4444 - accuracy: 0.8685 - val_loss: 0.4010 - val_accuracy: 0.8846\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4571 - accuracy: 0.8671 - val_loss: 0.4938 - val_accuracy: 0.8571\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4512 - accuracy: 0.8697 - val_loss: 0.4671 - val_accuracy: 0.8646\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4497 - accuracy: 0.8690 - val_loss: 0.4103 - val_accuracy: 0.8791\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4505 - accuracy: 0.8688 - val_loss: 0.4426 - val_accuracy: 0.8691\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4511 - accuracy: 0.8701 - val_loss: 0.4345 - val_accuracy: 0.8698\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4442 - accuracy: 0.8688 - val_loss: 0.4519 - val_accuracy: 0.8637\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4484 - accuracy: 0.8671 - val_loss: 0.3945 - val_accuracy: 0.8861\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4360 - accuracy: 0.8726 - val_loss: 0.4137 - val_accuracy: 0.8803\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4414 - accuracy: 0.8699 - val_loss: 0.5086 - val_accuracy: 0.8500\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4409 - accuracy: 0.8707 - val_loss: 0.4286 - val_accuracy: 0.8673\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4326 - accuracy: 0.8737 - val_loss: 0.4680 - val_accuracy: 0.8595\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4354 - accuracy: 0.8731 - val_loss: 0.3230 - val_accuracy: 0.9070\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4326 - accuracy: 0.8731 - val_loss: 0.5052 - val_accuracy: 0.8480\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4307 - accuracy: 0.8749 - val_loss: 0.3876 - val_accuracy: 0.8861\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4284 - accuracy: 0.8757 - val_loss: 0.3941 - val_accuracy: 0.8866\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4302 - accuracy: 0.8760 - val_loss: 0.3979 - val_accuracy: 0.8814\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4237 - accuracy: 0.8764 - val_loss: 0.5224 - val_accuracy: 0.8439\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4259 - accuracy: 0.8755 - val_loss: 0.3644 - val_accuracy: 0.8923\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4254 - accuracy: 0.8748 - val_loss: 0.3340 - val_accuracy: 0.9043\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4237 - accuracy: 0.8768 - val_loss: 0.3629 - val_accuracy: 0.8959\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4241 - accuracy: 0.8769 - val_loss: 0.3342 - val_accuracy: 0.9064\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4224 - accuracy: 0.8779 - val_loss: 0.4406 - val_accuracy: 0.8688\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4220 - accuracy: 0.8776 - val_loss: 0.4539 - val_accuracy: 0.8668\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4256 - accuracy: 0.8754 - val_loss: 0.7105 - val_accuracy: 0.7979\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4136 - accuracy: 0.8802 - val_loss: 0.3524 - val_accuracy: 0.8994\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4182 - accuracy: 0.8800 - val_loss: 0.3414 - val_accuracy: 0.9029\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4096 - accuracy: 0.8808 - val_loss: 0.3549 - val_accuracy: 0.8973\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4152 - accuracy: 0.8806 - val_loss: 0.3677 - val_accuracy: 0.8895\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4072 - accuracy: 0.8825 - val_loss: 0.3841 - val_accuracy: 0.8869\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4144 - accuracy: 0.8811 - val_loss: 0.3779 - val_accuracy: 0.8918\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4085 - accuracy: 0.8806 - val_loss: 0.4209 - val_accuracy: 0.8736\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4086 - accuracy: 0.8803 - val_loss: 0.4154 - val_accuracy: 0.8756\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4108 - accuracy: 0.8797 - val_loss: 0.3819 - val_accuracy: 0.8859\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4035 - accuracy: 0.8818 - val_loss: 0.3925 - val_accuracy: 0.8859\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4030 - accuracy: 0.8823 - val_loss: 0.6033 - val_accuracy: 0.8133\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8830 - val_loss: 0.3882 - val_accuracy: 0.8880\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3947 - accuracy: 0.8845 - val_loss: 0.3080 - val_accuracy: 0.9112\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3975 - accuracy: 0.8851 - val_loss: 0.5747 - val_accuracy: 0.8334\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8852 - val_loss: 0.5476 - val_accuracy: 0.8355\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3945 - accuracy: 0.8856 - val_loss: 0.3525 - val_accuracy: 0.9016\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3947 - accuracy: 0.8843 - val_loss: 0.3394 - val_accuracy: 0.9023\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3910 - accuracy: 0.8856 - val_loss: 0.3071 - val_accuracy: 0.9143\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3882 - accuracy: 0.8865 - val_loss: 0.3243 - val_accuracy: 0.9106\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8876 - val_loss: 0.5081 - val_accuracy: 0.8524\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3915 - accuracy: 0.8861 - val_loss: 0.3774 - val_accuracy: 0.8901\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3877 - accuracy: 0.8862 - val_loss: 0.3250 - val_accuracy: 0.9105\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3853 - accuracy: 0.8880 - val_loss: 0.3664 - val_accuracy: 0.8966\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3863 - accuracy: 0.8889 - val_loss: 0.3440 - val_accuracy: 0.9027\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3823 - accuracy: 0.8885 - val_loss: 0.3513 - val_accuracy: 0.9001\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.3834 - accuracy: 0.8885 - val_loss: 0.5346 - val_accuracy: 0.8428\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3824 - accuracy: 0.8885 - val_loss: 0.4256 - val_accuracy: 0.8771\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3848 - accuracy: 0.8880 - val_loss: 0.6180 - val_accuracy: 0.8181\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3776 - accuracy: 0.8906 - val_loss: 0.3677 - val_accuracy: 0.8944\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3823 - accuracy: 0.8897 - val_loss: 0.3329 - val_accuracy: 0.9044\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3779 - accuracy: 0.8905 - val_loss: 0.3701 - val_accuracy: 0.8940\n",
      "Try 4/100: Best_val_acc: [0.486179918050766, 0.8593888878822327], lr: 6.027611403498033e-05, Lambda: 5.842026004053601e-05\n",
      "\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7010 - accuracy: 0.1057 - val_loss: 2.2606 - val_accuracy: 0.2001\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5579 - accuracy: 0.1204 - val_loss: 2.1990 - val_accuracy: 0.2096\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4687 - accuracy: 0.1340 - val_loss: 2.2043 - val_accuracy: 0.2311\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3939 - accuracy: 0.1479 - val_loss: 2.1815 - val_accuracy: 0.2546\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3179 - accuracy: 0.1697 - val_loss: 2.1035 - val_accuracy: 0.2701\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2400 - accuracy: 0.1956 - val_loss: 2.0880 - val_accuracy: 0.2796\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1708 - accuracy: 0.2216 - val_loss: 1.9751 - val_accuracy: 0.3251\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0986 - accuracy: 0.2490 - val_loss: 1.9430 - val_accuracy: 0.3436\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0264 - accuracy: 0.2809 - val_loss: 1.8571 - val_accuracy: 0.4061\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9576 - accuracy: 0.3119 - val_loss: 1.7697 - val_accuracy: 0.4486\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8961 - accuracy: 0.3366 - val_loss: 1.7224 - val_accuracy: 0.5019\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8391 - accuracy: 0.3626 - val_loss: 1.6249 - val_accuracy: 0.5286\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7759 - accuracy: 0.3935 - val_loss: 1.5866 - val_accuracy: 0.5519\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7217 - accuracy: 0.4150 - val_loss: 1.4853 - val_accuracy: 0.6149\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6666 - accuracy: 0.4400 - val_loss: 1.4392 - val_accuracy: 0.6058\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6199 - accuracy: 0.4599 - val_loss: 1.3738 - val_accuracy: 0.6446\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5683 - accuracy: 0.4849 - val_loss: 1.3463 - val_accuracy: 0.6386\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5244 - accuracy: 0.5049 - val_loss: 1.2699 - val_accuracy: 0.6729\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4800 - accuracy: 0.5215 - val_loss: 1.2255 - val_accuracy: 0.6706\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4429 - accuracy: 0.5356 - val_loss: 1.2023 - val_accuracy: 0.6649\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4016 - accuracy: 0.5536 - val_loss: 1.1117 - val_accuracy: 0.7111\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3675 - accuracy: 0.5670 - val_loss: 1.1467 - val_accuracy: 0.6816\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3357 - accuracy: 0.5824 - val_loss: 1.1316 - val_accuracy: 0.6955\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2932 - accuracy: 0.5975 - val_loss: 1.0834 - val_accuracy: 0.7007\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2646 - accuracy: 0.6076 - val_loss: 1.0115 - val_accuracy: 0.7236\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2388 - accuracy: 0.6160 - val_loss: 1.0370 - val_accuracy: 0.7113\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2064 - accuracy: 0.6300 - val_loss: 0.8865 - val_accuracy: 0.7667\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1755 - accuracy: 0.6396 - val_loss: 0.9665 - val_accuracy: 0.7322\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1544 - accuracy: 0.6453 - val_loss: 0.9138 - val_accuracy: 0.7462\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1338 - accuracy: 0.6543 - val_loss: 0.8367 - val_accuracy: 0.7721\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1075 - accuracy: 0.6620 - val_loss: 0.8467 - val_accuracy: 0.7637\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0864 - accuracy: 0.6704 - val_loss: 0.8389 - val_accuracy: 0.7587\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0653 - accuracy: 0.6745 - val_loss: 0.8154 - val_accuracy: 0.7737\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0477 - accuracy: 0.6826 - val_loss: 0.9310 - val_accuracy: 0.7225\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0288 - accuracy: 0.6882 - val_loss: 0.8166 - val_accuracy: 0.7674\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0081 - accuracy: 0.6934 - val_loss: 0.7910 - val_accuracy: 0.7760\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9977 - accuracy: 0.6971 - val_loss: 0.8028 - val_accuracy: 0.7733\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9777 - accuracy: 0.7045 - val_loss: 0.8041 - val_accuracy: 0.7653\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9604 - accuracy: 0.7105 - val_loss: 0.8521 - val_accuracy: 0.7471\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9513 - accuracy: 0.7128 - val_loss: 0.7521 - val_accuracy: 0.7821\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9320 - accuracy: 0.7187 - val_loss: 0.6926 - val_accuracy: 0.8016\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9194 - accuracy: 0.7230 - val_loss: 0.7263 - val_accuracy: 0.7896\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9019 - accuracy: 0.7286 - val_loss: 0.7376 - val_accuracy: 0.7843\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8884 - accuracy: 0.7340 - val_loss: 0.7503 - val_accuracy: 0.7736\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8823 - accuracy: 0.7353 - val_loss: 0.6780 - val_accuracy: 0.7966\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8684 - accuracy: 0.7401 - val_loss: 0.6464 - val_accuracy: 0.8121\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8581 - accuracy: 0.7452 - val_loss: 0.7168 - val_accuracy: 0.7884\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8488 - accuracy: 0.7467 - val_loss: 0.5890 - val_accuracy: 0.8279\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8325 - accuracy: 0.7506 - val_loss: 0.6862 - val_accuracy: 0.7959\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8244 - accuracy: 0.7529 - val_loss: 0.6978 - val_accuracy: 0.7875\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8132 - accuracy: 0.7569 - val_loss: 0.5678 - val_accuracy: 0.8367\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8094 - accuracy: 0.7574 - val_loss: 0.7398 - val_accuracy: 0.7776\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7966 - accuracy: 0.7620 - val_loss: 0.6498 - val_accuracy: 0.8071\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7870 - accuracy: 0.7665 - val_loss: 0.5896 - val_accuracy: 0.8265\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7746 - accuracy: 0.7675 - val_loss: 0.5466 - val_accuracy: 0.8406\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7645 - accuracy: 0.7748 - val_loss: 0.6339 - val_accuracy: 0.8079\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7633 - accuracy: 0.7731 - val_loss: 0.5701 - val_accuracy: 0.8277\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7532 - accuracy: 0.7750 - val_loss: 0.6160 - val_accuracy: 0.8195\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7478 - accuracy: 0.7768 - val_loss: 0.6330 - val_accuracy: 0.8134\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7380 - accuracy: 0.7803 - val_loss: 0.5378 - val_accuracy: 0.8431\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7338 - accuracy: 0.7838 - val_loss: 0.5615 - val_accuracy: 0.8334\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7333 - accuracy: 0.7825 - val_loss: 0.6485 - val_accuracy: 0.8114\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7158 - accuracy: 0.7875 - val_loss: 0.6011 - val_accuracy: 0.8141\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7078 - accuracy: 0.7904 - val_loss: 0.6359 - val_accuracy: 0.8096\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7085 - accuracy: 0.7898 - val_loss: 0.5252 - val_accuracy: 0.8459\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7022 - accuracy: 0.7924 - val_loss: 0.6097 - val_accuracy: 0.8175\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6922 - accuracy: 0.7933 - val_loss: 0.5790 - val_accuracy: 0.8218\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6875 - accuracy: 0.7956 - val_loss: 0.6660 - val_accuracy: 0.7984\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6847 - accuracy: 0.7959 - val_loss: 0.5808 - val_accuracy: 0.8244\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6815 - accuracy: 0.7979 - val_loss: 0.5394 - val_accuracy: 0.8374\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6770 - accuracy: 0.8008 - val_loss: 0.5263 - val_accuracy: 0.8419\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6746 - accuracy: 0.8012 - val_loss: 0.5882 - val_accuracy: 0.8194\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6631 - accuracy: 0.8045 - val_loss: 0.5424 - val_accuracy: 0.8331\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6596 - accuracy: 0.8051 - val_loss: 0.6291 - val_accuracy: 0.8017\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6542 - accuracy: 0.8065 - val_loss: 0.5365 - val_accuracy: 0.8396\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6434 - accuracy: 0.8096 - val_loss: 0.4303 - val_accuracy: 0.8735\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6372 - accuracy: 0.8129 - val_loss: 0.6989 - val_accuracy: 0.7885\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6391 - accuracy: 0.8105 - val_loss: 0.4965 - val_accuracy: 0.8525\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6321 - accuracy: 0.8138 - val_loss: 0.6328 - val_accuracy: 0.8064\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6303 - accuracy: 0.8136 - val_loss: 0.5766 - val_accuracy: 0.8276\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6267 - accuracy: 0.8140 - val_loss: 0.5134 - val_accuracy: 0.8476\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6233 - accuracy: 0.8169 - val_loss: 0.4520 - val_accuracy: 0.8602\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6177 - accuracy: 0.8196 - val_loss: 0.5255 - val_accuracy: 0.8399\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6169 - accuracy: 0.8198 - val_loss: 0.4854 - val_accuracy: 0.8576\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6115 - accuracy: 0.8200 - val_loss: 0.4465 - val_accuracy: 0.8696\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6037 - accuracy: 0.8213 - val_loss: 0.5490 - val_accuracy: 0.8331\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6023 - accuracy: 0.8221 - val_loss: 0.5414 - val_accuracy: 0.8336\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5917 - accuracy: 0.8254 - val_loss: 0.5031 - val_accuracy: 0.8414\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5927 - accuracy: 0.8257 - val_loss: 0.4974 - val_accuracy: 0.8503\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5892 - accuracy: 0.8252 - val_loss: 0.5552 - val_accuracy: 0.8305\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5912 - accuracy: 0.8258 - val_loss: 0.4927 - val_accuracy: 0.8444\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5832 - accuracy: 0.8298 - val_loss: 0.4562 - val_accuracy: 0.8650\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5739 - accuracy: 0.8304 - val_loss: 0.4382 - val_accuracy: 0.8674\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5755 - accuracy: 0.8314 - val_loss: 0.4109 - val_accuracy: 0.8772\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5728 - accuracy: 0.8305 - val_loss: 0.5363 - val_accuracy: 0.8353\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5728 - accuracy: 0.8308 - val_loss: 0.5172 - val_accuracy: 0.8442\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5680 - accuracy: 0.8329 - val_loss: 0.5632 - val_accuracy: 0.8303\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5681 - accuracy: 0.8321 - val_loss: 0.3825 - val_accuracy: 0.8876\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5612 - accuracy: 0.8348 - val_loss: 0.4130 - val_accuracy: 0.8784\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5571 - accuracy: 0.8377 - val_loss: 0.5283 - val_accuracy: 0.8399\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5515 - accuracy: 0.8376 - val_loss: 0.4942 - val_accuracy: 0.8543\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5556 - accuracy: 0.8379 - val_loss: 0.4140 - val_accuracy: 0.8761\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5453 - accuracy: 0.8409 - val_loss: 0.6067 - val_accuracy: 0.8198\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5468 - accuracy: 0.8396 - val_loss: 0.4760 - val_accuracy: 0.8557\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5493 - accuracy: 0.8380 - val_loss: 0.5008 - val_accuracy: 0.8505\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5408 - accuracy: 0.8404 - val_loss: 0.5035 - val_accuracy: 0.8453\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5383 - accuracy: 0.8408 - val_loss: 0.4377 - val_accuracy: 0.8697\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5343 - accuracy: 0.8448 - val_loss: 0.3842 - val_accuracy: 0.8884\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5352 - accuracy: 0.8419 - val_loss: 0.4638 - val_accuracy: 0.8574\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5308 - accuracy: 0.8440 - val_loss: 0.4272 - val_accuracy: 0.8723\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5258 - accuracy: 0.8458 - val_loss: 0.4614 - val_accuracy: 0.8585\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5318 - accuracy: 0.8437 - val_loss: 0.4186 - val_accuracy: 0.8744\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5254 - accuracy: 0.8467 - val_loss: 0.6812 - val_accuracy: 0.7931\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5241 - accuracy: 0.8468 - val_loss: 0.4573 - val_accuracy: 0.8637\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5191 - accuracy: 0.8471 - val_loss: 0.3907 - val_accuracy: 0.8812\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5129 - accuracy: 0.8505 - val_loss: 0.4379 - val_accuracy: 0.8689\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5194 - accuracy: 0.8468 - val_loss: 0.4191 - val_accuracy: 0.8759\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8521 - val_loss: 0.4363 - val_accuracy: 0.8717\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5083 - accuracy: 0.8512 - val_loss: 0.4727 - val_accuracy: 0.8599\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5061 - accuracy: 0.8507 - val_loss: 0.4447 - val_accuracy: 0.8704\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5085 - accuracy: 0.8512 - val_loss: 0.3781 - val_accuracy: 0.8903\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5015 - accuracy: 0.8522 - val_loss: 0.4312 - val_accuracy: 0.8679\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4978 - accuracy: 0.8533 - val_loss: 0.4638 - val_accuracy: 0.8599\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4937 - accuracy: 0.8557 - val_loss: 0.3564 - val_accuracy: 0.8966\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4980 - accuracy: 0.8540 - val_loss: 0.3567 - val_accuracy: 0.8925\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4909 - accuracy: 0.8564 - val_loss: 0.4810 - val_accuracy: 0.8536\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4954 - accuracy: 0.8544 - val_loss: 0.3777 - val_accuracy: 0.8901\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4872 - accuracy: 0.8567 - val_loss: 0.4016 - val_accuracy: 0.8814\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4904 - accuracy: 0.8537 - val_loss: 0.3743 - val_accuracy: 0.8920\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4829 - accuracy: 0.8601 - val_loss: 0.4749 - val_accuracy: 0.8546\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4839 - accuracy: 0.8578 - val_loss: 0.4055 - val_accuracy: 0.8718\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4833 - accuracy: 0.8584 - val_loss: 0.4079 - val_accuracy: 0.8796\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4743 - accuracy: 0.8639 - val_loss: 0.4007 - val_accuracy: 0.8784\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4818 - accuracy: 0.8605 - val_loss: 0.4826 - val_accuracy: 0.8572\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4783 - accuracy: 0.8597 - val_loss: 0.4497 - val_accuracy: 0.8607\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4728 - accuracy: 0.8618 - val_loss: 0.4726 - val_accuracy: 0.8571\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4754 - accuracy: 0.8619 - val_loss: 0.3709 - val_accuracy: 0.8896\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4645 - accuracy: 0.8637 - val_loss: 0.5095 - val_accuracy: 0.8394\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4629 - accuracy: 0.8650 - val_loss: 0.4961 - val_accuracy: 0.8524\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4689 - accuracy: 0.8623 - val_loss: 0.5579 - val_accuracy: 0.8294\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4647 - accuracy: 0.8648 - val_loss: 0.5144 - val_accuracy: 0.8386\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4652 - accuracy: 0.8641 - val_loss: 0.3414 - val_accuracy: 0.9018\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4597 - accuracy: 0.8663 - val_loss: 0.4986 - val_accuracy: 0.8485\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4627 - accuracy: 0.8638 - val_loss: 0.4000 - val_accuracy: 0.8816\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4576 - accuracy: 0.8656 - val_loss: 0.4155 - val_accuracy: 0.8729\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4495 - accuracy: 0.8700 - val_loss: 0.3619 - val_accuracy: 0.8930\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4505 - accuracy: 0.8686 - val_loss: 0.4325 - val_accuracy: 0.8714\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4458 - accuracy: 0.8697 - val_loss: 0.4195 - val_accuracy: 0.8726\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4467 - accuracy: 0.8696 - val_loss: 0.3794 - val_accuracy: 0.8866\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4425 - accuracy: 0.8710 - val_loss: 0.4309 - val_accuracy: 0.8717\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4434 - accuracy: 0.8692 - val_loss: 0.4014 - val_accuracy: 0.8831\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4459 - accuracy: 0.8695 - val_loss: 0.5258 - val_accuracy: 0.8452\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4420 - accuracy: 0.8702 - val_loss: 0.4064 - val_accuracy: 0.8803\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4425 - accuracy: 0.8714 - val_loss: 0.3801 - val_accuracy: 0.8840\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4464 - accuracy: 0.8693 - val_loss: 0.4320 - val_accuracy: 0.8739\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4343 - accuracy: 0.8726 - val_loss: 0.3646 - val_accuracy: 0.8894\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4372 - accuracy: 0.8730 - val_loss: 0.4005 - val_accuracy: 0.8767\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4286 - accuracy: 0.8744 - val_loss: 0.4430 - val_accuracy: 0.8690\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4362 - accuracy: 0.8731 - val_loss: 0.4692 - val_accuracy: 0.8617\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4283 - accuracy: 0.8754 - val_loss: 0.3583 - val_accuracy: 0.8954\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4314 - accuracy: 0.8755 - val_loss: 0.3969 - val_accuracy: 0.8834\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4291 - accuracy: 0.8745 - val_loss: 0.5376 - val_accuracy: 0.8416\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4282 - accuracy: 0.8751 - val_loss: 0.5016 - val_accuracy: 0.8497\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4312 - accuracy: 0.8753 - val_loss: 0.3862 - val_accuracy: 0.8887\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4245 - accuracy: 0.8768 - val_loss: 0.2988 - val_accuracy: 0.9126\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4208 - accuracy: 0.8760 - val_loss: 0.3540 - val_accuracy: 0.8987\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4268 - accuracy: 0.8764 - val_loss: 0.4131 - val_accuracy: 0.8741\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4171 - accuracy: 0.8778 - val_loss: 0.4445 - val_accuracy: 0.8660\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4189 - accuracy: 0.8777 - val_loss: 0.3633 - val_accuracy: 0.8922\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4185 - accuracy: 0.8776 - val_loss: 0.4017 - val_accuracy: 0.8811\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4207 - accuracy: 0.8780 - val_loss: 0.3329 - val_accuracy: 0.9032\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4119 - accuracy: 0.8785 - val_loss: 0.3901 - val_accuracy: 0.8849\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4109 - accuracy: 0.8807 - val_loss: 0.3529 - val_accuracy: 0.8923\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4075 - accuracy: 0.8815 - val_loss: 0.3163 - val_accuracy: 0.9095\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4178 - accuracy: 0.8774 - val_loss: 0.3296 - val_accuracy: 0.9061\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4080 - accuracy: 0.8822 - val_loss: 0.3967 - val_accuracy: 0.8836\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8807 - val_loss: 0.3291 - val_accuracy: 0.9055\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4047 - accuracy: 0.8821 - val_loss: 0.4501 - val_accuracy: 0.8655\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4118 - accuracy: 0.8805 - val_loss: 0.5556 - val_accuracy: 0.8363\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4031 - accuracy: 0.8840 - val_loss: 0.4198 - val_accuracy: 0.8683\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4001 - accuracy: 0.8855 - val_loss: 0.3640 - val_accuracy: 0.8941\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4051 - accuracy: 0.8807 - val_loss: 0.3774 - val_accuracy: 0.8869\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4011 - accuracy: 0.8827 - val_loss: 0.3505 - val_accuracy: 0.8982\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3943 - accuracy: 0.8852 - val_loss: 0.4377 - val_accuracy: 0.8693\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3976 - accuracy: 0.8841 - val_loss: 0.3973 - val_accuracy: 0.8867\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3929 - accuracy: 0.8864 - val_loss: 0.3950 - val_accuracy: 0.8832\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3935 - accuracy: 0.8850 - val_loss: 0.3906 - val_accuracy: 0.8810\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3938 - accuracy: 0.8865 - val_loss: 0.3733 - val_accuracy: 0.8904\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3955 - accuracy: 0.8861 - val_loss: 0.4678 - val_accuracy: 0.8677\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3912 - accuracy: 0.8857 - val_loss: 0.3719 - val_accuracy: 0.8911\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3918 - accuracy: 0.8877 - val_loss: 0.3817 - val_accuracy: 0.8888\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3882 - accuracy: 0.8865 - val_loss: 0.3808 - val_accuracy: 0.8904\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3864 - accuracy: 0.8882 - val_loss: 0.4011 - val_accuracy: 0.8731\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3869 - accuracy: 0.8872 - val_loss: 0.4548 - val_accuracy: 0.8655\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3902 - accuracy: 0.8880 - val_loss: 0.3026 - val_accuracy: 0.9126\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3784 - accuracy: 0.8897 - val_loss: 0.3926 - val_accuracy: 0.8852\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3829 - accuracy: 0.8903 - val_loss: 0.3971 - val_accuracy: 0.8854\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8877 - val_loss: 0.3884 - val_accuracy: 0.8859\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3794 - accuracy: 0.8894 - val_loss: 0.3733 - val_accuracy: 0.8919\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3786 - accuracy: 0.8896 - val_loss: 0.4007 - val_accuracy: 0.8844\n",
      "Try 5/100: Best_val_acc: [0.4949725568294525, 0.8540555834770203], lr: 6.066189007840183e-05, Lambda: 5.852313827351977e-05\n",
      "\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7699 - accuracy: 0.1069 - val_loss: 2.3922 - val_accuracy: 0.1453\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.6132 - accuracy: 0.1218 - val_loss: 2.3644 - val_accuracy: 0.1828\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5205 - accuracy: 0.1308 - val_loss: 2.2539 - val_accuracy: 0.2246\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4317 - accuracy: 0.1466 - val_loss: 2.1821 - val_accuracy: 0.2392\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3444 - accuracy: 0.1690 - val_loss: 2.1189 - val_accuracy: 0.2683\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2646 - accuracy: 0.1941 - val_loss: 2.0928 - val_accuracy: 0.2520\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1904 - accuracy: 0.2194 - val_loss: 2.0317 - val_accuracy: 0.2838\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1104 - accuracy: 0.2495 - val_loss: 1.9772 - val_accuracy: 0.3224\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0376 - accuracy: 0.2790 - val_loss: 1.8865 - val_accuracy: 0.3770\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9738 - accuracy: 0.3075 - val_loss: 1.8357 - val_accuracy: 0.4091\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9093 - accuracy: 0.3332 - val_loss: 1.8290 - val_accuracy: 0.4052\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8532 - accuracy: 0.3574 - val_loss: 1.6383 - val_accuracy: 0.5259\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7928 - accuracy: 0.3835 - val_loss: 1.5338 - val_accuracy: 0.5389\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7400 - accuracy: 0.4111 - val_loss: 1.5246 - val_accuracy: 0.5494\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6848 - accuracy: 0.4354 - val_loss: 1.4721 - val_accuracy: 0.5739\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6335 - accuracy: 0.4588 - val_loss: 1.3980 - val_accuracy: 0.6029\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5828 - accuracy: 0.4799 - val_loss: 1.3899 - val_accuracy: 0.6126\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5399 - accuracy: 0.4989 - val_loss: 1.3192 - val_accuracy: 0.6385\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4919 - accuracy: 0.5145 - val_loss: 1.2472 - val_accuracy: 0.6705\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4556 - accuracy: 0.5302 - val_loss: 1.2439 - val_accuracy: 0.6578\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4165 - accuracy: 0.5475 - val_loss: 1.1794 - val_accuracy: 0.6844\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3851 - accuracy: 0.5623 - val_loss: 1.1521 - val_accuracy: 0.6786\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3529 - accuracy: 0.5696 - val_loss: 1.1542 - val_accuracy: 0.6766\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3163 - accuracy: 0.5842 - val_loss: 1.0496 - val_accuracy: 0.7151\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2853 - accuracy: 0.5941 - val_loss: 1.0328 - val_accuracy: 0.7149\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2616 - accuracy: 0.6040 - val_loss: 1.0212 - val_accuracy: 0.7191\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2299 - accuracy: 0.6146 - val_loss: 1.0489 - val_accuracy: 0.7062\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2097 - accuracy: 0.6237 - val_loss: 0.9311 - val_accuracy: 0.7467\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1792 - accuracy: 0.6361 - val_loss: 0.9018 - val_accuracy: 0.7517\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1603 - accuracy: 0.6415 - val_loss: 0.8438 - val_accuracy: 0.7722\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1378 - accuracy: 0.6490 - val_loss: 0.8797 - val_accuracy: 0.7541\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1122 - accuracy: 0.6605 - val_loss: 0.8981 - val_accuracy: 0.7261\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0903 - accuracy: 0.6655 - val_loss: 0.8732 - val_accuracy: 0.7546\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0710 - accuracy: 0.6700 - val_loss: 0.8173 - val_accuracy: 0.7698\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0503 - accuracy: 0.6768 - val_loss: 0.7907 - val_accuracy: 0.7712\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0352 - accuracy: 0.6848 - val_loss: 0.7618 - val_accuracy: 0.7879\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0195 - accuracy: 0.6895 - val_loss: 0.7985 - val_accuracy: 0.7682\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0017 - accuracy: 0.6953 - val_loss: 0.7981 - val_accuracy: 0.7590\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9875 - accuracy: 0.6989 - val_loss: 0.7596 - val_accuracy: 0.7820\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9693 - accuracy: 0.7094 - val_loss: 0.7513 - val_accuracy: 0.7843\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9539 - accuracy: 0.7120 - val_loss: 0.7140 - val_accuracy: 0.7926\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9463 - accuracy: 0.7132 - val_loss: 0.6853 - val_accuracy: 0.8113\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9283 - accuracy: 0.7174 - val_loss: 0.7631 - val_accuracy: 0.7811\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9147 - accuracy: 0.7246 - val_loss: 0.7496 - val_accuracy: 0.7782\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9005 - accuracy: 0.7308 - val_loss: 0.7698 - val_accuracy: 0.7707\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8926 - accuracy: 0.7322 - val_loss: 0.7256 - val_accuracy: 0.7826\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8779 - accuracy: 0.7373 - val_loss: 0.6732 - val_accuracy: 0.8046\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8653 - accuracy: 0.7394 - val_loss: 0.7702 - val_accuracy: 0.7703\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8555 - accuracy: 0.7422 - val_loss: 0.7011 - val_accuracy: 0.7928\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8427 - accuracy: 0.7474 - val_loss: 0.6954 - val_accuracy: 0.7909\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8329 - accuracy: 0.7488 - val_loss: 0.7092 - val_accuracy: 0.7918\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8312 - accuracy: 0.7498 - val_loss: 0.6584 - val_accuracy: 0.8030\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8187 - accuracy: 0.7553 - val_loss: 0.7936 - val_accuracy: 0.7618\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8081 - accuracy: 0.7599 - val_loss: 0.6604 - val_accuracy: 0.7988\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7994 - accuracy: 0.7636 - val_loss: 0.5892 - val_accuracy: 0.8226\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7883 - accuracy: 0.7655 - val_loss: 0.6242 - val_accuracy: 0.8101\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7857 - accuracy: 0.7651 - val_loss: 0.7446 - val_accuracy: 0.7736\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7736 - accuracy: 0.7691 - val_loss: 0.6212 - val_accuracy: 0.8141\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7688 - accuracy: 0.7721 - val_loss: 0.6300 - val_accuracy: 0.8121\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7575 - accuracy: 0.7734 - val_loss: 0.6041 - val_accuracy: 0.8224\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7460 - accuracy: 0.7769 - val_loss: 0.5827 - val_accuracy: 0.8249\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7509 - accuracy: 0.7764 - val_loss: 0.6307 - val_accuracy: 0.8123\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7353 - accuracy: 0.7824 - val_loss: 0.6486 - val_accuracy: 0.8051\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7320 - accuracy: 0.7847 - val_loss: 0.6103 - val_accuracy: 0.8182\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7241 - accuracy: 0.7867 - val_loss: 0.5950 - val_accuracy: 0.8236\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7215 - accuracy: 0.7843 - val_loss: 0.5972 - val_accuracy: 0.8165\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7112 - accuracy: 0.7882 - val_loss: 0.5565 - val_accuracy: 0.8372\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7120 - accuracy: 0.7893 - val_loss: 0.5321 - val_accuracy: 0.8364\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6982 - accuracy: 0.7925 - val_loss: 0.6518 - val_accuracy: 0.8051\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6941 - accuracy: 0.7948 - val_loss: 0.5763 - val_accuracy: 0.8237\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6887 - accuracy: 0.7977 - val_loss: 0.5210 - val_accuracy: 0.8406\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6843 - accuracy: 0.7978 - val_loss: 0.5668 - val_accuracy: 0.8239\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6762 - accuracy: 0.7996 - val_loss: 0.4832 - val_accuracy: 0.8559\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6697 - accuracy: 0.8015 - val_loss: 0.4765 - val_accuracy: 0.8572\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6694 - accuracy: 0.8025 - val_loss: 0.6982 - val_accuracy: 0.7921\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6636 - accuracy: 0.8052 - val_loss: 0.5799 - val_accuracy: 0.8276\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6596 - accuracy: 0.8055 - val_loss: 0.5093 - val_accuracy: 0.8458\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6569 - accuracy: 0.8055 - val_loss: 0.5836 - val_accuracy: 0.8280\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6560 - accuracy: 0.8070 - val_loss: 0.5228 - val_accuracy: 0.8449\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6406 - accuracy: 0.8136 - val_loss: 0.5293 - val_accuracy: 0.8414\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6365 - accuracy: 0.8107 - val_loss: 0.5120 - val_accuracy: 0.8511\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6343 - accuracy: 0.8131 - val_loss: 0.4366 - val_accuracy: 0.8723\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6274 - accuracy: 0.8152 - val_loss: 0.5030 - val_accuracy: 0.8447\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6230 - accuracy: 0.8175 - val_loss: 0.5428 - val_accuracy: 0.8396\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6267 - accuracy: 0.8150 - val_loss: 0.4568 - val_accuracy: 0.8621\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6135 - accuracy: 0.8194 - val_loss: 0.5782 - val_accuracy: 0.8264\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6141 - accuracy: 0.8198 - val_loss: 0.5558 - val_accuracy: 0.8356\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6111 - accuracy: 0.8204 - val_loss: 0.4572 - val_accuracy: 0.8695\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6001 - accuracy: 0.8224 - val_loss: 0.4552 - val_accuracy: 0.8624\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6104 - accuracy: 0.8204 - val_loss: 0.4355 - val_accuracy: 0.8740\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6018 - accuracy: 0.8223 - val_loss: 0.5083 - val_accuracy: 0.8474\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5965 - accuracy: 0.8243 - val_loss: 0.4675 - val_accuracy: 0.8625\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5944 - accuracy: 0.8251 - val_loss: 0.5039 - val_accuracy: 0.8502\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5892 - accuracy: 0.8276 - val_loss: 0.4293 - val_accuracy: 0.8764\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5922 - accuracy: 0.8265 - val_loss: 0.5683 - val_accuracy: 0.8349\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5870 - accuracy: 0.8298 - val_loss: 0.6123 - val_accuracy: 0.8190\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5837 - accuracy: 0.8285 - val_loss: 0.4858 - val_accuracy: 0.8600\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5704 - accuracy: 0.8322 - val_loss: 0.4889 - val_accuracy: 0.8544\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5701 - accuracy: 0.8329 - val_loss: 0.4984 - val_accuracy: 0.8559\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5707 - accuracy: 0.8332 - val_loss: 0.4920 - val_accuracy: 0.8516\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5601 - accuracy: 0.8372 - val_loss: 0.4348 - val_accuracy: 0.8738\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5629 - accuracy: 0.8348 - val_loss: 0.4200 - val_accuracy: 0.8733\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5637 - accuracy: 0.8342 - val_loss: 0.4932 - val_accuracy: 0.8594\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5561 - accuracy: 0.8377 - val_loss: 0.4752 - val_accuracy: 0.8611\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5583 - accuracy: 0.8364 - val_loss: 0.4862 - val_accuracy: 0.8568\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5546 - accuracy: 0.8369 - val_loss: 0.4104 - val_accuracy: 0.8782\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5499 - accuracy: 0.8390 - val_loss: 0.4188 - val_accuracy: 0.8761\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5501 - accuracy: 0.8403 - val_loss: 0.4563 - val_accuracy: 0.8670\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5445 - accuracy: 0.8405 - val_loss: 0.4523 - val_accuracy: 0.8613\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5381 - accuracy: 0.8426 - val_loss: 0.3985 - val_accuracy: 0.8839\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5387 - accuracy: 0.8420 - val_loss: 0.4708 - val_accuracy: 0.8650\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5323 - accuracy: 0.8443 - val_loss: 0.4768 - val_accuracy: 0.8621\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5379 - accuracy: 0.8422 - val_loss: 0.4723 - val_accuracy: 0.8555\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5341 - accuracy: 0.8443 - val_loss: 0.4907 - val_accuracy: 0.8486\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5339 - accuracy: 0.8429 - val_loss: 0.4306 - val_accuracy: 0.8705\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5310 - accuracy: 0.8443 - val_loss: 0.4511 - val_accuracy: 0.8659\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5256 - accuracy: 0.8472 - val_loss: 0.4000 - val_accuracy: 0.8840\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5183 - accuracy: 0.8478 - val_loss: 0.4036 - val_accuracy: 0.8829\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5182 - accuracy: 0.8498 - val_loss: 0.3830 - val_accuracy: 0.8876\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5144 - accuracy: 0.8508 - val_loss: 0.4576 - val_accuracy: 0.8643\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5110 - accuracy: 0.8511 - val_loss: 0.4624 - val_accuracy: 0.8602\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5172 - accuracy: 0.8485 - val_loss: 0.4990 - val_accuracy: 0.8534\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5090 - accuracy: 0.8525 - val_loss: 0.4101 - val_accuracy: 0.8832\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5047 - accuracy: 0.8543 - val_loss: 0.4424 - val_accuracy: 0.8677\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5028 - accuracy: 0.8521 - val_loss: 0.4401 - val_accuracy: 0.8708\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5008 - accuracy: 0.8543 - val_loss: 0.4184 - val_accuracy: 0.8723\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4980 - accuracy: 0.8554 - val_loss: 0.5300 - val_accuracy: 0.8377\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5009 - accuracy: 0.8553 - val_loss: 0.3785 - val_accuracy: 0.8899\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4964 - accuracy: 0.8551 - val_loss: 0.3798 - val_accuracy: 0.8904\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4899 - accuracy: 0.8565 - val_loss: 0.4105 - val_accuracy: 0.8760\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4958 - accuracy: 0.8554 - val_loss: 0.3984 - val_accuracy: 0.8866\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4956 - accuracy: 0.8543 - val_loss: 0.5019 - val_accuracy: 0.8471\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4965 - accuracy: 0.8554 - val_loss: 0.5473 - val_accuracy: 0.8353\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4833 - accuracy: 0.8596 - val_loss: 0.4423 - val_accuracy: 0.8714\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4820 - accuracy: 0.8581 - val_loss: 0.4491 - val_accuracy: 0.8709\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4817 - accuracy: 0.8614 - val_loss: 0.4326 - val_accuracy: 0.8766\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4812 - accuracy: 0.8597 - val_loss: 0.5383 - val_accuracy: 0.8512\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4773 - accuracy: 0.8613 - val_loss: 0.4050 - val_accuracy: 0.8829\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4782 - accuracy: 0.8622 - val_loss: 0.3787 - val_accuracy: 0.8873\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4727 - accuracy: 0.8628 - val_loss: 0.4650 - val_accuracy: 0.8633\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4725 - accuracy: 0.8627 - val_loss: 0.3912 - val_accuracy: 0.8870\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4686 - accuracy: 0.8637 - val_loss: 0.3796 - val_accuracy: 0.8904\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4671 - accuracy: 0.8632 - val_loss: 0.4079 - val_accuracy: 0.8815\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4656 - accuracy: 0.8635 - val_loss: 0.4241 - val_accuracy: 0.8744\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4598 - accuracy: 0.8658 - val_loss: 0.4039 - val_accuracy: 0.8855\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4593 - accuracy: 0.8647 - val_loss: 0.3878 - val_accuracy: 0.8871\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4628 - accuracy: 0.8649 - val_loss: 0.4248 - val_accuracy: 0.8764\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4597 - accuracy: 0.8646 - val_loss: 0.3823 - val_accuracy: 0.8871\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4579 - accuracy: 0.8655 - val_loss: 0.4869 - val_accuracy: 0.8566\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4567 - accuracy: 0.8683 - val_loss: 0.3824 - val_accuracy: 0.8901\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4569 - accuracy: 0.8669 - val_loss: 0.3922 - val_accuracy: 0.8868\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4569 - accuracy: 0.8683 - val_loss: 0.4076 - val_accuracy: 0.8780\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4492 - accuracy: 0.8713 - val_loss: 0.3471 - val_accuracy: 0.8999\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4519 - accuracy: 0.8679 - val_loss: 0.3376 - val_accuracy: 0.9072\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4431 - accuracy: 0.8699 - val_loss: 0.3710 - val_accuracy: 0.8936\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4418 - accuracy: 0.8719 - val_loss: 0.3396 - val_accuracy: 0.9044\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4424 - accuracy: 0.8715 - val_loss: 0.3665 - val_accuracy: 0.8941\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4409 - accuracy: 0.8715 - val_loss: 0.3664 - val_accuracy: 0.8959\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4384 - accuracy: 0.8725 - val_loss: 0.4068 - val_accuracy: 0.8821\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4395 - accuracy: 0.8717 - val_loss: 0.3662 - val_accuracy: 0.8926\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4358 - accuracy: 0.8753 - val_loss: 0.3642 - val_accuracy: 0.8964\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4394 - accuracy: 0.8736 - val_loss: 0.4593 - val_accuracy: 0.8677\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4330 - accuracy: 0.8739 - val_loss: 0.3493 - val_accuracy: 0.8999\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4353 - accuracy: 0.8741 - val_loss: 0.3976 - val_accuracy: 0.8870\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4311 - accuracy: 0.8749 - val_loss: 0.4503 - val_accuracy: 0.8655\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4360 - accuracy: 0.8753 - val_loss: 0.3655 - val_accuracy: 0.8924\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4292 - accuracy: 0.8751 - val_loss: 0.4257 - val_accuracy: 0.8759\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4233 - accuracy: 0.8766 - val_loss: 0.3478 - val_accuracy: 0.9004\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4204 - accuracy: 0.8775 - val_loss: 0.4702 - val_accuracy: 0.8654\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4233 - accuracy: 0.8772 - val_loss: 0.3759 - val_accuracy: 0.8917\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4124 - accuracy: 0.8792 - val_loss: 0.3461 - val_accuracy: 0.9039\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4193 - accuracy: 0.8778 - val_loss: 0.2994 - val_accuracy: 0.9185\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4199 - accuracy: 0.8784 - val_loss: 0.3702 - val_accuracy: 0.8929\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4170 - accuracy: 0.8802 - val_loss: 0.5065 - val_accuracy: 0.8549\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4165 - accuracy: 0.8783 - val_loss: 0.4397 - val_accuracy: 0.8749\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4123 - accuracy: 0.8801 - val_loss: 0.3873 - val_accuracy: 0.8881\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4141 - accuracy: 0.8808 - val_loss: 0.3477 - val_accuracy: 0.9010\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4099 - accuracy: 0.8815 - val_loss: 0.3520 - val_accuracy: 0.8995\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4160 - accuracy: 0.8806 - val_loss: 0.3560 - val_accuracy: 0.8970\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4128 - accuracy: 0.8801 - val_loss: 0.4332 - val_accuracy: 0.8736\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4119 - accuracy: 0.8820 - val_loss: 0.5025 - val_accuracy: 0.8569\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4106 - accuracy: 0.8818 - val_loss: 0.5855 - val_accuracy: 0.8329\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8836 - val_loss: 0.3123 - val_accuracy: 0.9126\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8816 - val_loss: 0.3612 - val_accuracy: 0.8942\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4000 - accuracy: 0.8833 - val_loss: 0.3951 - val_accuracy: 0.8838\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4011 - accuracy: 0.8835 - val_loss: 0.4292 - val_accuracy: 0.8753\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8832 - val_loss: 0.3375 - val_accuracy: 0.8987\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3969 - accuracy: 0.8851 - val_loss: 0.3448 - val_accuracy: 0.9017\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4038 - accuracy: 0.8842 - val_loss: 0.4755 - val_accuracy: 0.8636\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.3973 - accuracy: 0.8846 - val_loss: 0.4515 - val_accuracy: 0.8694\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8853 - val_loss: 0.3868 - val_accuracy: 0.8881\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3983 - accuracy: 0.8848 - val_loss: 0.3236 - val_accuracy: 0.9059\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3931 - accuracy: 0.8860 - val_loss: 0.3182 - val_accuracy: 0.9102\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3916 - accuracy: 0.8860 - val_loss: 0.5122 - val_accuracy: 0.8569\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3942 - accuracy: 0.8875 - val_loss: 0.3643 - val_accuracy: 0.8914\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3959 - accuracy: 0.8846 - val_loss: 0.5924 - val_accuracy: 0.8344\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3869 - accuracy: 0.8871 - val_loss: 0.4249 - val_accuracy: 0.8766\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8879 - val_loss: 0.5376 - val_accuracy: 0.8441\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3865 - accuracy: 0.8888 - val_loss: 0.4103 - val_accuracy: 0.8817\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3815 - accuracy: 0.8889 - val_loss: 0.3440 - val_accuracy: 0.8977\n",
      "Try 6/100: Best_val_acc: [0.48789340257644653, 0.8567222356796265], lr: 6.0936080670950044e-05, Lambda: 5.8154511078666894e-05\n",
      "\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6715 - accuracy: 0.1077 - val_loss: 2.2783 - val_accuracy: 0.1463\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5575 - accuracy: 0.1196 - val_loss: 2.2146 - val_accuracy: 0.1862\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4702 - accuracy: 0.1357 - val_loss: 2.1191 - val_accuracy: 0.2603\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3968 - accuracy: 0.1541 - val_loss: 2.0701 - val_accuracy: 0.3241\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3295 - accuracy: 0.1724 - val_loss: 2.0327 - val_accuracy: 0.3309\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2623 - accuracy: 0.1957 - val_loss: 1.9081 - val_accuracy: 0.4302\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1786 - accuracy: 0.2270 - val_loss: 1.8719 - val_accuracy: 0.4359\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1038 - accuracy: 0.2600 - val_loss: 1.8078 - val_accuracy: 0.4926\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0178 - accuracy: 0.2915 - val_loss: 1.7173 - val_accuracy: 0.5259\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9330 - accuracy: 0.3275 - val_loss: 1.6036 - val_accuracy: 0.5675\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8667 - accuracy: 0.3544 - val_loss: 1.5218 - val_accuracy: 0.5966\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7997 - accuracy: 0.3846 - val_loss: 1.4463 - val_accuracy: 0.6356\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7309 - accuracy: 0.4137 - val_loss: 1.4200 - val_accuracy: 0.6316\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6754 - accuracy: 0.4390 - val_loss: 1.3209 - val_accuracy: 0.6571\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6264 - accuracy: 0.4608 - val_loss: 1.3679 - val_accuracy: 0.6367\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5777 - accuracy: 0.4838 - val_loss: 1.3019 - val_accuracy: 0.6584\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5302 - accuracy: 0.5037 - val_loss: 1.2526 - val_accuracy: 0.6704\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4896 - accuracy: 0.5202 - val_loss: 1.1013 - val_accuracy: 0.7139\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4436 - accuracy: 0.5387 - val_loss: 1.1218 - val_accuracy: 0.7014\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4098 - accuracy: 0.5537 - val_loss: 1.0305 - val_accuracy: 0.7321\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3700 - accuracy: 0.5674 - val_loss: 1.0769 - val_accuracy: 0.7076\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3426 - accuracy: 0.5768 - val_loss: 0.9485 - val_accuracy: 0.7449\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3079 - accuracy: 0.5915 - val_loss: 1.0096 - val_accuracy: 0.7189\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2796 - accuracy: 0.5996 - val_loss: 0.9352 - val_accuracy: 0.7431\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2508 - accuracy: 0.6129 - val_loss: 1.0043 - val_accuracy: 0.7251\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2201 - accuracy: 0.6199 - val_loss: 1.0454 - val_accuracy: 0.7031\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1929 - accuracy: 0.6304 - val_loss: 0.9327 - val_accuracy: 0.7204\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1706 - accuracy: 0.6385 - val_loss: 1.0844 - val_accuracy: 0.6826\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1455 - accuracy: 0.6490 - val_loss: 0.8047 - val_accuracy: 0.7857\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1242 - accuracy: 0.6568 - val_loss: 0.9703 - val_accuracy: 0.7183\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1024 - accuracy: 0.6631 - val_loss: 0.8276 - val_accuracy: 0.7623\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0776 - accuracy: 0.6704 - val_loss: 0.8689 - val_accuracy: 0.7564\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0570 - accuracy: 0.6781 - val_loss: 0.7832 - val_accuracy: 0.7789\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0436 - accuracy: 0.6806 - val_loss: 0.9024 - val_accuracy: 0.7304\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0244 - accuracy: 0.6873 - val_loss: 0.7725 - val_accuracy: 0.7781\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0033 - accuracy: 0.6967 - val_loss: 0.7135 - val_accuracy: 0.7890\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9894 - accuracy: 0.7002 - val_loss: 0.7405 - val_accuracy: 0.7804\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9725 - accuracy: 0.7040 - val_loss: 0.7508 - val_accuracy: 0.7863\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9564 - accuracy: 0.7124 - val_loss: 0.8179 - val_accuracy: 0.7504\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9386 - accuracy: 0.7171 - val_loss: 0.6189 - val_accuracy: 0.8215\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9259 - accuracy: 0.7217 - val_loss: 0.8380 - val_accuracy: 0.7479\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9117 - accuracy: 0.7222 - val_loss: 0.6740 - val_accuracy: 0.8081\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9016 - accuracy: 0.7264 - val_loss: 0.6245 - val_accuracy: 0.8218\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8861 - accuracy: 0.7319 - val_loss: 0.6520 - val_accuracy: 0.8060\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8752 - accuracy: 0.7355 - val_loss: 0.5730 - val_accuracy: 0.8346\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8675 - accuracy: 0.7367 - val_loss: 0.6514 - val_accuracy: 0.8119\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8490 - accuracy: 0.7453 - val_loss: 0.6946 - val_accuracy: 0.7875\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8393 - accuracy: 0.7461 - val_loss: 0.6546 - val_accuracy: 0.8015\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8317 - accuracy: 0.7511 - val_loss: 0.5770 - val_accuracy: 0.8328\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8210 - accuracy: 0.7544 - val_loss: 0.6442 - val_accuracy: 0.8076\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8111 - accuracy: 0.7577 - val_loss: 0.5740 - val_accuracy: 0.8341\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7998 - accuracy: 0.7618 - val_loss: 0.6988 - val_accuracy: 0.7872\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7890 - accuracy: 0.7637 - val_loss: 0.6685 - val_accuracy: 0.7982\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7853 - accuracy: 0.7645 - val_loss: 0.6501 - val_accuracy: 0.8046\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7781 - accuracy: 0.7661 - val_loss: 0.5598 - val_accuracy: 0.8364\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7628 - accuracy: 0.7743 - val_loss: 0.5964 - val_accuracy: 0.8243\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7581 - accuracy: 0.7735 - val_loss: 0.5356 - val_accuracy: 0.8441\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7541 - accuracy: 0.7744 - val_loss: 0.5869 - val_accuracy: 0.8234\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7482 - accuracy: 0.7766 - val_loss: 0.6328 - val_accuracy: 0.8153\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7358 - accuracy: 0.7798 - val_loss: 0.5048 - val_accuracy: 0.8526\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7331 - accuracy: 0.7833 - val_loss: 0.6534 - val_accuracy: 0.8052\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7292 - accuracy: 0.7825 - val_loss: 0.5897 - val_accuracy: 0.8245\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7223 - accuracy: 0.7856 - val_loss: 0.6233 - val_accuracy: 0.8135\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7131 - accuracy: 0.7867 - val_loss: 0.5387 - val_accuracy: 0.8408\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7031 - accuracy: 0.7916 - val_loss: 0.6256 - val_accuracy: 0.8123\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6991 - accuracy: 0.7924 - val_loss: 0.5501 - val_accuracy: 0.8354\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6868 - accuracy: 0.7955 - val_loss: 0.4994 - val_accuracy: 0.8539\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6802 - accuracy: 0.7998 - val_loss: 0.6739 - val_accuracy: 0.7919\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6811 - accuracy: 0.7968 - val_loss: 0.6760 - val_accuracy: 0.7981\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6714 - accuracy: 0.8012 - val_loss: 0.5504 - val_accuracy: 0.8316\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6762 - accuracy: 0.8003 - val_loss: 0.4822 - val_accuracy: 0.8595\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6639 - accuracy: 0.8037 - val_loss: 0.4305 - val_accuracy: 0.8721\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6609 - accuracy: 0.8046 - val_loss: 0.4426 - val_accuracy: 0.8736\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6486 - accuracy: 0.8084 - val_loss: 0.4874 - val_accuracy: 0.8581\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6530 - accuracy: 0.8045 - val_loss: 0.4969 - val_accuracy: 0.8546\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6459 - accuracy: 0.8093 - val_loss: 0.5720 - val_accuracy: 0.8247\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6455 - accuracy: 0.8090 - val_loss: 0.5069 - val_accuracy: 0.8513\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6341 - accuracy: 0.8131 - val_loss: 0.5153 - val_accuracy: 0.8481\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6299 - accuracy: 0.8125 - val_loss: 0.5241 - val_accuracy: 0.8446\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6262 - accuracy: 0.8161 - val_loss: 0.4341 - val_accuracy: 0.8706\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6222 - accuracy: 0.8172 - val_loss: 0.5123 - val_accuracy: 0.8485\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6274 - accuracy: 0.8138 - val_loss: 0.5341 - val_accuracy: 0.8414\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6122 - accuracy: 0.8193 - val_loss: 0.4467 - val_accuracy: 0.8692\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6086 - accuracy: 0.8217 - val_loss: 0.4986 - val_accuracy: 0.8493\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6110 - accuracy: 0.8205 - val_loss: 0.5893 - val_accuracy: 0.8274\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6049 - accuracy: 0.8206 - val_loss: 0.5149 - val_accuracy: 0.8471\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5965 - accuracy: 0.8251 - val_loss: 0.4545 - val_accuracy: 0.8638\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5906 - accuracy: 0.8263 - val_loss: 0.4773 - val_accuracy: 0.8628\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5930 - accuracy: 0.8246 - val_loss: 0.5021 - val_accuracy: 0.8521\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5943 - accuracy: 0.8245 - val_loss: 0.5030 - val_accuracy: 0.8490\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5850 - accuracy: 0.8261 - val_loss: 0.5303 - val_accuracy: 0.8361\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5803 - accuracy: 0.8295 - val_loss: 0.4792 - val_accuracy: 0.8584\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5798 - accuracy: 0.8279 - val_loss: 0.4515 - val_accuracy: 0.8651\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5744 - accuracy: 0.8310 - val_loss: 0.4278 - val_accuracy: 0.8756\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5759 - accuracy: 0.8320 - val_loss: 0.4257 - val_accuracy: 0.8791\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5622 - accuracy: 0.8352 - val_loss: 0.4153 - val_accuracy: 0.8814\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5647 - accuracy: 0.8332 - val_loss: 0.4193 - val_accuracy: 0.8794\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5637 - accuracy: 0.8345 - val_loss: 0.4048 - val_accuracy: 0.8817\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5626 - accuracy: 0.8322 - val_loss: 0.4137 - val_accuracy: 0.8791\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5567 - accuracy: 0.8375 - val_loss: 0.5635 - val_accuracy: 0.8289\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5525 - accuracy: 0.8376 - val_loss: 0.4683 - val_accuracy: 0.8637\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5523 - accuracy: 0.8399 - val_loss: 0.4478 - val_accuracy: 0.8704\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5476 - accuracy: 0.8392 - val_loss: 0.3888 - val_accuracy: 0.8835\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5463 - accuracy: 0.8397 - val_loss: 0.5032 - val_accuracy: 0.8519\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5465 - accuracy: 0.8401 - val_loss: 0.4596 - val_accuracy: 0.8672\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5413 - accuracy: 0.8416 - val_loss: 0.5215 - val_accuracy: 0.8466\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5373 - accuracy: 0.8430 - val_loss: 0.6265 - val_accuracy: 0.8089\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5362 - accuracy: 0.8423 - val_loss: 0.5247 - val_accuracy: 0.8446\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5374 - accuracy: 0.8438 - val_loss: 0.4504 - val_accuracy: 0.8663\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5335 - accuracy: 0.8442 - val_loss: 0.4501 - val_accuracy: 0.8665\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5272 - accuracy: 0.8466 - val_loss: 0.4312 - val_accuracy: 0.8720\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5295 - accuracy: 0.8448 - val_loss: 0.3774 - val_accuracy: 0.8925\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5256 - accuracy: 0.8464 - val_loss: 0.5656 - val_accuracy: 0.8267\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5180 - accuracy: 0.8470 - val_loss: 0.3873 - val_accuracy: 0.8878\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5180 - accuracy: 0.8490 - val_loss: 0.4525 - val_accuracy: 0.8654\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5206 - accuracy: 0.8489 - val_loss: 0.5249 - val_accuracy: 0.8483\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5162 - accuracy: 0.8500 - val_loss: 0.3277 - val_accuracy: 0.9082\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5093 - accuracy: 0.8515 - val_loss: 0.4479 - val_accuracy: 0.8662\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5069 - accuracy: 0.8521 - val_loss: 0.4001 - val_accuracy: 0.8848\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5090 - accuracy: 0.8515 - val_loss: 0.6107 - val_accuracy: 0.8216\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8534 - val_loss: 0.4017 - val_accuracy: 0.8841\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5012 - accuracy: 0.8525 - val_loss: 0.5000 - val_accuracy: 0.8551\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4986 - accuracy: 0.8547 - val_loss: 0.5325 - val_accuracy: 0.8456\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4942 - accuracy: 0.8563 - val_loss: 0.4698 - val_accuracy: 0.8554\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4937 - accuracy: 0.8555 - val_loss: 0.3827 - val_accuracy: 0.8899\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4895 - accuracy: 0.8570 - val_loss: 0.7302 - val_accuracy: 0.7787\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4928 - accuracy: 0.8566 - val_loss: 0.3693 - val_accuracy: 0.8935\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4902 - accuracy: 0.8562 - val_loss: 0.3138 - val_accuracy: 0.9121\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8571 - val_loss: 0.4913 - val_accuracy: 0.8555\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.8587 - val_loss: 0.3349 - val_accuracy: 0.9034\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4801 - accuracy: 0.8593 - val_loss: 0.4484 - val_accuracy: 0.8694\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4772 - accuracy: 0.8604 - val_loss: 0.3896 - val_accuracy: 0.8836\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4831 - accuracy: 0.8590 - val_loss: 0.3297 - val_accuracy: 0.9046\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4772 - accuracy: 0.8614 - val_loss: 0.3625 - val_accuracy: 0.8951\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4711 - accuracy: 0.8608 - val_loss: 0.3582 - val_accuracy: 0.8992\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4757 - accuracy: 0.8624 - val_loss: 0.3976 - val_accuracy: 0.8879\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4740 - accuracy: 0.8620 - val_loss: 0.4367 - val_accuracy: 0.8706\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4693 - accuracy: 0.8633 - val_loss: 0.3649 - val_accuracy: 0.8971\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4698 - accuracy: 0.8620 - val_loss: 0.3354 - val_accuracy: 0.9042\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4731 - accuracy: 0.8633 - val_loss: 0.3675 - val_accuracy: 0.8956\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4618 - accuracy: 0.8648 - val_loss: 0.4283 - val_accuracy: 0.8765\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8671 - val_loss: 0.3547 - val_accuracy: 0.8999\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4564 - accuracy: 0.8662 - val_loss: 0.4149 - val_accuracy: 0.8796\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4597 - accuracy: 0.8656 - val_loss: 0.3336 - val_accuracy: 0.9078\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4549 - accuracy: 0.8684 - val_loss: 0.4378 - val_accuracy: 0.8676\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4587 - accuracy: 0.8656 - val_loss: 0.4458 - val_accuracy: 0.8682\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4470 - accuracy: 0.8693 - val_loss: 0.3400 - val_accuracy: 0.9046\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4536 - accuracy: 0.8681 - val_loss: 0.3266 - val_accuracy: 0.9076\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4471 - accuracy: 0.8702 - val_loss: 0.4067 - val_accuracy: 0.8828\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4449 - accuracy: 0.8701 - val_loss: 0.4306 - val_accuracy: 0.8701\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4478 - accuracy: 0.8687 - val_loss: 0.3602 - val_accuracy: 0.8997\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4404 - accuracy: 0.8725 - val_loss: 0.4665 - val_accuracy: 0.8670\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4433 - accuracy: 0.8706 - val_loss: 0.3375 - val_accuracy: 0.9029\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4400 - accuracy: 0.8720 - val_loss: 0.3751 - val_accuracy: 0.8891\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8718 - val_loss: 0.3517 - val_accuracy: 0.8984\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4363 - accuracy: 0.8739 - val_loss: 0.4628 - val_accuracy: 0.8670\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4399 - accuracy: 0.8717 - val_loss: 0.3590 - val_accuracy: 0.8943\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4327 - accuracy: 0.8736 - val_loss: 0.3497 - val_accuracy: 0.8991\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4365 - accuracy: 0.8730 - val_loss: 0.4504 - val_accuracy: 0.8692\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4344 - accuracy: 0.8745 - val_loss: 0.3702 - val_accuracy: 0.8924\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4313 - accuracy: 0.8743 - val_loss: 0.4931 - val_accuracy: 0.8549\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4301 - accuracy: 0.8745 - val_loss: 0.4693 - val_accuracy: 0.8660\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4363 - accuracy: 0.8740 - val_loss: 0.3538 - val_accuracy: 0.8943\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4299 - accuracy: 0.8752 - val_loss: 0.3088 - val_accuracy: 0.9137\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4261 - accuracy: 0.8756 - val_loss: 0.3187 - val_accuracy: 0.9081\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4243 - accuracy: 0.8772 - val_loss: 0.5370 - val_accuracy: 0.8442\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4229 - accuracy: 0.8772 - val_loss: 0.5234 - val_accuracy: 0.8495\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4149 - accuracy: 0.8799 - val_loss: 0.3736 - val_accuracy: 0.8951\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4203 - accuracy: 0.8772 - val_loss: 0.3781 - val_accuracy: 0.8907\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4183 - accuracy: 0.8784 - val_loss: 0.3309 - val_accuracy: 0.9060\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4167 - accuracy: 0.8793 - val_loss: 0.4392 - val_accuracy: 0.8766\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4200 - accuracy: 0.8775 - val_loss: 0.4444 - val_accuracy: 0.8674\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4141 - accuracy: 0.8776 - val_loss: 0.4036 - val_accuracy: 0.8833\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4152 - accuracy: 0.8796 - val_loss: 0.3480 - val_accuracy: 0.8995\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4126 - accuracy: 0.8807 - val_loss: 0.4528 - val_accuracy: 0.8701\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4060 - accuracy: 0.8823 - val_loss: 0.3888 - val_accuracy: 0.8889\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4151 - accuracy: 0.8792 - val_loss: 0.4758 - val_accuracy: 0.8605\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4062 - accuracy: 0.8818 - val_loss: 0.3894 - val_accuracy: 0.8819\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4093 - accuracy: 0.8811 - val_loss: 0.3421 - val_accuracy: 0.9041\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4055 - accuracy: 0.8818 - val_loss: 0.4624 - val_accuracy: 0.8633\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3999 - accuracy: 0.8844 - val_loss: 0.4883 - val_accuracy: 0.8575\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4038 - accuracy: 0.8831 - val_loss: 0.3170 - val_accuracy: 0.9104\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4041 - accuracy: 0.8823 - val_loss: 0.3041 - val_accuracy: 0.9171\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4035 - accuracy: 0.8824 - val_loss: 0.4945 - val_accuracy: 0.8575\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3977 - accuracy: 0.8852 - val_loss: 0.5649 - val_accuracy: 0.8304\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3951 - accuracy: 0.8857 - val_loss: 0.3104 - val_accuracy: 0.9149\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4001 - accuracy: 0.8833 - val_loss: 0.3757 - val_accuracy: 0.8907\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3969 - accuracy: 0.8847 - val_loss: 0.4076 - val_accuracy: 0.8836\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3951 - accuracy: 0.8861 - val_loss: 0.3913 - val_accuracy: 0.8846\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3946 - accuracy: 0.8855 - val_loss: 0.3765 - val_accuracy: 0.8947\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3894 - accuracy: 0.8867 - val_loss: 0.3332 - val_accuracy: 0.9054\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3878 - accuracy: 0.8867 - val_loss: 0.3875 - val_accuracy: 0.8894\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3873 - accuracy: 0.8881 - val_loss: 0.3282 - val_accuracy: 0.9054\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3868 - accuracy: 0.8871 - val_loss: 0.4165 - val_accuracy: 0.8744\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3868 - accuracy: 0.8877 - val_loss: 0.3566 - val_accuracy: 0.8950\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8882 - val_loss: 0.3252 - val_accuracy: 0.9074\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3765 - accuracy: 0.8911 - val_loss: 0.3753 - val_accuracy: 0.8899\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3868 - accuracy: 0.8885 - val_loss: 0.3669 - val_accuracy: 0.8939\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3735 - accuracy: 0.8934 - val_loss: 0.3516 - val_accuracy: 0.8994\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3807 - accuracy: 0.8880 - val_loss: 0.4321 - val_accuracy: 0.8694\n",
      "Try 7/100: Best_val_acc: [0.4848990738391876, 0.8565000295639038], lr: 6.079885540445708e-05, Lambda: 5.881506893314989e-05\n",
      "\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_358 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_359 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7563 - accuracy: 0.1067 - val_loss: 2.4862 - val_accuracy: 0.0831\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.6043 - accuracy: 0.1184 - val_loss: 2.3988 - val_accuracy: 0.1384\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4974 - accuracy: 0.1267 - val_loss: 2.4149 - val_accuracy: 0.1129\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4194 - accuracy: 0.1417 - val_loss: 2.3452 - val_accuracy: 0.1599\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3386 - accuracy: 0.1615 - val_loss: 2.2938 - val_accuracy: 0.1659\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2643 - accuracy: 0.1825 - val_loss: 2.2091 - val_accuracy: 0.1669\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1808 - accuracy: 0.2144 - val_loss: 2.0846 - val_accuracy: 0.2244\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1006 - accuracy: 0.2440 - val_loss: 2.0662 - val_accuracy: 0.2384\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0277 - accuracy: 0.2738 - val_loss: 1.9558 - val_accuracy: 0.2714\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9578 - accuracy: 0.3037 - val_loss: 1.8421 - val_accuracy: 0.3655\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8859 - accuracy: 0.3382 - val_loss: 1.8266 - val_accuracy: 0.3876\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8211 - accuracy: 0.3657 - val_loss: 1.6696 - val_accuracy: 0.4726\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7620 - accuracy: 0.3948 - val_loss: 1.5977 - val_accuracy: 0.5364\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7051 - accuracy: 0.4228 - val_loss: 1.5375 - val_accuracy: 0.5811\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6470 - accuracy: 0.4513 - val_loss: 1.4507 - val_accuracy: 0.5981\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5966 - accuracy: 0.4723 - val_loss: 1.4005 - val_accuracy: 0.6451\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5451 - accuracy: 0.4943 - val_loss: 1.4473 - val_accuracy: 0.6186\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5018 - accuracy: 0.5111 - val_loss: 1.3307 - val_accuracy: 0.6604\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4604 - accuracy: 0.5280 - val_loss: 1.1943 - val_accuracy: 0.6941\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4171 - accuracy: 0.5446 - val_loss: 1.2079 - val_accuracy: 0.6682\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3772 - accuracy: 0.5614 - val_loss: 1.1341 - val_accuracy: 0.7144\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3374 - accuracy: 0.5758 - val_loss: 1.0781 - val_accuracy: 0.7182\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3131 - accuracy: 0.5865 - val_loss: 1.0181 - val_accuracy: 0.7442\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2801 - accuracy: 0.5968 - val_loss: 1.0646 - val_accuracy: 0.7390\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2498 - accuracy: 0.6060 - val_loss: 0.9814 - val_accuracy: 0.7544\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2220 - accuracy: 0.6192 - val_loss: 0.9380 - val_accuracy: 0.7703\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2006 - accuracy: 0.6266 - val_loss: 0.8906 - val_accuracy: 0.7736\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1731 - accuracy: 0.6366 - val_loss: 0.9754 - val_accuracy: 0.7410\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1478 - accuracy: 0.6435 - val_loss: 0.8645 - val_accuracy: 0.7686\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1218 - accuracy: 0.6552 - val_loss: 0.8815 - val_accuracy: 0.7729\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1011 - accuracy: 0.6594 - val_loss: 0.8312 - val_accuracy: 0.7797\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0758 - accuracy: 0.6691 - val_loss: 0.9132 - val_accuracy: 0.7609\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0585 - accuracy: 0.6764 - val_loss: 0.8246 - val_accuracy: 0.7729\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0422 - accuracy: 0.6819 - val_loss: 0.7851 - val_accuracy: 0.7969\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0164 - accuracy: 0.6916 - val_loss: 0.7843 - val_accuracy: 0.7858\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0015 - accuracy: 0.6952 - val_loss: 0.7460 - val_accuracy: 0.7899\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9872 - accuracy: 0.6977 - val_loss: 0.7514 - val_accuracy: 0.7966\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9670 - accuracy: 0.7067 - val_loss: 0.8664 - val_accuracy: 0.7573\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9516 - accuracy: 0.7108 - val_loss: 0.7112 - val_accuracy: 0.8088\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9443 - accuracy: 0.7164 - val_loss: 0.7582 - val_accuracy: 0.7952\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9218 - accuracy: 0.7235 - val_loss: 0.6953 - val_accuracy: 0.8087\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9053 - accuracy: 0.7255 - val_loss: 0.6880 - val_accuracy: 0.8047\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8833 - accuracy: 0.7345 - val_loss: 0.6542 - val_accuracy: 0.8110\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8802 - accuracy: 0.7353 - val_loss: 0.7790 - val_accuracy: 0.7855\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8714 - accuracy: 0.7380 - val_loss: 0.6563 - val_accuracy: 0.8184\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8511 - accuracy: 0.7459 - val_loss: 0.6293 - val_accuracy: 0.8242\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8479 - accuracy: 0.7459 - val_loss: 0.5893 - val_accuracy: 0.8330\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8353 - accuracy: 0.7476 - val_loss: 0.5715 - val_accuracy: 0.8469\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8262 - accuracy: 0.7514 - val_loss: 0.6587 - val_accuracy: 0.8118\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8167 - accuracy: 0.7575 - val_loss: 0.6292 - val_accuracy: 0.8194\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8098 - accuracy: 0.7587 - val_loss: 0.6789 - val_accuracy: 0.8086\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7938 - accuracy: 0.7632 - val_loss: 0.7613 - val_accuracy: 0.7711\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7848 - accuracy: 0.7664 - val_loss: 0.5989 - val_accuracy: 0.8310\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7818 - accuracy: 0.7651 - val_loss: 0.7302 - val_accuracy: 0.7905\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7726 - accuracy: 0.7691 - val_loss: 0.5861 - val_accuracy: 0.8269\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7610 - accuracy: 0.7713 - val_loss: 0.5392 - val_accuracy: 0.8429\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7514 - accuracy: 0.7764 - val_loss: 0.6409 - val_accuracy: 0.8151\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7464 - accuracy: 0.7769 - val_loss: 0.6332 - val_accuracy: 0.8161\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7465 - accuracy: 0.7755 - val_loss: 0.7236 - val_accuracy: 0.7837\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7340 - accuracy: 0.7810 - val_loss: 0.6060 - val_accuracy: 0.8208\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7294 - accuracy: 0.7850 - val_loss: 0.6202 - val_accuracy: 0.8215\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7173 - accuracy: 0.7853 - val_loss: 0.5694 - val_accuracy: 0.8326\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7214 - accuracy: 0.7870 - val_loss: 0.5748 - val_accuracy: 0.8351\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7110 - accuracy: 0.7900 - val_loss: 0.6988 - val_accuracy: 0.7926\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7013 - accuracy: 0.7921 - val_loss: 0.6907 - val_accuracy: 0.8001\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6934 - accuracy: 0.7943 - val_loss: 0.5263 - val_accuracy: 0.8507\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6946 - accuracy: 0.7925 - val_loss: 0.6599 - val_accuracy: 0.8069\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6884 - accuracy: 0.7963 - val_loss: 0.5077 - val_accuracy: 0.8484\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6785 - accuracy: 0.7992 - val_loss: 0.5251 - val_accuracy: 0.8474\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6711 - accuracy: 0.7998 - val_loss: 0.6177 - val_accuracy: 0.8152\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6748 - accuracy: 0.7990 - val_loss: 0.6072 - val_accuracy: 0.8194\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6627 - accuracy: 0.8014 - val_loss: 0.5864 - val_accuracy: 0.8261\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6597 - accuracy: 0.8054 - val_loss: 0.5602 - val_accuracy: 0.8383\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6562 - accuracy: 0.8061 - val_loss: 0.6164 - val_accuracy: 0.8121\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6492 - accuracy: 0.8083 - val_loss: 0.5483 - val_accuracy: 0.8416\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6419 - accuracy: 0.8124 - val_loss: 0.5219 - val_accuracy: 0.8389\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6359 - accuracy: 0.8104 - val_loss: 0.5400 - val_accuracy: 0.8413\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6355 - accuracy: 0.8116 - val_loss: 0.5193 - val_accuracy: 0.8440\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6258 - accuracy: 0.8163 - val_loss: 0.5317 - val_accuracy: 0.8409\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6341 - accuracy: 0.8133 - val_loss: 0.4961 - val_accuracy: 0.8551\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6220 - accuracy: 0.8179 - val_loss: 0.5487 - val_accuracy: 0.8396\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6202 - accuracy: 0.8172 - val_loss: 0.6042 - val_accuracy: 0.8126\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6147 - accuracy: 0.8187 - val_loss: 0.5502 - val_accuracy: 0.8385\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.8207 - val_loss: 0.5213 - val_accuracy: 0.8485\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6053 - accuracy: 0.8226 - val_loss: 0.5896 - val_accuracy: 0.8259\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6042 - accuracy: 0.8240 - val_loss: 0.4850 - val_accuracy: 0.8582\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5909 - accuracy: 0.8277 - val_loss: 0.5709 - val_accuracy: 0.8216\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5903 - accuracy: 0.8275 - val_loss: 0.4855 - val_accuracy: 0.8589\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5894 - accuracy: 0.8259 - val_loss: 0.4919 - val_accuracy: 0.8569\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5850 - accuracy: 0.8260 - val_loss: 0.6228 - val_accuracy: 0.8111\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5844 - accuracy: 0.8291 - val_loss: 0.4280 - val_accuracy: 0.8762\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5796 - accuracy: 0.8308 - val_loss: 0.5530 - val_accuracy: 0.8342\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5783 - accuracy: 0.8300 - val_loss: 0.4504 - val_accuracy: 0.8641\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5724 - accuracy: 0.8314 - val_loss: 0.4309 - val_accuracy: 0.8713\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5676 - accuracy: 0.8345 - val_loss: 0.4984 - val_accuracy: 0.8551\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5658 - accuracy: 0.8328 - val_loss: 0.4449 - val_accuracy: 0.8720\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5680 - accuracy: 0.8327 - val_loss: 0.4598 - val_accuracy: 0.8650\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5646 - accuracy: 0.8354 - val_loss: 0.4225 - val_accuracy: 0.8796\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5613 - accuracy: 0.8356 - val_loss: 0.6556 - val_accuracy: 0.8000\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5524 - accuracy: 0.8376 - val_loss: 0.4521 - val_accuracy: 0.8677\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5555 - accuracy: 0.8372 - val_loss: 0.3667 - val_accuracy: 0.8941\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5518 - accuracy: 0.8375 - val_loss: 0.7030 - val_accuracy: 0.7961\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5497 - accuracy: 0.8395 - val_loss: 0.5090 - val_accuracy: 0.8469\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5425 - accuracy: 0.8408 - val_loss: 0.3802 - val_accuracy: 0.8932\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5410 - accuracy: 0.8408 - val_loss: 0.4293 - val_accuracy: 0.8745\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5373 - accuracy: 0.8413 - val_loss: 0.4393 - val_accuracy: 0.8726\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5392 - accuracy: 0.8416 - val_loss: 0.4416 - val_accuracy: 0.8699\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5381 - accuracy: 0.8424 - val_loss: 0.4650 - val_accuracy: 0.8642\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5288 - accuracy: 0.8443 - val_loss: 0.4464 - val_accuracy: 0.8714\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5243 - accuracy: 0.8453 - val_loss: 0.5408 - val_accuracy: 0.8388\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5229 - accuracy: 0.8467 - val_loss: 0.4880 - val_accuracy: 0.8529\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5251 - accuracy: 0.8466 - val_loss: 0.4695 - val_accuracy: 0.8616\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5253 - accuracy: 0.8470 - val_loss: 0.4947 - val_accuracy: 0.8562\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5160 - accuracy: 0.8494 - val_loss: 0.4341 - val_accuracy: 0.8761\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5123 - accuracy: 0.8515 - val_loss: 0.4624 - val_accuracy: 0.8649\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5097 - accuracy: 0.8520 - val_loss: 0.4608 - val_accuracy: 0.8647\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5125 - accuracy: 0.8506 - val_loss: 0.6003 - val_accuracy: 0.8229\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5182 - accuracy: 0.8491 - val_loss: 0.3869 - val_accuracy: 0.8876\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5097 - accuracy: 0.8517 - val_loss: 0.5158 - val_accuracy: 0.8455\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5050 - accuracy: 0.8521 - val_loss: 0.4265 - val_accuracy: 0.8746\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4964 - accuracy: 0.8557 - val_loss: 0.3968 - val_accuracy: 0.8841\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4969 - accuracy: 0.8540 - val_loss: 0.5229 - val_accuracy: 0.8495\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4971 - accuracy: 0.8547 - val_loss: 0.4936 - val_accuracy: 0.8504\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5004 - accuracy: 0.8530 - val_loss: 0.4479 - val_accuracy: 0.8665\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4975 - accuracy: 0.8552 - val_loss: 0.4558 - val_accuracy: 0.8680\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4882 - accuracy: 0.8561 - val_loss: 0.5309 - val_accuracy: 0.8440\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4864 - accuracy: 0.8585 - val_loss: 0.3936 - val_accuracy: 0.8859\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8568 - val_loss: 0.3523 - val_accuracy: 0.8994\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4862 - accuracy: 0.8584 - val_loss: 0.5520 - val_accuracy: 0.8331\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.8574 - val_loss: 0.4478 - val_accuracy: 0.8662\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4768 - accuracy: 0.8619 - val_loss: 0.7075 - val_accuracy: 0.7939\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4759 - accuracy: 0.8607 - val_loss: 0.3801 - val_accuracy: 0.8894\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4767 - accuracy: 0.8602 - val_loss: 0.3555 - val_accuracy: 0.9007\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4709 - accuracy: 0.8630 - val_loss: 0.5013 - val_accuracy: 0.8537\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4720 - accuracy: 0.8631 - val_loss: 0.4600 - val_accuracy: 0.8601\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4742 - accuracy: 0.8600 - val_loss: 0.3630 - val_accuracy: 0.8962\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4671 - accuracy: 0.8632 - val_loss: 0.6518 - val_accuracy: 0.8084\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4705 - accuracy: 0.8632 - val_loss: 0.4532 - val_accuracy: 0.8679\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4665 - accuracy: 0.8635 - val_loss: 0.4692 - val_accuracy: 0.8654\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4674 - accuracy: 0.8665 - val_loss: 0.3936 - val_accuracy: 0.8871\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4646 - accuracy: 0.8664 - val_loss: 0.4771 - val_accuracy: 0.8603\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4610 - accuracy: 0.8645 - val_loss: 0.6532 - val_accuracy: 0.8064\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4608 - accuracy: 0.8641 - val_loss: 0.4486 - val_accuracy: 0.8677\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8665 - val_loss: 0.2992 - val_accuracy: 0.9160\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4548 - accuracy: 0.8661 - val_loss: 0.4608 - val_accuracy: 0.8645\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4512 - accuracy: 0.8686 - val_loss: 0.3707 - val_accuracy: 0.8959\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4511 - accuracy: 0.8692 - val_loss: 0.4719 - val_accuracy: 0.8617\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4494 - accuracy: 0.8685 - val_loss: 0.4374 - val_accuracy: 0.8727\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4508 - accuracy: 0.8687 - val_loss: 0.4679 - val_accuracy: 0.8617\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4480 - accuracy: 0.8688 - val_loss: 0.3598 - val_accuracy: 0.8976\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4434 - accuracy: 0.8719 - val_loss: 0.4575 - val_accuracy: 0.8663\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4407 - accuracy: 0.8717 - val_loss: 0.3946 - val_accuracy: 0.8850\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4395 - accuracy: 0.8728 - val_loss: 0.4604 - val_accuracy: 0.8631\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4426 - accuracy: 0.8727 - val_loss: 0.4044 - val_accuracy: 0.8861\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4364 - accuracy: 0.8734 - val_loss: 0.3265 - val_accuracy: 0.9101\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4334 - accuracy: 0.8738 - val_loss: 0.3681 - val_accuracy: 0.8964\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4367 - accuracy: 0.8745 - val_loss: 0.3309 - val_accuracy: 0.9061\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4317 - accuracy: 0.8755 - val_loss: 0.5011 - val_accuracy: 0.8550\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4379 - accuracy: 0.8720 - val_loss: 0.3851 - val_accuracy: 0.8861\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4317 - accuracy: 0.8740 - val_loss: 0.3570 - val_accuracy: 0.9001\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4308 - accuracy: 0.8758 - val_loss: 0.3864 - val_accuracy: 0.8920\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4278 - accuracy: 0.8751 - val_loss: 0.3246 - val_accuracy: 0.9101\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4263 - accuracy: 0.8756 - val_loss: 0.5124 - val_accuracy: 0.8479\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4304 - accuracy: 0.8757 - val_loss: 0.4474 - val_accuracy: 0.8688\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4256 - accuracy: 0.8751 - val_loss: 0.4891 - val_accuracy: 0.8624\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4193 - accuracy: 0.8788 - val_loss: 0.3780 - val_accuracy: 0.8897\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4181 - accuracy: 0.8787 - val_loss: 0.4963 - val_accuracy: 0.8561\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4160 - accuracy: 0.8798 - val_loss: 0.4494 - val_accuracy: 0.8704\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4186 - accuracy: 0.8789 - val_loss: 0.4283 - val_accuracy: 0.8735\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4230 - accuracy: 0.8783 - val_loss: 0.3860 - val_accuracy: 0.8904\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8787 - val_loss: 0.5046 - val_accuracy: 0.8574\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4138 - accuracy: 0.8803 - val_loss: 0.3763 - val_accuracy: 0.8918\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4155 - accuracy: 0.8794 - val_loss: 0.4022 - val_accuracy: 0.8853\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4107 - accuracy: 0.8806 - val_loss: 0.4229 - val_accuracy: 0.8757\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4094 - accuracy: 0.8813 - val_loss: 0.3227 - val_accuracy: 0.9085\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4087 - accuracy: 0.8805 - val_loss: 0.6351 - val_accuracy: 0.8139\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4053 - accuracy: 0.8837 - val_loss: 0.4430 - val_accuracy: 0.8714\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4073 - accuracy: 0.8809 - val_loss: 0.5108 - val_accuracy: 0.8509\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8812 - val_loss: 0.4456 - val_accuracy: 0.8697\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3996 - accuracy: 0.8842 - val_loss: 0.5870 - val_accuracy: 0.8221\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8851 - val_loss: 0.4549 - val_accuracy: 0.8644\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3975 - accuracy: 0.8858 - val_loss: 0.3427 - val_accuracy: 0.9016\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4011 - accuracy: 0.8852 - val_loss: 0.3916 - val_accuracy: 0.8882\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3903 - accuracy: 0.8857 - val_loss: 0.3780 - val_accuracy: 0.8914\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3945 - accuracy: 0.8840 - val_loss: 0.4744 - val_accuracy: 0.8607\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3929 - accuracy: 0.8854 - val_loss: 0.3820 - val_accuracy: 0.8914\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3944 - accuracy: 0.8861 - val_loss: 0.5034 - val_accuracy: 0.8540\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3971 - accuracy: 0.8854 - val_loss: 0.5663 - val_accuracy: 0.8354\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3906 - accuracy: 0.8865 - val_loss: 0.4016 - val_accuracy: 0.8861\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3936 - accuracy: 0.8858 - val_loss: 0.5861 - val_accuracy: 0.8231\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3903 - accuracy: 0.8880 - val_loss: 0.4563 - val_accuracy: 0.8677\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3877 - accuracy: 0.8872 - val_loss: 0.3954 - val_accuracy: 0.8876\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3937 - accuracy: 0.8859 - val_loss: 0.4156 - val_accuracy: 0.8777\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3834 - accuracy: 0.8890 - val_loss: 0.3199 - val_accuracy: 0.9093\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3828 - accuracy: 0.8890 - val_loss: 0.2765 - val_accuracy: 0.9246\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3823 - accuracy: 0.8902 - val_loss: 0.2722 - val_accuracy: 0.9263\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3772 - accuracy: 0.8914 - val_loss: 0.3853 - val_accuracy: 0.8894\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3830 - accuracy: 0.8898 - val_loss: 0.3296 - val_accuracy: 0.9075\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3810 - accuracy: 0.8892 - val_loss: 0.3217 - val_accuracy: 0.9116\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3790 - accuracy: 0.8899 - val_loss: 0.3265 - val_accuracy: 0.9072\n",
      "Try 8/100: Best_val_acc: [0.47930261492729187, 0.8579444289207458], lr: 6.149328376361425e-05, Lambda: 5.8230158798530616e-05\n",
      "\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_360 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_361 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_362 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_363 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_364 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_365 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.7570 - accuracy: 0.1081 - val_loss: 2.2742 - val_accuracy: 0.1020\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5918 - accuracy: 0.1216 - val_loss: 2.2247 - val_accuracy: 0.1219\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4796 - accuracy: 0.1401 - val_loss: 2.1631 - val_accuracy: 0.1929\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3770 - accuracy: 0.1641 - val_loss: 2.1422 - val_accuracy: 0.2236\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2964 - accuracy: 0.1870 - val_loss: 2.1121 - val_accuracy: 0.2024\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2124 - accuracy: 0.2125 - val_loss: 1.9732 - val_accuracy: 0.3234\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1258 - accuracy: 0.2453 - val_loss: 1.8494 - val_accuracy: 0.3843\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0454 - accuracy: 0.2774 - val_loss: 1.8326 - val_accuracy: 0.3901\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9733 - accuracy: 0.3082 - val_loss: 1.6998 - val_accuracy: 0.4419\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9032 - accuracy: 0.3378 - val_loss: 1.6594 - val_accuracy: 0.4621\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8380 - accuracy: 0.3622 - val_loss: 1.6212 - val_accuracy: 0.4696\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7805 - accuracy: 0.3892 - val_loss: 1.5389 - val_accuracy: 0.5251\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7252 - accuracy: 0.4146 - val_loss: 1.4847 - val_accuracy: 0.5288\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6701 - accuracy: 0.4336 - val_loss: 1.4148 - val_accuracy: 0.6001\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6284 - accuracy: 0.4547 - val_loss: 1.3579 - val_accuracy: 0.6084\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5753 - accuracy: 0.4767 - val_loss: 1.3529 - val_accuracy: 0.6159\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5334 - accuracy: 0.4970 - val_loss: 1.2279 - val_accuracy: 0.6626\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4922 - accuracy: 0.5162 - val_loss: 1.2347 - val_accuracy: 0.6354\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4537 - accuracy: 0.5303 - val_loss: 1.2531 - val_accuracy: 0.6348\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4209 - accuracy: 0.5456 - val_loss: 1.2222 - val_accuracy: 0.6420\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3845 - accuracy: 0.5579 - val_loss: 1.1418 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3465 - accuracy: 0.5730 - val_loss: 1.1423 - val_accuracy: 0.6658\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3145 - accuracy: 0.5895 - val_loss: 1.0066 - val_accuracy: 0.7187\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2864 - accuracy: 0.5964 - val_loss: 1.0152 - val_accuracy: 0.7296\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2650 - accuracy: 0.6043 - val_loss: 1.0144 - val_accuracy: 0.7113\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2274 - accuracy: 0.6177 - val_loss: 0.9218 - val_accuracy: 0.7385\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2054 - accuracy: 0.6267 - val_loss: 0.9689 - val_accuracy: 0.7279\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1770 - accuracy: 0.6365 - val_loss: 0.9677 - val_accuracy: 0.7281\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1581 - accuracy: 0.6414 - val_loss: 0.9139 - val_accuracy: 0.7464\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1401 - accuracy: 0.6505 - val_loss: 0.9035 - val_accuracy: 0.7486\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1123 - accuracy: 0.6604 - val_loss: 0.7944 - val_accuracy: 0.7859\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0957 - accuracy: 0.6650 - val_loss: 0.9481 - val_accuracy: 0.7231\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0700 - accuracy: 0.6714 - val_loss: 0.8059 - val_accuracy: 0.7752\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0571 - accuracy: 0.6769 - val_loss: 0.7505 - val_accuracy: 0.7898\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0327 - accuracy: 0.6860 - val_loss: 0.8252 - val_accuracy: 0.7623\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0186 - accuracy: 0.6909 - val_loss: 0.7860 - val_accuracy: 0.7774\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9996 - accuracy: 0.6969 - val_loss: 0.7418 - val_accuracy: 0.7973\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9877 - accuracy: 0.7010 - val_loss: 0.8426 - val_accuracy: 0.7458\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9717 - accuracy: 0.7065 - val_loss: 0.7557 - val_accuracy: 0.7801\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9565 - accuracy: 0.7111 - val_loss: 0.7228 - val_accuracy: 0.7929\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9410 - accuracy: 0.7145 - val_loss: 0.7805 - val_accuracy: 0.7721\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9283 - accuracy: 0.7193 - val_loss: 0.7055 - val_accuracy: 0.8006\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9154 - accuracy: 0.7241 - val_loss: 0.7661 - val_accuracy: 0.7707\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9031 - accuracy: 0.7260 - val_loss: 0.6878 - val_accuracy: 0.8006\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8901 - accuracy: 0.7318 - val_loss: 0.7896 - val_accuracy: 0.7549\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8831 - accuracy: 0.7319 - val_loss: 0.6517 - val_accuracy: 0.8106\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8623 - accuracy: 0.7417 - val_loss: 0.8152 - val_accuracy: 0.7519\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8586 - accuracy: 0.7409 - val_loss: 0.5944 - val_accuracy: 0.8194\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8482 - accuracy: 0.7461 - val_loss: 0.6334 - val_accuracy: 0.8124\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8296 - accuracy: 0.7501 - val_loss: 0.6531 - val_accuracy: 0.8070\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8269 - accuracy: 0.7516 - val_loss: 0.6046 - val_accuracy: 0.8229\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8150 - accuracy: 0.7556 - val_loss: 0.6558 - val_accuracy: 0.8039\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8058 - accuracy: 0.7607 - val_loss: 0.5637 - val_accuracy: 0.8423\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8005 - accuracy: 0.7614 - val_loss: 0.6287 - val_accuracy: 0.8191\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7890 - accuracy: 0.7646 - val_loss: 0.5661 - val_accuracy: 0.8385\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7692 - accuracy: 0.7712 - val_loss: 0.6028 - val_accuracy: 0.8230\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7666 - accuracy: 0.7706 - val_loss: 0.5368 - val_accuracy: 0.8403\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7595 - accuracy: 0.7732 - val_loss: 0.6119 - val_accuracy: 0.8177\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7517 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.8456\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7460 - accuracy: 0.7777 - val_loss: 0.5883 - val_accuracy: 0.8215\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7406 - accuracy: 0.7789 - val_loss: 0.5302 - val_accuracy: 0.8418\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7374 - accuracy: 0.7811 - val_loss: 0.6552 - val_accuracy: 0.8036\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7311 - accuracy: 0.7840 - val_loss: 0.5712 - val_accuracy: 0.8304\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7149 - accuracy: 0.7878 - val_loss: 0.6890 - val_accuracy: 0.7914\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7110 - accuracy: 0.7905 - val_loss: 0.5546 - val_accuracy: 0.8320\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7025 - accuracy: 0.7915 - val_loss: 0.6242 - val_accuracy: 0.8124\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7034 - accuracy: 0.7915 - val_loss: 0.6492 - val_accuracy: 0.8012\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6951 - accuracy: 0.7938 - val_loss: 0.6403 - val_accuracy: 0.8061\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6914 - accuracy: 0.7950 - val_loss: 0.6434 - val_accuracy: 0.8072\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6810 - accuracy: 0.7994 - val_loss: 0.5862 - val_accuracy: 0.8286\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6786 - accuracy: 0.7997 - val_loss: 0.5882 - val_accuracy: 0.8253\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6761 - accuracy: 0.8009 - val_loss: 0.5942 - val_accuracy: 0.8168\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6615 - accuracy: 0.8050 - val_loss: 0.5694 - val_accuracy: 0.8287\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6616 - accuracy: 0.8046 - val_loss: 0.5770 - val_accuracy: 0.8275\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6599 - accuracy: 0.8050 - val_loss: 0.6211 - val_accuracy: 0.8110\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6486 - accuracy: 0.8082 - val_loss: 0.5097 - val_accuracy: 0.8452\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6371 - accuracy: 0.8125 - val_loss: 0.4974 - val_accuracy: 0.8561\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6423 - accuracy: 0.8098 - val_loss: 0.6228 - val_accuracy: 0.8086\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6367 - accuracy: 0.8112 - val_loss: 0.6215 - val_accuracy: 0.8133\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6324 - accuracy: 0.8143 - val_loss: 0.4504 - val_accuracy: 0.8681\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6259 - accuracy: 0.8164 - val_loss: 0.5854 - val_accuracy: 0.8220\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6239 - accuracy: 0.8168 - val_loss: 0.4259 - val_accuracy: 0.8724\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6178 - accuracy: 0.8181 - val_loss: 0.5319 - val_accuracy: 0.8370\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6148 - accuracy: 0.8185 - val_loss: 0.4588 - val_accuracy: 0.8601\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6155 - accuracy: 0.8176 - val_loss: 0.4792 - val_accuracy: 0.8583\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6036 - accuracy: 0.8217 - val_loss: 0.4795 - val_accuracy: 0.8616\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6040 - accuracy: 0.8214 - val_loss: 0.4916 - val_accuracy: 0.8562\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5956 - accuracy: 0.8242 - val_loss: 0.5848 - val_accuracy: 0.8121\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5997 - accuracy: 0.8236 - val_loss: 0.4430 - val_accuracy: 0.8706\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5945 - accuracy: 0.8267 - val_loss: 0.5235 - val_accuracy: 0.8416\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5910 - accuracy: 0.8282 - val_loss: 0.6241 - val_accuracy: 0.8066\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5880 - accuracy: 0.8275 - val_loss: 0.5376 - val_accuracy: 0.8328\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5892 - accuracy: 0.8273 - val_loss: 0.4278 - val_accuracy: 0.8779\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5750 - accuracy: 0.8312 - val_loss: 0.4302 - val_accuracy: 0.8758\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5739 - accuracy: 0.8311 - val_loss: 0.5025 - val_accuracy: 0.8466\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5687 - accuracy: 0.8342 - val_loss: 0.5524 - val_accuracy: 0.8355\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5638 - accuracy: 0.8348 - val_loss: 0.4626 - val_accuracy: 0.8621\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5626 - accuracy: 0.8350 - val_loss: 0.5254 - val_accuracy: 0.8461\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5681 - accuracy: 0.8325 - val_loss: 0.5756 - val_accuracy: 0.8302\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5630 - accuracy: 0.8352 - val_loss: 0.5603 - val_accuracy: 0.8296\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5556 - accuracy: 0.8369 - val_loss: 0.5265 - val_accuracy: 0.8424\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5540 - accuracy: 0.8375 - val_loss: 0.4928 - val_accuracy: 0.8541\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5513 - accuracy: 0.8375 - val_loss: 0.4106 - val_accuracy: 0.8789\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5501 - accuracy: 0.8380 - val_loss: 0.4862 - val_accuracy: 0.8523\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5429 - accuracy: 0.8414 - val_loss: 0.5248 - val_accuracy: 0.8439\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5435 - accuracy: 0.8424 - val_loss: 0.4672 - val_accuracy: 0.8643\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5412 - accuracy: 0.8420 - val_loss: 0.4096 - val_accuracy: 0.8768\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5362 - accuracy: 0.8449 - val_loss: 0.4465 - val_accuracy: 0.8691\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5320 - accuracy: 0.8436 - val_loss: 0.4527 - val_accuracy: 0.8698\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5283 - accuracy: 0.8460 - val_loss: 0.5408 - val_accuracy: 0.8396\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5336 - accuracy: 0.8429 - val_loss: 0.4598 - val_accuracy: 0.8638\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5193 - accuracy: 0.8493 - val_loss: 0.4150 - val_accuracy: 0.8806\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5222 - accuracy: 0.8471 - val_loss: 0.4463 - val_accuracy: 0.8646\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5219 - accuracy: 0.8485 - val_loss: 0.5219 - val_accuracy: 0.8434\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5180 - accuracy: 0.8483 - val_loss: 0.4308 - val_accuracy: 0.8689\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5173 - accuracy: 0.8489 - val_loss: 0.4847 - val_accuracy: 0.8538\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5111 - accuracy: 0.8498 - val_loss: 0.6184 - val_accuracy: 0.8115\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5102 - accuracy: 0.8519 - val_loss: 0.4436 - val_accuracy: 0.8711\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5114 - accuracy: 0.8495 - val_loss: 0.5090 - val_accuracy: 0.8481\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5037 - accuracy: 0.8521 - val_loss: 0.4973 - val_accuracy: 0.8544\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5008 - accuracy: 0.8539 - val_loss: 0.5391 - val_accuracy: 0.8390\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5025 - accuracy: 0.8530 - val_loss: 0.4508 - val_accuracy: 0.8667\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4974 - accuracy: 0.8565 - val_loss: 0.5122 - val_accuracy: 0.8456\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4953 - accuracy: 0.8550 - val_loss: 0.3616 - val_accuracy: 0.8965\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4941 - accuracy: 0.8578 - val_loss: 0.4046 - val_accuracy: 0.8798\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4915 - accuracy: 0.8572 - val_loss: 0.4891 - val_accuracy: 0.8536\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4953 - accuracy: 0.8545 - val_loss: 0.4360 - val_accuracy: 0.8729\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4901 - accuracy: 0.8580 - val_loss: 0.3230 - val_accuracy: 0.9112\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4852 - accuracy: 0.8585 - val_loss: 0.5528 - val_accuracy: 0.8326\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4869 - accuracy: 0.8575 - val_loss: 0.5096 - val_accuracy: 0.8503\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4820 - accuracy: 0.8596 - val_loss: 0.3625 - val_accuracy: 0.8964\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4911 - accuracy: 0.8575 - val_loss: 0.3890 - val_accuracy: 0.8856\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4761 - accuracy: 0.8627 - val_loss: 0.3504 - val_accuracy: 0.8994\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4760 - accuracy: 0.8597 - val_loss: 0.6619 - val_accuracy: 0.7995\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4740 - accuracy: 0.8622 - val_loss: 0.3435 - val_accuracy: 0.9021\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4740 - accuracy: 0.8630 - val_loss: 0.4356 - val_accuracy: 0.8735\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4690 - accuracy: 0.8627 - val_loss: 0.5293 - val_accuracy: 0.8388\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4752 - accuracy: 0.8622 - val_loss: 0.4357 - val_accuracy: 0.8727\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.8623 - val_loss: 0.3930 - val_accuracy: 0.8848\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4633 - accuracy: 0.8636 - val_loss: 0.4806 - val_accuracy: 0.8585\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4671 - accuracy: 0.8636 - val_loss: 0.3647 - val_accuracy: 0.8963\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4586 - accuracy: 0.8683 - val_loss: 0.4515 - val_accuracy: 0.8654\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4586 - accuracy: 0.8650 - val_loss: 0.4932 - val_accuracy: 0.8551\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4615 - accuracy: 0.8657 - val_loss: 0.3719 - val_accuracy: 0.8907\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4597 - accuracy: 0.8655 - val_loss: 0.3944 - val_accuracy: 0.8857\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4479 - accuracy: 0.8701 - val_loss: 0.4991 - val_accuracy: 0.8532\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4516 - accuracy: 0.8688 - val_loss: 0.3653 - val_accuracy: 0.8946\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4525 - accuracy: 0.8700 - val_loss: 0.4272 - val_accuracy: 0.8746\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4543 - accuracy: 0.8675 - val_loss: 0.3550 - val_accuracy: 0.8977\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4429 - accuracy: 0.8725 - val_loss: 0.5696 - val_accuracy: 0.8252\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4480 - accuracy: 0.8690 - val_loss: 0.3332 - val_accuracy: 0.9033\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4435 - accuracy: 0.8710 - val_loss: 0.3878 - val_accuracy: 0.8874\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4436 - accuracy: 0.8720 - val_loss: 0.4498 - val_accuracy: 0.8686\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4376 - accuracy: 0.8729 - val_loss: 0.4162 - val_accuracy: 0.8771\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4411 - accuracy: 0.8716 - val_loss: 0.3483 - val_accuracy: 0.8987\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4384 - accuracy: 0.8732 - val_loss: 0.3482 - val_accuracy: 0.8974\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4342 - accuracy: 0.8743 - val_loss: 0.4065 - val_accuracy: 0.8833\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4308 - accuracy: 0.8749 - val_loss: 0.4296 - val_accuracy: 0.8754\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4348 - accuracy: 0.8714 - val_loss: 0.5336 - val_accuracy: 0.8399\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4300 - accuracy: 0.8739 - val_loss: 0.4989 - val_accuracy: 0.8499\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4326 - accuracy: 0.8741 - val_loss: 0.3643 - val_accuracy: 0.8940\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4244 - accuracy: 0.8762 - val_loss: 0.3436 - val_accuracy: 0.9008\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4233 - accuracy: 0.8770 - val_loss: 0.3927 - val_accuracy: 0.8845\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4228 - accuracy: 0.8760 - val_loss: 0.3787 - val_accuracy: 0.8883\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4252 - accuracy: 0.8770 - val_loss: 0.4947 - val_accuracy: 0.8516\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4266 - accuracy: 0.8772 - val_loss: 0.3331 - val_accuracy: 0.9034\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4225 - accuracy: 0.8781 - val_loss: 0.4571 - val_accuracy: 0.8577\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4189 - accuracy: 0.8780 - val_loss: 0.4229 - val_accuracy: 0.8768\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4238 - accuracy: 0.8764 - val_loss: 0.4329 - val_accuracy: 0.8744\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4114 - accuracy: 0.8805 - val_loss: 0.3418 - val_accuracy: 0.9036\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4133 - accuracy: 0.8803 - val_loss: 0.4623 - val_accuracy: 0.8681\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4089 - accuracy: 0.8805 - val_loss: 0.5261 - val_accuracy: 0.8406\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4068 - accuracy: 0.8813 - val_loss: 0.5537 - val_accuracy: 0.8306\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4101 - accuracy: 0.8806 - val_loss: 0.4393 - val_accuracy: 0.8723\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4030 - accuracy: 0.8837 - val_loss: 0.3080 - val_accuracy: 0.9122\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4070 - accuracy: 0.8820 - val_loss: 0.4221 - val_accuracy: 0.8729\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8830 - val_loss: 0.5469 - val_accuracy: 0.8299\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4122 - accuracy: 0.8807 - val_loss: 0.4194 - val_accuracy: 0.8748\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8842 - val_loss: 0.3767 - val_accuracy: 0.8908\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3968 - accuracy: 0.8833 - val_loss: 0.4140 - val_accuracy: 0.8791\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8853 - val_loss: 0.3319 - val_accuracy: 0.9053\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4009 - accuracy: 0.8828 - val_loss: 0.3799 - val_accuracy: 0.8877\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4001 - accuracy: 0.8829 - val_loss: 0.4033 - val_accuracy: 0.8871\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3955 - accuracy: 0.8843 - val_loss: 0.3612 - val_accuracy: 0.8945\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3947 - accuracy: 0.8861 - val_loss: 0.4308 - val_accuracy: 0.8762\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8846 - val_loss: 0.4135 - val_accuracy: 0.8791\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3940 - accuracy: 0.8866 - val_loss: 0.3444 - val_accuracy: 0.8966\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3870 - accuracy: 0.8873 - val_loss: 0.4665 - val_accuracy: 0.8628\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8885 - val_loss: 0.4154 - val_accuracy: 0.8769\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3924 - accuracy: 0.8859 - val_loss: 0.3908 - val_accuracy: 0.8859\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3917 - accuracy: 0.8862 - val_loss: 0.3002 - val_accuracy: 0.9131\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8887 - val_loss: 0.4066 - val_accuracy: 0.8821\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8892 - val_loss: 0.4694 - val_accuracy: 0.8616\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3849 - accuracy: 0.8881 - val_loss: 0.3112 - val_accuracy: 0.9119\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3835 - accuracy: 0.8895 - val_loss: 0.4131 - val_accuracy: 0.8813\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3829 - accuracy: 0.8890 - val_loss: 0.3469 - val_accuracy: 0.9002\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3804 - accuracy: 0.8892 - val_loss: 0.5000 - val_accuracy: 0.8421\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3798 - accuracy: 0.8887 - val_loss: 0.3167 - val_accuracy: 0.9108\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3782 - accuracy: 0.8914 - val_loss: 0.3097 - val_accuracy: 0.9104\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3801 - accuracy: 0.8920 - val_loss: 0.3613 - val_accuracy: 0.8970\n",
      "Try 9/100: Best_val_acc: [0.5073733925819397, 0.8533333539962769], lr: 6.131251018561394e-05, Lambda: 5.803267479684179e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-4.22, -4.21))\n",
    "    Lambda = math.pow(10, np.random.uniform(-4.24, -4.23))\n",
    "    best_acc = basicDeepNN1(200, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy for test data is slightly better with an increase of 3-4% after adding Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 775565,
     "status": "ok",
     "timestamp": 1594473341060,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "75NOZJoP2qeW",
    "outputId": "df7c3125-1042-42f6-e178-1c7764218152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.6596 - accuracy: 0.1047 - val_loss: 2.2971 - val_accuracy: 0.1331\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5675 - accuracy: 0.1169 - val_loss: 2.2765 - val_accuracy: 0.1889\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 2.4961 - accuracy: 0.1280 - val_loss: 2.2338 - val_accuracy: 0.2274\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4333 - accuracy: 0.1423 - val_loss: 2.1752 - val_accuracy: 0.2756\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3778 - accuracy: 0.1561 - val_loss: 2.1271 - val_accuracy: 0.2771\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.3142 - accuracy: 0.1787 - val_loss: 2.0284 - val_accuracy: 0.3768\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2515 - accuracy: 0.2030 - val_loss: 1.9955 - val_accuracy: 0.3723\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.1985 - accuracy: 0.2185 - val_loss: 1.9230 - val_accuracy: 0.4158\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1381 - accuracy: 0.2415 - val_loss: 1.8630 - val_accuracy: 0.4350\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.0791 - accuracy: 0.2625 - val_loss: 1.8331 - val_accuracy: 0.4706\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0280 - accuracy: 0.2800 - val_loss: 1.7575 - val_accuracy: 0.5067\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9783 - accuracy: 0.3015 - val_loss: 1.6977 - val_accuracy: 0.5148\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9305 - accuracy: 0.3190 - val_loss: 1.6392 - val_accuracy: 0.5294\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8867 - accuracy: 0.3347 - val_loss: 1.5841 - val_accuracy: 0.5552\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.8370 - accuracy: 0.3585 - val_loss: 1.5164 - val_accuracy: 0.5900\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7977 - accuracy: 0.3724 - val_loss: 1.4334 - val_accuracy: 0.6357\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7651 - accuracy: 0.3893 - val_loss: 1.4378 - val_accuracy: 0.6084\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7319 - accuracy: 0.4030 - val_loss: 1.4026 - val_accuracy: 0.6177\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6917 - accuracy: 0.4211 - val_loss: 1.3835 - val_accuracy: 0.6331\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6554 - accuracy: 0.4358 - val_loss: 1.3009 - val_accuracy: 0.6609\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6239 - accuracy: 0.4505 - val_loss: 1.2753 - val_accuracy: 0.6700\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5873 - accuracy: 0.4667 - val_loss: 1.2452 - val_accuracy: 0.6849\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5601 - accuracy: 0.4769 - val_loss: 1.2458 - val_accuracy: 0.6756\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5268 - accuracy: 0.4926 - val_loss: 1.2033 - val_accuracy: 0.6807\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4962 - accuracy: 0.5031 - val_loss: 1.1481 - val_accuracy: 0.7066\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4698 - accuracy: 0.5160 - val_loss: 1.1716 - val_accuracy: 0.6888\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4452 - accuracy: 0.5260 - val_loss: 1.0851 - val_accuracy: 0.7200\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4204 - accuracy: 0.5364 - val_loss: 1.1300 - val_accuracy: 0.6899\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.3964 - accuracy: 0.5443 - val_loss: 1.0828 - val_accuracy: 0.7083\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3764 - accuracy: 0.5520 - val_loss: 1.0799 - val_accuracy: 0.6981\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3582 - accuracy: 0.5602 - val_loss: 0.9890 - val_accuracy: 0.7342\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3308 - accuracy: 0.5716 - val_loss: 0.9676 - val_accuracy: 0.7349\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3048 - accuracy: 0.5792 - val_loss: 1.0080 - val_accuracy: 0.7264\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2893 - accuracy: 0.5827 - val_loss: 0.9271 - val_accuracy: 0.7546\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2642 - accuracy: 0.5947 - val_loss: 0.9361 - val_accuracy: 0.7409\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2455 - accuracy: 0.6041 - val_loss: 0.9400 - val_accuracy: 0.7307\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2295 - accuracy: 0.6062 - val_loss: 0.9385 - val_accuracy: 0.7441\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2116 - accuracy: 0.6139 - val_loss: 0.8920 - val_accuracy: 0.7575\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1837 - accuracy: 0.6253 - val_loss: 0.9046 - val_accuracy: 0.7491\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1696 - accuracy: 0.6309 - val_loss: 0.8275 - val_accuracy: 0.7720\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1552 - accuracy: 0.6336 - val_loss: 0.8468 - val_accuracy: 0.7547\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1399 - accuracy: 0.6386 - val_loss: 0.9095 - val_accuracy: 0.7434\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1197 - accuracy: 0.6470 - val_loss: 0.8712 - val_accuracy: 0.7587\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1026 - accuracy: 0.6547 - val_loss: 0.9198 - val_accuracy: 0.7370\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0924 - accuracy: 0.6589 - val_loss: 0.8282 - val_accuracy: 0.7539\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0783 - accuracy: 0.6643 - val_loss: 0.8648 - val_accuracy: 0.7478\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0631 - accuracy: 0.6694 - val_loss: 0.8298 - val_accuracy: 0.7552\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0457 - accuracy: 0.6764 - val_loss: 0.9010 - val_accuracy: 0.7354\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0352 - accuracy: 0.6794 - val_loss: 0.8158 - val_accuracy: 0.7663\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0227 - accuracy: 0.6837 - val_loss: 0.6777 - val_accuracy: 0.8165\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0068 - accuracy: 0.6897 - val_loss: 0.7178 - val_accuracy: 0.8011\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0049 - accuracy: 0.6913 - val_loss: 0.7797 - val_accuracy: 0.7756\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9882 - accuracy: 0.6986 - val_loss: 0.7438 - val_accuracy: 0.7899\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9732 - accuracy: 0.7028 - val_loss: 0.6873 - val_accuracy: 0.8050\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9632 - accuracy: 0.7032 - val_loss: 0.6682 - val_accuracy: 0.8155\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9574 - accuracy: 0.7090 - val_loss: 0.7755 - val_accuracy: 0.7679\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9407 - accuracy: 0.7128 - val_loss: 0.6228 - val_accuracy: 0.8275\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9299 - accuracy: 0.7168 - val_loss: 0.6893 - val_accuracy: 0.8069\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9251 - accuracy: 0.7183 - val_loss: 0.7080 - val_accuracy: 0.7999\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9110 - accuracy: 0.7232 - val_loss: 0.7437 - val_accuracy: 0.7773\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9035 - accuracy: 0.7260 - val_loss: 0.6554 - val_accuracy: 0.8139\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8923 - accuracy: 0.7294 - val_loss: 0.7722 - val_accuracy: 0.7708\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8826 - accuracy: 0.7336 - val_loss: 0.7215 - val_accuracy: 0.7931\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8774 - accuracy: 0.7354 - val_loss: 0.6536 - val_accuracy: 0.8140\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8716 - accuracy: 0.7353 - val_loss: 0.6578 - val_accuracy: 0.8079\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8565 - accuracy: 0.7418 - val_loss: 0.6228 - val_accuracy: 0.8174\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8549 - accuracy: 0.7429 - val_loss: 0.6195 - val_accuracy: 0.8221\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8442 - accuracy: 0.7433 - val_loss: 0.5573 - val_accuracy: 0.8417\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8389 - accuracy: 0.7454 - val_loss: 0.5911 - val_accuracy: 0.8306\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8264 - accuracy: 0.7499 - val_loss: 0.6270 - val_accuracy: 0.8194\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8249 - accuracy: 0.7502 - val_loss: 0.6396 - val_accuracy: 0.8116\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8139 - accuracy: 0.7540 - val_loss: 0.6207 - val_accuracy: 0.8201\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8086 - accuracy: 0.7572 - val_loss: 0.5766 - val_accuracy: 0.8315\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7985 - accuracy: 0.7587 - val_loss: 0.6039 - val_accuracy: 0.8236\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7972 - accuracy: 0.7603 - val_loss: 0.6475 - val_accuracy: 0.8050\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.7629 - val_loss: 0.5883 - val_accuracy: 0.8282\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7830 - accuracy: 0.7657 - val_loss: 0.6028 - val_accuracy: 0.8224\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7714 - accuracy: 0.7691 - val_loss: 0.6464 - val_accuracy: 0.8054\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7709 - accuracy: 0.7706 - val_loss: 0.5345 - val_accuracy: 0.8431\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7690 - accuracy: 0.7688 - val_loss: 0.5362 - val_accuracy: 0.8431\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7619 - accuracy: 0.7702 - val_loss: 0.5372 - val_accuracy: 0.8457\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7598 - accuracy: 0.7721 - val_loss: 0.5232 - val_accuracy: 0.8498\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7455 - accuracy: 0.7781 - val_loss: 0.5684 - val_accuracy: 0.8364\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7440 - accuracy: 0.7772 - val_loss: 0.6161 - val_accuracy: 0.8131\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7422 - accuracy: 0.7768 - val_loss: 0.4933 - val_accuracy: 0.8583\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7291 - accuracy: 0.7809 - val_loss: 0.5486 - val_accuracy: 0.8416\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7316 - accuracy: 0.7791 - val_loss: 0.5850 - val_accuracy: 0.8181\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7199 - accuracy: 0.7838 - val_loss: 0.5234 - val_accuracy: 0.8490\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7181 - accuracy: 0.7855 - val_loss: 0.5417 - val_accuracy: 0.8426\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7139 - accuracy: 0.7884 - val_loss: 0.5177 - val_accuracy: 0.8524\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7086 - accuracy: 0.7874 - val_loss: 0.4830 - val_accuracy: 0.8594\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7047 - accuracy: 0.7882 - val_loss: 0.5665 - val_accuracy: 0.8325\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7018 - accuracy: 0.7902 - val_loss: 0.4762 - val_accuracy: 0.8606\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6948 - accuracy: 0.7939 - val_loss: 0.5423 - val_accuracy: 0.8361\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6877 - accuracy: 0.7950 - val_loss: 0.5524 - val_accuracy: 0.8371\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6848 - accuracy: 0.7954 - val_loss: 0.4825 - val_accuracy: 0.8634\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6871 - accuracy: 0.7959 - val_loss: 0.4888 - val_accuracy: 0.8612\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6790 - accuracy: 0.8005 - val_loss: 0.5972 - val_accuracy: 0.8114\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.7970 - val_loss: 0.4538 - val_accuracy: 0.8682\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6739 - accuracy: 0.8010 - val_loss: 0.5064 - val_accuracy: 0.8539\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6695 - accuracy: 0.7993 - val_loss: 0.5845 - val_accuracy: 0.8299\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6571 - accuracy: 0.8047 - val_loss: 0.5400 - val_accuracy: 0.8388\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6594 - accuracy: 0.8045 - val_loss: 0.4524 - val_accuracy: 0.8715\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6595 - accuracy: 0.8063 - val_loss: 0.4971 - val_accuracy: 0.8559\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.8075 - val_loss: 0.4991 - val_accuracy: 0.8514\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.8053 - val_loss: 0.4142 - val_accuracy: 0.8846\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6450 - accuracy: 0.8080 - val_loss: 0.4565 - val_accuracy: 0.8680\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6376 - accuracy: 0.8101 - val_loss: 0.7447 - val_accuracy: 0.7665\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6382 - accuracy: 0.8119 - val_loss: 0.4706 - val_accuracy: 0.8633\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6378 - accuracy: 0.8122 - val_loss: 0.4716 - val_accuracy: 0.8579\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6313 - accuracy: 0.8114 - val_loss: 0.4442 - val_accuracy: 0.8706\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6260 - accuracy: 0.8123 - val_loss: 0.4888 - val_accuracy: 0.8545\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6273 - accuracy: 0.8156 - val_loss: 0.4964 - val_accuracy: 0.8514\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6281 - accuracy: 0.8149 - val_loss: 0.5009 - val_accuracy: 0.8542\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6186 - accuracy: 0.8172 - val_loss: 0.4582 - val_accuracy: 0.8703\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6220 - accuracy: 0.8157 - val_loss: 0.4701 - val_accuracy: 0.8649\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6208 - accuracy: 0.8189 - val_loss: 0.5594 - val_accuracy: 0.8339\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6095 - accuracy: 0.8190 - val_loss: 0.4199 - val_accuracy: 0.8801\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.8192 - val_loss: 0.4741 - val_accuracy: 0.8627\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6034 - accuracy: 0.8233 - val_loss: 0.4375 - val_accuracy: 0.8756\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6086 - accuracy: 0.8201 - val_loss: 0.4164 - val_accuracy: 0.8798\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5970 - accuracy: 0.8227 - val_loss: 0.4759 - val_accuracy: 0.8621\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6004 - accuracy: 0.8233 - val_loss: 0.5209 - val_accuracy: 0.8475\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5950 - accuracy: 0.8233 - val_loss: 0.4274 - val_accuracy: 0.8742\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5931 - accuracy: 0.8242 - val_loss: 0.4953 - val_accuracy: 0.8558\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5850 - accuracy: 0.8288 - val_loss: 0.5218 - val_accuracy: 0.8380\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5881 - accuracy: 0.8257 - val_loss: 0.5141 - val_accuracy: 0.8494\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5833 - accuracy: 0.8285 - val_loss: 0.6147 - val_accuracy: 0.8158\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5836 - accuracy: 0.8282 - val_loss: 0.4822 - val_accuracy: 0.8574\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5849 - accuracy: 0.8285 - val_loss: 0.5649 - val_accuracy: 0.8325\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5789 - accuracy: 0.8286 - val_loss: 0.4775 - val_accuracy: 0.8619\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5772 - accuracy: 0.8295 - val_loss: 0.4943 - val_accuracy: 0.8561\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5759 - accuracy: 0.8285 - val_loss: 0.5509 - val_accuracy: 0.8368\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.8329 - val_loss: 0.4524 - val_accuracy: 0.8698\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5660 - accuracy: 0.8336 - val_loss: 0.4418 - val_accuracy: 0.8709\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5620 - accuracy: 0.8336 - val_loss: 0.4944 - val_accuracy: 0.8579\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5671 - accuracy: 0.8339 - val_loss: 0.5079 - val_accuracy: 0.8501\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5623 - accuracy: 0.8334 - val_loss: 0.4561 - val_accuracy: 0.8671\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5661 - accuracy: 0.8335 - val_loss: 0.4794 - val_accuracy: 0.8619\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5617 - accuracy: 0.8339 - val_loss: 0.4314 - val_accuracy: 0.8759\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5572 - accuracy: 0.8371 - val_loss: 0.4084 - val_accuracy: 0.8814\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5573 - accuracy: 0.8358 - val_loss: 0.4054 - val_accuracy: 0.8831\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5531 - accuracy: 0.8382 - val_loss: 0.3965 - val_accuracy: 0.8865\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5501 - accuracy: 0.8398 - val_loss: 0.4466 - val_accuracy: 0.8711\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5439 - accuracy: 0.8399 - val_loss: 0.4263 - val_accuracy: 0.8719\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5435 - accuracy: 0.8410 - val_loss: 0.4921 - val_accuracy: 0.8559\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5425 - accuracy: 0.8403 - val_loss: 0.4165 - val_accuracy: 0.8814\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5421 - accuracy: 0.8401 - val_loss: 0.3932 - val_accuracy: 0.8884\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5362 - accuracy: 0.8439 - val_loss: 0.4873 - val_accuracy: 0.8554\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5369 - accuracy: 0.8429 - val_loss: 0.4307 - val_accuracy: 0.8767\n",
      "Try 1/100: Best_val_acc: [0.504753053188324, 0.852055549621582], lr: 4.2441873723535886e-05, Lambda: 9.799246615983262e-05\n",
      "\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.6893 - accuracy: 0.1087 - val_loss: 2.3292 - val_accuracy: 0.0991\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5554 - accuracy: 0.1208 - val_loss: 2.3455 - val_accuracy: 0.1551\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4576 - accuracy: 0.1375 - val_loss: 2.2508 - val_accuracy: 0.2041\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3726 - accuracy: 0.1590 - val_loss: 2.1324 - val_accuracy: 0.2447\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.2951 - accuracy: 0.1827 - val_loss: 2.0639 - val_accuracy: 0.3129\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2205 - accuracy: 0.2087 - val_loss: 2.0285 - val_accuracy: 0.3380\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1388 - accuracy: 0.2390 - val_loss: 1.8273 - val_accuracy: 0.4511\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0612 - accuracy: 0.2673 - val_loss: 1.8634 - val_accuracy: 0.4072\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9909 - accuracy: 0.2998 - val_loss: 1.8059 - val_accuracy: 0.4426\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9280 - accuracy: 0.3225 - val_loss: 1.7057 - val_accuracy: 0.4944\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8633 - accuracy: 0.3542 - val_loss: 1.6878 - val_accuracy: 0.4877\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8031 - accuracy: 0.3798 - val_loss: 1.6166 - val_accuracy: 0.5251\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7514 - accuracy: 0.4018 - val_loss: 1.4672 - val_accuracy: 0.6072\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6906 - accuracy: 0.4291 - val_loss: 1.4408 - val_accuracy: 0.5896\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6401 - accuracy: 0.4508 - val_loss: 1.4337 - val_accuracy: 0.5986\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5988 - accuracy: 0.4722 - val_loss: 1.3385 - val_accuracy: 0.6481\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5452 - accuracy: 0.4953 - val_loss: 1.3585 - val_accuracy: 0.6062\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5090 - accuracy: 0.5099 - val_loss: 1.2774 - val_accuracy: 0.6348\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4635 - accuracy: 0.5294 - val_loss: 1.1634 - val_accuracy: 0.7042\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4274 - accuracy: 0.5433 - val_loss: 1.1849 - val_accuracy: 0.6872\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3950 - accuracy: 0.5565 - val_loss: 1.0939 - val_accuracy: 0.7190\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3581 - accuracy: 0.5717 - val_loss: 1.0639 - val_accuracy: 0.7188\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3210 - accuracy: 0.5865 - val_loss: 1.0429 - val_accuracy: 0.7305\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2924 - accuracy: 0.5958 - val_loss: 0.9635 - val_accuracy: 0.7581\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2623 - accuracy: 0.6065 - val_loss: 1.0164 - val_accuracy: 0.7226\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2323 - accuracy: 0.6191 - val_loss: 1.0119 - val_accuracy: 0.7247\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2070 - accuracy: 0.6301 - val_loss: 0.9554 - val_accuracy: 0.7379\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1834 - accuracy: 0.6344 - val_loss: 0.9269 - val_accuracy: 0.7477\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1573 - accuracy: 0.6446 - val_loss: 1.1277 - val_accuracy: 0.6574\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1425 - accuracy: 0.6502 - val_loss: 1.0008 - val_accuracy: 0.7079\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1126 - accuracy: 0.6612 - val_loss: 0.9068 - val_accuracy: 0.7486\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0921 - accuracy: 0.6655 - val_loss: 0.7301 - val_accuracy: 0.8112\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0810 - accuracy: 0.6698 - val_loss: 0.9788 - val_accuracy: 0.7116\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0585 - accuracy: 0.6778 - val_loss: 0.9132 - val_accuracy: 0.7266\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0374 - accuracy: 0.6867 - val_loss: 0.7994 - val_accuracy: 0.7759\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0225 - accuracy: 0.6894 - val_loss: 0.8730 - val_accuracy: 0.7409\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0088 - accuracy: 0.6907 - val_loss: 0.7219 - val_accuracy: 0.7918\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9860 - accuracy: 0.7033 - val_loss: 0.7622 - val_accuracy: 0.7772\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9743 - accuracy: 0.7043 - val_loss: 0.7545 - val_accuracy: 0.7848\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9533 - accuracy: 0.7132 - val_loss: 0.7180 - val_accuracy: 0.7940\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9483 - accuracy: 0.7133 - val_loss: 0.7668 - val_accuracy: 0.7759\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9279 - accuracy: 0.7217 - val_loss: 0.6568 - val_accuracy: 0.8167\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9152 - accuracy: 0.7266 - val_loss: 0.7972 - val_accuracy: 0.7563\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9029 - accuracy: 0.7289 - val_loss: 0.6310 - val_accuracy: 0.8249\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8939 - accuracy: 0.7312 - val_loss: 0.6574 - val_accuracy: 0.8103\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8707 - accuracy: 0.7403 - val_loss: 0.6068 - val_accuracy: 0.8267\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8648 - accuracy: 0.7421 - val_loss: 0.7696 - val_accuracy: 0.7683\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8561 - accuracy: 0.7447 - val_loss: 0.6354 - val_accuracy: 0.8209\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8474 - accuracy: 0.7461 - val_loss: 0.7439 - val_accuracy: 0.7797\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8357 - accuracy: 0.7504 - val_loss: 0.7087 - val_accuracy: 0.7852\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8263 - accuracy: 0.7528 - val_loss: 0.6501 - val_accuracy: 0.8065\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8176 - accuracy: 0.7562 - val_loss: 0.5454 - val_accuracy: 0.8414\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8039 - accuracy: 0.7604 - val_loss: 0.5541 - val_accuracy: 0.8396\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.7620 - val_loss: 0.5586 - val_accuracy: 0.8374\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7907 - accuracy: 0.7645 - val_loss: 0.5764 - val_accuracy: 0.8294\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7778 - accuracy: 0.7680 - val_loss: 0.5086 - val_accuracy: 0.8551\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7704 - accuracy: 0.7729 - val_loss: 0.7812 - val_accuracy: 0.7536\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7713 - accuracy: 0.7692 - val_loss: 0.5002 - val_accuracy: 0.8536\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7501 - accuracy: 0.7765 - val_loss: 0.5736 - val_accuracy: 0.8286\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7446 - accuracy: 0.7779 - val_loss: 0.5069 - val_accuracy: 0.8549\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7450 - accuracy: 0.7792 - val_loss: 0.5275 - val_accuracy: 0.8405\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7358 - accuracy: 0.7832 - val_loss: 0.5583 - val_accuracy: 0.8356\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7227 - accuracy: 0.7860 - val_loss: 0.6873 - val_accuracy: 0.7913\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7217 - accuracy: 0.7863 - val_loss: 0.5303 - val_accuracy: 0.8425\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7195 - accuracy: 0.7853 - val_loss: 0.8440 - val_accuracy: 0.7430\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7083 - accuracy: 0.7910 - val_loss: 0.5573 - val_accuracy: 0.8251\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7038 - accuracy: 0.7931 - val_loss: 0.5547 - val_accuracy: 0.8288\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6970 - accuracy: 0.7935 - val_loss: 0.5911 - val_accuracy: 0.8179\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6959 - accuracy: 0.7938 - val_loss: 0.5322 - val_accuracy: 0.8428\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6814 - accuracy: 0.7980 - val_loss: 0.4756 - val_accuracy: 0.8597\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6767 - accuracy: 0.8014 - val_loss: 0.5015 - val_accuracy: 0.8551\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6731 - accuracy: 0.8023 - val_loss: 0.5457 - val_accuracy: 0.8403\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6747 - accuracy: 0.7999 - val_loss: 0.4852 - val_accuracy: 0.8592\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6644 - accuracy: 0.8047 - val_loss: 0.8258 - val_accuracy: 0.7475\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6655 - accuracy: 0.8033 - val_loss: 0.6258 - val_accuracy: 0.8106\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6549 - accuracy: 0.8061 - val_loss: 0.5573 - val_accuracy: 0.8260\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6484 - accuracy: 0.8077 - val_loss: 0.5870 - val_accuracy: 0.8207\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6465 - accuracy: 0.8093 - val_loss: 0.4048 - val_accuracy: 0.8822\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6433 - accuracy: 0.8115 - val_loss: 0.6234 - val_accuracy: 0.8021\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6373 - accuracy: 0.8125 - val_loss: 0.4821 - val_accuracy: 0.8545\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6296 - accuracy: 0.8136 - val_loss: 0.5087 - val_accuracy: 0.8462\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6231 - accuracy: 0.8159 - val_loss: 0.5368 - val_accuracy: 0.8388\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6192 - accuracy: 0.8175 - val_loss: 0.5029 - val_accuracy: 0.8535\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6145 - accuracy: 0.8169 - val_loss: 0.5481 - val_accuracy: 0.8335\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6156 - accuracy: 0.8204 - val_loss: 0.4361 - val_accuracy: 0.8699\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6117 - accuracy: 0.8201 - val_loss: 0.4746 - val_accuracy: 0.8564\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6040 - accuracy: 0.8227 - val_loss: 0.5310 - val_accuracy: 0.8400\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6064 - accuracy: 0.8206 - val_loss: 0.5090 - val_accuracy: 0.8459\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6034 - accuracy: 0.8219 - val_loss: 0.4486 - val_accuracy: 0.8674\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5970 - accuracy: 0.8234 - val_loss: 0.4670 - val_accuracy: 0.8580\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5864 - accuracy: 0.8276 - val_loss: 0.5534 - val_accuracy: 0.8349\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5901 - accuracy: 0.8267 - val_loss: 0.6591 - val_accuracy: 0.8010\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5843 - accuracy: 0.8290 - val_loss: 0.4685 - val_accuracy: 0.8603\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5853 - accuracy: 0.8298 - val_loss: 0.4825 - val_accuracy: 0.8529\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5775 - accuracy: 0.8313 - val_loss: 0.4477 - val_accuracy: 0.8670\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5710 - accuracy: 0.8320 - val_loss: 0.4778 - val_accuracy: 0.8587\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5785 - accuracy: 0.8315 - val_loss: 0.4358 - val_accuracy: 0.8741\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5679 - accuracy: 0.8320 - val_loss: 0.4697 - val_accuracy: 0.8599\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5696 - accuracy: 0.8320 - val_loss: 0.4830 - val_accuracy: 0.8577\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5695 - accuracy: 0.8334 - val_loss: 0.5128 - val_accuracy: 0.8461\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5631 - accuracy: 0.8345 - val_loss: 0.5236 - val_accuracy: 0.8390\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5584 - accuracy: 0.8361 - val_loss: 0.4688 - val_accuracy: 0.8591\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5542 - accuracy: 0.8375 - val_loss: 0.4361 - val_accuracy: 0.8730\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5530 - accuracy: 0.8390 - val_loss: 0.4038 - val_accuracy: 0.8841\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5494 - accuracy: 0.8387 - val_loss: 0.4139 - val_accuracy: 0.8816\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5438 - accuracy: 0.8406 - val_loss: 0.4319 - val_accuracy: 0.8694\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5403 - accuracy: 0.8415 - val_loss: 0.3671 - val_accuracy: 0.8934\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5423 - accuracy: 0.8410 - val_loss: 0.6569 - val_accuracy: 0.8083\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5414 - accuracy: 0.8441 - val_loss: 0.4324 - val_accuracy: 0.8716\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5353 - accuracy: 0.8438 - val_loss: 0.4176 - val_accuracy: 0.8771\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5368 - accuracy: 0.8432 - val_loss: 0.4964 - val_accuracy: 0.8541\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5295 - accuracy: 0.8460 - val_loss: 0.6741 - val_accuracy: 0.7959\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5302 - accuracy: 0.8452 - val_loss: 0.4803 - val_accuracy: 0.8507\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5224 - accuracy: 0.8504 - val_loss: 0.4948 - val_accuracy: 0.8514\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5291 - accuracy: 0.8453 - val_loss: 0.5282 - val_accuracy: 0.8419\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5254 - accuracy: 0.8463 - val_loss: 0.5166 - val_accuracy: 0.8471\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5218 - accuracy: 0.8468 - val_loss: 0.4098 - val_accuracy: 0.8782\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5147 - accuracy: 0.8506 - val_loss: 0.3622 - val_accuracy: 0.8951\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5156 - accuracy: 0.8494 - val_loss: 0.4141 - val_accuracy: 0.8808\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5157 - accuracy: 0.8509 - val_loss: 0.4449 - val_accuracy: 0.8714\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5104 - accuracy: 0.8504 - val_loss: 0.4076 - val_accuracy: 0.8824\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5046 - accuracy: 0.8525 - val_loss: 0.4202 - val_accuracy: 0.8781\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5067 - accuracy: 0.8526 - val_loss: 0.4228 - val_accuracy: 0.8770\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5043 - accuracy: 0.8543 - val_loss: 0.4032 - val_accuracy: 0.8814\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5002 - accuracy: 0.8535 - val_loss: 0.5488 - val_accuracy: 0.8357\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5025 - accuracy: 0.8533 - val_loss: 0.4005 - val_accuracy: 0.8825\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4980 - accuracy: 0.8554 - val_loss: 0.5461 - val_accuracy: 0.8375\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4903 - accuracy: 0.8582 - val_loss: 0.3788 - val_accuracy: 0.8915\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4979 - accuracy: 0.8542 - val_loss: 0.4896 - val_accuracy: 0.8506\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4914 - accuracy: 0.8550 - val_loss: 0.7610 - val_accuracy: 0.7822\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4980 - accuracy: 0.8538 - val_loss: 0.3873 - val_accuracy: 0.8861\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4872 - accuracy: 0.8581 - val_loss: 0.5586 - val_accuracy: 0.8326\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4842 - accuracy: 0.8595 - val_loss: 0.4331 - val_accuracy: 0.8697\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4829 - accuracy: 0.8595 - val_loss: 0.4786 - val_accuracy: 0.8594\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4791 - accuracy: 0.8597 - val_loss: 0.4927 - val_accuracy: 0.8514\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4841 - accuracy: 0.8570 - val_loss: 0.4438 - val_accuracy: 0.8632\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4737 - accuracy: 0.8623 - val_loss: 0.4394 - val_accuracy: 0.8661\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4796 - accuracy: 0.8606 - val_loss: 0.4583 - val_accuracy: 0.8614\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4747 - accuracy: 0.8614 - val_loss: 0.5510 - val_accuracy: 0.8396\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4738 - accuracy: 0.8631 - val_loss: 0.4462 - val_accuracy: 0.8665\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4678 - accuracy: 0.8640 - val_loss: 0.4958 - val_accuracy: 0.8526\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4687 - accuracy: 0.8630 - val_loss: 0.4313 - val_accuracy: 0.8738\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4690 - accuracy: 0.8643 - val_loss: 0.4918 - val_accuracy: 0.8575\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4633 - accuracy: 0.8663 - val_loss: 0.4380 - val_accuracy: 0.8699\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4709 - accuracy: 0.8634 - val_loss: 0.3321 - val_accuracy: 0.9059\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4572 - accuracy: 0.8677 - val_loss: 0.3367 - val_accuracy: 0.9011\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4608 - accuracy: 0.8667 - val_loss: 0.4265 - val_accuracy: 0.8723\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4572 - accuracy: 0.8659 - val_loss: 0.3364 - val_accuracy: 0.9042\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4586 - accuracy: 0.8658 - val_loss: 0.4222 - val_accuracy: 0.8749\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4509 - accuracy: 0.8681 - val_loss: 0.4129 - val_accuracy: 0.8843\n",
      "Try 2/100: Best_val_acc: [0.5247927904129028, 0.8475000262260437], lr: 5.8755877091715665e-05, Lambda: 8.305383126782692e-05\n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.7205 - accuracy: 0.1016 - val_loss: 2.2176 - val_accuracy: 0.1928\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6796 - accuracy: 0.1065 - val_loss: 2.1553 - val_accuracy: 0.2285\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6440 - accuracy: 0.1095 - val_loss: 2.1618 - val_accuracy: 0.2131\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6108 - accuracy: 0.1114 - val_loss: 2.2013 - val_accuracy: 0.1809\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5760 - accuracy: 0.1141 - val_loss: 2.1861 - val_accuracy: 0.1969\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5438 - accuracy: 0.1216 - val_loss: 2.1735 - val_accuracy: 0.2121\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5151 - accuracy: 0.1259 - val_loss: 2.1691 - val_accuracy: 0.2099\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.4959 - accuracy: 0.1299 - val_loss: 2.1648 - val_accuracy: 0.2136\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4731 - accuracy: 0.1330 - val_loss: 2.1493 - val_accuracy: 0.2224\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.4498 - accuracy: 0.1358 - val_loss: 2.1442 - val_accuracy: 0.2307\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4301 - accuracy: 0.1410 - val_loss: 2.1310 - val_accuracy: 0.2429\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.4140 - accuracy: 0.1429 - val_loss: 2.1461 - val_accuracy: 0.2198\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3928 - accuracy: 0.1506 - val_loss: 2.1309 - val_accuracy: 0.2386\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3785 - accuracy: 0.1529 - val_loss: 2.1186 - val_accuracy: 0.2551\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3524 - accuracy: 0.1590 - val_loss: 2.1036 - val_accuracy: 0.2689\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.3436 - accuracy: 0.1628 - val_loss: 2.0892 - val_accuracy: 0.2824\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3189 - accuracy: 0.1702 - val_loss: 2.0930 - val_accuracy: 0.2845\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.3034 - accuracy: 0.1768 - val_loss: 2.0712 - val_accuracy: 0.3126\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2860 - accuracy: 0.1820 - val_loss: 2.0499 - val_accuracy: 0.3154\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2676 - accuracy: 0.1889 - val_loss: 2.0376 - val_accuracy: 0.3159\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2507 - accuracy: 0.1973 - val_loss: 2.0260 - val_accuracy: 0.3351\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.2331 - accuracy: 0.1999 - val_loss: 2.0246 - val_accuracy: 0.3322\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2150 - accuracy: 0.2079 - val_loss: 2.0080 - val_accuracy: 0.3391\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1992 - accuracy: 0.2134 - val_loss: 1.9963 - val_accuracy: 0.3581\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1874 - accuracy: 0.2180 - val_loss: 1.9635 - val_accuracy: 0.3924\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1724 - accuracy: 0.2244 - val_loss: 1.9631 - val_accuracy: 0.3736\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1487 - accuracy: 0.2355 - val_loss: 1.9581 - val_accuracy: 0.3706\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1322 - accuracy: 0.2386 - val_loss: 1.9483 - val_accuracy: 0.3788\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1259 - accuracy: 0.2398 - val_loss: 1.9182 - val_accuracy: 0.4062\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.1074 - accuracy: 0.2464 - val_loss: 1.9059 - val_accuracy: 0.4165\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.0884 - accuracy: 0.2590 - val_loss: 1.8871 - val_accuracy: 0.4301\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0724 - accuracy: 0.2646 - val_loss: 1.8679 - val_accuracy: 0.4429\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0565 - accuracy: 0.2710 - val_loss: 1.8675 - val_accuracy: 0.4267\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0470 - accuracy: 0.2745 - val_loss: 1.8356 - val_accuracy: 0.4419\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0246 - accuracy: 0.2841 - val_loss: 1.8260 - val_accuracy: 0.4569\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0109 - accuracy: 0.2879 - val_loss: 1.8086 - val_accuracy: 0.4634\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9945 - accuracy: 0.2978 - val_loss: 1.7849 - val_accuracy: 0.4822\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9834 - accuracy: 0.3005 - val_loss: 1.7690 - val_accuracy: 0.4791\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.9678 - accuracy: 0.3075 - val_loss: 1.7441 - val_accuracy: 0.4914\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9441 - accuracy: 0.3214 - val_loss: 1.7343 - val_accuracy: 0.4888\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9351 - accuracy: 0.3231 - val_loss: 1.7094 - val_accuracy: 0.4972\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9191 - accuracy: 0.3286 - val_loss: 1.6864 - val_accuracy: 0.5191\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8994 - accuracy: 0.3387 - val_loss: 1.6648 - val_accuracy: 0.5364\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8863 - accuracy: 0.3453 - val_loss: 1.6571 - val_accuracy: 0.5371\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8709 - accuracy: 0.3511 - val_loss: 1.6191 - val_accuracy: 0.5504\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.8584 - accuracy: 0.3559 - val_loss: 1.6285 - val_accuracy: 0.5307\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8412 - accuracy: 0.3654 - val_loss: 1.6152 - val_accuracy: 0.5361\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8253 - accuracy: 0.3697 - val_loss: 1.5769 - val_accuracy: 0.5636\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8138 - accuracy: 0.3771 - val_loss: 1.5601 - val_accuracy: 0.5785\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7964 - accuracy: 0.3819 - val_loss: 1.5395 - val_accuracy: 0.5743\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7831 - accuracy: 0.3910 - val_loss: 1.5057 - val_accuracy: 0.5858\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.7721 - accuracy: 0.3946 - val_loss: 1.5066 - val_accuracy: 0.5917\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7554 - accuracy: 0.4007 - val_loss: 1.4711 - val_accuracy: 0.6111\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.7374 - accuracy: 0.4110 - val_loss: 1.4640 - val_accuracy: 0.6005\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7264 - accuracy: 0.4125 - val_loss: 1.4427 - val_accuracy: 0.6214\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.7149 - accuracy: 0.4184 - val_loss: 1.4769 - val_accuracy: 0.6002\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6992 - accuracy: 0.4249 - val_loss: 1.4178 - val_accuracy: 0.6227\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6841 - accuracy: 0.4336 - val_loss: 1.4211 - val_accuracy: 0.6229\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6723 - accuracy: 0.4370 - val_loss: 1.3957 - val_accuracy: 0.6362\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6580 - accuracy: 0.4431 - val_loss: 1.3818 - val_accuracy: 0.6326\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6432 - accuracy: 0.4486 - val_loss: 1.3573 - val_accuracy: 0.6385\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6344 - accuracy: 0.4536 - val_loss: 1.3281 - val_accuracy: 0.6592\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.6279 - accuracy: 0.4573 - val_loss: 1.3231 - val_accuracy: 0.6503\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6131 - accuracy: 0.4638 - val_loss: 1.3315 - val_accuracy: 0.6481\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5932 - accuracy: 0.4717 - val_loss: 1.3109 - val_accuracy: 0.6614\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5846 - accuracy: 0.4738 - val_loss: 1.3018 - val_accuracy: 0.6581\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5732 - accuracy: 0.4805 - val_loss: 1.2826 - val_accuracy: 0.6658\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5595 - accuracy: 0.4846 - val_loss: 1.2651 - val_accuracy: 0.6718\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5534 - accuracy: 0.4882 - val_loss: 1.2507 - val_accuracy: 0.6713\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5407 - accuracy: 0.4946 - val_loss: 1.2309 - val_accuracy: 0.6724\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5306 - accuracy: 0.4995 - val_loss: 1.2525 - val_accuracy: 0.6806\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5141 - accuracy: 0.5077 - val_loss: 1.2043 - val_accuracy: 0.6934\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5075 - accuracy: 0.5083 - val_loss: 1.2072 - val_accuracy: 0.6876\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4937 - accuracy: 0.5141 - val_loss: 1.1635 - val_accuracy: 0.7046\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4826 - accuracy: 0.5187 - val_loss: 1.1728 - val_accuracy: 0.7024\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4748 - accuracy: 0.5188 - val_loss: 1.1445 - val_accuracy: 0.7021\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.4625 - accuracy: 0.5278 - val_loss: 1.1713 - val_accuracy: 0.7019\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4602 - accuracy: 0.5276 - val_loss: 1.1647 - val_accuracy: 0.7028\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4422 - accuracy: 0.5373 - val_loss: 1.1590 - val_accuracy: 0.7008\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.4312 - accuracy: 0.5403 - val_loss: 1.1286 - val_accuracy: 0.7083\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4255 - accuracy: 0.5439 - val_loss: 1.1791 - val_accuracy: 0.6881\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4122 - accuracy: 0.5490 - val_loss: 1.1141 - val_accuracy: 0.7209\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4017 - accuracy: 0.5502 - val_loss: 1.1222 - val_accuracy: 0.7149\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3988 - accuracy: 0.5531 - val_loss: 1.0838 - val_accuracy: 0.7261\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3897 - accuracy: 0.5558 - val_loss: 1.0687 - val_accuracy: 0.7352\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3764 - accuracy: 0.5624 - val_loss: 1.0696 - val_accuracy: 0.7294\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3675 - accuracy: 0.5650 - val_loss: 1.0760 - val_accuracy: 0.7275\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3673 - accuracy: 0.5639 - val_loss: 1.0648 - val_accuracy: 0.7164\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3508 - accuracy: 0.5706 - val_loss: 1.0250 - val_accuracy: 0.7433\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3497 - accuracy: 0.5726 - val_loss: 1.0169 - val_accuracy: 0.7476\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3374 - accuracy: 0.5772 - val_loss: 1.0003 - val_accuracy: 0.7403\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3309 - accuracy: 0.5795 - val_loss: 1.0687 - val_accuracy: 0.7064\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3246 - accuracy: 0.5808 - val_loss: 0.9958 - val_accuracy: 0.7479\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3093 - accuracy: 0.5881 - val_loss: 0.9846 - val_accuracy: 0.7514\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3021 - accuracy: 0.5913 - val_loss: 1.0211 - val_accuracy: 0.7313\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2931 - accuracy: 0.5970 - val_loss: 0.9840 - val_accuracy: 0.7467\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2892 - accuracy: 0.5966 - val_loss: 1.0066 - val_accuracy: 0.7363\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2831 - accuracy: 0.5965 - val_loss: 0.9476 - val_accuracy: 0.7561\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2708 - accuracy: 0.6014 - val_loss: 0.9732 - val_accuracy: 0.7463\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2645 - accuracy: 0.6047 - val_loss: 0.9976 - val_accuracy: 0.7384\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2570 - accuracy: 0.6068 - val_loss: 0.9671 - val_accuracy: 0.7456\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2540 - accuracy: 0.6070 - val_loss: 0.9589 - val_accuracy: 0.7506\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2434 - accuracy: 0.6125 - val_loss: 0.9458 - val_accuracy: 0.7560\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2366 - accuracy: 0.6123 - val_loss: 0.9231 - val_accuracy: 0.7585\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2259 - accuracy: 0.6195 - val_loss: 0.9540 - val_accuracy: 0.7489\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2251 - accuracy: 0.6212 - val_loss: 0.8763 - val_accuracy: 0.7717\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2170 - accuracy: 0.6222 - val_loss: 0.9380 - val_accuracy: 0.7529\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2072 - accuracy: 0.6258 - val_loss: 0.8987 - val_accuracy: 0.7646\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1996 - accuracy: 0.6261 - val_loss: 0.8866 - val_accuracy: 0.7694\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1988 - accuracy: 0.6262 - val_loss: 0.8651 - val_accuracy: 0.7770\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1877 - accuracy: 0.6320 - val_loss: 0.8958 - val_accuracy: 0.7620\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1799 - accuracy: 0.6374 - val_loss: 0.8689 - val_accuracy: 0.7686\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1758 - accuracy: 0.6354 - val_loss: 0.9102 - val_accuracy: 0.7605\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1709 - accuracy: 0.6379 - val_loss: 0.8329 - val_accuracy: 0.7853\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1608 - accuracy: 0.6418 - val_loss: 0.8793 - val_accuracy: 0.7689\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1559 - accuracy: 0.6444 - val_loss: 0.8474 - val_accuracy: 0.7773\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1475 - accuracy: 0.6455 - val_loss: 0.8244 - val_accuracy: 0.7796\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1463 - accuracy: 0.6446 - val_loss: 0.8481 - val_accuracy: 0.7764\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1419 - accuracy: 0.6483 - val_loss: 0.8317 - val_accuracy: 0.7754\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1282 - accuracy: 0.6516 - val_loss: 0.8250 - val_accuracy: 0.7791\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1218 - accuracy: 0.6571 - val_loss: 0.8420 - val_accuracy: 0.7731\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1230 - accuracy: 0.6554 - val_loss: 0.8447 - val_accuracy: 0.7721\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1139 - accuracy: 0.6606 - val_loss: 0.8204 - val_accuracy: 0.7791\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1114 - accuracy: 0.6573 - val_loss: 0.8497 - val_accuracy: 0.7670\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1069 - accuracy: 0.6589 - val_loss: 0.8051 - val_accuracy: 0.7866\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1024 - accuracy: 0.6625 - val_loss: 0.8181 - val_accuracy: 0.7771\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0934 - accuracy: 0.6636 - val_loss: 0.7997 - val_accuracy: 0.7799\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.0874 - accuracy: 0.6645 - val_loss: 0.7972 - val_accuracy: 0.7856\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0875 - accuracy: 0.6639 - val_loss: 0.7855 - val_accuracy: 0.7903\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0799 - accuracy: 0.6696 - val_loss: 0.7380 - val_accuracy: 0.8026\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0771 - accuracy: 0.6678 - val_loss: 0.7397 - val_accuracy: 0.8020\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0706 - accuracy: 0.6721 - val_loss: 0.7942 - val_accuracy: 0.7729\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0646 - accuracy: 0.6756 - val_loss: 0.7927 - val_accuracy: 0.7805\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0550 - accuracy: 0.6775 - val_loss: 0.7629 - val_accuracy: 0.7869\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0532 - accuracy: 0.6793 - val_loss: 0.7616 - val_accuracy: 0.7909\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0478 - accuracy: 0.6808 - val_loss: 0.7676 - val_accuracy: 0.7890\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0433 - accuracy: 0.6806 - val_loss: 0.7807 - val_accuracy: 0.7838\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0411 - accuracy: 0.6839 - val_loss: 0.7313 - val_accuracy: 0.8032\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0332 - accuracy: 0.6842 - val_loss: 0.7687 - val_accuracy: 0.7906\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0301 - accuracy: 0.6867 - val_loss: 0.7355 - val_accuracy: 0.7981\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0226 - accuracy: 0.6868 - val_loss: 0.7646 - val_accuracy: 0.7926\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0212 - accuracy: 0.6902 - val_loss: 0.7211 - val_accuracy: 0.8055\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0183 - accuracy: 0.6907 - val_loss: 0.7504 - val_accuracy: 0.7870\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0104 - accuracy: 0.6908 - val_loss: 0.7315 - val_accuracy: 0.7939\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0015 - accuracy: 0.6956 - val_loss: 0.7299 - val_accuracy: 0.7967\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0069 - accuracy: 0.6941 - val_loss: 0.7061 - val_accuracy: 0.8041\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9976 - accuracy: 0.6977 - val_loss: 0.7292 - val_accuracy: 0.7994\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9916 - accuracy: 0.6973 - val_loss: 0.7342 - val_accuracy: 0.7978\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9891 - accuracy: 0.6987 - val_loss: 0.7485 - val_accuracy: 0.7902\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9875 - accuracy: 0.7003 - val_loss: 0.7217 - val_accuracy: 0.7952\n",
      "Try 3/100: Best_val_acc: [0.7421140670776367, 0.785611093044281], lr: 1.3702185780556356e-05, Lambda: 2.301215360906446e-05\n",
      "\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.7078 - accuracy: 0.1016 - val_loss: 2.2868 - val_accuracy: 0.1016\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6756 - accuracy: 0.1049 - val_loss: 2.3201 - val_accuracy: 0.0936\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6550 - accuracy: 0.1067 - val_loss: 2.3097 - val_accuracy: 0.1099\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6219 - accuracy: 0.1106 - val_loss: 2.2892 - val_accuracy: 0.1314\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6019 - accuracy: 0.1151 - val_loss: 2.2729 - val_accuracy: 0.1441\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5819 - accuracy: 0.1170 - val_loss: 2.2379 - val_accuracy: 0.1664\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5663 - accuracy: 0.1188 - val_loss: 2.2286 - val_accuracy: 0.1819\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 2.5518 - accuracy: 0.1210 - val_loss: 2.2140 - val_accuracy: 0.2016\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5299 - accuracy: 0.1248 - val_loss: 2.2084 - val_accuracy: 0.1981\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5183 - accuracy: 0.1282 - val_loss: 2.1800 - val_accuracy: 0.2200\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4892 - accuracy: 0.1338 - val_loss: 2.1597 - val_accuracy: 0.2271\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4789 - accuracy: 0.1363 - val_loss: 2.1556 - val_accuracy: 0.2351\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 2.4587 - accuracy: 0.1382 - val_loss: 2.1585 - val_accuracy: 0.2439\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4430 - accuracy: 0.1450 - val_loss: 2.1501 - val_accuracy: 0.2521\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4295 - accuracy: 0.1487 - val_loss: 2.1223 - val_accuracy: 0.2780\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.4140 - accuracy: 0.1511 - val_loss: 2.1157 - val_accuracy: 0.2808\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3969 - accuracy: 0.1551 - val_loss: 2.0842 - val_accuracy: 0.3098\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3856 - accuracy: 0.1631 - val_loss: 2.0643 - val_accuracy: 0.3274\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3621 - accuracy: 0.1679 - val_loss: 2.0803 - val_accuracy: 0.3185\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3492 - accuracy: 0.1705 - val_loss: 2.0677 - val_accuracy: 0.3275\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3301 - accuracy: 0.1761 - val_loss: 2.0463 - val_accuracy: 0.3424\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3162 - accuracy: 0.1809 - val_loss: 2.0277 - val_accuracy: 0.3625\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2954 - accuracy: 0.1852 - val_loss: 2.0205 - val_accuracy: 0.3769\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2689 - accuracy: 0.1976 - val_loss: 1.9862 - val_accuracy: 0.4039\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2600 - accuracy: 0.2000 - val_loss: 1.9741 - val_accuracy: 0.4061\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2409 - accuracy: 0.2038 - val_loss: 1.9665 - val_accuracy: 0.4118\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2238 - accuracy: 0.2109 - val_loss: 1.9646 - val_accuracy: 0.4036\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1993 - accuracy: 0.2167 - val_loss: 1.9404 - val_accuracy: 0.4226\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1851 - accuracy: 0.2236 - val_loss: 1.9198 - val_accuracy: 0.4406\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1686 - accuracy: 0.2313 - val_loss: 1.9155 - val_accuracy: 0.4367\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1542 - accuracy: 0.2345 - val_loss: 1.9055 - val_accuracy: 0.4394\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1327 - accuracy: 0.2472 - val_loss: 1.8663 - val_accuracy: 0.4663\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1196 - accuracy: 0.2483 - val_loss: 1.8642 - val_accuracy: 0.4664\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0983 - accuracy: 0.2577 - val_loss: 1.8353 - val_accuracy: 0.4882\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0872 - accuracy: 0.2620 - val_loss: 1.8051 - val_accuracy: 0.5038\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0690 - accuracy: 0.2674 - val_loss: 1.8068 - val_accuracy: 0.5023\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0576 - accuracy: 0.2737 - val_loss: 1.7731 - val_accuracy: 0.5178\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0366 - accuracy: 0.2855 - val_loss: 1.7644 - val_accuracy: 0.5199\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0165 - accuracy: 0.2894 - val_loss: 1.7564 - val_accuracy: 0.5262\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0029 - accuracy: 0.2970 - val_loss: 1.7449 - val_accuracy: 0.5323\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9878 - accuracy: 0.3044 - val_loss: 1.7290 - val_accuracy: 0.5414\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9768 - accuracy: 0.3064 - val_loss: 1.6975 - val_accuracy: 0.5592\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9611 - accuracy: 0.3114 - val_loss: 1.6647 - val_accuracy: 0.5687\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9469 - accuracy: 0.3210 - val_loss: 1.6677 - val_accuracy: 0.5635\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9266 - accuracy: 0.3303 - val_loss: 1.6402 - val_accuracy: 0.5761\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9193 - accuracy: 0.3331 - val_loss: 1.6291 - val_accuracy: 0.5836\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9062 - accuracy: 0.3395 - val_loss: 1.6225 - val_accuracy: 0.5855\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8901 - accuracy: 0.3442 - val_loss: 1.5767 - val_accuracy: 0.6069\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8738 - accuracy: 0.3535 - val_loss: 1.5925 - val_accuracy: 0.5963\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8603 - accuracy: 0.3589 - val_loss: 1.5426 - val_accuracy: 0.6198\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8419 - accuracy: 0.3662 - val_loss: 1.5815 - val_accuracy: 0.5961\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8341 - accuracy: 0.3679 - val_loss: 1.5462 - val_accuracy: 0.6113\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8115 - accuracy: 0.3802 - val_loss: 1.5489 - val_accuracy: 0.6031\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8034 - accuracy: 0.3811 - val_loss: 1.5016 - val_accuracy: 0.6297\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7945 - accuracy: 0.3852 - val_loss: 1.5090 - val_accuracy: 0.6241\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7813 - accuracy: 0.3923 - val_loss: 1.4725 - val_accuracy: 0.6359\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7615 - accuracy: 0.4002 - val_loss: 1.4922 - val_accuracy: 0.6234\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7580 - accuracy: 0.4027 - val_loss: 1.4712 - val_accuracy: 0.6374\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7405 - accuracy: 0.4091 - val_loss: 1.4502 - val_accuracy: 0.6469\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7215 - accuracy: 0.4203 - val_loss: 1.4255 - val_accuracy: 0.6502\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7148 - accuracy: 0.4237 - val_loss: 1.4050 - val_accuracy: 0.6571\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6951 - accuracy: 0.4314 - val_loss: 1.4182 - val_accuracy: 0.6426\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6894 - accuracy: 0.4301 - val_loss: 1.3913 - val_accuracy: 0.6568\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6776 - accuracy: 0.4392 - val_loss: 1.3893 - val_accuracy: 0.6591\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6666 - accuracy: 0.4431 - val_loss: 1.3692 - val_accuracy: 0.6586\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6547 - accuracy: 0.4479 - val_loss: 1.3645 - val_accuracy: 0.6583\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6436 - accuracy: 0.4523 - val_loss: 1.3398 - val_accuracy: 0.6714\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6380 - accuracy: 0.4546 - val_loss: 1.3315 - val_accuracy: 0.6696\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6199 - accuracy: 0.4601 - val_loss: 1.3116 - val_accuracy: 0.6806\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6133 - accuracy: 0.4644 - val_loss: 1.3114 - val_accuracy: 0.6739\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5974 - accuracy: 0.4732 - val_loss: 1.2935 - val_accuracy: 0.6848\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5898 - accuracy: 0.4765 - val_loss: 1.3049 - val_accuracy: 0.6752\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5756 - accuracy: 0.4826 - val_loss: 1.2833 - val_accuracy: 0.6787\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5659 - accuracy: 0.4825 - val_loss: 1.2421 - val_accuracy: 0.6951\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5565 - accuracy: 0.4902 - val_loss: 1.2559 - val_accuracy: 0.6892\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5501 - accuracy: 0.4895 - val_loss: 1.2316 - val_accuracy: 0.6962\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5368 - accuracy: 0.4998 - val_loss: 1.2315 - val_accuracy: 0.6951\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5270 - accuracy: 0.5013 - val_loss: 1.2248 - val_accuracy: 0.6958\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5136 - accuracy: 0.5059 - val_loss: 1.2203 - val_accuracy: 0.6893\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5095 - accuracy: 0.5074 - val_loss: 1.2044 - val_accuracy: 0.7011\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4949 - accuracy: 0.5124 - val_loss: 1.1938 - val_accuracy: 0.7017\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4914 - accuracy: 0.5180 - val_loss: 1.1945 - val_accuracy: 0.6978\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4765 - accuracy: 0.5220 - val_loss: 1.1479 - val_accuracy: 0.7176\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4685 - accuracy: 0.5240 - val_loss: 1.1533 - val_accuracy: 0.7083\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4613 - accuracy: 0.5259 - val_loss: 1.1344 - val_accuracy: 0.7140\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4491 - accuracy: 0.5360 - val_loss: 1.1226 - val_accuracy: 0.7226\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4378 - accuracy: 0.5369 - val_loss: 1.1376 - val_accuracy: 0.7138\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4352 - accuracy: 0.5378 - val_loss: 1.1287 - val_accuracy: 0.7169\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4218 - accuracy: 0.5452 - val_loss: 1.1258 - val_accuracy: 0.7168\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4150 - accuracy: 0.5474 - val_loss: 1.1255 - val_accuracy: 0.7110\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4050 - accuracy: 0.5497 - val_loss: 1.0788 - val_accuracy: 0.7284\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3928 - accuracy: 0.5568 - val_loss: 1.0739 - val_accuracy: 0.7296\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3916 - accuracy: 0.5571 - val_loss: 1.1044 - val_accuracy: 0.7174\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3854 - accuracy: 0.5596 - val_loss: 1.0701 - val_accuracy: 0.7244\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3746 - accuracy: 0.5634 - val_loss: 1.0534 - val_accuracy: 0.7291\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3638 - accuracy: 0.5678 - val_loss: 1.0563 - val_accuracy: 0.7306\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3538 - accuracy: 0.5707 - val_loss: 1.0787 - val_accuracy: 0.7224\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3519 - accuracy: 0.5729 - val_loss: 1.0266 - val_accuracy: 0.7396\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3418 - accuracy: 0.5765 - val_loss: 1.0228 - val_accuracy: 0.7367\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3322 - accuracy: 0.5795 - val_loss: 1.0293 - val_accuracy: 0.7311\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3259 - accuracy: 0.5798 - val_loss: 1.0148 - val_accuracy: 0.7386\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3235 - accuracy: 0.5837 - val_loss: 1.0185 - val_accuracy: 0.7358\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3142 - accuracy: 0.5856 - val_loss: 0.9997 - val_accuracy: 0.7404\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3058 - accuracy: 0.5893 - val_loss: 1.0185 - val_accuracy: 0.7294\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2972 - accuracy: 0.5943 - val_loss: 1.0273 - val_accuracy: 0.7222\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2896 - accuracy: 0.5938 - val_loss: 0.9950 - val_accuracy: 0.7397\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2858 - accuracy: 0.5986 - val_loss: 0.9969 - val_accuracy: 0.7376\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2796 - accuracy: 0.5998 - val_loss: 0.9877 - val_accuracy: 0.7398\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2709 - accuracy: 0.6033 - val_loss: 0.9552 - val_accuracy: 0.7526\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2626 - accuracy: 0.6060 - val_loss: 0.9646 - val_accuracy: 0.7463\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2546 - accuracy: 0.6087 - val_loss: 0.9699 - val_accuracy: 0.7419\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2468 - accuracy: 0.6111 - val_loss: 0.9301 - val_accuracy: 0.7579\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2402 - accuracy: 0.6147 - val_loss: 0.9546 - val_accuracy: 0.7429\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2350 - accuracy: 0.6175 - val_loss: 0.9590 - val_accuracy: 0.7420\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2292 - accuracy: 0.6179 - val_loss: 0.9482 - val_accuracy: 0.7439\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2228 - accuracy: 0.6227 - val_loss: 0.9038 - val_accuracy: 0.7646\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2186 - accuracy: 0.6248 - val_loss: 0.9085 - val_accuracy: 0.7578\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2108 - accuracy: 0.6260 - val_loss: 0.9106 - val_accuracy: 0.7586\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1989 - accuracy: 0.6320 - val_loss: 0.8895 - val_accuracy: 0.7656\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2021 - accuracy: 0.6288 - val_loss: 0.9206 - val_accuracy: 0.7506\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1907 - accuracy: 0.6327 - val_loss: 0.8984 - val_accuracy: 0.7579\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1927 - accuracy: 0.6309 - val_loss: 0.8801 - val_accuracy: 0.7615\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1803 - accuracy: 0.6369 - val_loss: 0.8938 - val_accuracy: 0.7592\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1762 - accuracy: 0.6360 - val_loss: 0.9131 - val_accuracy: 0.7499\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1693 - accuracy: 0.6417 - val_loss: 0.8829 - val_accuracy: 0.7619\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1643 - accuracy: 0.6458 - val_loss: 0.8527 - val_accuracy: 0.7655\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1616 - accuracy: 0.6452 - val_loss: 0.8543 - val_accuracy: 0.7699\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1523 - accuracy: 0.6469 - val_loss: 0.8753 - val_accuracy: 0.7574\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1481 - accuracy: 0.6472 - val_loss: 0.8502 - val_accuracy: 0.7706\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1445 - accuracy: 0.6494 - val_loss: 0.8818 - val_accuracy: 0.7546\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1415 - accuracy: 0.6494 - val_loss: 0.8472 - val_accuracy: 0.7705\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1268 - accuracy: 0.6540 - val_loss: 0.8619 - val_accuracy: 0.7608\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1246 - accuracy: 0.6577 - val_loss: 0.8374 - val_accuracy: 0.7713\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1221 - accuracy: 0.6550 - val_loss: 0.8896 - val_accuracy: 0.7459\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1124 - accuracy: 0.6600 - val_loss: 0.8197 - val_accuracy: 0.7744\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1174 - accuracy: 0.6589 - val_loss: 0.8301 - val_accuracy: 0.7706\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1052 - accuracy: 0.6637 - val_loss: 0.8047 - val_accuracy: 0.7807\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1026 - accuracy: 0.6656 - val_loss: 0.8069 - val_accuracy: 0.7806\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1009 - accuracy: 0.6659 - val_loss: 0.8301 - val_accuracy: 0.7690\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0942 - accuracy: 0.6670 - val_loss: 0.8211 - val_accuracy: 0.7741\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.0889 - accuracy: 0.6678 - val_loss: 0.8008 - val_accuracy: 0.7806\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0825 - accuracy: 0.6708 - val_loss: 0.7925 - val_accuracy: 0.7793\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0782 - accuracy: 0.6773 - val_loss: 0.8203 - val_accuracy: 0.7679\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0729 - accuracy: 0.6722 - val_loss: 0.8095 - val_accuracy: 0.7742\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0733 - accuracy: 0.6747 - val_loss: 0.7795 - val_accuracy: 0.7848\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0666 - accuracy: 0.6775 - val_loss: 0.7848 - val_accuracy: 0.7832\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0633 - accuracy: 0.6766 - val_loss: 0.7787 - val_accuracy: 0.7829\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0586 - accuracy: 0.6790 - val_loss: 0.7471 - val_accuracy: 0.7980\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0516 - accuracy: 0.6838 - val_loss: 0.8010 - val_accuracy: 0.7736\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0484 - accuracy: 0.6825 - val_loss: 0.7620 - val_accuracy: 0.7877\n",
      "Try 4/100: Best_val_acc: [0.8035387396812439, 0.7665555477142334], lr: 1.1700087665566814e-05, Lambda: 4.756141005952947e-05\n",
      "\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_143 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_144 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.8277 - accuracy: 0.0995 - val_loss: 2.4947 - val_accuracy: 0.0788\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.7180 - accuracy: 0.1095 - val_loss: 2.6217 - val_accuracy: 0.0792\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6412 - accuracy: 0.1177 - val_loss: 2.5544 - val_accuracy: 0.0572\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5803 - accuracy: 0.1244 - val_loss: 2.4712 - val_accuracy: 0.1148\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5283 - accuracy: 0.1312 - val_loss: 2.4481 - val_accuracy: 0.1158\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4697 - accuracy: 0.1413 - val_loss: 2.4087 - val_accuracy: 0.1175\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4167 - accuracy: 0.1543 - val_loss: 2.3501 - val_accuracy: 0.1459\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3635 - accuracy: 0.1691 - val_loss: 2.2974 - val_accuracy: 0.1750\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3118 - accuracy: 0.1811 - val_loss: 2.2017 - val_accuracy: 0.2421\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2572 - accuracy: 0.2025 - val_loss: 2.1295 - val_accuracy: 0.2936\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2137 - accuracy: 0.2170 - val_loss: 2.0781 - val_accuracy: 0.3154\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1629 - accuracy: 0.2353 - val_loss: 1.9465 - val_accuracy: 0.4065\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1119 - accuracy: 0.2553 - val_loss: 1.9217 - val_accuracy: 0.4128\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0636 - accuracy: 0.2733 - val_loss: 1.8543 - val_accuracy: 0.4676\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0165 - accuracy: 0.2924 - val_loss: 1.7539 - val_accuracy: 0.5190\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9765 - accuracy: 0.3061 - val_loss: 1.7458 - val_accuracy: 0.5102\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9304 - accuracy: 0.3255 - val_loss: 1.6604 - val_accuracy: 0.5434\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8843 - accuracy: 0.3458 - val_loss: 1.6341 - val_accuracy: 0.5554\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8427 - accuracy: 0.3650 - val_loss: 1.5106 - val_accuracy: 0.6311\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8033 - accuracy: 0.3816 - val_loss: 1.4258 - val_accuracy: 0.6636\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7691 - accuracy: 0.3964 - val_loss: 1.4145 - val_accuracy: 0.6614\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7373 - accuracy: 0.4124 - val_loss: 1.3576 - val_accuracy: 0.6808\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6951 - accuracy: 0.4294 - val_loss: 1.3777 - val_accuracy: 0.6666\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6591 - accuracy: 0.4452 - val_loss: 1.2452 - val_accuracy: 0.7176\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6342 - accuracy: 0.4576 - val_loss: 1.2109 - val_accuracy: 0.7342\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.6029 - accuracy: 0.4725 - val_loss: 1.2107 - val_accuracy: 0.7231\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5672 - accuracy: 0.4859 - val_loss: 1.2309 - val_accuracy: 0.7082\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5415 - accuracy: 0.4960 - val_loss: 1.1700 - val_accuracy: 0.7296\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5138 - accuracy: 0.5095 - val_loss: 1.1387 - val_accuracy: 0.7296\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4851 - accuracy: 0.5170 - val_loss: 1.1407 - val_accuracy: 0.7274\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4663 - accuracy: 0.5268 - val_loss: 1.0878 - val_accuracy: 0.7434\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4410 - accuracy: 0.5375 - val_loss: 1.0681 - val_accuracy: 0.7490\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4139 - accuracy: 0.5494 - val_loss: 1.0785 - val_accuracy: 0.7366\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3918 - accuracy: 0.5586 - val_loss: 1.0304 - val_accuracy: 0.7537\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.3663 - accuracy: 0.5655 - val_loss: 1.0347 - val_accuracy: 0.7444\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3483 - accuracy: 0.5728 - val_loss: 0.9717 - val_accuracy: 0.7636\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.3194 - accuracy: 0.5871 - val_loss: 0.9649 - val_accuracy: 0.7701\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3011 - accuracy: 0.5924 - val_loss: 0.9466 - val_accuracy: 0.7623\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2801 - accuracy: 0.5988 - val_loss: 0.9864 - val_accuracy: 0.7551\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2702 - accuracy: 0.6022 - val_loss: 0.9391 - val_accuracy: 0.7556\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2469 - accuracy: 0.6111 - val_loss: 0.9417 - val_accuracy: 0.7593\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2315 - accuracy: 0.6143 - val_loss: 0.9409 - val_accuracy: 0.7597\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2146 - accuracy: 0.6241 - val_loss: 0.8867 - val_accuracy: 0.7769\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1968 - accuracy: 0.6277 - val_loss: 0.9013 - val_accuracy: 0.7659\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1808 - accuracy: 0.6344 - val_loss: 0.8317 - val_accuracy: 0.7924\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1755 - accuracy: 0.6369 - val_loss: 0.8166 - val_accuracy: 0.7950\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1517 - accuracy: 0.6453 - val_loss: 0.8455 - val_accuracy: 0.7831\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1347 - accuracy: 0.6520 - val_loss: 0.7999 - val_accuracy: 0.7938\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1229 - accuracy: 0.6538 - val_loss: 0.8431 - val_accuracy: 0.7774\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1116 - accuracy: 0.6583 - val_loss: 0.8353 - val_accuracy: 0.7774\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0903 - accuracy: 0.6663 - val_loss: 0.8483 - val_accuracy: 0.7704\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0817 - accuracy: 0.6714 - val_loss: 0.7951 - val_accuracy: 0.7929\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0706 - accuracy: 0.6720 - val_loss: 0.7770 - val_accuracy: 0.7868\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0556 - accuracy: 0.6774 - val_loss: 0.6980 - val_accuracy: 0.8196\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0491 - accuracy: 0.6779 - val_loss: 0.8233 - val_accuracy: 0.7712\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0289 - accuracy: 0.6856 - val_loss: 0.7973 - val_accuracy: 0.7824\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0235 - accuracy: 0.6882 - val_loss: 0.7702 - val_accuracy: 0.7904\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0058 - accuracy: 0.6925 - val_loss: 0.8052 - val_accuracy: 0.7690\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9983 - accuracy: 0.6960 - val_loss: 0.7884 - val_accuracy: 0.7789\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9889 - accuracy: 0.6976 - val_loss: 0.6926 - val_accuracy: 0.8151\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.9815 - accuracy: 0.7032 - val_loss: 0.7297 - val_accuracy: 0.8011\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9689 - accuracy: 0.7050 - val_loss: 0.7527 - val_accuracy: 0.7874\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9649 - accuracy: 0.7057 - val_loss: 0.6373 - val_accuracy: 0.8274\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9546 - accuracy: 0.7111 - val_loss: 0.7149 - val_accuracy: 0.7993\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9407 - accuracy: 0.7157 - val_loss: 0.7302 - val_accuracy: 0.7959\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9331 - accuracy: 0.7189 - val_loss: 0.6928 - val_accuracy: 0.8072\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9256 - accuracy: 0.7198 - val_loss: 0.6007 - val_accuracy: 0.8366\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9166 - accuracy: 0.7233 - val_loss: 0.7301 - val_accuracy: 0.7914\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9088 - accuracy: 0.7236 - val_loss: 0.6889 - val_accuracy: 0.8028\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9026 - accuracy: 0.7283 - val_loss: 0.6448 - val_accuracy: 0.8229\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8917 - accuracy: 0.7275 - val_loss: 0.6874 - val_accuracy: 0.8044\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8822 - accuracy: 0.7331 - val_loss: 0.6213 - val_accuracy: 0.8296\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8772 - accuracy: 0.7345 - val_loss: 0.6503 - val_accuracy: 0.8159\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8697 - accuracy: 0.7377 - val_loss: 0.7262 - val_accuracy: 0.7900\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8614 - accuracy: 0.7396 - val_loss: 0.6164 - val_accuracy: 0.8276\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8631 - accuracy: 0.7408 - val_loss: 0.6095 - val_accuracy: 0.8271\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8489 - accuracy: 0.7460 - val_loss: 0.6102 - val_accuracy: 0.8257\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8364 - accuracy: 0.7478 - val_loss: 0.5960 - val_accuracy: 0.8311\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8399 - accuracy: 0.7466 - val_loss: 0.5505 - val_accuracy: 0.8475\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8321 - accuracy: 0.7482 - val_loss: 0.5992 - val_accuracy: 0.8313\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8262 - accuracy: 0.7538 - val_loss: 0.6638 - val_accuracy: 0.8030\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.8184 - accuracy: 0.7556 - val_loss: 0.5903 - val_accuracy: 0.8354\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8010 - accuracy: 0.7584 - val_loss: 0.6621 - val_accuracy: 0.8139\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8084 - accuracy: 0.7572 - val_loss: 0.6705 - val_accuracy: 0.8058\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.7602 - val_loss: 0.6361 - val_accuracy: 0.8188\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.7634 - val_loss: 0.6773 - val_accuracy: 0.7981\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7835 - accuracy: 0.7645 - val_loss: 0.5385 - val_accuracy: 0.8510\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7815 - accuracy: 0.7661 - val_loss: 0.5753 - val_accuracy: 0.8370\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7802 - accuracy: 0.7690 - val_loss: 0.5232 - val_accuracy: 0.8585\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7724 - accuracy: 0.7687 - val_loss: 0.6092 - val_accuracy: 0.8189\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7634 - accuracy: 0.7713 - val_loss: 0.5886 - val_accuracy: 0.8221\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7612 - accuracy: 0.7696 - val_loss: 0.5254 - val_accuracy: 0.8536\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7548 - accuracy: 0.7738 - val_loss: 0.6251 - val_accuracy: 0.8221\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7511 - accuracy: 0.7759 - val_loss: 0.5817 - val_accuracy: 0.8298\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7451 - accuracy: 0.7778 - val_loss: 0.5568 - val_accuracy: 0.8439\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7441 - accuracy: 0.7790 - val_loss: 0.5293 - val_accuracy: 0.8513\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7374 - accuracy: 0.7784 - val_loss: 0.5061 - val_accuracy: 0.8605\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7304 - accuracy: 0.7824 - val_loss: 0.5227 - val_accuracy: 0.8511\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7291 - accuracy: 0.7827 - val_loss: 0.5218 - val_accuracy: 0.8480\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7185 - accuracy: 0.7858 - val_loss: 0.5319 - val_accuracy: 0.8472\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7247 - accuracy: 0.7826 - val_loss: 0.5004 - val_accuracy: 0.8554\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7180 - accuracy: 0.7862 - val_loss: 0.5347 - val_accuracy: 0.8463\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7156 - accuracy: 0.7872 - val_loss: 0.5184 - val_accuracy: 0.8511\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7076 - accuracy: 0.7884 - val_loss: 0.5223 - val_accuracy: 0.8540\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7027 - accuracy: 0.7906 - val_loss: 0.4781 - val_accuracy: 0.8604\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6980 - accuracy: 0.7897 - val_loss: 0.5005 - val_accuracy: 0.8539\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.7927 - val_loss: 0.5298 - val_accuracy: 0.8516\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6918 - accuracy: 0.7932 - val_loss: 0.4649 - val_accuracy: 0.8662\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.7945 - val_loss: 0.5444 - val_accuracy: 0.8416\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6886 - accuracy: 0.7953 - val_loss: 0.4983 - val_accuracy: 0.8549\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6835 - accuracy: 0.7949 - val_loss: 0.5562 - val_accuracy: 0.8381\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6859 - accuracy: 0.7944 - val_loss: 0.4223 - val_accuracy: 0.8850\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6787 - accuracy: 0.7986 - val_loss: 0.5193 - val_accuracy: 0.8513\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6761 - accuracy: 0.7973 - val_loss: 0.5034 - val_accuracy: 0.8546\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6627 - accuracy: 0.8030 - val_loss: 0.4696 - val_accuracy: 0.8632\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6687 - accuracy: 0.8016 - val_loss: 0.4938 - val_accuracy: 0.8578\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6615 - accuracy: 0.8044 - val_loss: 0.5268 - val_accuracy: 0.8481\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.8068 - val_loss: 0.4710 - val_accuracy: 0.8689\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6523 - accuracy: 0.8047 - val_loss: 0.5161 - val_accuracy: 0.8462\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6556 - accuracy: 0.8069 - val_loss: 0.5076 - val_accuracy: 0.8526\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.8080 - val_loss: 0.5149 - val_accuracy: 0.8526\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6493 - accuracy: 0.8073 - val_loss: 0.4965 - val_accuracy: 0.8576\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6392 - accuracy: 0.8120 - val_loss: 0.4868 - val_accuracy: 0.8595\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.8113 - val_loss: 0.4794 - val_accuracy: 0.8557\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6342 - accuracy: 0.8097 - val_loss: 0.4593 - val_accuracy: 0.8671\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6336 - accuracy: 0.8127 - val_loss: 0.4659 - val_accuracy: 0.8698\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6243 - accuracy: 0.8166 - val_loss: 0.4149 - val_accuracy: 0.8861\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6277 - accuracy: 0.8128 - val_loss: 0.4145 - val_accuracy: 0.8831\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6233 - accuracy: 0.8145 - val_loss: 0.5126 - val_accuracy: 0.8491\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6192 - accuracy: 0.8178 - val_loss: 0.5733 - val_accuracy: 0.8286\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6164 - accuracy: 0.8164 - val_loss: 0.4467 - val_accuracy: 0.8679\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6150 - accuracy: 0.8174 - val_loss: 0.5471 - val_accuracy: 0.8369\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6191 - accuracy: 0.8174 - val_loss: 0.4206 - val_accuracy: 0.8809\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6184 - accuracy: 0.8155 - val_loss: 0.5000 - val_accuracy: 0.8556\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6055 - accuracy: 0.8216 - val_loss: 0.5579 - val_accuracy: 0.8361\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6054 - accuracy: 0.8189 - val_loss: 0.4955 - val_accuracy: 0.8544\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6050 - accuracy: 0.8204 - val_loss: 0.5217 - val_accuracy: 0.8478\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5999 - accuracy: 0.8236 - val_loss: 0.4685 - val_accuracy: 0.8654\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6002 - accuracy: 0.8235 - val_loss: 0.4300 - val_accuracy: 0.8771\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5942 - accuracy: 0.8242 - val_loss: 0.4849 - val_accuracy: 0.8573\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5915 - accuracy: 0.8245 - val_loss: 0.4831 - val_accuracy: 0.8587\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5942 - accuracy: 0.8227 - val_loss: 0.4373 - val_accuracy: 0.8764\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5941 - accuracy: 0.8240 - val_loss: 0.3909 - val_accuracy: 0.8938\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5932 - accuracy: 0.8243 - val_loss: 0.4647 - val_accuracy: 0.8669\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5907 - accuracy: 0.8267 - val_loss: 0.4539 - val_accuracy: 0.8681\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5893 - accuracy: 0.8260 - val_loss: 0.4554 - val_accuracy: 0.8703\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5804 - accuracy: 0.8284 - val_loss: 0.5145 - val_accuracy: 0.8429\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5818 - accuracy: 0.8301 - val_loss: 0.4418 - val_accuracy: 0.8756\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5729 - accuracy: 0.8307 - val_loss: 0.4464 - val_accuracy: 0.8676\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5753 - accuracy: 0.8303 - val_loss: 0.4646 - val_accuracy: 0.8683\n",
      "Try 5/100: Best_val_acc: [0.5325013995170593, 0.8388333320617676], lr: 3.684857534856536e-05, Lambda: 3.431990714445342e-05\n",
      "\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.7370 - accuracy: 0.1050 - val_loss: 2.3299 - val_accuracy: 0.1294\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6815 - accuracy: 0.1096 - val_loss: 2.3135 - val_accuracy: 0.1485\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6290 - accuracy: 0.1182 - val_loss: 2.3182 - val_accuracy: 0.1508\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5822 - accuracy: 0.1233 - val_loss: 2.2838 - val_accuracy: 0.1959\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5411 - accuracy: 0.1254 - val_loss: 2.2506 - val_accuracy: 0.2149\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5067 - accuracy: 0.1346 - val_loss: 2.2102 - val_accuracy: 0.2258\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4685 - accuracy: 0.1450 - val_loss: 2.1931 - val_accuracy: 0.2361\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4251 - accuracy: 0.1513 - val_loss: 2.1504 - val_accuracy: 0.2651\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3936 - accuracy: 0.1607 - val_loss: 2.1178 - val_accuracy: 0.2602\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3487 - accuracy: 0.1695 - val_loss: 2.0812 - val_accuracy: 0.2814\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3180 - accuracy: 0.1803 - val_loss: 2.0661 - val_accuracy: 0.2981\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2721 - accuracy: 0.1914 - val_loss: 2.0236 - val_accuracy: 0.3126\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2343 - accuracy: 0.2045 - val_loss: 2.0017 - val_accuracy: 0.3295\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2064 - accuracy: 0.2143 - val_loss: 1.9857 - val_accuracy: 0.3400\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1726 - accuracy: 0.2265 - val_loss: 1.9483 - val_accuracy: 0.3592\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1475 - accuracy: 0.2308 - val_loss: 1.9045 - val_accuracy: 0.3847\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1117 - accuracy: 0.2418 - val_loss: 1.8909 - val_accuracy: 0.3946\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0862 - accuracy: 0.2559 - val_loss: 1.8552 - val_accuracy: 0.3966\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0573 - accuracy: 0.2620 - val_loss: 1.8360 - val_accuracy: 0.4136\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0257 - accuracy: 0.2761 - val_loss: 1.8164 - val_accuracy: 0.4274\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9964 - accuracy: 0.2896 - val_loss: 1.7889 - val_accuracy: 0.4318\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9789 - accuracy: 0.2985 - val_loss: 1.7764 - val_accuracy: 0.4336\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9519 - accuracy: 0.3074 - val_loss: 1.7546 - val_accuracy: 0.4499\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9313 - accuracy: 0.3196 - val_loss: 1.7127 - val_accuracy: 0.4819\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.9027 - accuracy: 0.3297 - val_loss: 1.6630 - val_accuracy: 0.5054\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8850 - accuracy: 0.3349 - val_loss: 1.6818 - val_accuracy: 0.4868\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8632 - accuracy: 0.3448 - val_loss: 1.6623 - val_accuracy: 0.5014\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8471 - accuracy: 0.3553 - val_loss: 1.6534 - val_accuracy: 0.4884\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8149 - accuracy: 0.3711 - val_loss: 1.6369 - val_accuracy: 0.5122\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7991 - accuracy: 0.3726 - val_loss: 1.5843 - val_accuracy: 0.5504\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7725 - accuracy: 0.3895 - val_loss: 1.5740 - val_accuracy: 0.5348\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7581 - accuracy: 0.3949 - val_loss: 1.5524 - val_accuracy: 0.5626\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.7372 - accuracy: 0.4052 - val_loss: 1.5245 - val_accuracy: 0.5644\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7167 - accuracy: 0.4137 - val_loss: 1.5341 - val_accuracy: 0.5655\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6979 - accuracy: 0.4228 - val_loss: 1.5033 - val_accuracy: 0.5643\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6832 - accuracy: 0.4287 - val_loss: 1.4722 - val_accuracy: 0.5926\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6624 - accuracy: 0.4386 - val_loss: 1.4674 - val_accuracy: 0.5890\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6436 - accuracy: 0.4479 - val_loss: 1.4248 - val_accuracy: 0.6182\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6241 - accuracy: 0.4530 - val_loss: 1.4430 - val_accuracy: 0.6094\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6020 - accuracy: 0.4659 - val_loss: 1.3664 - val_accuracy: 0.6466\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5945 - accuracy: 0.4685 - val_loss: 1.3610 - val_accuracy: 0.6410\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.5760 - accuracy: 0.4768 - val_loss: 1.3388 - val_accuracy: 0.6513\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5551 - accuracy: 0.4837 - val_loss: 1.3401 - val_accuracy: 0.6515\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5459 - accuracy: 0.4908 - val_loss: 1.3079 - val_accuracy: 0.6609\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5274 - accuracy: 0.4977 - val_loss: 1.3102 - val_accuracy: 0.6611\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5083 - accuracy: 0.5066 - val_loss: 1.2842 - val_accuracy: 0.6684\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4910 - accuracy: 0.5127 - val_loss: 1.2850 - val_accuracy: 0.6672\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4809 - accuracy: 0.5178 - val_loss: 1.2648 - val_accuracy: 0.6602\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4631 - accuracy: 0.5269 - val_loss: 1.2118 - val_accuracy: 0.6898\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4515 - accuracy: 0.5299 - val_loss: 1.2266 - val_accuracy: 0.6829\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.4337 - accuracy: 0.5366 - val_loss: 1.1784 - val_accuracy: 0.7021\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4295 - accuracy: 0.5391 - val_loss: 1.1827 - val_accuracy: 0.6960\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4143 - accuracy: 0.5459 - val_loss: 1.1827 - val_accuracy: 0.6961\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4040 - accuracy: 0.5494 - val_loss: 1.1342 - val_accuracy: 0.7139\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3873 - accuracy: 0.5557 - val_loss: 1.1493 - val_accuracy: 0.7034\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3748 - accuracy: 0.5623 - val_loss: 1.1554 - val_accuracy: 0.7029\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3617 - accuracy: 0.5613 - val_loss: 1.1106 - val_accuracy: 0.7159\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3470 - accuracy: 0.5696 - val_loss: 1.0913 - val_accuracy: 0.7170\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3361 - accuracy: 0.5762 - val_loss: 1.0983 - val_accuracy: 0.7147\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3267 - accuracy: 0.5761 - val_loss: 1.0642 - val_accuracy: 0.7281\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3121 - accuracy: 0.5832 - val_loss: 1.0588 - val_accuracy: 0.7247\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2964 - accuracy: 0.5899 - val_loss: 1.0509 - val_accuracy: 0.7281\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2903 - accuracy: 0.5920 - val_loss: 1.0411 - val_accuracy: 0.7309\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.2782 - accuracy: 0.5973 - val_loss: 1.0336 - val_accuracy: 0.7292\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2650 - accuracy: 0.6022 - val_loss: 0.9826 - val_accuracy: 0.7466\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2566 - accuracy: 0.6041 - val_loss: 1.0245 - val_accuracy: 0.7309\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2451 - accuracy: 0.6115 - val_loss: 0.9498 - val_accuracy: 0.7579\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2347 - accuracy: 0.6136 - val_loss: 0.9702 - val_accuracy: 0.7433\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2300 - accuracy: 0.6150 - val_loss: 1.0203 - val_accuracy: 0.7252\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2153 - accuracy: 0.6207 - val_loss: 0.9339 - val_accuracy: 0.7532\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2063 - accuracy: 0.6238 - val_loss: 0.9217 - val_accuracy: 0.7564\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1912 - accuracy: 0.6278 - val_loss: 0.9353 - val_accuracy: 0.7496\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1861 - accuracy: 0.6321 - val_loss: 0.9099 - val_accuracy: 0.7606\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1740 - accuracy: 0.6336 - val_loss: 0.9224 - val_accuracy: 0.7521\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1603 - accuracy: 0.6399 - val_loss: 0.9341 - val_accuracy: 0.7488\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1545 - accuracy: 0.6413 - val_loss: 0.8919 - val_accuracy: 0.7598\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1443 - accuracy: 0.6455 - val_loss: 0.8588 - val_accuracy: 0.7701\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1397 - accuracy: 0.6494 - val_loss: 0.8890 - val_accuracy: 0.7607\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1314 - accuracy: 0.6497 - val_loss: 0.8865 - val_accuracy: 0.7591\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1192 - accuracy: 0.6544 - val_loss: 0.8755 - val_accuracy: 0.7630\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1065 - accuracy: 0.6606 - val_loss: 0.8287 - val_accuracy: 0.7808\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.1066 - accuracy: 0.6574 - val_loss: 0.8575 - val_accuracy: 0.7645\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0971 - accuracy: 0.6611 - val_loss: 0.8098 - val_accuracy: 0.7825\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.0877 - accuracy: 0.6654 - val_loss: 0.8322 - val_accuracy: 0.7739\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0811 - accuracy: 0.6648 - val_loss: 0.8281 - val_accuracy: 0.7693\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0691 - accuracy: 0.6710 - val_loss: 0.8351 - val_accuracy: 0.7694\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0660 - accuracy: 0.6720 - val_loss: 0.7862 - val_accuracy: 0.7873\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0580 - accuracy: 0.6755 - val_loss: 0.8270 - val_accuracy: 0.7754\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0502 - accuracy: 0.6764 - val_loss: 0.8572 - val_accuracy: 0.7588\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0434 - accuracy: 0.6819 - val_loss: 0.7992 - val_accuracy: 0.7789\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0379 - accuracy: 0.6809 - val_loss: 0.7566 - val_accuracy: 0.7911\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0284 - accuracy: 0.6861 - val_loss: 0.7571 - val_accuracy: 0.7929\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0191 - accuracy: 0.6898 - val_loss: 0.7875 - val_accuracy: 0.7824\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0148 - accuracy: 0.6882 - val_loss: 0.7964 - val_accuracy: 0.7789\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0050 - accuracy: 0.6936 - val_loss: 0.7686 - val_accuracy: 0.7862\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0023 - accuracy: 0.6937 - val_loss: 0.7728 - val_accuracy: 0.7862\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.9948 - accuracy: 0.6974 - val_loss: 0.7534 - val_accuracy: 0.7867\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9860 - accuracy: 0.6980 - val_loss: 0.7745 - val_accuracy: 0.7752\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9833 - accuracy: 0.7005 - val_loss: 0.7546 - val_accuracy: 0.7840\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9742 - accuracy: 0.7049 - val_loss: 0.7273 - val_accuracy: 0.7943\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9681 - accuracy: 0.7065 - val_loss: 0.7369 - val_accuracy: 0.7938\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9652 - accuracy: 0.7039 - val_loss: 0.7172 - val_accuracy: 0.7989\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9612 - accuracy: 0.7062 - val_loss: 0.7457 - val_accuracy: 0.7870\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9534 - accuracy: 0.7090 - val_loss: 0.6799 - val_accuracy: 0.8115\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9446 - accuracy: 0.7127 - val_loss: 0.6883 - val_accuracy: 0.8070\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9447 - accuracy: 0.7138 - val_loss: 0.6957 - val_accuracy: 0.8035\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9320 - accuracy: 0.7156 - val_loss: 0.7028 - val_accuracy: 0.8009\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9276 - accuracy: 0.7187 - val_loss: 0.6912 - val_accuracy: 0.8020\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9219 - accuracy: 0.7221 - val_loss: 0.7151 - val_accuracy: 0.7924\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9229 - accuracy: 0.7210 - val_loss: 0.6586 - val_accuracy: 0.8103\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9156 - accuracy: 0.7209 - val_loss: 0.6626 - val_accuracy: 0.8112\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9087 - accuracy: 0.7234 - val_loss: 0.6799 - val_accuracy: 0.8064\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9110 - accuracy: 0.7224 - val_loss: 0.6770 - val_accuracy: 0.8038\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.9035 - accuracy: 0.7250 - val_loss: 0.6627 - val_accuracy: 0.8119\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8982 - accuracy: 0.7277 - val_loss: 0.6497 - val_accuracy: 0.8128\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8939 - accuracy: 0.7273 - val_loss: 0.6525 - val_accuracy: 0.8144\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8864 - accuracy: 0.7314 - val_loss: 0.6344 - val_accuracy: 0.8207\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8795 - accuracy: 0.7332 - val_loss: 0.6285 - val_accuracy: 0.8193\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8740 - accuracy: 0.7366 - val_loss: 0.6625 - val_accuracy: 0.8119\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8765 - accuracy: 0.7378 - val_loss: 0.6720 - val_accuracy: 0.8067\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8711 - accuracy: 0.7347 - val_loss: 0.6503 - val_accuracy: 0.8106\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8623 - accuracy: 0.7395 - val_loss: 0.6231 - val_accuracy: 0.8234\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8577 - accuracy: 0.7404 - val_loss: 0.6902 - val_accuracy: 0.7959\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8593 - accuracy: 0.7406 - val_loss: 0.6397 - val_accuracy: 0.8129\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8545 - accuracy: 0.7426 - val_loss: 0.6378 - val_accuracy: 0.8162\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8555 - accuracy: 0.7397 - val_loss: 0.5760 - val_accuracy: 0.8347\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8517 - accuracy: 0.7400 - val_loss: 0.6409 - val_accuracy: 0.8154\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8408 - accuracy: 0.7450 - val_loss: 0.6451 - val_accuracy: 0.8139\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8366 - accuracy: 0.7452 - val_loss: 0.6177 - val_accuracy: 0.8174\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8319 - accuracy: 0.7480 - val_loss: 0.5986 - val_accuracy: 0.8294\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8294 - accuracy: 0.7479 - val_loss: 0.6437 - val_accuracy: 0.8107\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8255 - accuracy: 0.7493 - val_loss: 0.6162 - val_accuracy: 0.8198\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8266 - accuracy: 0.7509 - val_loss: 0.6311 - val_accuracy: 0.8186\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8159 - accuracy: 0.7542 - val_loss: 0.6358 - val_accuracy: 0.8139\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8123 - accuracy: 0.7544 - val_loss: 0.6302 - val_accuracy: 0.8173\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8086 - accuracy: 0.7536 - val_loss: 0.5938 - val_accuracy: 0.8264\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.8115 - accuracy: 0.7547 - val_loss: 0.5551 - val_accuracy: 0.8399\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8029 - accuracy: 0.7560 - val_loss: 0.6071 - val_accuracy: 0.8230\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8031 - accuracy: 0.7581 - val_loss: 0.6061 - val_accuracy: 0.8235\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7951 - accuracy: 0.7590 - val_loss: 0.5865 - val_accuracy: 0.8306\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7955 - accuracy: 0.7595 - val_loss: 0.5880 - val_accuracy: 0.8297\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.7605 - val_loss: 0.6218 - val_accuracy: 0.8195\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7910 - accuracy: 0.7610 - val_loss: 0.5984 - val_accuracy: 0.8271\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7931 - accuracy: 0.7595 - val_loss: 0.5837 - val_accuracy: 0.8290\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7873 - accuracy: 0.7603 - val_loss: 0.5732 - val_accuracy: 0.8363\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7810 - accuracy: 0.7630 - val_loss: 0.5838 - val_accuracy: 0.8313\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7751 - accuracy: 0.7654 - val_loss: 0.5914 - val_accuracy: 0.8265\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7744 - accuracy: 0.7663 - val_loss: 0.5990 - val_accuracy: 0.8246\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.7663 - accuracy: 0.7685 - val_loss: 0.5744 - val_accuracy: 0.8330\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7722 - accuracy: 0.7660 - val_loss: 0.6620 - val_accuracy: 0.7991\n",
      "Try 6/100: Best_val_acc: [0.6329810619354248, 0.8069999814033508], lr: 2.0711349934560884e-05, Lambda: 1.8055749995786324e-05\n",
      "\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.7063 - accuracy: 0.1032 - val_loss: 2.4619 - val_accuracy: 0.0494\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6786 - accuracy: 0.1048 - val_loss: 2.5134 - val_accuracy: 0.0927\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6463 - accuracy: 0.1081 - val_loss: 2.5035 - val_accuracy: 0.1224\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.6160 - accuracy: 0.1133 - val_loss: 2.4951 - val_accuracy: 0.1473\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5983 - accuracy: 0.1149 - val_loss: 2.5078 - val_accuracy: 0.1264\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5559 - accuracy: 0.1220 - val_loss: 2.4771 - val_accuracy: 0.1430\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5431 - accuracy: 0.1240 - val_loss: 2.4566 - val_accuracy: 0.1539\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5260 - accuracy: 0.1244 - val_loss: 2.4207 - val_accuracy: 0.1551\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4961 - accuracy: 0.1330 - val_loss: 2.3872 - val_accuracy: 0.1688\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4815 - accuracy: 0.1315 - val_loss: 2.3856 - val_accuracy: 0.1691\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4568 - accuracy: 0.1355 - val_loss: 2.3877 - val_accuracy: 0.1713\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.4378 - accuracy: 0.1400 - val_loss: 2.3510 - val_accuracy: 0.1720\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.4223 - accuracy: 0.1483 - val_loss: 2.3332 - val_accuracy: 0.1876\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.4028 - accuracy: 0.1520 - val_loss: 2.3266 - val_accuracy: 0.1957\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3829 - accuracy: 0.1580 - val_loss: 2.2915 - val_accuracy: 0.2140\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3682 - accuracy: 0.1605 - val_loss: 2.2544 - val_accuracy: 0.2284\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.3412 - accuracy: 0.1658 - val_loss: 2.2221 - val_accuracy: 0.2426\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3267 - accuracy: 0.1730 - val_loss: 2.1997 - val_accuracy: 0.2554\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3077 - accuracy: 0.1753 - val_loss: 2.1662 - val_accuracy: 0.2760\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.3014 - accuracy: 0.1809 - val_loss: 2.1503 - val_accuracy: 0.2816\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2752 - accuracy: 0.1867 - val_loss: 2.1417 - val_accuracy: 0.2941\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2538 - accuracy: 0.1941 - val_loss: 2.1258 - val_accuracy: 0.3005\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2339 - accuracy: 0.1996 - val_loss: 2.0988 - val_accuracy: 0.3083\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2199 - accuracy: 0.2075 - val_loss: 2.0758 - val_accuracy: 0.3214\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.1999 - accuracy: 0.2117 - val_loss: 2.0506 - val_accuracy: 0.3404\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1795 - accuracy: 0.2212 - val_loss: 2.0339 - val_accuracy: 0.3459\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1653 - accuracy: 0.2296 - val_loss: 2.0051 - val_accuracy: 0.3586\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1508 - accuracy: 0.2305 - val_loss: 1.9714 - val_accuracy: 0.3881\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1374 - accuracy: 0.2326 - val_loss: 1.9301 - val_accuracy: 0.4076\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1110 - accuracy: 0.2434 - val_loss: 1.9639 - val_accuracy: 0.3800\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0985 - accuracy: 0.2484 - val_loss: 1.9127 - val_accuracy: 0.4126\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0774 - accuracy: 0.2602 - val_loss: 1.8815 - val_accuracy: 0.4373\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0667 - accuracy: 0.2626 - val_loss: 1.8628 - val_accuracy: 0.4409\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0460 - accuracy: 0.2753 - val_loss: 1.8466 - val_accuracy: 0.4508\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0271 - accuracy: 0.2791 - val_loss: 1.8304 - val_accuracy: 0.4449\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0143 - accuracy: 0.2824 - val_loss: 1.7925 - val_accuracy: 0.4672\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.0002 - accuracy: 0.2881 - val_loss: 1.7727 - val_accuracy: 0.4811\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9806 - accuracy: 0.2965 - val_loss: 1.7502 - val_accuracy: 0.4894\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9602 - accuracy: 0.3048 - val_loss: 1.7339 - val_accuracy: 0.4901\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9505 - accuracy: 0.3097 - val_loss: 1.7003 - val_accuracy: 0.5115\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9275 - accuracy: 0.3172 - val_loss: 1.6715 - val_accuracy: 0.5291\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9176 - accuracy: 0.3239 - val_loss: 1.6563 - val_accuracy: 0.5490\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9034 - accuracy: 0.3269 - val_loss: 1.6208 - val_accuracy: 0.5586\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8877 - accuracy: 0.3370 - val_loss: 1.6029 - val_accuracy: 0.5629\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8694 - accuracy: 0.3449 - val_loss: 1.5874 - val_accuracy: 0.5756\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8566 - accuracy: 0.3479 - val_loss: 1.5434 - val_accuracy: 0.6082\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.8423 - accuracy: 0.3569 - val_loss: 1.5655 - val_accuracy: 0.5881\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8284 - accuracy: 0.3620 - val_loss: 1.5374 - val_accuracy: 0.6038\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8158 - accuracy: 0.3673 - val_loss: 1.5300 - val_accuracy: 0.6060\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7981 - accuracy: 0.3746 - val_loss: 1.5015 - val_accuracy: 0.6131\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7871 - accuracy: 0.3825 - val_loss: 1.4731 - val_accuracy: 0.6284\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7787 - accuracy: 0.3869 - val_loss: 1.4704 - val_accuracy: 0.6284\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7627 - accuracy: 0.3935 - val_loss: 1.4240 - val_accuracy: 0.6449\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7505 - accuracy: 0.4024 - val_loss: 1.4143 - val_accuracy: 0.6413\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7390 - accuracy: 0.4020 - val_loss: 1.4072 - val_accuracy: 0.6537\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7287 - accuracy: 0.4064 - val_loss: 1.3725 - val_accuracy: 0.6677\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7131 - accuracy: 0.4135 - val_loss: 1.3762 - val_accuracy: 0.6571\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7030 - accuracy: 0.4195 - val_loss: 1.3834 - val_accuracy: 0.6589\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6888 - accuracy: 0.4265 - val_loss: 1.3511 - val_accuracy: 0.6760\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6753 - accuracy: 0.4318 - val_loss: 1.3138 - val_accuracy: 0.6929\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.6665 - accuracy: 0.4372 - val_loss: 1.3029 - val_accuracy: 0.6859\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6516 - accuracy: 0.4427 - val_loss: 1.2996 - val_accuracy: 0.6837\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6442 - accuracy: 0.4456 - val_loss: 1.2863 - val_accuracy: 0.6954\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6332 - accuracy: 0.4509 - val_loss: 1.2694 - val_accuracy: 0.6975\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6202 - accuracy: 0.4592 - val_loss: 1.2711 - val_accuracy: 0.6866\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6083 - accuracy: 0.4636 - val_loss: 1.2591 - val_accuracy: 0.7025\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6012 - accuracy: 0.4670 - val_loss: 1.2222 - val_accuracy: 0.7148\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5944 - accuracy: 0.4695 - val_loss: 1.2373 - val_accuracy: 0.6951\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5793 - accuracy: 0.4758 - val_loss: 1.1943 - val_accuracy: 0.7124\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5683 - accuracy: 0.4792 - val_loss: 1.2045 - val_accuracy: 0.7146\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5594 - accuracy: 0.4843 - val_loss: 1.1961 - val_accuracy: 0.7146\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.5509 - accuracy: 0.4891 - val_loss: 1.1887 - val_accuracy: 0.7159\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5394 - accuracy: 0.4925 - val_loss: 1.1609 - val_accuracy: 0.7300\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5264 - accuracy: 0.4989 - val_loss: 1.1501 - val_accuracy: 0.7281\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5204 - accuracy: 0.5031 - val_loss: 1.1360 - val_accuracy: 0.7384\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5113 - accuracy: 0.5064 - val_loss: 1.1606 - val_accuracy: 0.7214\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.4990 - accuracy: 0.5108 - val_loss: 1.1197 - val_accuracy: 0.7290\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4940 - accuracy: 0.5139 - val_loss: 1.1114 - val_accuracy: 0.7345\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4835 - accuracy: 0.5166 - val_loss: 1.1169 - val_accuracy: 0.7251\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4770 - accuracy: 0.5184 - val_loss: 1.0966 - val_accuracy: 0.7398\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4615 - accuracy: 0.5310 - val_loss: 1.0936 - val_accuracy: 0.7324\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4527 - accuracy: 0.5330 - val_loss: 1.1059 - val_accuracy: 0.7358\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4404 - accuracy: 0.5353 - val_loss: 1.0642 - val_accuracy: 0.7456\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4374 - accuracy: 0.5396 - val_loss: 1.0587 - val_accuracy: 0.7433\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4307 - accuracy: 0.5439 - val_loss: 1.0834 - val_accuracy: 0.7320\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4145 - accuracy: 0.5470 - val_loss: 1.0425 - val_accuracy: 0.7510\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4107 - accuracy: 0.5521 - val_loss: 1.0701 - val_accuracy: 0.7342\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3980 - accuracy: 0.5562 - val_loss: 1.0467 - val_accuracy: 0.7466\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3943 - accuracy: 0.5580 - val_loss: 1.0205 - val_accuracy: 0.7546\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3822 - accuracy: 0.5637 - val_loss: 1.0169 - val_accuracy: 0.7545\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3763 - accuracy: 0.5648 - val_loss: 1.0065 - val_accuracy: 0.7505\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3683 - accuracy: 0.5675 - val_loss: 0.9930 - val_accuracy: 0.7557\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3536 - accuracy: 0.5716 - val_loss: 1.0044 - val_accuracy: 0.7559\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3493 - accuracy: 0.5737 - val_loss: 0.9940 - val_accuracy: 0.7601\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.3381 - accuracy: 0.5796 - val_loss: 0.9909 - val_accuracy: 0.7598\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3336 - accuracy: 0.5821 - val_loss: 0.9905 - val_accuracy: 0.7514\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3273 - accuracy: 0.5836 - val_loss: 0.9743 - val_accuracy: 0.7565\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3199 - accuracy: 0.5884 - val_loss: 0.9562 - val_accuracy: 0.7622\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 1.3103 - accuracy: 0.5902 - val_loss: 0.9376 - val_accuracy: 0.7747\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3062 - accuracy: 0.5937 - val_loss: 0.9390 - val_accuracy: 0.7671\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2934 - accuracy: 0.5984 - val_loss: 0.9671 - val_accuracy: 0.7604\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2889 - accuracy: 0.5994 - val_loss: 0.9479 - val_accuracy: 0.7654\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2831 - accuracy: 0.5998 - val_loss: 0.9596 - val_accuracy: 0.7606\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.2732 - accuracy: 0.6071 - val_loss: 0.9252 - val_accuracy: 0.7708\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2630 - accuracy: 0.6102 - val_loss: 0.9212 - val_accuracy: 0.7721\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2591 - accuracy: 0.6106 - val_loss: 0.9119 - val_accuracy: 0.7734\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2525 - accuracy: 0.6151 - val_loss: 0.8887 - val_accuracy: 0.7801\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2437 - accuracy: 0.6155 - val_loss: 0.9247 - val_accuracy: 0.7684\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2323 - accuracy: 0.6202 - val_loss: 0.9209 - val_accuracy: 0.7701\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2380 - accuracy: 0.6180 - val_loss: 0.8987 - val_accuracy: 0.7729\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2255 - accuracy: 0.6225 - val_loss: 0.8669 - val_accuracy: 0.7857\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2163 - accuracy: 0.6282 - val_loss: 0.8722 - val_accuracy: 0.7808\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2104 - accuracy: 0.6295 - val_loss: 0.8787 - val_accuracy: 0.7792\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2048 - accuracy: 0.6296 - val_loss: 0.8754 - val_accuracy: 0.7767\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1981 - accuracy: 0.6335 - val_loss: 0.8682 - val_accuracy: 0.7772\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1930 - accuracy: 0.6355 - val_loss: 0.8617 - val_accuracy: 0.7827\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1820 - accuracy: 0.6385 - val_loss: 0.8782 - val_accuracy: 0.7724\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1821 - accuracy: 0.6384 - val_loss: 0.8680 - val_accuracy: 0.7782\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1705 - accuracy: 0.6424 - val_loss: 0.8276 - val_accuracy: 0.7872\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1643 - accuracy: 0.6458 - val_loss: 0.8502 - val_accuracy: 0.7837\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1583 - accuracy: 0.6453 - val_loss: 0.8323 - val_accuracy: 0.7876\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1516 - accuracy: 0.6506 - val_loss: 0.8395 - val_accuracy: 0.7855\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1495 - accuracy: 0.6511 - val_loss: 0.8099 - val_accuracy: 0.7923\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1459 - accuracy: 0.6528 - val_loss: 0.8310 - val_accuracy: 0.7800\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1384 - accuracy: 0.6548 - val_loss: 0.8240 - val_accuracy: 0.7868\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1301 - accuracy: 0.6571 - val_loss: 0.8238 - val_accuracy: 0.7873\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1261 - accuracy: 0.6567 - val_loss: 0.8198 - val_accuracy: 0.7889\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1200 - accuracy: 0.6591 - val_loss: 0.8268 - val_accuracy: 0.7800\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1157 - accuracy: 0.6595 - val_loss: 0.8015 - val_accuracy: 0.7939\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1114 - accuracy: 0.6639 - val_loss: 0.7749 - val_accuracy: 0.8049\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1005 - accuracy: 0.6675 - val_loss: 0.7688 - val_accuracy: 0.8054\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1036 - accuracy: 0.6650 - val_loss: 0.7889 - val_accuracy: 0.7911\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0965 - accuracy: 0.6643 - val_loss: 0.7761 - val_accuracy: 0.7954\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0878 - accuracy: 0.6702 - val_loss: 0.8167 - val_accuracy: 0.7801\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0898 - accuracy: 0.6684 - val_loss: 0.7966 - val_accuracy: 0.7919\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0755 - accuracy: 0.6742 - val_loss: 0.7772 - val_accuracy: 0.7968\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0734 - accuracy: 0.6734 - val_loss: 0.7449 - val_accuracy: 0.8093\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0697 - accuracy: 0.6777 - val_loss: 0.7511 - val_accuracy: 0.8044\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0648 - accuracy: 0.6768 - val_loss: 0.7759 - val_accuracy: 0.7973\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0517 - accuracy: 0.6810 - val_loss: 0.7629 - val_accuracy: 0.7989\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0527 - accuracy: 0.6824 - val_loss: 0.7712 - val_accuracy: 0.7949\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0503 - accuracy: 0.6824 - val_loss: 0.7252 - val_accuracy: 0.8101\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0426 - accuracy: 0.6856 - val_loss: 0.7597 - val_accuracy: 0.7998\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0432 - accuracy: 0.6846 - val_loss: 0.7207 - val_accuracy: 0.8093\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0330 - accuracy: 0.6877 - val_loss: 0.7758 - val_accuracy: 0.7919\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0310 - accuracy: 0.6897 - val_loss: 0.7380 - val_accuracy: 0.8071\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0291 - accuracy: 0.6885 - val_loss: 0.7398 - val_accuracy: 0.8032\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0247 - accuracy: 0.6898 - val_loss: 0.7441 - val_accuracy: 0.8006\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0223 - accuracy: 0.6916 - val_loss: 0.7222 - val_accuracy: 0.8070\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0170 - accuracy: 0.6941 - val_loss: 0.7310 - val_accuracy: 0.8071\n",
      "Try 7/100: Best_val_acc: [0.7773866653442383, 0.7735555768013], lr: 1.2408004591548833e-05, Lambda: 3.9526961768325714e-05\n",
      "\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 2.7932 - accuracy: 0.1030 - val_loss: 2.3405 - val_accuracy: 0.1709\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.6651 - accuracy: 0.1200 - val_loss: 2.2679 - val_accuracy: 0.1844\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.5569 - accuracy: 0.1355 - val_loss: 2.1450 - val_accuracy: 0.2561\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.4532 - accuracy: 0.1581 - val_loss: 2.1488 - val_accuracy: 0.2486\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3543 - accuracy: 0.1842 - val_loss: 2.0890 - val_accuracy: 0.2850\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2535 - accuracy: 0.2142 - val_loss: 1.9627 - val_accuracy: 0.3463\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1595 - accuracy: 0.2475 - val_loss: 1.8519 - val_accuracy: 0.3940\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0704 - accuracy: 0.2778 - val_loss: 1.7028 - val_accuracy: 0.4935\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9983 - accuracy: 0.3072 - val_loss: 1.6491 - val_accuracy: 0.5042\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.9226 - accuracy: 0.3329 - val_loss: 1.5757 - val_accuracy: 0.5603\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8598 - accuracy: 0.3575 - val_loss: 1.5294 - val_accuracy: 0.5824\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8001 - accuracy: 0.3838 - val_loss: 1.4723 - val_accuracy: 0.5992\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7471 - accuracy: 0.4083 - val_loss: 1.4075 - val_accuracy: 0.6345\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6965 - accuracy: 0.4275 - val_loss: 1.3269 - val_accuracy: 0.6864\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6482 - accuracy: 0.4466 - val_loss: 1.3018 - val_accuracy: 0.6850\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6008 - accuracy: 0.4662 - val_loss: 1.2579 - val_accuracy: 0.6880\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5575 - accuracy: 0.4851 - val_loss: 1.2391 - val_accuracy: 0.6966\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5180 - accuracy: 0.5000 - val_loss: 1.1628 - val_accuracy: 0.7117\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4657 - accuracy: 0.5265 - val_loss: 1.1291 - val_accuracy: 0.7228\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4286 - accuracy: 0.5390 - val_loss: 1.0895 - val_accuracy: 0.7389\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3960 - accuracy: 0.5522 - val_loss: 1.0471 - val_accuracy: 0.7414\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3529 - accuracy: 0.5715 - val_loss: 1.0811 - val_accuracy: 0.7210\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.3209 - accuracy: 0.5825 - val_loss: 1.0161 - val_accuracy: 0.7429\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2957 - accuracy: 0.5925 - val_loss: 1.0382 - val_accuracy: 0.7264\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2619 - accuracy: 0.6051 - val_loss: 0.9807 - val_accuracy: 0.7448\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2302 - accuracy: 0.6187 - val_loss: 1.0079 - val_accuracy: 0.7261\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2071 - accuracy: 0.6271 - val_loss: 0.9166 - val_accuracy: 0.7666\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1819 - accuracy: 0.6345 - val_loss: 1.0067 - val_accuracy: 0.7153\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1564 - accuracy: 0.6441 - val_loss: 0.8772 - val_accuracy: 0.7673\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1290 - accuracy: 0.6545 - val_loss: 0.8804 - val_accuracy: 0.7674\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1087 - accuracy: 0.6622 - val_loss: 0.8677 - val_accuracy: 0.7655\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0869 - accuracy: 0.6673 - val_loss: 0.8274 - val_accuracy: 0.7774\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0677 - accuracy: 0.6747 - val_loss: 0.7988 - val_accuracy: 0.7881\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0489 - accuracy: 0.6828 - val_loss: 0.9110 - val_accuracy: 0.7394\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0248 - accuracy: 0.6919 - val_loss: 0.7671 - val_accuracy: 0.7923\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.0069 - accuracy: 0.6952 - val_loss: 0.7972 - val_accuracy: 0.7729\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9938 - accuracy: 0.6990 - val_loss: 0.7312 - val_accuracy: 0.8006\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9767 - accuracy: 0.7043 - val_loss: 0.8793 - val_accuracy: 0.7383\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9689 - accuracy: 0.7084 - val_loss: 0.8297 - val_accuracy: 0.7526\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9490 - accuracy: 0.7138 - val_loss: 0.7619 - val_accuracy: 0.7801\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9319 - accuracy: 0.7194 - val_loss: 0.6827 - val_accuracy: 0.8050\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9214 - accuracy: 0.7213 - val_loss: 0.6726 - val_accuracy: 0.8195\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9082 - accuracy: 0.7279 - val_loss: 0.6135 - val_accuracy: 0.8316\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8953 - accuracy: 0.7320 - val_loss: 0.6850 - val_accuracy: 0.8074\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8866 - accuracy: 0.7342 - val_loss: 0.8037 - val_accuracy: 0.7557\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8665 - accuracy: 0.7409 - val_loss: 0.6745 - val_accuracy: 0.8017\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8579 - accuracy: 0.7437 - val_loss: 0.6198 - val_accuracy: 0.8276\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8432 - accuracy: 0.7460 - val_loss: 0.6816 - val_accuracy: 0.8022\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8390 - accuracy: 0.7506 - val_loss: 0.6066 - val_accuracy: 0.8229\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8304 - accuracy: 0.7506 - val_loss: 0.6956 - val_accuracy: 0.7946\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8201 - accuracy: 0.7543 - val_loss: 0.6130 - val_accuracy: 0.8231\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8055 - accuracy: 0.7595 - val_loss: 0.5613 - val_accuracy: 0.8389\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8020 - accuracy: 0.7605 - val_loss: 0.6063 - val_accuracy: 0.8270\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7833 - accuracy: 0.7669 - val_loss: 0.5925 - val_accuracy: 0.8347\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7773 - accuracy: 0.7715 - val_loss: 0.5801 - val_accuracy: 0.8311\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7747 - accuracy: 0.7685 - val_loss: 0.5974 - val_accuracy: 0.8294\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7646 - accuracy: 0.7718 - val_loss: 0.6018 - val_accuracy: 0.8211\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7637 - accuracy: 0.7742 - val_loss: 0.6233 - val_accuracy: 0.8131\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7481 - accuracy: 0.7772 - val_loss: 0.6800 - val_accuracy: 0.7918\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7376 - accuracy: 0.7812 - val_loss: 0.6110 - val_accuracy: 0.8181\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7370 - accuracy: 0.7794 - val_loss: 0.5996 - val_accuracy: 0.8185\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7210 - accuracy: 0.7863 - val_loss: 0.5353 - val_accuracy: 0.8460\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7211 - accuracy: 0.7855 - val_loss: 0.5295 - val_accuracy: 0.8491\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7083 - accuracy: 0.7916 - val_loss: 0.5678 - val_accuracy: 0.8343\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7140 - accuracy: 0.7891 - val_loss: 0.5361 - val_accuracy: 0.8456\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7015 - accuracy: 0.7913 - val_loss: 0.5419 - val_accuracy: 0.8401\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6997 - accuracy: 0.7930 - val_loss: 0.6046 - val_accuracy: 0.8201\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6953 - accuracy: 0.7937 - val_loss: 0.6084 - val_accuracy: 0.8206\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6848 - accuracy: 0.8003 - val_loss: 0.5468 - val_accuracy: 0.8389\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6758 - accuracy: 0.8024 - val_loss: 0.5503 - val_accuracy: 0.8377\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6657 - accuracy: 0.8042 - val_loss: 0.5945 - val_accuracy: 0.8241\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6680 - accuracy: 0.8037 - val_loss: 0.5505 - val_accuracy: 0.8394\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6635 - accuracy: 0.8031 - val_loss: 0.5739 - val_accuracy: 0.8331\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6537 - accuracy: 0.8068 - val_loss: 0.5183 - val_accuracy: 0.8526\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6488 - accuracy: 0.8102 - val_loss: 0.4915 - val_accuracy: 0.8545\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6525 - accuracy: 0.8078 - val_loss: 0.5892 - val_accuracy: 0.8224\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6429 - accuracy: 0.8099 - val_loss: 0.5772 - val_accuracy: 0.8273\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.8114 - val_loss: 0.6208 - val_accuracy: 0.8131\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6371 - accuracy: 0.8114 - val_loss: 0.5493 - val_accuracy: 0.8339\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6308 - accuracy: 0.8146 - val_loss: 0.5880 - val_accuracy: 0.8214\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6253 - accuracy: 0.8159 - val_loss: 0.5428 - val_accuracy: 0.8360\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6173 - accuracy: 0.8196 - val_loss: 0.5028 - val_accuracy: 0.8524\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6132 - accuracy: 0.8217 - val_loss: 0.6186 - val_accuracy: 0.8086\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6135 - accuracy: 0.8194 - val_loss: 0.5123 - val_accuracy: 0.8441\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6061 - accuracy: 0.8226 - val_loss: 0.5881 - val_accuracy: 0.8259\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6040 - accuracy: 0.8235 - val_loss: 0.4723 - val_accuracy: 0.8615\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5997 - accuracy: 0.8226 - val_loss: 0.4736 - val_accuracy: 0.8616\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5946 - accuracy: 0.8239 - val_loss: 0.5300 - val_accuracy: 0.8409\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5915 - accuracy: 0.8248 - val_loss: 0.5444 - val_accuracy: 0.8367\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5950 - accuracy: 0.8229 - val_loss: 0.5016 - val_accuracy: 0.8503\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5849 - accuracy: 0.8292 - val_loss: 0.4631 - val_accuracy: 0.8684\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5846 - accuracy: 0.8262 - val_loss: 0.4533 - val_accuracy: 0.8607\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5851 - accuracy: 0.8280 - val_loss: 0.4928 - val_accuracy: 0.8561\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5793 - accuracy: 0.8297 - val_loss: 0.5476 - val_accuracy: 0.8356\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5739 - accuracy: 0.8336 - val_loss: 0.4928 - val_accuracy: 0.8500\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5739 - accuracy: 0.8294 - val_loss: 0.4435 - val_accuracy: 0.8718\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5670 - accuracy: 0.8341 - val_loss: 0.4700 - val_accuracy: 0.8569\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5628 - accuracy: 0.8345 - val_loss: 0.4569 - val_accuracy: 0.8656\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5582 - accuracy: 0.8358 - val_loss: 0.6162 - val_accuracy: 0.8148\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5593 - accuracy: 0.8375 - val_loss: 0.5275 - val_accuracy: 0.8404\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5542 - accuracy: 0.8374 - val_loss: 0.3985 - val_accuracy: 0.8843\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5600 - accuracy: 0.8374 - val_loss: 0.4147 - val_accuracy: 0.8773\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5514 - accuracy: 0.8402 - val_loss: 0.4664 - val_accuracy: 0.8606\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5462 - accuracy: 0.8409 - val_loss: 0.5039 - val_accuracy: 0.8524\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5446 - accuracy: 0.8402 - val_loss: 0.5548 - val_accuracy: 0.8356\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5428 - accuracy: 0.8420 - val_loss: 0.6111 - val_accuracy: 0.8029\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5401 - accuracy: 0.8422 - val_loss: 0.4436 - val_accuracy: 0.8674\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5333 - accuracy: 0.8456 - val_loss: 0.4615 - val_accuracy: 0.8620\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5379 - accuracy: 0.8420 - val_loss: 0.4901 - val_accuracy: 0.8549\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5327 - accuracy: 0.8453 - val_loss: 0.4581 - val_accuracy: 0.8650\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5292 - accuracy: 0.8461 - val_loss: 0.4252 - val_accuracy: 0.8701\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5294 - accuracy: 0.8452 - val_loss: 0.5241 - val_accuracy: 0.8409\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5269 - accuracy: 0.8451 - val_loss: 0.5117 - val_accuracy: 0.8451\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5199 - accuracy: 0.8498 - val_loss: 0.4281 - val_accuracy: 0.8771\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5147 - accuracy: 0.8516 - val_loss: 0.6581 - val_accuracy: 0.8041\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5220 - accuracy: 0.8466 - val_loss: 0.4495 - val_accuracy: 0.8668\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5149 - accuracy: 0.8499 - val_loss: 0.4728 - val_accuracy: 0.8624\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5109 - accuracy: 0.8523 - val_loss: 0.6546 - val_accuracy: 0.8017\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5103 - accuracy: 0.8507 - val_loss: 0.4864 - val_accuracy: 0.8560\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5011 - accuracy: 0.8540 - val_loss: 0.4056 - val_accuracy: 0.8837\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5010 - accuracy: 0.8533 - val_loss: 0.4945 - val_accuracy: 0.8532\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5031 - accuracy: 0.8530 - val_loss: 0.8223 - val_accuracy: 0.7602\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5012 - accuracy: 0.8530 - val_loss: 0.5163 - val_accuracy: 0.8464\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4955 - accuracy: 0.8544 - val_loss: 0.4383 - val_accuracy: 0.8697\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4919 - accuracy: 0.8574 - val_loss: 0.5750 - val_accuracy: 0.8331\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4924 - accuracy: 0.8585 - val_loss: 0.3562 - val_accuracy: 0.9008\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4887 - accuracy: 0.8575 - val_loss: 0.5029 - val_accuracy: 0.8502\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4862 - accuracy: 0.8596 - val_loss: 0.5123 - val_accuracy: 0.8480\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4864 - accuracy: 0.8597 - val_loss: 0.4076 - val_accuracy: 0.8792\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4874 - accuracy: 0.8583 - val_loss: 0.4925 - val_accuracy: 0.8544\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4835 - accuracy: 0.8597 - val_loss: 0.4501 - val_accuracy: 0.8684\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4822 - accuracy: 0.8596 - val_loss: 0.4056 - val_accuracy: 0.8821\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4784 - accuracy: 0.8613 - val_loss: 0.4843 - val_accuracy: 0.8536\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4825 - accuracy: 0.8601 - val_loss: 0.4631 - val_accuracy: 0.8629\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4783 - accuracy: 0.8606 - val_loss: 0.4681 - val_accuracy: 0.8611\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4785 - accuracy: 0.8601 - val_loss: 0.3547 - val_accuracy: 0.8970\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4680 - accuracy: 0.8645 - val_loss: 0.4807 - val_accuracy: 0.8579\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4627 - accuracy: 0.8658 - val_loss: 0.4634 - val_accuracy: 0.8643\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4705 - accuracy: 0.8620 - val_loss: 0.4912 - val_accuracy: 0.8486\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4671 - accuracy: 0.8626 - val_loss: 0.4026 - val_accuracy: 0.8845\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4640 - accuracy: 0.8655 - val_loss: 0.4496 - val_accuracy: 0.8669\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4575 - accuracy: 0.8675 - val_loss: 0.4283 - val_accuracy: 0.8758\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4594 - accuracy: 0.8674 - val_loss: 0.4076 - val_accuracy: 0.8799\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4579 - accuracy: 0.8671 - val_loss: 0.4934 - val_accuracy: 0.8491\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4535 - accuracy: 0.8685 - val_loss: 0.4544 - val_accuracy: 0.8584\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4549 - accuracy: 0.8669 - val_loss: 0.3688 - val_accuracy: 0.8923\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4568 - accuracy: 0.8674 - val_loss: 0.4228 - val_accuracy: 0.8783\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.4580 - accuracy: 0.8676 - val_loss: 0.3783 - val_accuracy: 0.8894\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4488 - accuracy: 0.8705 - val_loss: 0.3595 - val_accuracy: 0.8986\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4457 - accuracy: 0.8713 - val_loss: 0.3659 - val_accuracy: 0.8940\n",
      "Try 8/100: Best_val_acc: [0.48781782388687134, 0.8539999723434448], lr: 6.137809361954345e-05, Lambda: 5.8364246649734e-05\n",
      "\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 2.6375 - accuracy: 0.1047 - val_loss: 2.2842 - val_accuracy: 0.1806\n",
      "Epoch 2/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.5558 - accuracy: 0.1206 - val_loss: 2.3390 - val_accuracy: 0.1898\n",
      "Epoch 3/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.4838 - accuracy: 0.1353 - val_loss: 2.3015 - val_accuracy: 0.1870\n",
      "Epoch 4/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.4110 - accuracy: 0.1513 - val_loss: 2.2052 - val_accuracy: 0.2288\n",
      "Epoch 5/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.3448 - accuracy: 0.1699 - val_loss: 2.1856 - val_accuracy: 0.2344\n",
      "Epoch 6/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.2845 - accuracy: 0.1864 - val_loss: 2.1061 - val_accuracy: 0.2710\n",
      "Epoch 7/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.2363 - accuracy: 0.1996 - val_loss: 2.0606 - val_accuracy: 0.3018\n",
      "Epoch 8/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1750 - accuracy: 0.2214 - val_loss: 1.9813 - val_accuracy: 0.3382\n",
      "Epoch 9/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.1261 - accuracy: 0.2378 - val_loss: 1.9222 - val_accuracy: 0.3805\n",
      "Epoch 10/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0745 - accuracy: 0.2584 - val_loss: 1.8601 - val_accuracy: 0.4125\n",
      "Epoch 11/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 2.0312 - accuracy: 0.2766 - val_loss: 1.8228 - val_accuracy: 0.4173\n",
      "Epoch 12/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9853 - accuracy: 0.2966 - val_loss: 1.8027 - val_accuracy: 0.4295\n",
      "Epoch 13/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.9435 - accuracy: 0.3103 - val_loss: 1.7178 - val_accuracy: 0.4788\n",
      "Epoch 14/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8980 - accuracy: 0.3340 - val_loss: 1.6692 - val_accuracy: 0.5255\n",
      "Epoch 15/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8633 - accuracy: 0.3499 - val_loss: 1.6131 - val_accuracy: 0.5574\n",
      "Epoch 16/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.8228 - accuracy: 0.3665 - val_loss: 1.5682 - val_accuracy: 0.5808\n",
      "Epoch 17/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.7885 - accuracy: 0.3825 - val_loss: 1.5527 - val_accuracy: 0.5894\n",
      "Epoch 18/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7528 - accuracy: 0.3974 - val_loss: 1.4775 - val_accuracy: 0.6197\n",
      "Epoch 19/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.7180 - accuracy: 0.4133 - val_loss: 1.4597 - val_accuracy: 0.6194\n",
      "Epoch 20/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6838 - accuracy: 0.4292 - val_loss: 1.4394 - val_accuracy: 0.6151\n",
      "Epoch 21/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6555 - accuracy: 0.4407 - val_loss: 1.4017 - val_accuracy: 0.6560\n",
      "Epoch 22/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.6223 - accuracy: 0.4543 - val_loss: 1.3498 - val_accuracy: 0.6654\n",
      "Epoch 23/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5981 - accuracy: 0.4650 - val_loss: 1.3324 - val_accuracy: 0.6563\n",
      "Epoch 24/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5638 - accuracy: 0.4804 - val_loss: 1.2887 - val_accuracy: 0.6881\n",
      "Epoch 25/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5375 - accuracy: 0.4948 - val_loss: 1.2617 - val_accuracy: 0.6866\n",
      "Epoch 26/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.5088 - accuracy: 0.5056 - val_loss: 1.2365 - val_accuracy: 0.6929\n",
      "Epoch 27/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4794 - accuracy: 0.5160 - val_loss: 1.1922 - val_accuracy: 0.6944\n",
      "Epoch 28/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4565 - accuracy: 0.5256 - val_loss: 1.1682 - val_accuracy: 0.7154\n",
      "Epoch 29/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.4305 - accuracy: 0.5347 - val_loss: 1.1232 - val_accuracy: 0.7216\n",
      "Epoch 30/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.4028 - accuracy: 0.5455 - val_loss: 1.1513 - val_accuracy: 0.7044\n",
      "Epoch 31/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3881 - accuracy: 0.5520 - val_loss: 1.1284 - val_accuracy: 0.7134\n",
      "Epoch 32/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3621 - accuracy: 0.5606 - val_loss: 1.0675 - val_accuracy: 0.7289\n",
      "Epoch 33/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.3348 - accuracy: 0.5752 - val_loss: 1.0852 - val_accuracy: 0.7171\n",
      "Epoch 34/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.3177 - accuracy: 0.5824 - val_loss: 1.0383 - val_accuracy: 0.7388\n",
      "Epoch 35/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2952 - accuracy: 0.5885 - val_loss: 1.0241 - val_accuracy: 0.7424\n",
      "Epoch 36/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2730 - accuracy: 0.5980 - val_loss: 0.9570 - val_accuracy: 0.7574\n",
      "Epoch 37/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2588 - accuracy: 0.6030 - val_loss: 0.9863 - val_accuracy: 0.7524\n",
      "Epoch 38/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2403 - accuracy: 0.6083 - val_loss: 0.9356 - val_accuracy: 0.7568\n",
      "Epoch 39/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2242 - accuracy: 0.6132 - val_loss: 0.9190 - val_accuracy: 0.7511\n",
      "Epoch 40/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.2031 - accuracy: 0.6242 - val_loss: 0.9527 - val_accuracy: 0.7291\n",
      "Epoch 41/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1847 - accuracy: 0.6292 - val_loss: 0.9262 - val_accuracy: 0.7555\n",
      "Epoch 42/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.1690 - accuracy: 0.6342 - val_loss: 0.9155 - val_accuracy: 0.7546\n",
      "Epoch 43/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1528 - accuracy: 0.6401 - val_loss: 0.8622 - val_accuracy: 0.7766\n",
      "Epoch 44/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1395 - accuracy: 0.6467 - val_loss: 0.8567 - val_accuracy: 0.7720\n",
      "Epoch 45/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.1210 - accuracy: 0.6536 - val_loss: 0.8095 - val_accuracy: 0.7919\n",
      "Epoch 46/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.1050 - accuracy: 0.6576 - val_loss: 0.8813 - val_accuracy: 0.7646\n",
      "Epoch 47/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0908 - accuracy: 0.6627 - val_loss: 0.8224 - val_accuracy: 0.7814\n",
      "Epoch 48/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.0748 - accuracy: 0.6685 - val_loss: 0.8222 - val_accuracy: 0.7729\n",
      "Epoch 49/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0652 - accuracy: 0.6740 - val_loss: 0.7923 - val_accuracy: 0.7826\n",
      "Epoch 50/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0520 - accuracy: 0.6795 - val_loss: 0.7395 - val_accuracy: 0.7971\n",
      "Epoch 51/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0417 - accuracy: 0.6818 - val_loss: 0.7798 - val_accuracy: 0.7833\n",
      "Epoch 52/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 1.0290 - accuracy: 0.6857 - val_loss: 0.7649 - val_accuracy: 0.7856\n",
      "Epoch 53/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0104 - accuracy: 0.6908 - val_loss: 0.7774 - val_accuracy: 0.7876\n",
      "Epoch 54/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0028 - accuracy: 0.6945 - val_loss: 0.8289 - val_accuracy: 0.7650\n",
      "Epoch 55/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9900 - accuracy: 0.6988 - val_loss: 0.7493 - val_accuracy: 0.7931\n",
      "Epoch 56/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9830 - accuracy: 0.7018 - val_loss: 0.7798 - val_accuracy: 0.7751\n",
      "Epoch 57/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9708 - accuracy: 0.7047 - val_loss: 0.7029 - val_accuracy: 0.8068\n",
      "Epoch 58/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9587 - accuracy: 0.7078 - val_loss: 0.7346 - val_accuracy: 0.7944\n",
      "Epoch 59/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9498 - accuracy: 0.7097 - val_loss: 0.7004 - val_accuracy: 0.8026\n",
      "Epoch 60/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9408 - accuracy: 0.7146 - val_loss: 0.7219 - val_accuracy: 0.7956\n",
      "Epoch 61/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9280 - accuracy: 0.7165 - val_loss: 0.7250 - val_accuracy: 0.7980\n",
      "Epoch 62/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9202 - accuracy: 0.7202 - val_loss: 0.6580 - val_accuracy: 0.8159\n",
      "Epoch 63/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.9138 - accuracy: 0.7245 - val_loss: 0.6901 - val_accuracy: 0.8043\n",
      "Epoch 64/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.9085 - accuracy: 0.7252 - val_loss: 0.6452 - val_accuracy: 0.8153\n",
      "Epoch 65/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8896 - accuracy: 0.7323 - val_loss: 0.6342 - val_accuracy: 0.8239\n",
      "Epoch 66/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8888 - accuracy: 0.7315 - val_loss: 0.6276 - val_accuracy: 0.8191\n",
      "Epoch 67/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8807 - accuracy: 0.7330 - val_loss: 0.6952 - val_accuracy: 0.8027\n",
      "Epoch 68/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8709 - accuracy: 0.7392 - val_loss: 0.6279 - val_accuracy: 0.8225\n",
      "Epoch 69/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8639 - accuracy: 0.7403 - val_loss: 0.6433 - val_accuracy: 0.8176\n",
      "Epoch 70/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8573 - accuracy: 0.7418 - val_loss: 0.7271 - val_accuracy: 0.7909\n",
      "Epoch 71/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8460 - accuracy: 0.7480 - val_loss: 0.6781 - val_accuracy: 0.8085\n",
      "Epoch 72/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8411 - accuracy: 0.7480 - val_loss: 0.6425 - val_accuracy: 0.8120\n",
      "Epoch 73/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8356 - accuracy: 0.7488 - val_loss: 0.5830 - val_accuracy: 0.8345\n",
      "Epoch 74/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.8227 - accuracy: 0.7540 - val_loss: 0.6199 - val_accuracy: 0.8224\n",
      "Epoch 75/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.8210 - accuracy: 0.7556 - val_loss: 0.6367 - val_accuracy: 0.7994\n",
      "Epoch 76/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.8130 - accuracy: 0.7545 - val_loss: 0.5632 - val_accuracy: 0.8408\n",
      "Epoch 77/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.8097 - accuracy: 0.7561 - val_loss: 0.6178 - val_accuracy: 0.8222\n",
      "Epoch 78/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7978 - accuracy: 0.7604 - val_loss: 0.5952 - val_accuracy: 0.8215\n",
      "Epoch 79/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7978 - accuracy: 0.7605 - val_loss: 0.5832 - val_accuracy: 0.8315\n",
      "Epoch 80/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.7595 - val_loss: 0.6620 - val_accuracy: 0.8089\n",
      "Epoch 81/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7894 - accuracy: 0.7642 - val_loss: 0.6392 - val_accuracy: 0.8161\n",
      "Epoch 82/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7796 - accuracy: 0.7666 - val_loss: 0.6875 - val_accuracy: 0.7918\n",
      "Epoch 83/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7740 - accuracy: 0.7691 - val_loss: 0.5520 - val_accuracy: 0.8405\n",
      "Epoch 84/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7695 - accuracy: 0.7688 - val_loss: 0.5594 - val_accuracy: 0.8372\n",
      "Epoch 85/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7642 - accuracy: 0.7724 - val_loss: 0.5878 - val_accuracy: 0.8274\n",
      "Epoch 86/150\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7542 - accuracy: 0.7756 - val_loss: 0.5619 - val_accuracy: 0.8392\n",
      "Epoch 87/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7527 - accuracy: 0.7759 - val_loss: 0.5450 - val_accuracy: 0.8420\n",
      "Epoch 88/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7472 - accuracy: 0.7766 - val_loss: 0.5457 - val_accuracy: 0.8405\n",
      "Epoch 89/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7462 - accuracy: 0.7769 - val_loss: 0.6265 - val_accuracy: 0.8083\n",
      "Epoch 90/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7446 - accuracy: 0.7774 - val_loss: 0.6037 - val_accuracy: 0.8210\n",
      "Epoch 91/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7310 - accuracy: 0.7818 - val_loss: 0.5406 - val_accuracy: 0.8408\n",
      "Epoch 92/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7290 - accuracy: 0.7837 - val_loss: 0.6058 - val_accuracy: 0.8212\n",
      "Epoch 93/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7250 - accuracy: 0.7835 - val_loss: 0.5458 - val_accuracy: 0.8393\n",
      "Epoch 94/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7210 - accuracy: 0.7852 - val_loss: 0.5602 - val_accuracy: 0.8341\n",
      "Epoch 95/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7178 - accuracy: 0.7881 - val_loss: 0.5397 - val_accuracy: 0.8417\n",
      "Epoch 96/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7105 - accuracy: 0.7889 - val_loss: 0.5167 - val_accuracy: 0.8480\n",
      "Epoch 97/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7134 - accuracy: 0.7868 - val_loss: 0.6023 - val_accuracy: 0.8186\n",
      "Epoch 98/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7035 - accuracy: 0.7893 - val_loss: 0.5044 - val_accuracy: 0.8521\n",
      "Epoch 99/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6979 - accuracy: 0.7920 - val_loss: 0.4914 - val_accuracy: 0.8595\n",
      "Epoch 100/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.7006 - accuracy: 0.7895 - val_loss: 0.4761 - val_accuracy: 0.8616\n",
      "Epoch 101/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6988 - accuracy: 0.7920 - val_loss: 0.5237 - val_accuracy: 0.8425\n",
      "Epoch 102/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6881 - accuracy: 0.7965 - val_loss: 0.5757 - val_accuracy: 0.8289\n",
      "Epoch 103/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.7965 - val_loss: 0.5328 - val_accuracy: 0.8436\n",
      "Epoch 104/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6838 - accuracy: 0.7964 - val_loss: 0.4739 - val_accuracy: 0.8620\n",
      "Epoch 105/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6786 - accuracy: 0.7982 - val_loss: 0.5096 - val_accuracy: 0.8506\n",
      "Epoch 106/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6752 - accuracy: 0.8002 - val_loss: 0.5786 - val_accuracy: 0.8286\n",
      "Epoch 107/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6762 - accuracy: 0.7983 - val_loss: 0.5030 - val_accuracy: 0.8496\n",
      "Epoch 108/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6648 - accuracy: 0.8008 - val_loss: 0.5472 - val_accuracy: 0.8376\n",
      "Epoch 109/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6679 - accuracy: 0.8021 - val_loss: 0.4899 - val_accuracy: 0.8586\n",
      "Epoch 110/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.8031 - val_loss: 0.4571 - val_accuracy: 0.8701\n",
      "Epoch 111/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6573 - accuracy: 0.8050 - val_loss: 0.4740 - val_accuracy: 0.8621\n",
      "Epoch 112/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6546 - accuracy: 0.8058 - val_loss: 0.4368 - val_accuracy: 0.8734\n",
      "Epoch 113/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6562 - accuracy: 0.8047 - val_loss: 0.4805 - val_accuracy: 0.8564\n",
      "Epoch 114/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6509 - accuracy: 0.8058 - val_loss: 0.5061 - val_accuracy: 0.8535\n",
      "Epoch 115/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6515 - accuracy: 0.8068 - val_loss: 0.4555 - val_accuracy: 0.8694\n",
      "Epoch 116/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6430 - accuracy: 0.8085 - val_loss: 0.6478 - val_accuracy: 0.8039\n",
      "Epoch 117/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6371 - accuracy: 0.8121 - val_loss: 0.5191 - val_accuracy: 0.8488\n",
      "Epoch 118/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6381 - accuracy: 0.8099 - val_loss: 0.5392 - val_accuracy: 0.8389\n",
      "Epoch 119/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6357 - accuracy: 0.8137 - val_loss: 0.4129 - val_accuracy: 0.8831\n",
      "Epoch 120/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6301 - accuracy: 0.8154 - val_loss: 0.4882 - val_accuracy: 0.8529\n",
      "Epoch 121/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6242 - accuracy: 0.8159 - val_loss: 0.4581 - val_accuracy: 0.8654\n",
      "Epoch 122/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.8165 - val_loss: 0.5021 - val_accuracy: 0.8536\n",
      "Epoch 123/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6279 - accuracy: 0.8151 - val_loss: 0.4636 - val_accuracy: 0.8661\n",
      "Epoch 124/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.8184 - val_loss: 0.4303 - val_accuracy: 0.8755\n",
      "Epoch 125/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6169 - accuracy: 0.8170 - val_loss: 0.4744 - val_accuracy: 0.8585\n",
      "Epoch 126/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6141 - accuracy: 0.8183 - val_loss: 0.4991 - val_accuracy: 0.8529\n",
      "Epoch 127/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.6123 - accuracy: 0.8187 - val_loss: 0.4514 - val_accuracy: 0.8668\n",
      "Epoch 128/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6036 - accuracy: 0.8214 - val_loss: 0.4448 - val_accuracy: 0.8687\n",
      "Epoch 129/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6100 - accuracy: 0.8184 - val_loss: 0.5179 - val_accuracy: 0.8466\n",
      "Epoch 130/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6042 - accuracy: 0.8227 - val_loss: 0.5582 - val_accuracy: 0.8339\n",
      "Epoch 131/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5985 - accuracy: 0.8216 - val_loss: 0.4843 - val_accuracy: 0.8519\n",
      "Epoch 132/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.6052 - accuracy: 0.8201 - val_loss: 0.5336 - val_accuracy: 0.8424\n",
      "Epoch 133/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5959 - accuracy: 0.8237 - val_loss: 0.4521 - val_accuracy: 0.8697\n",
      "Epoch 134/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5956 - accuracy: 0.8235 - val_loss: 0.5074 - val_accuracy: 0.8506\n",
      "Epoch 135/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5950 - accuracy: 0.8225 - val_loss: 0.6567 - val_accuracy: 0.8004\n",
      "Epoch 136/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5920 - accuracy: 0.8259 - val_loss: 0.5110 - val_accuracy: 0.8477\n",
      "Epoch 137/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5882 - accuracy: 0.8281 - val_loss: 0.4579 - val_accuracy: 0.8622\n",
      "Epoch 138/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5876 - accuracy: 0.8258 - val_loss: 0.3904 - val_accuracy: 0.8865\n",
      "Epoch 139/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5818 - accuracy: 0.8287 - val_loss: 0.4671 - val_accuracy: 0.8622\n",
      "Epoch 140/150\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.5835 - accuracy: 0.8295 - val_loss: 0.4587 - val_accuracy: 0.8635\n",
      "Epoch 141/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5770 - accuracy: 0.8308 - val_loss: 0.4410 - val_accuracy: 0.8733\n",
      "Epoch 142/150\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.5765 - accuracy: 0.8302 - val_loss: 0.4178 - val_accuracy: 0.8787\n",
      "Epoch 143/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5742 - accuracy: 0.8301 - val_loss: 0.4568 - val_accuracy: 0.8656\n",
      "Epoch 144/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5738 - accuracy: 0.8316 - val_loss: 0.4430 - val_accuracy: 0.8747\n",
      "Epoch 145/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5658 - accuracy: 0.8332 - val_loss: 0.4188 - val_accuracy: 0.8796\n",
      "Epoch 146/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5676 - accuracy: 0.8334 - val_loss: 0.4397 - val_accuracy: 0.8709\n",
      "Epoch 147/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5644 - accuracy: 0.8340 - val_loss: 0.4153 - val_accuracy: 0.8816\n",
      "Epoch 148/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5655 - accuracy: 0.8327 - val_loss: 0.4209 - val_accuracy: 0.8777\n",
      "Epoch 149/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5583 - accuracy: 0.8353 - val_loss: 0.4153 - val_accuracy: 0.8797\n",
      "Epoch 150/150\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.5570 - accuracy: 0.8356 - val_loss: 0.4832 - val_accuracy: 0.8601\n",
      "Try 9/100: Best_val_acc: [0.5417643785476685, 0.8362777829170227], lr: 3.668370363375151e-05, Lambda: 3.136079824928236e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-5.0, -4.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,-4))\n",
    "    best_acc = basicDeepNN2(150, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1594542625233,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "T_s38Zw0-GPy"
   },
   "outputs": [],
   "source": [
    "# Removing Dropout at last two layers\n",
    "def basicDeepNN2(iterations, lr, Lambda, activation, k_initial, verb=True):\n",
    "    ## hyperparameters\n",
    "    epochs = iterations\n",
    "    learning_rate = lr\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape = (X_train.shape[1], ), kernel_initializer=k_initial, name='Input'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, kernel_initializer=k_initial, name='Hidden_Layer_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, kernel_initializer=k_initial, name='Hidden_Layer_2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, kernel_initializer=k_initial, name='Hidden_Layer_3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, kernel_initializer=k_initial, name='Hidden_Layer_4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer=k_initial ,kernel_regularizer=regularizers.l2(Lambda), name='Output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #opt = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9)\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]),\n",
    "              epochs=iterations, batch_size=500, verbose= 1)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2043890,
     "status": "ok",
     "timestamp": 1594544782165,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "9j8xF-wLJgyO",
    "outputId": "10284e59-0b62-4f22-e87a-4e299f2e4ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_366 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_367 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_368 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_369 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_370 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_371 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.5929 - accuracy: 0.1097 - val_loss: 2.3642 - val_accuracy: 0.0526\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4280 - accuracy: 0.1385 - val_loss: 2.4037 - val_accuracy: 0.0959\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3033 - accuracy: 0.1749 - val_loss: 2.4111 - val_accuracy: 0.1171\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1847 - accuracy: 0.2182 - val_loss: 2.3110 - val_accuracy: 0.1543\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0706 - accuracy: 0.2668 - val_loss: 2.2033 - val_accuracy: 0.2199\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9656 - accuracy: 0.3174 - val_loss: 2.0376 - val_accuracy: 0.2948\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8660 - accuracy: 0.3678 - val_loss: 1.9070 - val_accuracy: 0.3644\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7710 - accuracy: 0.4082 - val_loss: 1.8000 - val_accuracy: 0.4181\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6840 - accuracy: 0.4523 - val_loss: 1.6915 - val_accuracy: 0.4724\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6083 - accuracy: 0.4849 - val_loss: 1.5716 - val_accuracy: 0.5228\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5375 - accuracy: 0.5139 - val_loss: 1.5255 - val_accuracy: 0.5480\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4716 - accuracy: 0.5418 - val_loss: 1.4654 - val_accuracy: 0.5651\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4078 - accuracy: 0.5670 - val_loss: 1.3563 - val_accuracy: 0.5965\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3544 - accuracy: 0.5871 - val_loss: 1.2867 - val_accuracy: 0.6310\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2999 - accuracy: 0.6087 - val_loss: 1.2085 - val_accuracy: 0.6661\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2553 - accuracy: 0.6253 - val_loss: 1.2238 - val_accuracy: 0.6529\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2124 - accuracy: 0.6357 - val_loss: 1.1699 - val_accuracy: 0.6754\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1766 - accuracy: 0.6478 - val_loss: 1.1289 - val_accuracy: 0.6749\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1392 - accuracy: 0.6609 - val_loss: 1.0893 - val_accuracy: 0.6881\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1036 - accuracy: 0.6720 - val_loss: 1.0661 - val_accuracy: 0.6961\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0749 - accuracy: 0.6799 - val_loss: 1.0219 - val_accuracy: 0.7179\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0491 - accuracy: 0.6870 - val_loss: 0.9969 - val_accuracy: 0.7226\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0219 - accuracy: 0.6935 - val_loss: 0.9618 - val_accuracy: 0.7278\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9920 - accuracy: 0.7017 - val_loss: 0.8995 - val_accuracy: 0.7384\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9724 - accuracy: 0.7086 - val_loss: 0.8641 - val_accuracy: 0.7476\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9517 - accuracy: 0.7150 - val_loss: 0.8636 - val_accuracy: 0.7590\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9322 - accuracy: 0.7192 - val_loss: 0.8508 - val_accuracy: 0.7573\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9115 - accuracy: 0.7226 - val_loss: 0.8298 - val_accuracy: 0.7644\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8965 - accuracy: 0.7262 - val_loss: 0.8362 - val_accuracy: 0.7557\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8802 - accuracy: 0.7327 - val_loss: 0.8291 - val_accuracy: 0.7506\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8616 - accuracy: 0.7372 - val_loss: 0.7903 - val_accuracy: 0.7683\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8413 - accuracy: 0.7450 - val_loss: 0.7366 - val_accuracy: 0.7877\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8310 - accuracy: 0.7478 - val_loss: 0.7280 - val_accuracy: 0.7835\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8132 - accuracy: 0.7532 - val_loss: 0.7269 - val_accuracy: 0.7851\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8036 - accuracy: 0.7549 - val_loss: 0.7181 - val_accuracy: 0.7906\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7934 - accuracy: 0.7569 - val_loss: 0.7712 - val_accuracy: 0.7701\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7798 - accuracy: 0.7613 - val_loss: 0.6597 - val_accuracy: 0.8086\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7664 - accuracy: 0.7651 - val_loss: 0.8266 - val_accuracy: 0.7549\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7586 - accuracy: 0.7672 - val_loss: 0.7202 - val_accuracy: 0.7826\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7443 - accuracy: 0.7739 - val_loss: 0.6671 - val_accuracy: 0.8069\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7339 - accuracy: 0.7763 - val_loss: 0.7463 - val_accuracy: 0.7749\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7268 - accuracy: 0.7761 - val_loss: 0.6563 - val_accuracy: 0.7984\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7176 - accuracy: 0.7808 - val_loss: 0.6706 - val_accuracy: 0.7920\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7084 - accuracy: 0.7820 - val_loss: 0.7400 - val_accuracy: 0.7771\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6991 - accuracy: 0.7853 - val_loss: 0.7045 - val_accuracy: 0.7882\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6919 - accuracy: 0.7865 - val_loss: 0.6508 - val_accuracy: 0.8008\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6874 - accuracy: 0.7882 - val_loss: 0.6508 - val_accuracy: 0.8039\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6759 - accuracy: 0.7919 - val_loss: 0.5959 - val_accuracy: 0.8189\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6702 - accuracy: 0.7923 - val_loss: 0.6473 - val_accuracy: 0.8037\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6613 - accuracy: 0.7952 - val_loss: 0.6053 - val_accuracy: 0.8221\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6520 - accuracy: 0.7974 - val_loss: 0.6193 - val_accuracy: 0.8183\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6412 - accuracy: 0.8018 - val_loss: 0.5765 - val_accuracy: 0.8309\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6426 - accuracy: 0.8016 - val_loss: 0.5898 - val_accuracy: 0.8205\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6360 - accuracy: 0.8033 - val_loss: 0.6459 - val_accuracy: 0.7980\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6289 - accuracy: 0.8064 - val_loss: 0.5492 - val_accuracy: 0.8329\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6234 - accuracy: 0.8066 - val_loss: 0.5699 - val_accuracy: 0.8287\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6163 - accuracy: 0.8079 - val_loss: 0.6375 - val_accuracy: 0.8034\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6080 - accuracy: 0.8144 - val_loss: 0.7035 - val_accuracy: 0.7879\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6043 - accuracy: 0.8137 - val_loss: 0.4850 - val_accuracy: 0.8538\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6014 - accuracy: 0.8143 - val_loss: 0.5346 - val_accuracy: 0.8406\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5920 - accuracy: 0.8183 - val_loss: 0.5498 - val_accuracy: 0.8363\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5865 - accuracy: 0.8177 - val_loss: 0.5618 - val_accuracy: 0.8281\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5842 - accuracy: 0.8170 - val_loss: 0.5779 - val_accuracy: 0.8205\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5800 - accuracy: 0.8207 - val_loss: 0.6042 - val_accuracy: 0.8159\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5787 - accuracy: 0.8198 - val_loss: 0.5224 - val_accuracy: 0.8452\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5718 - accuracy: 0.8239 - val_loss: 0.5424 - val_accuracy: 0.8360\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5702 - accuracy: 0.8218 - val_loss: 0.6273 - val_accuracy: 0.8099\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5592 - accuracy: 0.8265 - val_loss: 0.5071 - val_accuracy: 0.8493\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5566 - accuracy: 0.8272 - val_loss: 0.5458 - val_accuracy: 0.8379\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5545 - accuracy: 0.8294 - val_loss: 0.6669 - val_accuracy: 0.7974\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5556 - accuracy: 0.8280 - val_loss: 0.5735 - val_accuracy: 0.8298\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5487 - accuracy: 0.8298 - val_loss: 0.4487 - val_accuracy: 0.8649\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5462 - accuracy: 0.8308 - val_loss: 0.5205 - val_accuracy: 0.8459\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5406 - accuracy: 0.8326 - val_loss: 0.5198 - val_accuracy: 0.8417\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5351 - accuracy: 0.8352 - val_loss: 0.5584 - val_accuracy: 0.8361\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5309 - accuracy: 0.8343 - val_loss: 0.4674 - val_accuracy: 0.8628\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5295 - accuracy: 0.8364 - val_loss: 0.4779 - val_accuracy: 0.8539\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5240 - accuracy: 0.8364 - val_loss: 0.4590 - val_accuracy: 0.8652\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5153 - accuracy: 0.8402 - val_loss: 0.5413 - val_accuracy: 0.8433\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5213 - accuracy: 0.8383 - val_loss: 0.5250 - val_accuracy: 0.8397\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5164 - accuracy: 0.8398 - val_loss: 0.4294 - val_accuracy: 0.8736\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5090 - accuracy: 0.8425 - val_loss: 0.5045 - val_accuracy: 0.8568\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5085 - accuracy: 0.8429 - val_loss: 0.5619 - val_accuracy: 0.8318\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5077 - accuracy: 0.8418 - val_loss: 0.5450 - val_accuracy: 0.8296\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5036 - accuracy: 0.8431 - val_loss: 0.4761 - val_accuracy: 0.8574\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4983 - accuracy: 0.8460 - val_loss: 0.5483 - val_accuracy: 0.8311\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4915 - accuracy: 0.8473 - val_loss: 0.5443 - val_accuracy: 0.8345\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4949 - accuracy: 0.8475 - val_loss: 0.4349 - val_accuracy: 0.8735\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4857 - accuracy: 0.8503 - val_loss: 0.4455 - val_accuracy: 0.8684\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4876 - accuracy: 0.8490 - val_loss: 0.4935 - val_accuracy: 0.8525\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4834 - accuracy: 0.8515 - val_loss: 0.4364 - val_accuracy: 0.8701\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4808 - accuracy: 0.8503 - val_loss: 0.4837 - val_accuracy: 0.8508\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.8550 - val_loss: 0.5683 - val_accuracy: 0.8266\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4801 - accuracy: 0.8493 - val_loss: 0.5494 - val_accuracy: 0.8311\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4729 - accuracy: 0.8541 - val_loss: 0.4293 - val_accuracy: 0.8774\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4697 - accuracy: 0.8541 - val_loss: 0.5495 - val_accuracy: 0.8384\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4657 - accuracy: 0.8554 - val_loss: 0.5404 - val_accuracy: 0.8393\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4666 - accuracy: 0.8549 - val_loss: 0.4671 - val_accuracy: 0.8614\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4627 - accuracy: 0.8560 - val_loss: 0.5262 - val_accuracy: 0.8450\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4620 - accuracy: 0.8570 - val_loss: 0.4268 - val_accuracy: 0.8752\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4593 - accuracy: 0.8593 - val_loss: 0.4439 - val_accuracy: 0.8695\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4537 - accuracy: 0.8580 - val_loss: 0.5204 - val_accuracy: 0.8378\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4494 - accuracy: 0.8623 - val_loss: 0.4380 - val_accuracy: 0.8694\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4504 - accuracy: 0.8622 - val_loss: 0.5419 - val_accuracy: 0.8374\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4512 - accuracy: 0.8611 - val_loss: 0.4256 - val_accuracy: 0.8723\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4450 - accuracy: 0.8618 - val_loss: 0.4845 - val_accuracy: 0.8607\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4470 - accuracy: 0.8619 - val_loss: 0.3971 - val_accuracy: 0.8879\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4396 - accuracy: 0.8633 - val_loss: 0.4929 - val_accuracy: 0.8531\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4358 - accuracy: 0.8655 - val_loss: 0.6065 - val_accuracy: 0.8181\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4391 - accuracy: 0.8626 - val_loss: 0.4398 - val_accuracy: 0.8716\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4341 - accuracy: 0.8650 - val_loss: 0.5251 - val_accuracy: 0.8432\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4363 - accuracy: 0.8638 - val_loss: 0.4163 - val_accuracy: 0.8795\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4293 - accuracy: 0.8665 - val_loss: 0.4289 - val_accuracy: 0.8724\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4266 - accuracy: 0.8681 - val_loss: 0.3673 - val_accuracy: 0.8952\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4264 - accuracy: 0.8687 - val_loss: 0.4442 - val_accuracy: 0.8618\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4255 - accuracy: 0.8678 - val_loss: 0.3588 - val_accuracy: 0.8954\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4195 - accuracy: 0.8704 - val_loss: 0.4357 - val_accuracy: 0.8695\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4202 - accuracy: 0.8693 - val_loss: 0.4580 - val_accuracy: 0.8619\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4184 - accuracy: 0.8710 - val_loss: 0.4431 - val_accuracy: 0.8670\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4172 - accuracy: 0.8726 - val_loss: 0.4095 - val_accuracy: 0.8754\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4162 - accuracy: 0.8694 - val_loss: 0.4673 - val_accuracy: 0.8627\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4083 - accuracy: 0.8731 - val_loss: 0.4008 - val_accuracy: 0.8832\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4096 - accuracy: 0.8714 - val_loss: 0.4044 - val_accuracy: 0.8819\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4062 - accuracy: 0.8743 - val_loss: 0.5308 - val_accuracy: 0.8333\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4047 - accuracy: 0.8753 - val_loss: 0.4218 - val_accuracy: 0.8774\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4082 - accuracy: 0.8729 - val_loss: 0.4397 - val_accuracy: 0.8691\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4036 - accuracy: 0.8761 - val_loss: 0.3913 - val_accuracy: 0.8874\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4033 - accuracy: 0.8747 - val_loss: 0.4730 - val_accuracy: 0.8466\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4011 - accuracy: 0.8764 - val_loss: 0.4271 - val_accuracy: 0.8732\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8745 - val_loss: 0.4095 - val_accuracy: 0.8739\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3960 - accuracy: 0.8779 - val_loss: 0.3875 - val_accuracy: 0.8876\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3927 - accuracy: 0.8780 - val_loss: 0.4394 - val_accuracy: 0.8698\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3920 - accuracy: 0.8783 - val_loss: 0.4010 - val_accuracy: 0.8827\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3904 - accuracy: 0.8794 - val_loss: 0.4381 - val_accuracy: 0.8694\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3930 - accuracy: 0.8770 - val_loss: 0.4032 - val_accuracy: 0.8751\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3851 - accuracy: 0.8785 - val_loss: 0.4002 - val_accuracy: 0.8863\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8781 - val_loss: 0.4291 - val_accuracy: 0.8740\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3837 - accuracy: 0.8819 - val_loss: 0.3797 - val_accuracy: 0.8889\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3840 - accuracy: 0.8807 - val_loss: 0.5205 - val_accuracy: 0.8447\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3850 - accuracy: 0.8802 - val_loss: 0.4034 - val_accuracy: 0.8811\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3832 - accuracy: 0.8810 - val_loss: 0.3385 - val_accuracy: 0.9041\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3765 - accuracy: 0.8826 - val_loss: 0.3453 - val_accuracy: 0.9030\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3812 - accuracy: 0.8811 - val_loss: 0.5851 - val_accuracy: 0.8251\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3748 - accuracy: 0.8846 - val_loss: 0.5787 - val_accuracy: 0.8315\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3752 - accuracy: 0.8840 - val_loss: 0.4350 - val_accuracy: 0.8693\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3726 - accuracy: 0.8855 - val_loss: 0.4381 - val_accuracy: 0.8743\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3706 - accuracy: 0.8845 - val_loss: 0.4664 - val_accuracy: 0.8560\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3718 - accuracy: 0.8848 - val_loss: 0.5009 - val_accuracy: 0.8494\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3695 - accuracy: 0.8847 - val_loss: 0.3772 - val_accuracy: 0.8891\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3680 - accuracy: 0.8845 - val_loss: 0.3682 - val_accuracy: 0.8954\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3650 - accuracy: 0.8853 - val_loss: 0.3803 - val_accuracy: 0.8921\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3635 - accuracy: 0.8855 - val_loss: 0.4540 - val_accuracy: 0.8673\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3637 - accuracy: 0.8898 - val_loss: 0.3680 - val_accuracy: 0.8933\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3628 - accuracy: 0.8872 - val_loss: 0.3864 - val_accuracy: 0.8860\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3600 - accuracy: 0.8896 - val_loss: 0.3666 - val_accuracy: 0.8945\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3551 - accuracy: 0.8906 - val_loss: 0.5129 - val_accuracy: 0.8481\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3555 - accuracy: 0.8884 - val_loss: 0.4388 - val_accuracy: 0.8724\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3530 - accuracy: 0.8908 - val_loss: 0.4237 - val_accuracy: 0.8766\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3515 - accuracy: 0.8908 - val_loss: 0.3965 - val_accuracy: 0.8853\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3508 - accuracy: 0.8922 - val_loss: 0.3135 - val_accuracy: 0.9096\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3503 - accuracy: 0.8913 - val_loss: 0.3631 - val_accuracy: 0.8945\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3427 - accuracy: 0.8925 - val_loss: 0.3763 - val_accuracy: 0.8936\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3458 - accuracy: 0.8918 - val_loss: 0.3670 - val_accuracy: 0.8944\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3524 - accuracy: 0.8896 - val_loss: 0.3820 - val_accuracy: 0.8884\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3501 - accuracy: 0.8916 - val_loss: 0.3388 - val_accuracy: 0.8995\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3514 - accuracy: 0.8893 - val_loss: 0.3802 - val_accuracy: 0.8888\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3443 - accuracy: 0.8921 - val_loss: 0.4286 - val_accuracy: 0.8703\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3397 - accuracy: 0.8933 - val_loss: 0.3506 - val_accuracy: 0.8986\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3407 - accuracy: 0.8953 - val_loss: 0.3305 - val_accuracy: 0.9062\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3396 - accuracy: 0.8945 - val_loss: 0.4632 - val_accuracy: 0.8636\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3373 - accuracy: 0.8950 - val_loss: 0.3869 - val_accuracy: 0.8879\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3350 - accuracy: 0.8951 - val_loss: 0.3831 - val_accuracy: 0.8915\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3385 - accuracy: 0.8941 - val_loss: 0.4938 - val_accuracy: 0.8536\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3312 - accuracy: 0.8966 - val_loss: 0.4218 - val_accuracy: 0.8810\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3329 - accuracy: 0.8960 - val_loss: 0.3114 - val_accuracy: 0.9133\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3337 - accuracy: 0.8951 - val_loss: 0.3544 - val_accuracy: 0.8963\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3334 - accuracy: 0.8960 - val_loss: 0.3210 - val_accuracy: 0.9106\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3294 - accuracy: 0.8976 - val_loss: 0.3869 - val_accuracy: 0.8803\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3259 - accuracy: 0.8990 - val_loss: 0.3311 - val_accuracy: 0.9043\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3259 - accuracy: 0.8980 - val_loss: 0.3443 - val_accuracy: 0.8991\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3233 - accuracy: 0.8994 - val_loss: 0.3581 - val_accuracy: 0.8964\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3244 - accuracy: 0.8993 - val_loss: 0.4173 - val_accuracy: 0.8764\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3290 - accuracy: 0.8980 - val_loss: 0.3932 - val_accuracy: 0.8821\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3237 - accuracy: 0.8978 - val_loss: 0.3855 - val_accuracy: 0.8865\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3226 - accuracy: 0.9002 - val_loss: 0.3530 - val_accuracy: 0.8966\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3218 - accuracy: 0.9008 - val_loss: 0.3712 - val_accuracy: 0.8932\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3194 - accuracy: 0.9010 - val_loss: 0.3328 - val_accuracy: 0.9033\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3176 - accuracy: 0.9005 - val_loss: 0.2702 - val_accuracy: 0.9244\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3208 - accuracy: 0.9009 - val_loss: 0.3136 - val_accuracy: 0.9109\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3105 - accuracy: 0.9038 - val_loss: 0.3127 - val_accuracy: 0.9121\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3144 - accuracy: 0.9023 - val_loss: 0.3082 - val_accuracy: 0.9101\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3134 - accuracy: 0.9027 - val_loss: 0.3424 - val_accuracy: 0.9017\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3149 - accuracy: 0.9018 - val_loss: 0.4184 - val_accuracy: 0.8782\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3112 - accuracy: 0.9026 - val_loss: 0.4050 - val_accuracy: 0.8825\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3095 - accuracy: 0.9041 - val_loss: 0.4122 - val_accuracy: 0.8752\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3096 - accuracy: 0.9036 - val_loss: 0.3594 - val_accuracy: 0.8982\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3136 - accuracy: 0.9040 - val_loss: 0.3293 - val_accuracy: 0.9072\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3062 - accuracy: 0.9035 - val_loss: 0.3159 - val_accuracy: 0.9092\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3063 - accuracy: 0.9057 - val_loss: 0.4089 - val_accuracy: 0.8795\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3055 - accuracy: 0.9045 - val_loss: 0.3364 - val_accuracy: 0.9031\n",
      "Try 1/100: Best_val_acc: [0.4911455512046814, 0.8566111326217651], lr: 6.136273682945685e-05, Lambda: 5.7790769353008696e-05\n",
      "\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_372 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_373 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_374 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_375 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_376 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_377 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6946 - accuracy: 0.1041 - val_loss: 2.3233 - val_accuracy: 0.1187\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5044 - accuracy: 0.1282 - val_loss: 2.2584 - val_accuracy: 0.1990\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3827 - accuracy: 0.1548 - val_loss: 2.1909 - val_accuracy: 0.2380\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2645 - accuracy: 0.1921 - val_loss: 2.0698 - val_accuracy: 0.3292\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1392 - accuracy: 0.2357 - val_loss: 1.9517 - val_accuracy: 0.4072\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0296 - accuracy: 0.2814 - val_loss: 1.8099 - val_accuracy: 0.4689\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9283 - accuracy: 0.3272 - val_loss: 1.7165 - val_accuracy: 0.5061\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8379 - accuracy: 0.3717 - val_loss: 1.6653 - val_accuracy: 0.5050\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7581 - accuracy: 0.4084 - val_loss: 1.5873 - val_accuracy: 0.5420\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6805 - accuracy: 0.4483 - val_loss: 1.5073 - val_accuracy: 0.5620\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6151 - accuracy: 0.4795 - val_loss: 1.4345 - val_accuracy: 0.5994\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5473 - accuracy: 0.5089 - val_loss: 1.4049 - val_accuracy: 0.6194\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4879 - accuracy: 0.5353 - val_loss: 1.2856 - val_accuracy: 0.6446\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4390 - accuracy: 0.5523 - val_loss: 1.3496 - val_accuracy: 0.6161\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3923 - accuracy: 0.5722 - val_loss: 1.2184 - val_accuracy: 0.6656\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3416 - accuracy: 0.5918 - val_loss: 1.3026 - val_accuracy: 0.6175\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3033 - accuracy: 0.6044 - val_loss: 1.2181 - val_accuracy: 0.6527\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2645 - accuracy: 0.6173 - val_loss: 1.1193 - val_accuracy: 0.7096\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2240 - accuracy: 0.6319 - val_loss: 1.0681 - val_accuracy: 0.7213\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1908 - accuracy: 0.6438 - val_loss: 1.0531 - val_accuracy: 0.7118\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1584 - accuracy: 0.6527 - val_loss: 1.0672 - val_accuracy: 0.7061\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1257 - accuracy: 0.6633 - val_loss: 1.0518 - val_accuracy: 0.6935\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0962 - accuracy: 0.6724 - val_loss: 1.0086 - val_accuracy: 0.7240\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0674 - accuracy: 0.6812 - val_loss: 0.9567 - val_accuracy: 0.7298\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0509 - accuracy: 0.6863 - val_loss: 0.9153 - val_accuracy: 0.7439\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0202 - accuracy: 0.6960 - val_loss: 0.9743 - val_accuracy: 0.7219\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9945 - accuracy: 0.7033 - val_loss: 0.9115 - val_accuracy: 0.7449\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9776 - accuracy: 0.7068 - val_loss: 0.8545 - val_accuracy: 0.7581\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9571 - accuracy: 0.7151 - val_loss: 0.8948 - val_accuracy: 0.7460\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9441 - accuracy: 0.7172 - val_loss: 0.8687 - val_accuracy: 0.7511\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9205 - accuracy: 0.7244 - val_loss: 0.9075 - val_accuracy: 0.7431\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8996 - accuracy: 0.7291 - val_loss: 0.7921 - val_accuracy: 0.7771\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8808 - accuracy: 0.7365 - val_loss: 0.8397 - val_accuracy: 0.7578\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8668 - accuracy: 0.7400 - val_loss: 0.7628 - val_accuracy: 0.7790\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8477 - accuracy: 0.7418 - val_loss: 0.7132 - val_accuracy: 0.8000\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8329 - accuracy: 0.7474 - val_loss: 0.8551 - val_accuracy: 0.7487\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8260 - accuracy: 0.7495 - val_loss: 0.7623 - val_accuracy: 0.7816\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8083 - accuracy: 0.7540 - val_loss: 0.9885 - val_accuracy: 0.7016\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7987 - accuracy: 0.7569 - val_loss: 0.6672 - val_accuracy: 0.8124\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7837 - accuracy: 0.7612 - val_loss: 0.7376 - val_accuracy: 0.7721\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7668 - accuracy: 0.7655 - val_loss: 0.7182 - val_accuracy: 0.7884\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7661 - accuracy: 0.7645 - val_loss: 0.8463 - val_accuracy: 0.7531\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7498 - accuracy: 0.7726 - val_loss: 0.7882 - val_accuracy: 0.7575\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7329 - accuracy: 0.7775 - val_loss: 0.6775 - val_accuracy: 0.8025\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7330 - accuracy: 0.7765 - val_loss: 0.7098 - val_accuracy: 0.7811\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7195 - accuracy: 0.7793 - val_loss: 0.6760 - val_accuracy: 0.7974\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7137 - accuracy: 0.7816 - val_loss: 0.6077 - val_accuracy: 0.8246\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7015 - accuracy: 0.7851 - val_loss: 0.6557 - val_accuracy: 0.8073\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6982 - accuracy: 0.7870 - val_loss: 0.5582 - val_accuracy: 0.8412\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6844 - accuracy: 0.7915 - val_loss: 0.7227 - val_accuracy: 0.7835\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6783 - accuracy: 0.7919 - val_loss: 0.5904 - val_accuracy: 0.8258\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6680 - accuracy: 0.7979 - val_loss: 0.6981 - val_accuracy: 0.7848\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6621 - accuracy: 0.7951 - val_loss: 0.6461 - val_accuracy: 0.8044\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6573 - accuracy: 0.7994 - val_loss: 0.5400 - val_accuracy: 0.8392\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6455 - accuracy: 0.8020 - val_loss: 0.6184 - val_accuracy: 0.8134\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6436 - accuracy: 0.8005 - val_loss: 0.5284 - val_accuracy: 0.8500\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6363 - accuracy: 0.8047 - val_loss: 0.6866 - val_accuracy: 0.7928\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6289 - accuracy: 0.8073 - val_loss: 0.7043 - val_accuracy: 0.7964\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6238 - accuracy: 0.8078 - val_loss: 0.5578 - val_accuracy: 0.8340\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6225 - accuracy: 0.8071 - val_loss: 0.5677 - val_accuracy: 0.8296\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6131 - accuracy: 0.8117 - val_loss: 0.5509 - val_accuracy: 0.8320\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6085 - accuracy: 0.8120 - val_loss: 0.5468 - val_accuracy: 0.8341\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6033 - accuracy: 0.8131 - val_loss: 0.5366 - val_accuracy: 0.8422\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5966 - accuracy: 0.8160 - val_loss: 0.6028 - val_accuracy: 0.8211\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5945 - accuracy: 0.8162 - val_loss: 0.5204 - val_accuracy: 0.8466\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5864 - accuracy: 0.8190 - val_loss: 0.5523 - val_accuracy: 0.8310\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5799 - accuracy: 0.8212 - val_loss: 0.5280 - val_accuracy: 0.8439\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5795 - accuracy: 0.8214 - val_loss: 0.5308 - val_accuracy: 0.8411\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5751 - accuracy: 0.8221 - val_loss: 0.5359 - val_accuracy: 0.8410\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5660 - accuracy: 0.8251 - val_loss: 0.4754 - val_accuracy: 0.8543\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5635 - accuracy: 0.8275 - val_loss: 0.5175 - val_accuracy: 0.8477\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5579 - accuracy: 0.8282 - val_loss: 0.4938 - val_accuracy: 0.8513\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5604 - accuracy: 0.8266 - val_loss: 0.4793 - val_accuracy: 0.8584\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5500 - accuracy: 0.8301 - val_loss: 0.4452 - val_accuracy: 0.8695\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5426 - accuracy: 0.8318 - val_loss: 0.4362 - val_accuracy: 0.8741\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5420 - accuracy: 0.8327 - val_loss: 0.4310 - val_accuracy: 0.8743\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5367 - accuracy: 0.8345 - val_loss: 0.6284 - val_accuracy: 0.8116\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5370 - accuracy: 0.8326 - val_loss: 0.4196 - val_accuracy: 0.8777\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5355 - accuracy: 0.8345 - val_loss: 0.6678 - val_accuracy: 0.8022\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5261 - accuracy: 0.8364 - val_loss: 0.5076 - val_accuracy: 0.8496\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5222 - accuracy: 0.8390 - val_loss: 0.5995 - val_accuracy: 0.8253\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5203 - accuracy: 0.8395 - val_loss: 0.5545 - val_accuracy: 0.8358\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5119 - accuracy: 0.8439 - val_loss: 0.4453 - val_accuracy: 0.8690\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5154 - accuracy: 0.8416 - val_loss: 0.4654 - val_accuracy: 0.8629\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5141 - accuracy: 0.8407 - val_loss: 0.5429 - val_accuracy: 0.8346\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5032 - accuracy: 0.8453 - val_loss: 0.5015 - val_accuracy: 0.8479\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5078 - accuracy: 0.8435 - val_loss: 0.5076 - val_accuracy: 0.8483\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5010 - accuracy: 0.8461 - val_loss: 0.4488 - val_accuracy: 0.8695\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4952 - accuracy: 0.8457 - val_loss: 0.4390 - val_accuracy: 0.8704\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4928 - accuracy: 0.8493 - val_loss: 0.4980 - val_accuracy: 0.8523\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4887 - accuracy: 0.8478 - val_loss: 0.4661 - val_accuracy: 0.8646\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4916 - accuracy: 0.8466 - val_loss: 0.4761 - val_accuracy: 0.8596\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4839 - accuracy: 0.8518 - val_loss: 0.4929 - val_accuracy: 0.8526\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4830 - accuracy: 0.8524 - val_loss: 0.4593 - val_accuracy: 0.8619\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4803 - accuracy: 0.8517 - val_loss: 0.5037 - val_accuracy: 0.8444\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4797 - accuracy: 0.8524 - val_loss: 0.4025 - val_accuracy: 0.8804\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4745 - accuracy: 0.8533 - val_loss: 0.4808 - val_accuracy: 0.8581\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4747 - accuracy: 0.8537 - val_loss: 0.4022 - val_accuracy: 0.8833\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4680 - accuracy: 0.8562 - val_loss: 0.4704 - val_accuracy: 0.8618\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4610 - accuracy: 0.8579 - val_loss: 0.5104 - val_accuracy: 0.8460\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4605 - accuracy: 0.8590 - val_loss: 0.4079 - val_accuracy: 0.8841\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4611 - accuracy: 0.8582 - val_loss: 0.4617 - val_accuracy: 0.8645\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4590 - accuracy: 0.8577 - val_loss: 0.3791 - val_accuracy: 0.8879\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4580 - accuracy: 0.8607 - val_loss: 0.5339 - val_accuracy: 0.8361\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4515 - accuracy: 0.8626 - val_loss: 0.5761 - val_accuracy: 0.8311\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4490 - accuracy: 0.8639 - val_loss: 0.4811 - val_accuracy: 0.8588\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4552 - accuracy: 0.8598 - val_loss: 0.4564 - val_accuracy: 0.8649\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4444 - accuracy: 0.8624 - val_loss: 0.4604 - val_accuracy: 0.8564\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4466 - accuracy: 0.8617 - val_loss: 0.4277 - val_accuracy: 0.8748\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4400 - accuracy: 0.8642 - val_loss: 0.5673 - val_accuracy: 0.8254\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4395 - accuracy: 0.8630 - val_loss: 0.3496 - val_accuracy: 0.8999\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4336 - accuracy: 0.8655 - val_loss: 0.4079 - val_accuracy: 0.8811\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4330 - accuracy: 0.8675 - val_loss: 0.4963 - val_accuracy: 0.8536\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4371 - accuracy: 0.8638 - val_loss: 0.3893 - val_accuracy: 0.8895\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4331 - accuracy: 0.8643 - val_loss: 0.4563 - val_accuracy: 0.8661\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4237 - accuracy: 0.8686 - val_loss: 0.3785 - val_accuracy: 0.8919\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4196 - accuracy: 0.8707 - val_loss: 0.4215 - val_accuracy: 0.8736\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4224 - accuracy: 0.8701 - val_loss: 0.3772 - val_accuracy: 0.8899\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4220 - accuracy: 0.8698 - val_loss: 0.4089 - val_accuracy: 0.8797\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4230 - accuracy: 0.8691 - val_loss: 0.3955 - val_accuracy: 0.8853\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4199 - accuracy: 0.8705 - val_loss: 0.4362 - val_accuracy: 0.8721\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8707 - val_loss: 0.4554 - val_accuracy: 0.8681\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4151 - accuracy: 0.8711 - val_loss: 0.3648 - val_accuracy: 0.8954\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4125 - accuracy: 0.8714 - val_loss: 0.5376 - val_accuracy: 0.8414\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4080 - accuracy: 0.8737 - val_loss: 0.4368 - val_accuracy: 0.8717\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4101 - accuracy: 0.8718 - val_loss: 0.3397 - val_accuracy: 0.9004\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4048 - accuracy: 0.8750 - val_loss: 0.4876 - val_accuracy: 0.8526\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4070 - accuracy: 0.8747 - val_loss: 0.4144 - val_accuracy: 0.8771\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4061 - accuracy: 0.8748 - val_loss: 0.3693 - val_accuracy: 0.8964\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4028 - accuracy: 0.8753 - val_loss: 0.4592 - val_accuracy: 0.8636\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3993 - accuracy: 0.8762 - val_loss: 0.4670 - val_accuracy: 0.8637\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3962 - accuracy: 0.8775 - val_loss: 0.4775 - val_accuracy: 0.8544\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3971 - accuracy: 0.8767 - val_loss: 0.3446 - val_accuracy: 0.9004\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3960 - accuracy: 0.8777 - val_loss: 0.3495 - val_accuracy: 0.9000\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3910 - accuracy: 0.8803 - val_loss: 0.3235 - val_accuracy: 0.9071\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8772 - val_loss: 0.4130 - val_accuracy: 0.8831\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3912 - accuracy: 0.8793 - val_loss: 0.3765 - val_accuracy: 0.8910\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3863 - accuracy: 0.8802 - val_loss: 0.4030 - val_accuracy: 0.8836\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3830 - accuracy: 0.8810 - val_loss: 0.3572 - val_accuracy: 0.8984\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3781 - accuracy: 0.8826 - val_loss: 0.4291 - val_accuracy: 0.8698\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3836 - accuracy: 0.8816 - val_loss: 0.3909 - val_accuracy: 0.8864\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3849 - accuracy: 0.8806 - val_loss: 0.4365 - val_accuracy: 0.8696\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3828 - accuracy: 0.8805 - val_loss: 0.3647 - val_accuracy: 0.8930\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3760 - accuracy: 0.8852 - val_loss: 0.3660 - val_accuracy: 0.8935\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3736 - accuracy: 0.8855 - val_loss: 0.3052 - val_accuracy: 0.9130\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3770 - accuracy: 0.8834 - val_loss: 0.3853 - val_accuracy: 0.8849\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3746 - accuracy: 0.8836 - val_loss: 0.3941 - val_accuracy: 0.8897\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3759 - accuracy: 0.8833 - val_loss: 0.4003 - val_accuracy: 0.8785\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3709 - accuracy: 0.8858 - val_loss: 0.4100 - val_accuracy: 0.8767\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3686 - accuracy: 0.8849 - val_loss: 0.3153 - val_accuracy: 0.9076\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3683 - accuracy: 0.8871 - val_loss: 0.3960 - val_accuracy: 0.8829\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3729 - accuracy: 0.8835 - val_loss: 0.4310 - val_accuracy: 0.8719\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3655 - accuracy: 0.8866 - val_loss: 0.4005 - val_accuracy: 0.8837\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3636 - accuracy: 0.8870 - val_loss: 0.3363 - val_accuracy: 0.9049\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3597 - accuracy: 0.8892 - val_loss: 0.3500 - val_accuracy: 0.8976\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3604 - accuracy: 0.8880 - val_loss: 0.3262 - val_accuracy: 0.9074\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3555 - accuracy: 0.8897 - val_loss: 0.3653 - val_accuracy: 0.8941\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3548 - accuracy: 0.8908 - val_loss: 0.3335 - val_accuracy: 0.9064\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3522 - accuracy: 0.8899 - val_loss: 0.4490 - val_accuracy: 0.8703\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3527 - accuracy: 0.8894 - val_loss: 0.5590 - val_accuracy: 0.8412\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3589 - accuracy: 0.8878 - val_loss: 0.3595 - val_accuracy: 0.8972\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3539 - accuracy: 0.8910 - val_loss: 0.4716 - val_accuracy: 0.8654\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3516 - accuracy: 0.8897 - val_loss: 0.3804 - val_accuracy: 0.8861\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3477 - accuracy: 0.8916 - val_loss: 0.4535 - val_accuracy: 0.8667\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3512 - accuracy: 0.8906 - val_loss: 0.3657 - val_accuracy: 0.8957\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3437 - accuracy: 0.8932 - val_loss: 0.3400 - val_accuracy: 0.9022\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3488 - accuracy: 0.8913 - val_loss: 0.3419 - val_accuracy: 0.8999\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3486 - accuracy: 0.8914 - val_loss: 0.3560 - val_accuracy: 0.8974\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3408 - accuracy: 0.8946 - val_loss: 0.3629 - val_accuracy: 0.8974\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3389 - accuracy: 0.8959 - val_loss: 0.3666 - val_accuracy: 0.8949\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3394 - accuracy: 0.8951 - val_loss: 0.3242 - val_accuracy: 0.9062\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3436 - accuracy: 0.8934 - val_loss: 0.2866 - val_accuracy: 0.9200\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3346 - accuracy: 0.8959 - val_loss: 0.3141 - val_accuracy: 0.9114\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3410 - accuracy: 0.8947 - val_loss: 0.3002 - val_accuracy: 0.9146\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3357 - accuracy: 0.8939 - val_loss: 0.4073 - val_accuracy: 0.8795\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3341 - accuracy: 0.8967 - val_loss: 0.3183 - val_accuracy: 0.9071\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3328 - accuracy: 0.8975 - val_loss: 0.3680 - val_accuracy: 0.8904\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3329 - accuracy: 0.8960 - val_loss: 0.3679 - val_accuracy: 0.8884\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3330 - accuracy: 0.8981 - val_loss: 0.3990 - val_accuracy: 0.8836\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3316 - accuracy: 0.8966 - val_loss: 0.4245 - val_accuracy: 0.8756\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3280 - accuracy: 0.8970 - val_loss: 0.5196 - val_accuracy: 0.8414\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3295 - accuracy: 0.8971 - val_loss: 0.4491 - val_accuracy: 0.8707\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3267 - accuracy: 0.8992 - val_loss: 0.4308 - val_accuracy: 0.8762\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3248 - accuracy: 0.8996 - val_loss: 0.3612 - val_accuracy: 0.8969\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3221 - accuracy: 0.8985 - val_loss: 0.3985 - val_accuracy: 0.8885\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3255 - accuracy: 0.8983 - val_loss: 0.3187 - val_accuracy: 0.9104\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3203 - accuracy: 0.9010 - val_loss: 0.3823 - val_accuracy: 0.8848\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3212 - accuracy: 0.8993 - val_loss: 0.3585 - val_accuracy: 0.8962\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3189 - accuracy: 0.9017 - val_loss: 0.3245 - val_accuracy: 0.9044\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3206 - accuracy: 0.9004 - val_loss: 0.3143 - val_accuracy: 0.9089\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3128 - accuracy: 0.9015 - val_loss: 0.4083 - val_accuracy: 0.8770\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3135 - accuracy: 0.9026 - val_loss: 0.3846 - val_accuracy: 0.8870\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3102 - accuracy: 0.9025 - val_loss: 0.3581 - val_accuracy: 0.8984\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3182 - accuracy: 0.9010 - val_loss: 0.4299 - val_accuracy: 0.8671\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3132 - accuracy: 0.9037 - val_loss: 0.2964 - val_accuracy: 0.9162\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3141 - accuracy: 0.9028 - val_loss: 0.3224 - val_accuracy: 0.9096\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3082 - accuracy: 0.9037 - val_loss: 0.3563 - val_accuracy: 0.8966\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3101 - accuracy: 0.9030 - val_loss: 0.2943 - val_accuracy: 0.9161\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3061 - accuracy: 0.9066 - val_loss: 0.3093 - val_accuracy: 0.9109\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3138 - accuracy: 0.9014 - val_loss: 0.3377 - val_accuracy: 0.9049\n",
      "Try 2/100: Best_val_acc: [0.5416287779808044, 0.8478333353996277], lr: 6.10666479211338e-05, Lambda: 5.875290841194539e-05\n",
      "\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_378 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_379 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_380 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_381 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_382 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_383 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6177 - accuracy: 0.1123 - val_loss: 2.3861 - val_accuracy: 0.0456\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4396 - accuracy: 0.1362 - val_loss: 2.3604 - val_accuracy: 0.0745\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3131 - accuracy: 0.1651 - val_loss: 2.2123 - val_accuracy: 0.1626\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1927 - accuracy: 0.2065 - val_loss: 2.1630 - val_accuracy: 0.2011\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0804 - accuracy: 0.2586 - val_loss: 2.0486 - val_accuracy: 0.2660\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9741 - accuracy: 0.3075 - val_loss: 1.8667 - val_accuracy: 0.3362\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8718 - accuracy: 0.3612 - val_loss: 1.8119 - val_accuracy: 0.3713\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7878 - accuracy: 0.4040 - val_loss: 1.7047 - val_accuracy: 0.4192\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7125 - accuracy: 0.4383 - val_loss: 1.7200 - val_accuracy: 0.4259\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6442 - accuracy: 0.4718 - val_loss: 1.5341 - val_accuracy: 0.5333\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5730 - accuracy: 0.5052 - val_loss: 1.4012 - val_accuracy: 0.5906\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5122 - accuracy: 0.5298 - val_loss: 1.4151 - val_accuracy: 0.5794\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4592 - accuracy: 0.5545 - val_loss: 1.3138 - val_accuracy: 0.6291\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4076 - accuracy: 0.5730 - val_loss: 1.2507 - val_accuracy: 0.6460\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3604 - accuracy: 0.5890 - val_loss: 1.2248 - val_accuracy: 0.6604\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3163 - accuracy: 0.6088 - val_loss: 1.1833 - val_accuracy: 0.6519\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2708 - accuracy: 0.6211 - val_loss: 1.2019 - val_accuracy: 0.6523\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2343 - accuracy: 0.6331 - val_loss: 1.1274 - val_accuracy: 0.6707\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1992 - accuracy: 0.6440 - val_loss: 1.0153 - val_accuracy: 0.7169\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1650 - accuracy: 0.6543 - val_loss: 1.0100 - val_accuracy: 0.7171\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1372 - accuracy: 0.6635 - val_loss: 1.0014 - val_accuracy: 0.7193\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1060 - accuracy: 0.6710 - val_loss: 1.0147 - val_accuracy: 0.6895\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0819 - accuracy: 0.6787 - val_loss: 0.9392 - val_accuracy: 0.7370\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0531 - accuracy: 0.6850 - val_loss: 0.8860 - val_accuracy: 0.7441\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0281 - accuracy: 0.6938 - val_loss: 0.9382 - val_accuracy: 0.7188\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0046 - accuracy: 0.7000 - val_loss: 0.8813 - val_accuracy: 0.7402\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.9845 - accuracy: 0.7056 - val_loss: 0.8150 - val_accuracy: 0.7683\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9621 - accuracy: 0.7119 - val_loss: 0.8044 - val_accuracy: 0.7631\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9406 - accuracy: 0.7196 - val_loss: 0.7819 - val_accuracy: 0.7726\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9228 - accuracy: 0.7236 - val_loss: 0.7925 - val_accuracy: 0.7668\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9046 - accuracy: 0.7281 - val_loss: 0.8090 - val_accuracy: 0.7591\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8872 - accuracy: 0.7342 - val_loss: 0.8006 - val_accuracy: 0.7664\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8710 - accuracy: 0.7379 - val_loss: 0.8413 - val_accuracy: 0.7469\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8493 - accuracy: 0.7450 - val_loss: 0.9749 - val_accuracy: 0.7029\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8384 - accuracy: 0.7469 - val_loss: 0.7300 - val_accuracy: 0.7848\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8256 - accuracy: 0.7503 - val_loss: 0.7396 - val_accuracy: 0.7809\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8090 - accuracy: 0.7536 - val_loss: 0.8143 - val_accuracy: 0.7531\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8000 - accuracy: 0.7550 - val_loss: 0.6694 - val_accuracy: 0.8024\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7873 - accuracy: 0.7597 - val_loss: 0.6447 - val_accuracy: 0.8119\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7753 - accuracy: 0.7648 - val_loss: 0.7218 - val_accuracy: 0.7797\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7630 - accuracy: 0.7676 - val_loss: 0.6096 - val_accuracy: 0.8187\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7529 - accuracy: 0.7711 - val_loss: 0.6579 - val_accuracy: 0.8004\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7444 - accuracy: 0.7723 - val_loss: 0.6125 - val_accuracy: 0.8081\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7295 - accuracy: 0.7771 - val_loss: 0.5855 - val_accuracy: 0.8265\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7199 - accuracy: 0.7807 - val_loss: 0.6708 - val_accuracy: 0.8007\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7116 - accuracy: 0.7830 - val_loss: 0.6001 - val_accuracy: 0.8252\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7032 - accuracy: 0.7849 - val_loss: 0.5754 - val_accuracy: 0.8309\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6949 - accuracy: 0.7875 - val_loss: 0.6375 - val_accuracy: 0.8059\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6856 - accuracy: 0.7902 - val_loss: 0.8075 - val_accuracy: 0.7505\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6782 - accuracy: 0.7924 - val_loss: 0.6687 - val_accuracy: 0.7976\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6750 - accuracy: 0.7927 - val_loss: 0.5907 - val_accuracy: 0.8265\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6635 - accuracy: 0.7949 - val_loss: 0.5467 - val_accuracy: 0.8358\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6588 - accuracy: 0.7984 - val_loss: 0.6628 - val_accuracy: 0.7999\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6510 - accuracy: 0.8006 - val_loss: 0.5783 - val_accuracy: 0.8243\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6395 - accuracy: 0.8039 - val_loss: 0.6444 - val_accuracy: 0.8112\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6398 - accuracy: 0.8044 - val_loss: 0.5698 - val_accuracy: 0.8234\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6348 - accuracy: 0.8030 - val_loss: 0.5585 - val_accuracy: 0.8357\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6258 - accuracy: 0.8083 - val_loss: 0.5721 - val_accuracy: 0.8298\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6185 - accuracy: 0.8083 - val_loss: 0.5117 - val_accuracy: 0.8483\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6105 - accuracy: 0.8121 - val_loss: 0.5452 - val_accuracy: 0.8384\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.8131 - val_loss: 0.5120 - val_accuracy: 0.8463\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5994 - accuracy: 0.8158 - val_loss: 0.5986 - val_accuracy: 0.8147\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5945 - accuracy: 0.8158 - val_loss: 0.6307 - val_accuracy: 0.8109\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5898 - accuracy: 0.8197 - val_loss: 0.5086 - val_accuracy: 0.8446\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5841 - accuracy: 0.8206 - val_loss: 0.5423 - val_accuracy: 0.8380\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5779 - accuracy: 0.8221 - val_loss: 0.4668 - val_accuracy: 0.8602\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5768 - accuracy: 0.8210 - val_loss: 0.4602 - val_accuracy: 0.8637\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5699 - accuracy: 0.8252 - val_loss: 0.4365 - val_accuracy: 0.8742\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5675 - accuracy: 0.8235 - val_loss: 0.5343 - val_accuracy: 0.8394\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5619 - accuracy: 0.8272 - val_loss: 0.5563 - val_accuracy: 0.8326\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5609 - accuracy: 0.8272 - val_loss: 0.6563 - val_accuracy: 0.8058\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5527 - accuracy: 0.8284 - val_loss: 0.5485 - val_accuracy: 0.8351\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5507 - accuracy: 0.8317 - val_loss: 0.4420 - val_accuracy: 0.8649\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5438 - accuracy: 0.8318 - val_loss: 0.4555 - val_accuracy: 0.8569\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5415 - accuracy: 0.8314 - val_loss: 0.6093 - val_accuracy: 0.8160\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5405 - accuracy: 0.8325 - val_loss: 0.4962 - val_accuracy: 0.8522\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5322 - accuracy: 0.8359 - val_loss: 0.6156 - val_accuracy: 0.8156\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5329 - accuracy: 0.8339 - val_loss: 0.4620 - val_accuracy: 0.8654\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5271 - accuracy: 0.8360 - val_loss: 0.5266 - val_accuracy: 0.8408\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5227 - accuracy: 0.8410 - val_loss: 0.4774 - val_accuracy: 0.8491\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5186 - accuracy: 0.8394 - val_loss: 0.4522 - val_accuracy: 0.8657\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5161 - accuracy: 0.8403 - val_loss: 0.5247 - val_accuracy: 0.8439\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5091 - accuracy: 0.8419 - val_loss: 0.6537 - val_accuracy: 0.7988\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5089 - accuracy: 0.8431 - val_loss: 0.4661 - val_accuracy: 0.8621\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5070 - accuracy: 0.8432 - val_loss: 0.5680 - val_accuracy: 0.8322\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5050 - accuracy: 0.8433 - val_loss: 0.4188 - val_accuracy: 0.8765\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4938 - accuracy: 0.8485 - val_loss: 0.4207 - val_accuracy: 0.8737\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4951 - accuracy: 0.8483 - val_loss: 0.4856 - val_accuracy: 0.8561\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4923 - accuracy: 0.8485 - val_loss: 0.6131 - val_accuracy: 0.8139\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4901 - accuracy: 0.8494 - val_loss: 0.3721 - val_accuracy: 0.8919\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.8495 - val_loss: 0.4116 - val_accuracy: 0.8802\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4885 - accuracy: 0.8481 - val_loss: 0.5545 - val_accuracy: 0.8285\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4831 - accuracy: 0.8497 - val_loss: 0.4408 - val_accuracy: 0.8682\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4861 - accuracy: 0.8502 - val_loss: 0.4204 - val_accuracy: 0.8777\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4752 - accuracy: 0.8511 - val_loss: 0.4428 - val_accuracy: 0.8665\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4765 - accuracy: 0.8530 - val_loss: 0.5518 - val_accuracy: 0.8371\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4716 - accuracy: 0.8549 - val_loss: 0.5311 - val_accuracy: 0.8431\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4639 - accuracy: 0.8569 - val_loss: 0.5889 - val_accuracy: 0.8238\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4636 - accuracy: 0.8560 - val_loss: 0.4337 - val_accuracy: 0.8720\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4616 - accuracy: 0.8567 - val_loss: 0.4685 - val_accuracy: 0.8573\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4639 - accuracy: 0.8557 - val_loss: 0.4415 - val_accuracy: 0.8681\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4522 - accuracy: 0.8607 - val_loss: 0.6211 - val_accuracy: 0.8156\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4498 - accuracy: 0.8603 - val_loss: 0.4266 - val_accuracy: 0.8779\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4537 - accuracy: 0.8585 - val_loss: 0.4616 - val_accuracy: 0.8598\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4493 - accuracy: 0.8609 - val_loss: 0.5232 - val_accuracy: 0.8463\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4474 - accuracy: 0.8623 - val_loss: 0.4391 - val_accuracy: 0.8702\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4423 - accuracy: 0.8615 - val_loss: 0.4294 - val_accuracy: 0.8767\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4478 - accuracy: 0.8614 - val_loss: 0.4415 - val_accuracy: 0.8699\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4417 - accuracy: 0.8624 - val_loss: 0.4376 - val_accuracy: 0.8711\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4340 - accuracy: 0.8656 - val_loss: 0.6086 - val_accuracy: 0.8176\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4380 - accuracy: 0.8638 - val_loss: 0.4664 - val_accuracy: 0.8646\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4385 - accuracy: 0.8641 - val_loss: 0.4363 - val_accuracy: 0.8696\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4307 - accuracy: 0.8678 - val_loss: 0.5106 - val_accuracy: 0.8448\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4266 - accuracy: 0.8674 - val_loss: 0.4702 - val_accuracy: 0.8596\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4275 - accuracy: 0.8669 - val_loss: 0.4490 - val_accuracy: 0.8701\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4248 - accuracy: 0.8683 - val_loss: 0.3542 - val_accuracy: 0.8977\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4186 - accuracy: 0.8704 - val_loss: 0.4055 - val_accuracy: 0.8810\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4201 - accuracy: 0.8705 - val_loss: 0.3480 - val_accuracy: 0.8986\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4150 - accuracy: 0.8725 - val_loss: 0.4088 - val_accuracy: 0.8797\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4200 - accuracy: 0.8691 - val_loss: 0.3519 - val_accuracy: 0.8990\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8712 - val_loss: 0.4288 - val_accuracy: 0.8714\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4164 - accuracy: 0.8707 - val_loss: 0.3608 - val_accuracy: 0.8941\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4101 - accuracy: 0.8732 - val_loss: 0.3626 - val_accuracy: 0.8946\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4119 - accuracy: 0.8716 - val_loss: 0.4075 - val_accuracy: 0.8810\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4084 - accuracy: 0.8719 - val_loss: 0.5298 - val_accuracy: 0.8436\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8751 - val_loss: 0.3361 - val_accuracy: 0.9064\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4058 - accuracy: 0.8739 - val_loss: 0.3938 - val_accuracy: 0.8837\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4031 - accuracy: 0.8758 - val_loss: 0.4019 - val_accuracy: 0.8815\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3996 - accuracy: 0.8757 - val_loss: 0.3729 - val_accuracy: 0.8905\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4014 - accuracy: 0.8752 - val_loss: 0.4656 - val_accuracy: 0.8673\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3992 - accuracy: 0.8760 - val_loss: 0.4298 - val_accuracy: 0.8739\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3986 - accuracy: 0.8782 - val_loss: 0.3391 - val_accuracy: 0.9037\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3949 - accuracy: 0.8769 - val_loss: 0.3472 - val_accuracy: 0.8939\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3898 - accuracy: 0.8800 - val_loss: 0.4024 - val_accuracy: 0.8836\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3961 - accuracy: 0.8778 - val_loss: 0.3679 - val_accuracy: 0.8944\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3900 - accuracy: 0.8785 - val_loss: 0.4740 - val_accuracy: 0.8587\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3857 - accuracy: 0.8808 - val_loss: 0.6187 - val_accuracy: 0.8149\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3897 - accuracy: 0.8778 - val_loss: 0.5227 - val_accuracy: 0.8459\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3818 - accuracy: 0.8822 - val_loss: 0.3481 - val_accuracy: 0.8988\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3809 - accuracy: 0.8819 - val_loss: 0.3859 - val_accuracy: 0.8878\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3759 - accuracy: 0.8844 - val_loss: 0.3981 - val_accuracy: 0.8839\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3804 - accuracy: 0.8822 - val_loss: 0.3421 - val_accuracy: 0.8972\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3751 - accuracy: 0.8828 - val_loss: 0.3601 - val_accuracy: 0.8960\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3769 - accuracy: 0.8817 - val_loss: 0.4984 - val_accuracy: 0.8547\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3717 - accuracy: 0.8850 - val_loss: 0.3807 - val_accuracy: 0.8906\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3717 - accuracy: 0.8851 - val_loss: 0.3177 - val_accuracy: 0.9083\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3711 - accuracy: 0.8847 - val_loss: 0.3569 - val_accuracy: 0.8977\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3683 - accuracy: 0.8847 - val_loss: 0.5200 - val_accuracy: 0.8475\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3732 - accuracy: 0.8827 - val_loss: 0.3541 - val_accuracy: 0.8985\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3671 - accuracy: 0.8864 - val_loss: 0.2783 - val_accuracy: 0.9216\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3677 - accuracy: 0.8840 - val_loss: 0.3767 - val_accuracy: 0.8904\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3677 - accuracy: 0.8862 - val_loss: 0.3532 - val_accuracy: 0.8984\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3672 - accuracy: 0.8852 - val_loss: 0.3369 - val_accuracy: 0.9041\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3581 - accuracy: 0.8878 - val_loss: 0.3926 - val_accuracy: 0.8866\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3617 - accuracy: 0.8882 - val_loss: 0.3753 - val_accuracy: 0.8925\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3566 - accuracy: 0.8893 - val_loss: 0.4057 - val_accuracy: 0.8793\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3637 - accuracy: 0.8865 - val_loss: 0.3675 - val_accuracy: 0.8933\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3545 - accuracy: 0.8898 - val_loss: 0.3159 - val_accuracy: 0.9114\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3503 - accuracy: 0.8915 - val_loss: 0.4058 - val_accuracy: 0.8765\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3487 - accuracy: 0.8910 - val_loss: 0.3476 - val_accuracy: 0.8998\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3534 - accuracy: 0.8903 - val_loss: 0.4409 - val_accuracy: 0.8657\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3539 - accuracy: 0.8906 - val_loss: 0.3824 - val_accuracy: 0.8877\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3502 - accuracy: 0.8909 - val_loss: 0.3688 - val_accuracy: 0.8914\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3470 - accuracy: 0.8924 - val_loss: 0.3029 - val_accuracy: 0.9152\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3464 - accuracy: 0.8933 - val_loss: 0.3381 - val_accuracy: 0.9024\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3393 - accuracy: 0.8944 - val_loss: 0.3987 - val_accuracy: 0.8867\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3492 - accuracy: 0.8912 - val_loss: 0.3670 - val_accuracy: 0.8949\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3420 - accuracy: 0.8930 - val_loss: 0.3838 - val_accuracy: 0.8891\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3421 - accuracy: 0.8947 - val_loss: 0.3210 - val_accuracy: 0.9065\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3389 - accuracy: 0.8934 - val_loss: 0.2575 - val_accuracy: 0.9276\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3404 - accuracy: 0.8952 - val_loss: 0.5143 - val_accuracy: 0.8539\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3384 - accuracy: 0.8959 - val_loss: 0.4381 - val_accuracy: 0.8680\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3367 - accuracy: 0.8951 - val_loss: 0.3803 - val_accuracy: 0.8923\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3352 - accuracy: 0.8958 - val_loss: 0.3215 - val_accuracy: 0.9094\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3363 - accuracy: 0.8945 - val_loss: 0.3336 - val_accuracy: 0.9048\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3326 - accuracy: 0.8966 - val_loss: 0.3732 - val_accuracy: 0.8879\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3289 - accuracy: 0.8982 - val_loss: 0.2848 - val_accuracy: 0.9204\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3318 - accuracy: 0.8966 - val_loss: 0.4337 - val_accuracy: 0.8736\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3347 - accuracy: 0.8968 - val_loss: 0.3474 - val_accuracy: 0.9030\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3264 - accuracy: 0.8984 - val_loss: 0.3158 - val_accuracy: 0.9086\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3270 - accuracy: 0.8979 - val_loss: 0.3357 - val_accuracy: 0.9017\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3263 - accuracy: 0.8994 - val_loss: 0.3465 - val_accuracy: 0.8987\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3226 - accuracy: 0.9007 - val_loss: 0.2931 - val_accuracy: 0.9144\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3237 - accuracy: 0.8989 - val_loss: 0.3815 - val_accuracy: 0.8891\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3181 - accuracy: 0.9024 - val_loss: 0.2973 - val_accuracy: 0.9179\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3235 - accuracy: 0.8997 - val_loss: 0.3242 - val_accuracy: 0.9079\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3194 - accuracy: 0.9002 - val_loss: 0.3239 - val_accuracy: 0.9072\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3161 - accuracy: 0.9009 - val_loss: 0.2583 - val_accuracy: 0.9298\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3169 - accuracy: 0.9022 - val_loss: 0.3854 - val_accuracy: 0.8885\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3179 - accuracy: 0.9014 - val_loss: 0.2903 - val_accuracy: 0.9199\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3144 - accuracy: 0.9013 - val_loss: 0.3997 - val_accuracy: 0.8814\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3121 - accuracy: 0.9011 - val_loss: 0.2942 - val_accuracy: 0.9181\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3128 - accuracy: 0.9030 - val_loss: 0.6016 - val_accuracy: 0.8272\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3168 - accuracy: 0.9010 - val_loss: 0.3211 - val_accuracy: 0.9084\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3130 - accuracy: 0.9026 - val_loss: 0.3811 - val_accuracy: 0.8906\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3114 - accuracy: 0.9026 - val_loss: 0.4210 - val_accuracy: 0.8742\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3074 - accuracy: 0.9047 - val_loss: 0.4075 - val_accuracy: 0.8836\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3099 - accuracy: 0.9027 - val_loss: 0.3492 - val_accuracy: 0.8970\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3083 - accuracy: 0.9033 - val_loss: 0.3396 - val_accuracy: 0.9035\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3109 - accuracy: 0.9018 - val_loss: 0.3750 - val_accuracy: 0.8932\n",
      "Try 3/100: Best_val_acc: [0.48880690336227417, 0.859333336353302], lr: 6.025930951054962e-05, Lambda: 5.79418786098933e-05\n",
      "\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_384 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_385 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_386 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_387 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_388 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_389 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 2.6028 - accuracy: 0.1188 - val_loss: 2.1505 - val_accuracy: 0.1202\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4322 - accuracy: 0.1491 - val_loss: 2.1205 - val_accuracy: 0.1650\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2923 - accuracy: 0.1848 - val_loss: 2.0094 - val_accuracy: 0.2741\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1584 - accuracy: 0.2330 - val_loss: 1.9131 - val_accuracy: 0.3172\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0289 - accuracy: 0.2859 - val_loss: 1.8302 - val_accuracy: 0.3648\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9236 - accuracy: 0.3348 - val_loss: 1.7438 - val_accuracy: 0.4021\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8259 - accuracy: 0.3839 - val_loss: 1.7217 - val_accuracy: 0.4150\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.7356 - accuracy: 0.4256 - val_loss: 1.6070 - val_accuracy: 0.4942\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6554 - accuracy: 0.4625 - val_loss: 1.4879 - val_accuracy: 0.5366\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5849 - accuracy: 0.4935 - val_loss: 1.4113 - val_accuracy: 0.5957\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5109 - accuracy: 0.5251 - val_loss: 1.2892 - val_accuracy: 0.6446\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4510 - accuracy: 0.5482 - val_loss: 1.2728 - val_accuracy: 0.6354\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3949 - accuracy: 0.5722 - val_loss: 1.3477 - val_accuracy: 0.6059\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3450 - accuracy: 0.5862 - val_loss: 1.1655 - val_accuracy: 0.6848\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2974 - accuracy: 0.6056 - val_loss: 1.1615 - val_accuracy: 0.6771\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2534 - accuracy: 0.6209 - val_loss: 1.1754 - val_accuracy: 0.6591\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2141 - accuracy: 0.6337 - val_loss: 1.0810 - val_accuracy: 0.7090\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1775 - accuracy: 0.6456 - val_loss: 1.0468 - val_accuracy: 0.7215\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.1444 - accuracy: 0.6560 - val_loss: 1.0133 - val_accuracy: 0.7165\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1144 - accuracy: 0.6650 - val_loss: 0.9815 - val_accuracy: 0.7271\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0772 - accuracy: 0.6773 - val_loss: 0.9261 - val_accuracy: 0.7493\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0525 - accuracy: 0.6831 - val_loss: 0.9512 - val_accuracy: 0.7344\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0287 - accuracy: 0.6915 - val_loss: 0.9334 - val_accuracy: 0.7225\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0021 - accuracy: 0.6975 - val_loss: 0.9049 - val_accuracy: 0.7374\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9780 - accuracy: 0.7064 - val_loss: 0.8227 - val_accuracy: 0.7708\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9578 - accuracy: 0.7114 - val_loss: 0.7843 - val_accuracy: 0.7749\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9392 - accuracy: 0.7157 - val_loss: 0.9079 - val_accuracy: 0.7286\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9226 - accuracy: 0.7223 - val_loss: 0.9092 - val_accuracy: 0.7297\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8961 - accuracy: 0.7282 - val_loss: 0.7749 - val_accuracy: 0.7746\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8906 - accuracy: 0.7300 - val_loss: 0.6860 - val_accuracy: 0.8070\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8608 - accuracy: 0.7384 - val_loss: 0.7506 - val_accuracy: 0.7773\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8553 - accuracy: 0.7387 - val_loss: 0.8132 - val_accuracy: 0.7607\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8377 - accuracy: 0.7456 - val_loss: 0.6963 - val_accuracy: 0.7941\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8191 - accuracy: 0.7514 - val_loss: 0.8288 - val_accuracy: 0.7526\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8085 - accuracy: 0.7541 - val_loss: 0.7416 - val_accuracy: 0.7841\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7935 - accuracy: 0.7585 - val_loss: 0.6460 - val_accuracy: 0.8112\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7828 - accuracy: 0.7614 - val_loss: 0.6611 - val_accuracy: 0.8049\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7695 - accuracy: 0.7657 - val_loss: 0.6588 - val_accuracy: 0.8032\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7617 - accuracy: 0.7674 - val_loss: 0.8757 - val_accuracy: 0.7321\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7517 - accuracy: 0.7700 - val_loss: 0.6846 - val_accuracy: 0.7983\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7420 - accuracy: 0.7726 - val_loss: 0.7344 - val_accuracy: 0.7770\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7319 - accuracy: 0.7756 - val_loss: 0.6326 - val_accuracy: 0.8109\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7262 - accuracy: 0.7764 - val_loss: 0.5546 - val_accuracy: 0.8349\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7109 - accuracy: 0.7822 - val_loss: 0.6637 - val_accuracy: 0.8001\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7095 - accuracy: 0.7823 - val_loss: 0.6358 - val_accuracy: 0.8062\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7001 - accuracy: 0.7839 - val_loss: 0.5604 - val_accuracy: 0.8294\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6903 - accuracy: 0.7874 - val_loss: 0.6893 - val_accuracy: 0.7905\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6791 - accuracy: 0.7904 - val_loss: 0.5600 - val_accuracy: 0.8296\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6717 - accuracy: 0.7916 - val_loss: 0.5919 - val_accuracy: 0.8213\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6690 - accuracy: 0.7934 - val_loss: 0.6236 - val_accuracy: 0.8093\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6501 - accuracy: 0.7995 - val_loss: 0.5557 - val_accuracy: 0.8330\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6517 - accuracy: 0.7988 - val_loss: 0.5128 - val_accuracy: 0.8466\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6459 - accuracy: 0.8018 - val_loss: 0.5840 - val_accuracy: 0.8233\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6389 - accuracy: 0.8032 - val_loss: 0.5328 - val_accuracy: 0.8434\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6304 - accuracy: 0.8041 - val_loss: 0.7124 - val_accuracy: 0.7796\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6255 - accuracy: 0.8083 - val_loss: 0.5468 - val_accuracy: 0.8321\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6228 - accuracy: 0.8097 - val_loss: 0.6560 - val_accuracy: 0.7986\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6133 - accuracy: 0.8110 - val_loss: 0.4802 - val_accuracy: 0.8543\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6073 - accuracy: 0.8132 - val_loss: 0.6503 - val_accuracy: 0.7994\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6051 - accuracy: 0.8120 - val_loss: 0.5264 - val_accuracy: 0.8419\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6001 - accuracy: 0.8148 - val_loss: 0.5109 - val_accuracy: 0.8431\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5947 - accuracy: 0.8162 - val_loss: 0.4651 - val_accuracy: 0.8606\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5824 - accuracy: 0.8214 - val_loss: 0.5120 - val_accuracy: 0.8482\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5832 - accuracy: 0.8212 - val_loss: 0.5491 - val_accuracy: 0.8276\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5721 - accuracy: 0.8232 - val_loss: 0.5457 - val_accuracy: 0.8356\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5705 - accuracy: 0.8233 - val_loss: 0.4849 - val_accuracy: 0.8572\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5723 - accuracy: 0.8232 - val_loss: 0.4685 - val_accuracy: 0.8605\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5654 - accuracy: 0.8249 - val_loss: 0.4984 - val_accuracy: 0.8495\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5537 - accuracy: 0.8297 - val_loss: 0.5731 - val_accuracy: 0.8242\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5512 - accuracy: 0.8299 - val_loss: 0.5270 - val_accuracy: 0.8418\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5465 - accuracy: 0.8307 - val_loss: 0.4706 - val_accuracy: 0.8602\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5447 - accuracy: 0.8315 - val_loss: 0.6217 - val_accuracy: 0.8066\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5442 - accuracy: 0.8302 - val_loss: 0.5555 - val_accuracy: 0.8292\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5386 - accuracy: 0.8348 - val_loss: 0.5573 - val_accuracy: 0.8276\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5355 - accuracy: 0.8345 - val_loss: 0.4935 - val_accuracy: 0.8519\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5331 - accuracy: 0.8352 - val_loss: 0.4474 - val_accuracy: 0.8645\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5306 - accuracy: 0.8367 - val_loss: 0.5075 - val_accuracy: 0.8461\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5275 - accuracy: 0.8351 - val_loss: 0.4543 - val_accuracy: 0.8634\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5246 - accuracy: 0.8378 - val_loss: 0.5248 - val_accuracy: 0.8459\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5123 - accuracy: 0.8415 - val_loss: 0.4879 - val_accuracy: 0.8524\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5123 - accuracy: 0.8407 - val_loss: 0.4893 - val_accuracy: 0.8543\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5130 - accuracy: 0.8420 - val_loss: 0.4972 - val_accuracy: 0.8501\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5037 - accuracy: 0.8447 - val_loss: 0.4862 - val_accuracy: 0.8539\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5021 - accuracy: 0.8444 - val_loss: 0.4025 - val_accuracy: 0.8825\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5019 - accuracy: 0.8462 - val_loss: 0.5593 - val_accuracy: 0.8292\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4937 - accuracy: 0.8448 - val_loss: 0.4921 - val_accuracy: 0.8531\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4911 - accuracy: 0.8475 - val_loss: 0.4671 - val_accuracy: 0.8580\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4945 - accuracy: 0.8471 - val_loss: 0.4839 - val_accuracy: 0.8572\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4855 - accuracy: 0.8493 - val_loss: 0.5374 - val_accuracy: 0.8380\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4836 - accuracy: 0.8484 - val_loss: 0.4646 - val_accuracy: 0.8609\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4795 - accuracy: 0.8515 - val_loss: 0.4161 - val_accuracy: 0.8750\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4808 - accuracy: 0.8502 - val_loss: 0.3984 - val_accuracy: 0.8836\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4777 - accuracy: 0.8529 - val_loss: 0.4706 - val_accuracy: 0.8570\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4779 - accuracy: 0.8520 - val_loss: 0.4158 - val_accuracy: 0.8775\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4757 - accuracy: 0.8521 - val_loss: 0.5094 - val_accuracy: 0.8464\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4697 - accuracy: 0.8555 - val_loss: 0.4584 - val_accuracy: 0.8589\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4656 - accuracy: 0.8556 - val_loss: 0.4244 - val_accuracy: 0.8751\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4584 - accuracy: 0.8584 - val_loss: 0.4688 - val_accuracy: 0.8538\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4575 - accuracy: 0.8579 - val_loss: 0.4962 - val_accuracy: 0.8529\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4594 - accuracy: 0.8570 - val_loss: 0.4087 - val_accuracy: 0.8824\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4570 - accuracy: 0.8575 - val_loss: 0.4904 - val_accuracy: 0.8522\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4485 - accuracy: 0.8596 - val_loss: 0.5180 - val_accuracy: 0.8461\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4494 - accuracy: 0.8616 - val_loss: 0.4934 - val_accuracy: 0.8519\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4453 - accuracy: 0.8619 - val_loss: 0.4583 - val_accuracy: 0.8625\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4518 - accuracy: 0.8588 - val_loss: 0.5241 - val_accuracy: 0.8401\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4477 - accuracy: 0.8609 - val_loss: 0.4409 - val_accuracy: 0.8672\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4457 - accuracy: 0.8619 - val_loss: 0.4544 - val_accuracy: 0.8669\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4389 - accuracy: 0.8630 - val_loss: 0.4738 - val_accuracy: 0.8585\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4330 - accuracy: 0.8650 - val_loss: 0.5382 - val_accuracy: 0.8380\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4394 - accuracy: 0.8638 - val_loss: 0.4525 - val_accuracy: 0.8642\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4299 - accuracy: 0.8671 - val_loss: 0.4988 - val_accuracy: 0.8493\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4295 - accuracy: 0.8685 - val_loss: 0.4511 - val_accuracy: 0.8681\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4290 - accuracy: 0.8668 - val_loss: 0.4666 - val_accuracy: 0.8604\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4288 - accuracy: 0.8672 - val_loss: 0.3625 - val_accuracy: 0.8924\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4255 - accuracy: 0.8682 - val_loss: 0.3541 - val_accuracy: 0.8964\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4256 - accuracy: 0.8685 - val_loss: 0.4524 - val_accuracy: 0.8663\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4229 - accuracy: 0.8697 - val_loss: 0.4098 - val_accuracy: 0.8804\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8701 - val_loss: 0.3909 - val_accuracy: 0.8858\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4212 - accuracy: 0.8694 - val_loss: 0.3901 - val_accuracy: 0.8832\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4131 - accuracy: 0.8729 - val_loss: 0.3496 - val_accuracy: 0.8989\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4123 - accuracy: 0.8731 - val_loss: 0.4107 - val_accuracy: 0.8808\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4121 - accuracy: 0.8729 - val_loss: 0.3357 - val_accuracy: 0.9020\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4069 - accuracy: 0.8725 - val_loss: 0.4416 - val_accuracy: 0.8619\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4090 - accuracy: 0.8735 - val_loss: 0.3852 - val_accuracy: 0.8883\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4054 - accuracy: 0.8719 - val_loss: 0.5002 - val_accuracy: 0.8554\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4008 - accuracy: 0.8766 - val_loss: 0.4943 - val_accuracy: 0.8515\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3997 - accuracy: 0.8764 - val_loss: 0.3523 - val_accuracy: 0.8987\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8762 - val_loss: 0.4750 - val_accuracy: 0.8552\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8751 - val_loss: 0.4842 - val_accuracy: 0.8491\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3956 - accuracy: 0.8769 - val_loss: 0.3442 - val_accuracy: 0.9018\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3951 - accuracy: 0.8770 - val_loss: 0.5432 - val_accuracy: 0.8355\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3909 - accuracy: 0.8776 - val_loss: 0.3860 - val_accuracy: 0.8831\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3922 - accuracy: 0.8784 - val_loss: 0.3300 - val_accuracy: 0.9061\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3907 - accuracy: 0.8786 - val_loss: 0.4621 - val_accuracy: 0.8622\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3784 - accuracy: 0.8848 - val_loss: 0.3806 - val_accuracy: 0.8912\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3822 - accuracy: 0.8798 - val_loss: 0.4081 - val_accuracy: 0.8788\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3837 - accuracy: 0.8791 - val_loss: 0.4690 - val_accuracy: 0.8615\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3792 - accuracy: 0.8819 - val_loss: 0.3842 - val_accuracy: 0.8856\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3844 - accuracy: 0.8787 - val_loss: 0.4372 - val_accuracy: 0.8701\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3826 - accuracy: 0.8802 - val_loss: 0.3165 - val_accuracy: 0.9092\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3744 - accuracy: 0.8831 - val_loss: 0.3559 - val_accuracy: 0.8969\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3716 - accuracy: 0.8835 - val_loss: 0.3689 - val_accuracy: 0.8941\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3689 - accuracy: 0.8862 - val_loss: 0.3711 - val_accuracy: 0.8911\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3690 - accuracy: 0.8840 - val_loss: 0.3306 - val_accuracy: 0.9056\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3723 - accuracy: 0.8847 - val_loss: 0.3496 - val_accuracy: 0.9001\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3710 - accuracy: 0.8853 - val_loss: 0.3552 - val_accuracy: 0.8987\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3683 - accuracy: 0.8848 - val_loss: 0.3923 - val_accuracy: 0.8816\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3677 - accuracy: 0.8855 - val_loss: 0.3526 - val_accuracy: 0.8979\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3661 - accuracy: 0.8844 - val_loss: 0.4325 - val_accuracy: 0.8701\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3659 - accuracy: 0.8855 - val_loss: 0.3849 - val_accuracy: 0.8881\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3652 - accuracy: 0.8868 - val_loss: 0.3496 - val_accuracy: 0.8967\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3569 - accuracy: 0.8882 - val_loss: 0.4440 - val_accuracy: 0.8673\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3559 - accuracy: 0.8882 - val_loss: 0.4184 - val_accuracy: 0.8776\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3600 - accuracy: 0.8886 - val_loss: 0.3952 - val_accuracy: 0.8815\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3566 - accuracy: 0.8901 - val_loss: 0.3788 - val_accuracy: 0.8908\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3509 - accuracy: 0.8921 - val_loss: 0.3668 - val_accuracy: 0.8906\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3581 - accuracy: 0.8880 - val_loss: 0.3293 - val_accuracy: 0.9029\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3542 - accuracy: 0.8887 - val_loss: 0.3151 - val_accuracy: 0.9102\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3497 - accuracy: 0.8900 - val_loss: 0.3347 - val_accuracy: 0.9045\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3506 - accuracy: 0.8914 - val_loss: 0.4038 - val_accuracy: 0.8819\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3441 - accuracy: 0.8931 - val_loss: 0.3445 - val_accuracy: 0.8984\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3476 - accuracy: 0.8909 - val_loss: 0.3591 - val_accuracy: 0.8944\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3427 - accuracy: 0.8937 - val_loss: 0.3900 - val_accuracy: 0.8812\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3394 - accuracy: 0.8938 - val_loss: 0.3162 - val_accuracy: 0.9082\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3407 - accuracy: 0.8939 - val_loss: 0.4128 - val_accuracy: 0.8788\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3425 - accuracy: 0.8932 - val_loss: 0.2826 - val_accuracy: 0.9180\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3381 - accuracy: 0.8950 - val_loss: 0.3634 - val_accuracy: 0.8889\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3393 - accuracy: 0.8936 - val_loss: 0.6648 - val_accuracy: 0.7995\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3385 - accuracy: 0.8943 - val_loss: 0.3719 - val_accuracy: 0.8864\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3336 - accuracy: 0.8950 - val_loss: 0.4387 - val_accuracy: 0.8706\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3393 - accuracy: 0.8947 - val_loss: 0.3402 - val_accuracy: 0.9034\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3343 - accuracy: 0.8970 - val_loss: 0.3397 - val_accuracy: 0.8990\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3331 - accuracy: 0.8954 - val_loss: 0.3115 - val_accuracy: 0.9117\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3340 - accuracy: 0.8961 - val_loss: 0.2827 - val_accuracy: 0.9197\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3336 - accuracy: 0.8974 - val_loss: 0.2775 - val_accuracy: 0.9213\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3366 - accuracy: 0.8941 - val_loss: 0.3160 - val_accuracy: 0.9089\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3311 - accuracy: 0.8959 - val_loss: 0.3081 - val_accuracy: 0.9100\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3306 - accuracy: 0.8963 - val_loss: 0.2812 - val_accuracy: 0.9209\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3262 - accuracy: 0.8975 - val_loss: 0.3699 - val_accuracy: 0.8891\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3271 - accuracy: 0.8982 - val_loss: 0.4296 - val_accuracy: 0.8738\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3218 - accuracy: 0.8999 - val_loss: 0.4536 - val_accuracy: 0.8659\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3287 - accuracy: 0.8962 - val_loss: 0.3617 - val_accuracy: 0.8954\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3250 - accuracy: 0.8999 - val_loss: 0.3677 - val_accuracy: 0.8904\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3205 - accuracy: 0.8995 - val_loss: 0.3846 - val_accuracy: 0.8879\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3231 - accuracy: 0.8995 - val_loss: 0.3643 - val_accuracy: 0.8928\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3182 - accuracy: 0.9009 - val_loss: 0.2978 - val_accuracy: 0.9134\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3171 - accuracy: 0.9012 - val_loss: 0.3366 - val_accuracy: 0.9024\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3174 - accuracy: 0.9007 - val_loss: 0.3653 - val_accuracy: 0.8920\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3148 - accuracy: 0.9022 - val_loss: 0.3309 - val_accuracy: 0.9047\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3139 - accuracy: 0.9027 - val_loss: 0.3561 - val_accuracy: 0.8898\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3155 - accuracy: 0.9023 - val_loss: 0.3001 - val_accuracy: 0.9148\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3117 - accuracy: 0.9035 - val_loss: 0.3106 - val_accuracy: 0.9092\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3108 - accuracy: 0.9037 - val_loss: 0.3203 - val_accuracy: 0.9042\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3099 - accuracy: 0.9040 - val_loss: 0.2951 - val_accuracy: 0.9164\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3066 - accuracy: 0.9028 - val_loss: 0.3634 - val_accuracy: 0.8946\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3106 - accuracy: 0.9034 - val_loss: 0.3099 - val_accuracy: 0.9113\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3059 - accuracy: 0.9045 - val_loss: 0.2616 - val_accuracy: 0.9271\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3079 - accuracy: 0.9046 - val_loss: 0.3071 - val_accuracy: 0.9118\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3042 - accuracy: 0.9040 - val_loss: 0.3012 - val_accuracy: 0.9154\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3004 - accuracy: 0.9061 - val_loss: 0.3879 - val_accuracy: 0.8858\n",
      "Try 4/100: Best_val_acc: [0.5171998143196106, 0.8496111035346985], lr: 6.160845479979014e-05, Lambda: 5.7650822923168565e-05\n",
      "\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_390 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_391 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_392 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_393 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_394 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_395 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6717 - accuracy: 0.1137 - val_loss: 2.2387 - val_accuracy: 0.2016\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4735 - accuracy: 0.1479 - val_loss: 2.2305 - val_accuracy: 0.1784\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3150 - accuracy: 0.1865 - val_loss: 2.1813 - val_accuracy: 0.1948\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1839 - accuracy: 0.2303 - val_loss: 2.0679 - val_accuracy: 0.2558\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0667 - accuracy: 0.2733 - val_loss: 2.0082 - val_accuracy: 0.2741\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9602 - accuracy: 0.3240 - val_loss: 1.9301 - val_accuracy: 0.3266\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8658 - accuracy: 0.3678 - val_loss: 1.8144 - val_accuracy: 0.4124\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7857 - accuracy: 0.4101 - val_loss: 1.7631 - val_accuracy: 0.4385\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6967 - accuracy: 0.4500 - val_loss: 1.6103 - val_accuracy: 0.5253\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6272 - accuracy: 0.4844 - val_loss: 1.5928 - val_accuracy: 0.5321\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5632 - accuracy: 0.5104 - val_loss: 1.4959 - val_accuracy: 0.5886\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4952 - accuracy: 0.5387 - val_loss: 1.3737 - val_accuracy: 0.6377\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4366 - accuracy: 0.5629 - val_loss: 1.3347 - val_accuracy: 0.6330\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3797 - accuracy: 0.5834 - val_loss: 1.2276 - val_accuracy: 0.6777\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3317 - accuracy: 0.6007 - val_loss: 1.1936 - val_accuracy: 0.6979\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2910 - accuracy: 0.6150 - val_loss: 1.1452 - val_accuracy: 0.6926\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2469 - accuracy: 0.6275 - val_loss: 1.1026 - val_accuracy: 0.6896\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2078 - accuracy: 0.6396 - val_loss: 1.0462 - val_accuracy: 0.7216\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1762 - accuracy: 0.6491 - val_loss: 1.0110 - val_accuracy: 0.7291\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1421 - accuracy: 0.6605 - val_loss: 1.0398 - val_accuracy: 0.7122\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1135 - accuracy: 0.6682 - val_loss: 0.9888 - val_accuracy: 0.7302\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0837 - accuracy: 0.6756 - val_loss: 0.9747 - val_accuracy: 0.7304\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0603 - accuracy: 0.6832 - val_loss: 0.9874 - val_accuracy: 0.7176\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0344 - accuracy: 0.6910 - val_loss: 0.8970 - val_accuracy: 0.7456\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.0128 - accuracy: 0.6983 - val_loss: 0.9188 - val_accuracy: 0.7352\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9907 - accuracy: 0.7017 - val_loss: 0.9220 - val_accuracy: 0.7315\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9656 - accuracy: 0.7102 - val_loss: 0.8546 - val_accuracy: 0.7639\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9525 - accuracy: 0.7138 - val_loss: 0.7848 - val_accuracy: 0.7821\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9309 - accuracy: 0.7193 - val_loss: 0.8333 - val_accuracy: 0.7611\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9093 - accuracy: 0.7262 - val_loss: 0.7537 - val_accuracy: 0.7908\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.8917 - accuracy: 0.7317 - val_loss: 0.7608 - val_accuracy: 0.7822\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8798 - accuracy: 0.7347 - val_loss: 0.8108 - val_accuracy: 0.7600\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8655 - accuracy: 0.7383 - val_loss: 0.7377 - val_accuracy: 0.7861\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8494 - accuracy: 0.7423 - val_loss: 0.7201 - val_accuracy: 0.7916\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8312 - accuracy: 0.7479 - val_loss: 0.7505 - val_accuracy: 0.7814\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8191 - accuracy: 0.7511 - val_loss: 0.7012 - val_accuracy: 0.7987\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8060 - accuracy: 0.7551 - val_loss: 0.7114 - val_accuracy: 0.7928\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7971 - accuracy: 0.7585 - val_loss: 0.6488 - val_accuracy: 0.8136\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7893 - accuracy: 0.7606 - val_loss: 0.6887 - val_accuracy: 0.8026\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7675 - accuracy: 0.7655 - val_loss: 0.6640 - val_accuracy: 0.8042\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7653 - accuracy: 0.7667 - val_loss: 0.7003 - val_accuracy: 0.7945\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7500 - accuracy: 0.7720 - val_loss: 0.6772 - val_accuracy: 0.8035\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7374 - accuracy: 0.7770 - val_loss: 0.6932 - val_accuracy: 0.7947\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7325 - accuracy: 0.7772 - val_loss: 0.6288 - val_accuracy: 0.8179\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7189 - accuracy: 0.7827 - val_loss: 0.5896 - val_accuracy: 0.8302\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7117 - accuracy: 0.7829 - val_loss: 0.5615 - val_accuracy: 0.8372\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7038 - accuracy: 0.7842 - val_loss: 0.5646 - val_accuracy: 0.8350\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6991 - accuracy: 0.7872 - val_loss: 0.6686 - val_accuracy: 0.8041\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6840 - accuracy: 0.7914 - val_loss: 0.6620 - val_accuracy: 0.7986\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6830 - accuracy: 0.7907 - val_loss: 0.5939 - val_accuracy: 0.8274\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6739 - accuracy: 0.7953 - val_loss: 0.5849 - val_accuracy: 0.8274\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6630 - accuracy: 0.7977 - val_loss: 0.5704 - val_accuracy: 0.8329\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6605 - accuracy: 0.7971 - val_loss: 0.5924 - val_accuracy: 0.8219\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6479 - accuracy: 0.8015 - val_loss: 0.5888 - val_accuracy: 0.8246\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6376 - accuracy: 0.8054 - val_loss: 0.5616 - val_accuracy: 0.8309\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6382 - accuracy: 0.8046 - val_loss: 0.5544 - val_accuracy: 0.8360\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6367 - accuracy: 0.8036 - val_loss: 0.5779 - val_accuracy: 0.8296\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6229 - accuracy: 0.8082 - val_loss: 0.5633 - val_accuracy: 0.8264\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6189 - accuracy: 0.8097 - val_loss: 0.5917 - val_accuracy: 0.8256\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6131 - accuracy: 0.8123 - val_loss: 0.5511 - val_accuracy: 0.8282\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6058 - accuracy: 0.8135 - val_loss: 0.5410 - val_accuracy: 0.8405\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5953 - accuracy: 0.8175 - val_loss: 0.5367 - val_accuracy: 0.8398\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5980 - accuracy: 0.8143 - val_loss: 0.5274 - val_accuracy: 0.8441\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5895 - accuracy: 0.8182 - val_loss: 0.5517 - val_accuracy: 0.8377\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5869 - accuracy: 0.8185 - val_loss: 0.5395 - val_accuracy: 0.8424\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5808 - accuracy: 0.8216 - val_loss: 0.5477 - val_accuracy: 0.8381\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5756 - accuracy: 0.8230 - val_loss: 0.5944 - val_accuracy: 0.8234\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5749 - accuracy: 0.8247 - val_loss: 0.5238 - val_accuracy: 0.8411\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5685 - accuracy: 0.8242 - val_loss: 0.5937 - val_accuracy: 0.8227\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5616 - accuracy: 0.8283 - val_loss: 0.4811 - val_accuracy: 0.8507\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5623 - accuracy: 0.8278 - val_loss: 0.4819 - val_accuracy: 0.8580\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5536 - accuracy: 0.8292 - val_loss: 0.5213 - val_accuracy: 0.8442\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5526 - accuracy: 0.8298 - val_loss: 0.4549 - val_accuracy: 0.8640\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5448 - accuracy: 0.8320 - val_loss: 0.4706 - val_accuracy: 0.8617\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5471 - accuracy: 0.8317 - val_loss: 0.5387 - val_accuracy: 0.8372\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5395 - accuracy: 0.8341 - val_loss: 0.5274 - val_accuracy: 0.8428\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5365 - accuracy: 0.8360 - val_loss: 0.5219 - val_accuracy: 0.8436\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5297 - accuracy: 0.8360 - val_loss: 0.5396 - val_accuracy: 0.8399\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5310 - accuracy: 0.8367 - val_loss: 0.4651 - val_accuracy: 0.8551\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5237 - accuracy: 0.8385 - val_loss: 0.5455 - val_accuracy: 0.8367\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5244 - accuracy: 0.8386 - val_loss: 0.5135 - val_accuracy: 0.8434\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5206 - accuracy: 0.8391 - val_loss: 0.4920 - val_accuracy: 0.8549\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5096 - accuracy: 0.8448 - val_loss: 0.4446 - val_accuracy: 0.8661\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5127 - accuracy: 0.8417 - val_loss: 0.5674 - val_accuracy: 0.8296\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5090 - accuracy: 0.8414 - val_loss: 0.4337 - val_accuracy: 0.8728\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5073 - accuracy: 0.8420 - val_loss: 0.4917 - val_accuracy: 0.8521\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5092 - accuracy: 0.8440 - val_loss: 0.5286 - val_accuracy: 0.8395\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4975 - accuracy: 0.8466 - val_loss: 0.5892 - val_accuracy: 0.8263\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4961 - accuracy: 0.8473 - val_loss: 0.4463 - val_accuracy: 0.8696\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4892 - accuracy: 0.8491 - val_loss: 0.4866 - val_accuracy: 0.8537\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4899 - accuracy: 0.8496 - val_loss: 0.3977 - val_accuracy: 0.8841\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4859 - accuracy: 0.8496 - val_loss: 0.7234 - val_accuracy: 0.7949\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4788 - accuracy: 0.8520 - val_loss: 0.4071 - val_accuracy: 0.8823\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8502 - val_loss: 0.4611 - val_accuracy: 0.8603\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4775 - accuracy: 0.8523 - val_loss: 0.4374 - val_accuracy: 0.8744\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4777 - accuracy: 0.8533 - val_loss: 0.4502 - val_accuracy: 0.8651\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4687 - accuracy: 0.8553 - val_loss: 0.4313 - val_accuracy: 0.8736\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4693 - accuracy: 0.8555 - val_loss: 0.4118 - val_accuracy: 0.8768\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4643 - accuracy: 0.8555 - val_loss: 0.3881 - val_accuracy: 0.8854\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4654 - accuracy: 0.8572 - val_loss: 0.5157 - val_accuracy: 0.8414\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4619 - accuracy: 0.8566 - val_loss: 0.5536 - val_accuracy: 0.8391\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4635 - accuracy: 0.8560 - val_loss: 0.4865 - val_accuracy: 0.8554\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4606 - accuracy: 0.8571 - val_loss: 0.4501 - val_accuracy: 0.8579\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4559 - accuracy: 0.8608 - val_loss: 0.4166 - val_accuracy: 0.8756\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4526 - accuracy: 0.8590 - val_loss: 0.3902 - val_accuracy: 0.8844\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4495 - accuracy: 0.8607 - val_loss: 0.3847 - val_accuracy: 0.8845\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4501 - accuracy: 0.8617 - val_loss: 0.5091 - val_accuracy: 0.8489\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4448 - accuracy: 0.8637 - val_loss: 0.4717 - val_accuracy: 0.8613\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4414 - accuracy: 0.8640 - val_loss: 0.4528 - val_accuracy: 0.8589\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4403 - accuracy: 0.8631 - val_loss: 0.3815 - val_accuracy: 0.8850\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4389 - accuracy: 0.8654 - val_loss: 0.3705 - val_accuracy: 0.8927\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4353 - accuracy: 0.8667 - val_loss: 0.3777 - val_accuracy: 0.8896\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4384 - accuracy: 0.8643 - val_loss: 0.4808 - val_accuracy: 0.8524\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4272 - accuracy: 0.8695 - val_loss: 0.4232 - val_accuracy: 0.8770\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4307 - accuracy: 0.8651 - val_loss: 0.3936 - val_accuracy: 0.8874\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4272 - accuracy: 0.8678 - val_loss: 0.4892 - val_accuracy: 0.8529\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4291 - accuracy: 0.8667 - val_loss: 0.4718 - val_accuracy: 0.8599\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4206 - accuracy: 0.8703 - val_loss: 0.4144 - val_accuracy: 0.8779\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4204 - accuracy: 0.8703 - val_loss: 0.3596 - val_accuracy: 0.8922\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4195 - accuracy: 0.8683 - val_loss: 0.4016 - val_accuracy: 0.8821\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8704 - val_loss: 0.3880 - val_accuracy: 0.8851\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4160 - accuracy: 0.8699 - val_loss: 0.4355 - val_accuracy: 0.8734\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4142 - accuracy: 0.8711 - val_loss: 0.4192 - val_accuracy: 0.8781\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4111 - accuracy: 0.8722 - val_loss: 0.4151 - val_accuracy: 0.8718\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4075 - accuracy: 0.8743 - val_loss: 0.4115 - val_accuracy: 0.8778\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4062 - accuracy: 0.8745 - val_loss: 0.4661 - val_accuracy: 0.8576\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4087 - accuracy: 0.8723 - val_loss: 0.4477 - val_accuracy: 0.8661\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4112 - accuracy: 0.8737 - val_loss: 0.4577 - val_accuracy: 0.8603\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4037 - accuracy: 0.8753 - val_loss: 0.3721 - val_accuracy: 0.8899\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3982 - accuracy: 0.8763 - val_loss: 0.4687 - val_accuracy: 0.8636\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3972 - accuracy: 0.8756 - val_loss: 0.3601 - val_accuracy: 0.8947\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3960 - accuracy: 0.8760 - val_loss: 0.3620 - val_accuracy: 0.8959\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3942 - accuracy: 0.8780 - val_loss: 0.4078 - val_accuracy: 0.8804\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3956 - accuracy: 0.8766 - val_loss: 0.3366 - val_accuracy: 0.9033\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3920 - accuracy: 0.8793 - val_loss: 0.3647 - val_accuracy: 0.8941\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3901 - accuracy: 0.8791 - val_loss: 0.3758 - val_accuracy: 0.8885\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3877 - accuracy: 0.8805 - val_loss: 0.3712 - val_accuracy: 0.8869\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3875 - accuracy: 0.8793 - val_loss: 0.4274 - val_accuracy: 0.8766\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3841 - accuracy: 0.8799 - val_loss: 0.3867 - val_accuracy: 0.8879\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3788 - accuracy: 0.8829 - val_loss: 0.4671 - val_accuracy: 0.8642\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3836 - accuracy: 0.8796 - val_loss: 0.4102 - val_accuracy: 0.8759\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3793 - accuracy: 0.8812 - val_loss: 0.4823 - val_accuracy: 0.8551\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3816 - accuracy: 0.8804 - val_loss: 0.4715 - val_accuracy: 0.8657\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3818 - accuracy: 0.8814 - val_loss: 0.3621 - val_accuracy: 0.8962\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3734 - accuracy: 0.8841 - val_loss: 0.3138 - val_accuracy: 0.9102\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3755 - accuracy: 0.8833 - val_loss: 0.3553 - val_accuracy: 0.8936\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3699 - accuracy: 0.8858 - val_loss: 0.4072 - val_accuracy: 0.8757\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3728 - accuracy: 0.8824 - val_loss: 0.3400 - val_accuracy: 0.8994\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3690 - accuracy: 0.8839 - val_loss: 0.3629 - val_accuracy: 0.8941\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3714 - accuracy: 0.8830 - val_loss: 0.5406 - val_accuracy: 0.8418\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3665 - accuracy: 0.8843 - val_loss: 0.4204 - val_accuracy: 0.8802\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3625 - accuracy: 0.8866 - val_loss: 0.3416 - val_accuracy: 0.9038\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3661 - accuracy: 0.8858 - val_loss: 0.4827 - val_accuracy: 0.8544\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3671 - accuracy: 0.8868 - val_loss: 0.3588 - val_accuracy: 0.8939\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3551 - accuracy: 0.8906 - val_loss: 0.3220 - val_accuracy: 0.9072\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3597 - accuracy: 0.8874 - val_loss: 0.3318 - val_accuracy: 0.9059\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3621 - accuracy: 0.8871 - val_loss: 0.3572 - val_accuracy: 0.8923\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3614 - accuracy: 0.8885 - val_loss: 0.3902 - val_accuracy: 0.8844\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3509 - accuracy: 0.8897 - val_loss: 0.3096 - val_accuracy: 0.9102\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3520 - accuracy: 0.8910 - val_loss: 0.3362 - val_accuracy: 0.9021\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3506 - accuracy: 0.8919 - val_loss: 0.5753 - val_accuracy: 0.8326\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3518 - accuracy: 0.8922 - val_loss: 0.3168 - val_accuracy: 0.9094\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3476 - accuracy: 0.8914 - val_loss: 0.3311 - val_accuracy: 0.9035\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3525 - accuracy: 0.8905 - val_loss: 0.3938 - val_accuracy: 0.8827\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3389 - accuracy: 0.8954 - val_loss: 0.3596 - val_accuracy: 0.8941\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3502 - accuracy: 0.8909 - val_loss: 0.3166 - val_accuracy: 0.9109\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3463 - accuracy: 0.8930 - val_loss: 0.4286 - val_accuracy: 0.8751\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3422 - accuracy: 0.8929 - val_loss: 0.3125 - val_accuracy: 0.9118\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3455 - accuracy: 0.8920 - val_loss: 0.3216 - val_accuracy: 0.9077\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3401 - accuracy: 0.8958 - val_loss: 0.4170 - val_accuracy: 0.8793\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3346 - accuracy: 0.8950 - val_loss: 0.4100 - val_accuracy: 0.8821\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3377 - accuracy: 0.8932 - val_loss: 0.3312 - val_accuracy: 0.9040\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3342 - accuracy: 0.8957 - val_loss: 0.3007 - val_accuracy: 0.9144\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3296 - accuracy: 0.8960 - val_loss: 0.4180 - val_accuracy: 0.8791\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3373 - accuracy: 0.8944 - val_loss: 0.3136 - val_accuracy: 0.9108\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3310 - accuracy: 0.8985 - val_loss: 0.3016 - val_accuracy: 0.9146\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3295 - accuracy: 0.8968 - val_loss: 0.3431 - val_accuracy: 0.8992\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3350 - accuracy: 0.8957 - val_loss: 0.3739 - val_accuracy: 0.8871\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3299 - accuracy: 0.8967 - val_loss: 0.4274 - val_accuracy: 0.8753\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3323 - accuracy: 0.8975 - val_loss: 0.3085 - val_accuracy: 0.9136\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3288 - accuracy: 0.8988 - val_loss: 0.3419 - val_accuracy: 0.9017\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3277 - accuracy: 0.8987 - val_loss: 0.3545 - val_accuracy: 0.8948\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3259 - accuracy: 0.8996 - val_loss: 0.2703 - val_accuracy: 0.9256\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3217 - accuracy: 0.9010 - val_loss: 0.3365 - val_accuracy: 0.9052\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3213 - accuracy: 0.8997 - val_loss: 0.3507 - val_accuracy: 0.9009\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3200 - accuracy: 0.9012 - val_loss: 0.3492 - val_accuracy: 0.8965\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3209 - accuracy: 0.9001 - val_loss: 0.3808 - val_accuracy: 0.8886\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3188 - accuracy: 0.9007 - val_loss: 0.2652 - val_accuracy: 0.9255\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3168 - accuracy: 0.9028 - val_loss: 0.3646 - val_accuracy: 0.8964\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3182 - accuracy: 0.9006 - val_loss: 0.3696 - val_accuracy: 0.8928\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3174 - accuracy: 0.8999 - val_loss: 0.3106 - val_accuracy: 0.9123\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3170 - accuracy: 0.9023 - val_loss: 0.2568 - val_accuracy: 0.9293\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3171 - accuracy: 0.8996 - val_loss: 0.3081 - val_accuracy: 0.9121\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3117 - accuracy: 0.9024 - val_loss: 0.3131 - val_accuracy: 0.9089\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3161 - accuracy: 0.9013 - val_loss: 0.3213 - val_accuracy: 0.9087\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3123 - accuracy: 0.9014 - val_loss: 0.3380 - val_accuracy: 0.9046\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3125 - accuracy: 0.9025 - val_loss: 0.3232 - val_accuracy: 0.9056\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3132 - accuracy: 0.9029 - val_loss: 0.2843 - val_accuracy: 0.9196\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3068 - accuracy: 0.9039 - val_loss: 0.4787 - val_accuracy: 0.8543\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3089 - accuracy: 0.9030 - val_loss: 0.3267 - val_accuracy: 0.9032\n",
      "Try 5/100: Best_val_acc: [0.5039315223693848, 0.8527222275733948], lr: 6.098795236427485e-05, Lambda: 5.854105873527945e-05\n",
      "\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_396 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_397 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_398 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_399 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_400 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_401 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.5835 - accuracy: 0.1115 - val_loss: 2.1841 - val_accuracy: 0.2316\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4247 - accuracy: 0.1408 - val_loss: 2.0926 - val_accuracy: 0.2236\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2848 - accuracy: 0.1817 - val_loss: 1.9552 - val_accuracy: 0.3470\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1523 - accuracy: 0.2245 - val_loss: 1.8503 - val_accuracy: 0.4036\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0255 - accuracy: 0.2815 - val_loss: 1.7755 - val_accuracy: 0.4384\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9141 - accuracy: 0.3345 - val_loss: 1.6187 - val_accuracy: 0.5265\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8138 - accuracy: 0.3803 - val_loss: 1.5660 - val_accuracy: 0.5441\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7152 - accuracy: 0.4263 - val_loss: 1.4274 - val_accuracy: 0.6112\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6367 - accuracy: 0.4610 - val_loss: 1.4100 - val_accuracy: 0.6332\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5586 - accuracy: 0.4995 - val_loss: 1.2962 - val_accuracy: 0.6740\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4933 - accuracy: 0.5238 - val_loss: 1.2536 - val_accuracy: 0.6668\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4311 - accuracy: 0.5484 - val_loss: 1.1931 - val_accuracy: 0.6842\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3748 - accuracy: 0.5685 - val_loss: 1.2025 - val_accuracy: 0.6631\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3255 - accuracy: 0.5890 - val_loss: 1.0921 - val_accuracy: 0.7006\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2826 - accuracy: 0.6037 - val_loss: 1.1243 - val_accuracy: 0.6949\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2422 - accuracy: 0.6208 - val_loss: 1.0751 - val_accuracy: 0.7006\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2057 - accuracy: 0.6320 - val_loss: 0.9919 - val_accuracy: 0.7324\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1691 - accuracy: 0.6431 - val_loss: 0.9509 - val_accuracy: 0.7301\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1377 - accuracy: 0.6545 - val_loss: 0.9571 - val_accuracy: 0.7340\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1025 - accuracy: 0.6646 - val_loss: 0.9567 - val_accuracy: 0.7204\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.0745 - accuracy: 0.6748 - val_loss: 0.8598 - val_accuracy: 0.7560\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0500 - accuracy: 0.6807 - val_loss: 0.9021 - val_accuracy: 0.7385\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0229 - accuracy: 0.6905 - val_loss: 0.9898 - val_accuracy: 0.7062\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9920 - accuracy: 0.7011 - val_loss: 0.9377 - val_accuracy: 0.7277\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9739 - accuracy: 0.7046 - val_loss: 0.8610 - val_accuracy: 0.7412\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9508 - accuracy: 0.7122 - val_loss: 0.7928 - val_accuracy: 0.7732\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9304 - accuracy: 0.7185 - val_loss: 0.7508 - val_accuracy: 0.7896\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9100 - accuracy: 0.7244 - val_loss: 0.7953 - val_accuracy: 0.7668\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8894 - accuracy: 0.7292 - val_loss: 0.7128 - val_accuracy: 0.7960\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8744 - accuracy: 0.7340 - val_loss: 0.8266 - val_accuracy: 0.7461\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8553 - accuracy: 0.7401 - val_loss: 0.7434 - val_accuracy: 0.7884\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8488 - accuracy: 0.7402 - val_loss: 0.7934 - val_accuracy: 0.7691\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.8276 - accuracy: 0.7467 - val_loss: 0.7561 - val_accuracy: 0.7798\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8128 - accuracy: 0.7528 - val_loss: 0.6749 - val_accuracy: 0.8060\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7997 - accuracy: 0.7556 - val_loss: 0.6115 - val_accuracy: 0.8241\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7842 - accuracy: 0.7600 - val_loss: 0.6214 - val_accuracy: 0.8241\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7766 - accuracy: 0.7614 - val_loss: 0.6580 - val_accuracy: 0.8066\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7591 - accuracy: 0.7688 - val_loss: 0.6813 - val_accuracy: 0.7949\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7526 - accuracy: 0.7684 - val_loss: 0.6266 - val_accuracy: 0.8182\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7343 - accuracy: 0.7753 - val_loss: 0.7661 - val_accuracy: 0.7691\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7266 - accuracy: 0.7764 - val_loss: 0.5982 - val_accuracy: 0.8244\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7189 - accuracy: 0.7804 - val_loss: 0.5556 - val_accuracy: 0.8371\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7082 - accuracy: 0.7826 - val_loss: 0.5667 - val_accuracy: 0.8390\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6990 - accuracy: 0.7854 - val_loss: 0.6029 - val_accuracy: 0.8237\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6896 - accuracy: 0.7876 - val_loss: 0.6654 - val_accuracy: 0.7996\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6840 - accuracy: 0.7895 - val_loss: 0.6532 - val_accuracy: 0.8031\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6797 - accuracy: 0.7901 - val_loss: 0.6045 - val_accuracy: 0.8209\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6689 - accuracy: 0.7947 - val_loss: 0.5849 - val_accuracy: 0.8245\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6616 - accuracy: 0.7966 - val_loss: 0.5318 - val_accuracy: 0.8429\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6513 - accuracy: 0.8003 - val_loss: 0.6469 - val_accuracy: 0.8054\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6509 - accuracy: 0.8000 - val_loss: 0.6477 - val_accuracy: 0.8042\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6417 - accuracy: 0.8013 - val_loss: 0.6171 - val_accuracy: 0.8112\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6308 - accuracy: 0.8064 - val_loss: 0.5828 - val_accuracy: 0.8301\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6296 - accuracy: 0.8068 - val_loss: 0.6075 - val_accuracy: 0.8203\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6225 - accuracy: 0.8072 - val_loss: 0.5197 - val_accuracy: 0.8486\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6129 - accuracy: 0.8102 - val_loss: 0.4919 - val_accuracy: 0.8569\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6071 - accuracy: 0.8136 - val_loss: 0.5718 - val_accuracy: 0.8269\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6061 - accuracy: 0.8120 - val_loss: 0.5285 - val_accuracy: 0.8420\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5948 - accuracy: 0.8166 - val_loss: 0.5986 - val_accuracy: 0.8169\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5930 - accuracy: 0.8151 - val_loss: 0.5393 - val_accuracy: 0.8417\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5922 - accuracy: 0.8153 - val_loss: 0.5157 - val_accuracy: 0.8406\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5818 - accuracy: 0.8197 - val_loss: 0.5290 - val_accuracy: 0.8416\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5792 - accuracy: 0.8216 - val_loss: 0.5106 - val_accuracy: 0.8463\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5748 - accuracy: 0.8215 - val_loss: 0.5461 - val_accuracy: 0.8315\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5736 - accuracy: 0.8222 - val_loss: 0.6491 - val_accuracy: 0.8039\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5591 - accuracy: 0.8268 - val_loss: 0.5136 - val_accuracy: 0.8505\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5665 - accuracy: 0.8247 - val_loss: 0.4946 - val_accuracy: 0.8550\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5543 - accuracy: 0.8290 - val_loss: 0.4418 - val_accuracy: 0.8691\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5508 - accuracy: 0.8291 - val_loss: 0.4943 - val_accuracy: 0.8524\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5538 - accuracy: 0.8271 - val_loss: 0.5167 - val_accuracy: 0.8444\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5390 - accuracy: 0.8321 - val_loss: 0.4628 - val_accuracy: 0.8671\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5394 - accuracy: 0.8329 - val_loss: 0.4348 - val_accuracy: 0.8701\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5407 - accuracy: 0.8325 - val_loss: 0.4488 - val_accuracy: 0.8721\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5333 - accuracy: 0.8350 - val_loss: 0.4256 - val_accuracy: 0.8768\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5249 - accuracy: 0.8377 - val_loss: 0.4805 - val_accuracy: 0.8600\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5223 - accuracy: 0.8381 - val_loss: 0.5084 - val_accuracy: 0.8435\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5206 - accuracy: 0.8395 - val_loss: 0.4886 - val_accuracy: 0.8539\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5155 - accuracy: 0.8405 - val_loss: 0.4114 - val_accuracy: 0.8798\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5134 - accuracy: 0.8408 - val_loss: 0.4702 - val_accuracy: 0.8599\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5100 - accuracy: 0.8409 - val_loss: 0.4465 - val_accuracy: 0.8706\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5091 - accuracy: 0.8414 - val_loss: 0.4495 - val_accuracy: 0.8680\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5013 - accuracy: 0.8451 - val_loss: 0.4797 - val_accuracy: 0.8542\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5043 - accuracy: 0.8425 - val_loss: 0.4452 - val_accuracy: 0.8654\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4987 - accuracy: 0.8454 - val_loss: 0.5212 - val_accuracy: 0.8414\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4941 - accuracy: 0.8466 - val_loss: 0.4789 - val_accuracy: 0.8546\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4901 - accuracy: 0.8482 - val_loss: 0.3927 - val_accuracy: 0.8844\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4873 - accuracy: 0.8499 - val_loss: 0.4603 - val_accuracy: 0.8624\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4875 - accuracy: 0.8476 - val_loss: 0.5304 - val_accuracy: 0.8404\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4768 - accuracy: 0.8534 - val_loss: 0.4222 - val_accuracy: 0.8715\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4786 - accuracy: 0.8513 - val_loss: 0.4026 - val_accuracy: 0.8841\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8500 - val_loss: 0.4418 - val_accuracy: 0.8678\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4735 - accuracy: 0.8536 - val_loss: 0.4195 - val_accuracy: 0.8732\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4704 - accuracy: 0.8548 - val_loss: 0.5029 - val_accuracy: 0.8441\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4689 - accuracy: 0.8536 - val_loss: 0.4037 - val_accuracy: 0.8811\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4651 - accuracy: 0.8562 - val_loss: 0.5020 - val_accuracy: 0.8536\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4663 - accuracy: 0.8567 - val_loss: 0.3677 - val_accuracy: 0.8939\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4624 - accuracy: 0.8570 - val_loss: 0.5216 - val_accuracy: 0.8430\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4602 - accuracy: 0.8564 - val_loss: 0.3568 - val_accuracy: 0.8949\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4552 - accuracy: 0.8577 - val_loss: 0.3996 - val_accuracy: 0.8823\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4541 - accuracy: 0.8594 - val_loss: 0.4066 - val_accuracy: 0.8807\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4522 - accuracy: 0.8592 - val_loss: 0.4734 - val_accuracy: 0.8593\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4486 - accuracy: 0.8605 - val_loss: 0.4622 - val_accuracy: 0.8629\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4486 - accuracy: 0.8606 - val_loss: 0.3429 - val_accuracy: 0.9016\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4436 - accuracy: 0.8643 - val_loss: 0.5059 - val_accuracy: 0.8506\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4445 - accuracy: 0.8644 - val_loss: 0.4457 - val_accuracy: 0.8631\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4442 - accuracy: 0.8632 - val_loss: 0.4106 - val_accuracy: 0.8767\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4389 - accuracy: 0.8643 - val_loss: 0.3799 - val_accuracy: 0.8884\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4376 - accuracy: 0.8643 - val_loss: 0.3586 - val_accuracy: 0.8970\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4360 - accuracy: 0.8645 - val_loss: 0.4180 - val_accuracy: 0.8774\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4319 - accuracy: 0.8664 - val_loss: 0.6744 - val_accuracy: 0.7989\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4336 - accuracy: 0.8653 - val_loss: 0.3858 - val_accuracy: 0.8840\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4287 - accuracy: 0.8653 - val_loss: 0.3819 - val_accuracy: 0.8917\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4278 - accuracy: 0.8665 - val_loss: 0.4339 - val_accuracy: 0.8774\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4202 - accuracy: 0.8699 - val_loss: 0.3864 - val_accuracy: 0.8856\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8701 - val_loss: 0.3929 - val_accuracy: 0.8861\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4214 - accuracy: 0.8693 - val_loss: 0.4836 - val_accuracy: 0.8557\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8701 - val_loss: 0.4412 - val_accuracy: 0.8656\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4126 - accuracy: 0.8718 - val_loss: 0.3963 - val_accuracy: 0.8871\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4118 - accuracy: 0.8728 - val_loss: 0.4492 - val_accuracy: 0.8671\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4070 - accuracy: 0.8747 - val_loss: 0.3692 - val_accuracy: 0.8916\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4104 - accuracy: 0.8722 - val_loss: 0.4003 - val_accuracy: 0.8789\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4142 - accuracy: 0.8722 - val_loss: 0.3801 - val_accuracy: 0.8874\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4049 - accuracy: 0.8750 - val_loss: 0.4171 - val_accuracy: 0.8813\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4059 - accuracy: 0.8739 - val_loss: 0.4163 - val_accuracy: 0.8783\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4067 - accuracy: 0.8749 - val_loss: 0.3571 - val_accuracy: 0.8957\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4006 - accuracy: 0.8744 - val_loss: 0.3741 - val_accuracy: 0.8929\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4000 - accuracy: 0.8760 - val_loss: 0.4748 - val_accuracy: 0.8600\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4009 - accuracy: 0.8744 - val_loss: 0.4025 - val_accuracy: 0.8822\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3919 - accuracy: 0.8793 - val_loss: 0.3746 - val_accuracy: 0.8932\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3983 - accuracy: 0.8762 - val_loss: 0.4190 - val_accuracy: 0.8756\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4016 - accuracy: 0.8756 - val_loss: 0.4259 - val_accuracy: 0.8771\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3871 - accuracy: 0.8790 - val_loss: 0.4648 - val_accuracy: 0.8598\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3865 - accuracy: 0.8804 - val_loss: 0.4288 - val_accuracy: 0.8755\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3885 - accuracy: 0.8800 - val_loss: 0.4638 - val_accuracy: 0.8629\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3843 - accuracy: 0.8808 - val_loss: 0.3983 - val_accuracy: 0.8834\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3845 - accuracy: 0.8793 - val_loss: 0.4525 - val_accuracy: 0.8661\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3828 - accuracy: 0.8799 - val_loss: 0.3686 - val_accuracy: 0.8947\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3809 - accuracy: 0.8825 - val_loss: 0.4326 - val_accuracy: 0.8681\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3809 - accuracy: 0.8826 - val_loss: 0.4771 - val_accuracy: 0.8582\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3780 - accuracy: 0.8829 - val_loss: 0.3682 - val_accuracy: 0.8976\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3778 - accuracy: 0.8831 - val_loss: 0.3418 - val_accuracy: 0.8987\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3751 - accuracy: 0.8833 - val_loss: 0.3720 - val_accuracy: 0.8921\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3797 - accuracy: 0.8793 - val_loss: 0.4119 - val_accuracy: 0.8831\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3688 - accuracy: 0.8856 - val_loss: 0.4968 - val_accuracy: 0.8567\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3682 - accuracy: 0.8858 - val_loss: 0.4307 - val_accuracy: 0.8728\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3671 - accuracy: 0.8859 - val_loss: 0.4497 - val_accuracy: 0.8659\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3626 - accuracy: 0.8857 - val_loss: 0.3871 - val_accuracy: 0.8890\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3655 - accuracy: 0.8856 - val_loss: 0.4388 - val_accuracy: 0.8701\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3618 - accuracy: 0.8883 - val_loss: 0.4362 - val_accuracy: 0.8741\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3625 - accuracy: 0.8891 - val_loss: 0.4158 - val_accuracy: 0.8790\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3623 - accuracy: 0.8875 - val_loss: 0.3203 - val_accuracy: 0.9094\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3591 - accuracy: 0.8886 - val_loss: 0.3173 - val_accuracy: 0.9106\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3585 - accuracy: 0.8890 - val_loss: 0.3656 - val_accuracy: 0.8919\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3515 - accuracy: 0.8909 - val_loss: 0.3913 - val_accuracy: 0.8862\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3609 - accuracy: 0.8879 - val_loss: 0.3578 - val_accuracy: 0.8969\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3565 - accuracy: 0.8900 - val_loss: 0.4586 - val_accuracy: 0.8651\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3543 - accuracy: 0.8914 - val_loss: 0.4617 - val_accuracy: 0.8581\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3501 - accuracy: 0.8910 - val_loss: 0.3928 - val_accuracy: 0.8849\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3481 - accuracy: 0.8920 - val_loss: 0.3675 - val_accuracy: 0.8955\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.3469 - accuracy: 0.8905 - val_loss: 0.4662 - val_accuracy: 0.8615\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3471 - accuracy: 0.8919 - val_loss: 0.3059 - val_accuracy: 0.9130\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3480 - accuracy: 0.8928 - val_loss: 0.3628 - val_accuracy: 0.8935\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3482 - accuracy: 0.8908 - val_loss: 0.4431 - val_accuracy: 0.8705\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3485 - accuracy: 0.8927 - val_loss: 0.2785 - val_accuracy: 0.9215\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3406 - accuracy: 0.8941 - val_loss: 0.3215 - val_accuracy: 0.9037\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3394 - accuracy: 0.8955 - val_loss: 0.3365 - val_accuracy: 0.9043\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3382 - accuracy: 0.8937 - val_loss: 0.3131 - val_accuracy: 0.9086\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3410 - accuracy: 0.8934 - val_loss: 0.5328 - val_accuracy: 0.8481\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3366 - accuracy: 0.8962 - val_loss: 0.3461 - val_accuracy: 0.8996\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3339 - accuracy: 0.8971 - val_loss: 0.4469 - val_accuracy: 0.8655\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3373 - accuracy: 0.8965 - val_loss: 0.3173 - val_accuracy: 0.9106\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3346 - accuracy: 0.8955 - val_loss: 0.3640 - val_accuracy: 0.8942\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3348 - accuracy: 0.8951 - val_loss: 0.4850 - val_accuracy: 0.8505\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3325 - accuracy: 0.8973 - val_loss: 0.3447 - val_accuracy: 0.9015\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3373 - accuracy: 0.8959 - val_loss: 0.3239 - val_accuracy: 0.9082\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3322 - accuracy: 0.8954 - val_loss: 0.2988 - val_accuracy: 0.9156\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3281 - accuracy: 0.8973 - val_loss: 0.3197 - val_accuracy: 0.9081\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3235 - accuracy: 0.9000 - val_loss: 0.3197 - val_accuracy: 0.9109\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3252 - accuracy: 0.8979 - val_loss: 0.3110 - val_accuracy: 0.9094\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3248 - accuracy: 0.8992 - val_loss: 0.2931 - val_accuracy: 0.9168\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3211 - accuracy: 0.9006 - val_loss: 0.2987 - val_accuracy: 0.9157\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3215 - accuracy: 0.9009 - val_loss: 0.3702 - val_accuracy: 0.8921\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3280 - accuracy: 0.8969 - val_loss: 0.3678 - val_accuracy: 0.8960\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3273 - accuracy: 0.8965 - val_loss: 0.4743 - val_accuracy: 0.8584\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3180 - accuracy: 0.8996 - val_loss: 0.3663 - val_accuracy: 0.8916\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3180 - accuracy: 0.9008 - val_loss: 0.2809 - val_accuracy: 0.9216\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3179 - accuracy: 0.9004 - val_loss: 0.3702 - val_accuracy: 0.8901\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3123 - accuracy: 0.9017 - val_loss: 0.3135 - val_accuracy: 0.9099\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3122 - accuracy: 0.9035 - val_loss: 0.3063 - val_accuracy: 0.9129\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3126 - accuracy: 0.9030 - val_loss: 0.3381 - val_accuracy: 0.9046\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3133 - accuracy: 0.9029 - val_loss: 0.3493 - val_accuracy: 0.9001\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3108 - accuracy: 0.9035 - val_loss: 0.3988 - val_accuracy: 0.8814\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3139 - accuracy: 0.9027 - val_loss: 0.7686 - val_accuracy: 0.7822\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3156 - accuracy: 0.9017 - val_loss: 0.3356 - val_accuracy: 0.9015\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3098 - accuracy: 0.9038 - val_loss: 0.2786 - val_accuracy: 0.9223\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3080 - accuracy: 0.9051 - val_loss: 0.3344 - val_accuracy: 0.9026\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3037 - accuracy: 0.9053 - val_loss: 0.3290 - val_accuracy: 0.9062\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3072 - accuracy: 0.9035 - val_loss: 0.3805 - val_accuracy: 0.8939\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3065 - accuracy: 0.9047 - val_loss: 0.3018 - val_accuracy: 0.9148\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3049 - accuracy: 0.9059 - val_loss: 0.3760 - val_accuracy: 0.8926\n",
      "Try 6/100: Best_val_acc: [0.48323360085487366, 0.8588888645172119], lr: 6.105933474049894e-05, Lambda: 5.821894809254731e-05\n",
      "\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_402 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_403 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_404 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_405 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_406 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_407 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.5827 - accuracy: 0.1104 - val_loss: 2.3246 - val_accuracy: 0.0944\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4001 - accuracy: 0.1429 - val_loss: 2.3104 - val_accuracy: 0.1749\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2699 - accuracy: 0.1849 - val_loss: 2.2806 - val_accuracy: 0.1801\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1427 - accuracy: 0.2338 - val_loss: 2.1846 - val_accuracy: 0.2420\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0277 - accuracy: 0.2852 - val_loss: 2.0595 - val_accuracy: 0.3016\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9115 - accuracy: 0.3357 - val_loss: 1.9076 - val_accuracy: 0.4025\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8169 - accuracy: 0.3794 - val_loss: 1.7619 - val_accuracy: 0.4459\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7198 - accuracy: 0.4232 - val_loss: 1.6409 - val_accuracy: 0.4879\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6423 - accuracy: 0.4590 - val_loss: 1.5036 - val_accuracy: 0.5608\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5678 - accuracy: 0.4924 - val_loss: 1.4991 - val_accuracy: 0.5300\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5018 - accuracy: 0.5195 - val_loss: 1.3544 - val_accuracy: 0.6366\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4405 - accuracy: 0.5472 - val_loss: 1.2882 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3835 - accuracy: 0.5693 - val_loss: 1.2384 - val_accuracy: 0.6601\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3346 - accuracy: 0.5856 - val_loss: 1.2570 - val_accuracy: 0.6466\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2874 - accuracy: 0.6050 - val_loss: 1.2266 - val_accuracy: 0.6496\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2442 - accuracy: 0.6191 - val_loss: 1.1170 - val_accuracy: 0.6884\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2044 - accuracy: 0.6352 - val_loss: 1.0790 - val_accuracy: 0.6954\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1669 - accuracy: 0.6456 - val_loss: 1.0751 - val_accuracy: 0.6951\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1354 - accuracy: 0.6565 - val_loss: 0.9743 - val_accuracy: 0.7356\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1015 - accuracy: 0.6662 - val_loss: 1.0154 - val_accuracy: 0.7111\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0746 - accuracy: 0.6744 - val_loss: 0.9739 - val_accuracy: 0.7159\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0475 - accuracy: 0.6850 - val_loss: 1.0211 - val_accuracy: 0.6954\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0234 - accuracy: 0.6912 - val_loss: 0.9100 - val_accuracy: 0.7454\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9975 - accuracy: 0.6993 - val_loss: 0.8598 - val_accuracy: 0.7574\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9755 - accuracy: 0.7042 - val_loss: 0.8763 - val_accuracy: 0.7511\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9580 - accuracy: 0.7094 - val_loss: 0.8664 - val_accuracy: 0.7444\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9303 - accuracy: 0.7175 - val_loss: 0.7979 - val_accuracy: 0.7761\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9179 - accuracy: 0.7209 - val_loss: 0.9007 - val_accuracy: 0.7311\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9017 - accuracy: 0.7236 - val_loss: 0.8481 - val_accuracy: 0.7589\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.8837 - accuracy: 0.7304 - val_loss: 0.7989 - val_accuracy: 0.7693\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8703 - accuracy: 0.7358 - val_loss: 0.8335 - val_accuracy: 0.7500\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8523 - accuracy: 0.7427 - val_loss: 0.7586 - val_accuracy: 0.7785\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8388 - accuracy: 0.7445 - val_loss: 0.7856 - val_accuracy: 0.7664\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8204 - accuracy: 0.7516 - val_loss: 0.7460 - val_accuracy: 0.7794\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.8086 - accuracy: 0.7538 - val_loss: 0.7921 - val_accuracy: 0.7624\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7941 - accuracy: 0.7594 - val_loss: 0.7739 - val_accuracy: 0.7644\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7864 - accuracy: 0.7611 - val_loss: 0.7573 - val_accuracy: 0.7767\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7754 - accuracy: 0.7622 - val_loss: 0.7458 - val_accuracy: 0.7756\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7610 - accuracy: 0.7666 - val_loss: 0.6652 - val_accuracy: 0.8103\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7492 - accuracy: 0.7717 - val_loss: 0.5962 - val_accuracy: 0.8306\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7434 - accuracy: 0.7715 - val_loss: 0.6385 - val_accuracy: 0.8149\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7318 - accuracy: 0.7781 - val_loss: 0.5954 - val_accuracy: 0.8319\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7212 - accuracy: 0.7795 - val_loss: 0.6804 - val_accuracy: 0.7986\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7150 - accuracy: 0.7797 - val_loss: 0.6963 - val_accuracy: 0.7901\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7061 - accuracy: 0.7833 - val_loss: 0.6636 - val_accuracy: 0.8041\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6928 - accuracy: 0.7890 - val_loss: 0.7088 - val_accuracy: 0.7847\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6825 - accuracy: 0.7905 - val_loss: 0.6158 - val_accuracy: 0.8182\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6734 - accuracy: 0.7940 - val_loss: 0.5322 - val_accuracy: 0.8455\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6713 - accuracy: 0.7936 - val_loss: 0.6016 - val_accuracy: 0.8206\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6646 - accuracy: 0.7967 - val_loss: 0.5572 - val_accuracy: 0.8366\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6572 - accuracy: 0.7976 - val_loss: 0.6426 - val_accuracy: 0.7976\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6562 - accuracy: 0.7960 - val_loss: 0.6624 - val_accuracy: 0.7979\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6375 - accuracy: 0.8048 - val_loss: 0.7424 - val_accuracy: 0.7767\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6417 - accuracy: 0.8007 - val_loss: 0.5672 - val_accuracy: 0.8373\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6257 - accuracy: 0.8067 - val_loss: 0.6388 - val_accuracy: 0.8071\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6277 - accuracy: 0.8078 - val_loss: 0.5693 - val_accuracy: 0.8307\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6188 - accuracy: 0.8088 - val_loss: 0.6731 - val_accuracy: 0.7899\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6136 - accuracy: 0.8099 - val_loss: 0.5457 - val_accuracy: 0.8332\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6105 - accuracy: 0.8131 - val_loss: 0.5382 - val_accuracy: 0.8419\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6003 - accuracy: 0.8141 - val_loss: 0.5154 - val_accuracy: 0.8456\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5925 - accuracy: 0.8170 - val_loss: 0.6125 - val_accuracy: 0.8146\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5904 - accuracy: 0.8182 - val_loss: 0.5739 - val_accuracy: 0.8285\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5893 - accuracy: 0.8189 - val_loss: 0.6024 - val_accuracy: 0.8216\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5820 - accuracy: 0.8217 - val_loss: 0.5275 - val_accuracy: 0.8438\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5767 - accuracy: 0.8210 - val_loss: 0.5107 - val_accuracy: 0.8514\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5760 - accuracy: 0.8216 - val_loss: 0.6137 - val_accuracy: 0.8114\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5668 - accuracy: 0.8249 - val_loss: 0.4818 - val_accuracy: 0.8526\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5634 - accuracy: 0.8272 - val_loss: 0.4572 - val_accuracy: 0.8697\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5618 - accuracy: 0.8260 - val_loss: 0.6078 - val_accuracy: 0.8174\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5554 - accuracy: 0.8285 - val_loss: 0.5173 - val_accuracy: 0.8461\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5502 - accuracy: 0.8308 - val_loss: 0.4770 - val_accuracy: 0.8578\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5459 - accuracy: 0.8287 - val_loss: 0.5770 - val_accuracy: 0.8215\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5446 - accuracy: 0.8318 - val_loss: 0.4857 - val_accuracy: 0.8582\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5383 - accuracy: 0.8329 - val_loss: 0.4497 - val_accuracy: 0.8673\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5395 - accuracy: 0.8317 - val_loss: 0.4841 - val_accuracy: 0.8543\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5350 - accuracy: 0.8338 - val_loss: 0.5125 - val_accuracy: 0.8524\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5294 - accuracy: 0.8358 - val_loss: 0.5013 - val_accuracy: 0.8534\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5266 - accuracy: 0.8372 - val_loss: 0.4928 - val_accuracy: 0.8467\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5232 - accuracy: 0.8383 - val_loss: 0.4814 - val_accuracy: 0.8561\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5197 - accuracy: 0.8391 - val_loss: 0.5750 - val_accuracy: 0.8280\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5211 - accuracy: 0.8389 - val_loss: 0.5147 - val_accuracy: 0.8479\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5159 - accuracy: 0.8398 - val_loss: 0.5157 - val_accuracy: 0.8430\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5022 - accuracy: 0.8449 - val_loss: 0.4627 - val_accuracy: 0.8674\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5046 - accuracy: 0.8463 - val_loss: 0.4225 - val_accuracy: 0.8796\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5039 - accuracy: 0.8439 - val_loss: 0.4847 - val_accuracy: 0.8571\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4974 - accuracy: 0.8451 - val_loss: 0.4638 - val_accuracy: 0.8644\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4911 - accuracy: 0.8487 - val_loss: 0.4332 - val_accuracy: 0.8712\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4942 - accuracy: 0.8466 - val_loss: 0.3932 - val_accuracy: 0.8836\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4897 - accuracy: 0.8494 - val_loss: 0.5368 - val_accuracy: 0.8347\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4883 - accuracy: 0.8487 - val_loss: 0.4219 - val_accuracy: 0.8790\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4837 - accuracy: 0.8511 - val_loss: 0.4037 - val_accuracy: 0.8786\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4781 - accuracy: 0.8512 - val_loss: 0.4746 - val_accuracy: 0.8570\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4808 - accuracy: 0.8517 - val_loss: 0.4906 - val_accuracy: 0.8564\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4776 - accuracy: 0.8525 - val_loss: 0.4041 - val_accuracy: 0.8824\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4730 - accuracy: 0.8543 - val_loss: 0.4688 - val_accuracy: 0.8631\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4685 - accuracy: 0.8556 - val_loss: 0.4434 - val_accuracy: 0.8672\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4652 - accuracy: 0.8563 - val_loss: 0.3699 - val_accuracy: 0.8932\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4655 - accuracy: 0.8570 - val_loss: 0.4221 - val_accuracy: 0.8773\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8587 - val_loss: 0.4983 - val_accuracy: 0.8520\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4658 - accuracy: 0.8554 - val_loss: 0.4560 - val_accuracy: 0.8652\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4594 - accuracy: 0.8585 - val_loss: 0.4569 - val_accuracy: 0.8699\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4554 - accuracy: 0.8598 - val_loss: 0.4976 - val_accuracy: 0.8531\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4562 - accuracy: 0.8577 - val_loss: 0.4829 - val_accuracy: 0.8518\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4508 - accuracy: 0.8612 - val_loss: 0.4554 - val_accuracy: 0.8678\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4528 - accuracy: 0.8601 - val_loss: 0.3967 - val_accuracy: 0.8839\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4514 - accuracy: 0.8607 - val_loss: 0.4210 - val_accuracy: 0.8699\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8638 - val_loss: 0.4659 - val_accuracy: 0.8641\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4416 - accuracy: 0.8632 - val_loss: 0.3627 - val_accuracy: 0.8950\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4338 - accuracy: 0.8656 - val_loss: 0.4311 - val_accuracy: 0.8689\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4423 - accuracy: 0.8647 - val_loss: 0.4663 - val_accuracy: 0.8613\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4375 - accuracy: 0.8660 - val_loss: 0.3738 - val_accuracy: 0.8916\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4298 - accuracy: 0.8665 - val_loss: 0.3838 - val_accuracy: 0.8878\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4301 - accuracy: 0.8665 - val_loss: 0.3912 - val_accuracy: 0.8841\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4291 - accuracy: 0.8665 - val_loss: 0.4335 - val_accuracy: 0.8691\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4279 - accuracy: 0.8664 - val_loss: 0.3792 - val_accuracy: 0.8904\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4255 - accuracy: 0.8679 - val_loss: 0.3898 - val_accuracy: 0.8855\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4227 - accuracy: 0.8690 - val_loss: 0.4297 - val_accuracy: 0.8733\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4185 - accuracy: 0.8696 - val_loss: 0.4638 - val_accuracy: 0.8591\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4219 - accuracy: 0.8687 - val_loss: 0.4763 - val_accuracy: 0.8490\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4212 - accuracy: 0.8699 - val_loss: 0.5317 - val_accuracy: 0.8457\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4192 - accuracy: 0.8695 - val_loss: 0.4603 - val_accuracy: 0.8591\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4139 - accuracy: 0.8719 - val_loss: 0.3702 - val_accuracy: 0.8909\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4086 - accuracy: 0.8739 - val_loss: 0.3145 - val_accuracy: 0.9089\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4065 - accuracy: 0.8740 - val_loss: 0.4563 - val_accuracy: 0.8661\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4135 - accuracy: 0.8733 - val_loss: 0.3542 - val_accuracy: 0.8964\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4076 - accuracy: 0.8746 - val_loss: 0.3901 - val_accuracy: 0.8816\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4044 - accuracy: 0.8750 - val_loss: 0.3814 - val_accuracy: 0.8891\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3990 - accuracy: 0.8769 - val_loss: 0.4555 - val_accuracy: 0.8646\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8765 - val_loss: 0.4437 - val_accuracy: 0.8713\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3973 - accuracy: 0.8756 - val_loss: 0.3814 - val_accuracy: 0.8896\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3958 - accuracy: 0.8778 - val_loss: 0.4026 - val_accuracy: 0.8831\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3999 - accuracy: 0.8765 - val_loss: 0.3512 - val_accuracy: 0.8994\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3952 - accuracy: 0.8771 - val_loss: 0.4334 - val_accuracy: 0.8706\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3935 - accuracy: 0.8769 - val_loss: 0.3297 - val_accuracy: 0.9051\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3902 - accuracy: 0.8803 - val_loss: 0.3526 - val_accuracy: 0.8956\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3874 - accuracy: 0.8800 - val_loss: 0.3435 - val_accuracy: 0.9003\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3873 - accuracy: 0.8794 - val_loss: 0.6150 - val_accuracy: 0.8191\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3844 - accuracy: 0.8798 - val_loss: 0.3263 - val_accuracy: 0.9070\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3806 - accuracy: 0.8816 - val_loss: 0.3398 - val_accuracy: 0.8998\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3822 - accuracy: 0.8819 - val_loss: 0.3577 - val_accuracy: 0.8992\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3858 - accuracy: 0.8798 - val_loss: 0.4653 - val_accuracy: 0.8594\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3784 - accuracy: 0.8826 - val_loss: 0.3897 - val_accuracy: 0.8871\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3730 - accuracy: 0.8853 - val_loss: 0.3558 - val_accuracy: 0.8959\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3771 - accuracy: 0.8830 - val_loss: 0.4114 - val_accuracy: 0.8754\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3722 - accuracy: 0.8827 - val_loss: 0.3592 - val_accuracy: 0.8943\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3719 - accuracy: 0.8849 - val_loss: 0.3671 - val_accuracy: 0.8937\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3703 - accuracy: 0.8840 - val_loss: 0.3416 - val_accuracy: 0.9013\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3664 - accuracy: 0.8857 - val_loss: 0.3396 - val_accuracy: 0.9026\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3687 - accuracy: 0.8861 - val_loss: 0.4982 - val_accuracy: 0.8492\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3679 - accuracy: 0.8865 - val_loss: 0.4102 - val_accuracy: 0.8794\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3663 - accuracy: 0.8849 - val_loss: 0.3551 - val_accuracy: 0.8994\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3635 - accuracy: 0.8871 - val_loss: 0.3917 - val_accuracy: 0.8844\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3659 - accuracy: 0.8858 - val_loss: 0.3179 - val_accuracy: 0.9059\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3611 - accuracy: 0.8862 - val_loss: 0.3870 - val_accuracy: 0.8896\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3565 - accuracy: 0.8881 - val_loss: 0.3743 - val_accuracy: 0.8874\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3588 - accuracy: 0.8877 - val_loss: 0.3810 - val_accuracy: 0.8800\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3548 - accuracy: 0.8889 - val_loss: 0.3913 - val_accuracy: 0.8795\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3612 - accuracy: 0.8876 - val_loss: 0.5086 - val_accuracy: 0.8499\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3542 - accuracy: 0.8904 - val_loss: 0.3331 - val_accuracy: 0.9041\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3540 - accuracy: 0.8905 - val_loss: 0.3988 - val_accuracy: 0.8806\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3545 - accuracy: 0.8911 - val_loss: 0.4518 - val_accuracy: 0.8655\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3547 - accuracy: 0.8887 - val_loss: 0.4359 - val_accuracy: 0.8664\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3468 - accuracy: 0.8912 - val_loss: 0.3269 - val_accuracy: 0.9061\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3471 - accuracy: 0.8922 - val_loss: 0.4619 - val_accuracy: 0.8646\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3395 - accuracy: 0.8945 - val_loss: 0.3766 - val_accuracy: 0.8874\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3417 - accuracy: 0.8933 - val_loss: 0.2979 - val_accuracy: 0.9157\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3459 - accuracy: 0.8915 - val_loss: 0.4451 - val_accuracy: 0.8686\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3406 - accuracy: 0.8917 - val_loss: 0.3966 - val_accuracy: 0.8785\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3385 - accuracy: 0.8952 - val_loss: 0.3306 - val_accuracy: 0.9054\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3428 - accuracy: 0.8936 - val_loss: 0.3168 - val_accuracy: 0.9085\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3429 - accuracy: 0.8940 - val_loss: 0.3454 - val_accuracy: 0.8976\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3388 - accuracy: 0.8945 - val_loss: 0.3655 - val_accuracy: 0.8936\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3315 - accuracy: 0.8987 - val_loss: 0.5646 - val_accuracy: 0.8359\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3353 - accuracy: 0.8960 - val_loss: 0.3794 - val_accuracy: 0.8875\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3313 - accuracy: 0.8975 - val_loss: 0.4951 - val_accuracy: 0.8548\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3336 - accuracy: 0.8967 - val_loss: 0.3989 - val_accuracy: 0.8851\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3332 - accuracy: 0.8959 - val_loss: 0.3687 - val_accuracy: 0.8912\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3292 - accuracy: 0.8983 - val_loss: 0.3404 - val_accuracy: 0.9041\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3277 - accuracy: 0.8989 - val_loss: 0.3791 - val_accuracy: 0.8901\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3310 - accuracy: 0.8977 - val_loss: 0.3407 - val_accuracy: 0.8999\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3295 - accuracy: 0.8975 - val_loss: 0.2349 - val_accuracy: 0.9359\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3277 - accuracy: 0.8976 - val_loss: 0.4418 - val_accuracy: 0.8701\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3238 - accuracy: 0.8976 - val_loss: 0.2560 - val_accuracy: 0.9276\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3215 - accuracy: 0.9006 - val_loss: 0.3066 - val_accuracy: 0.9132\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3233 - accuracy: 0.8996 - val_loss: 0.3387 - val_accuracy: 0.9026\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3249 - accuracy: 0.8979 - val_loss: 0.4692 - val_accuracy: 0.8662\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3185 - accuracy: 0.9019 - val_loss: 0.4969 - val_accuracy: 0.8530\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3175 - accuracy: 0.9005 - val_loss: 0.3656 - val_accuracy: 0.8901\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3163 - accuracy: 0.9012 - val_loss: 0.3674 - val_accuracy: 0.8911\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3184 - accuracy: 0.9005 - val_loss: 0.3112 - val_accuracy: 0.9127\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3166 - accuracy: 0.9020 - val_loss: 0.2938 - val_accuracy: 0.9172\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3123 - accuracy: 0.9019 - val_loss: 0.3411 - val_accuracy: 0.9021\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3101 - accuracy: 0.9039 - val_loss: 0.4059 - val_accuracy: 0.8826\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3158 - accuracy: 0.9024 - val_loss: 0.2692 - val_accuracy: 0.9235\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3111 - accuracy: 0.9022 - val_loss: 0.5160 - val_accuracy: 0.8471\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3110 - accuracy: 0.9025 - val_loss: 0.4006 - val_accuracy: 0.8796\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3104 - accuracy: 0.9050 - val_loss: 0.3617 - val_accuracy: 0.8951\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3076 - accuracy: 0.9044 - val_loss: 0.3271 - val_accuracy: 0.9056\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3090 - accuracy: 0.9029 - val_loss: 0.3077 - val_accuracy: 0.9123\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3087 - accuracy: 0.9035 - val_loss: 0.3226 - val_accuracy: 0.9084\n",
      "Try 7/100: Best_val_acc: [0.48271116614341736, 0.8617222309112549], lr: 6.081144502868459e-05, Lambda: 5.8332833809803764e-05\n",
      "\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_408 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_409 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_410 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_411 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_412 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_413 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.5299 - accuracy: 0.1138 - val_loss: 2.3778 - val_accuracy: 0.0617\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3695 - accuracy: 0.1500 - val_loss: 2.4209 - val_accuracy: 0.0617\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2451 - accuracy: 0.1971 - val_loss: 2.3951 - val_accuracy: 0.0822\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1356 - accuracy: 0.2399 - val_loss: 2.2394 - val_accuracy: 0.1304\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0333 - accuracy: 0.2861 - val_loss: 2.2039 - val_accuracy: 0.1718\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9358 - accuracy: 0.3303 - val_loss: 2.0297 - val_accuracy: 0.2845\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.8458 - accuracy: 0.3759 - val_loss: 1.9141 - val_accuracy: 0.3406\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7629 - accuracy: 0.4151 - val_loss: 1.6970 - val_accuracy: 0.4734\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.6903 - accuracy: 0.4494 - val_loss: 1.7106 - val_accuracy: 0.4610\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6215 - accuracy: 0.4844 - val_loss: 1.4785 - val_accuracy: 0.5751\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5541 - accuracy: 0.5104 - val_loss: 1.3895 - val_accuracy: 0.5944\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4917 - accuracy: 0.5393 - val_loss: 1.3955 - val_accuracy: 0.6020\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4373 - accuracy: 0.5594 - val_loss: 1.2281 - val_accuracy: 0.6565\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3812 - accuracy: 0.5823 - val_loss: 1.2669 - val_accuracy: 0.6511\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3355 - accuracy: 0.6002 - val_loss: 1.1399 - val_accuracy: 0.6923\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2844 - accuracy: 0.6158 - val_loss: 1.1494 - val_accuracy: 0.6712\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2454 - accuracy: 0.6301 - val_loss: 1.2327 - val_accuracy: 0.6340\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2060 - accuracy: 0.6422 - val_loss: 1.0864 - val_accuracy: 0.6834\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1658 - accuracy: 0.6556 - val_loss: 1.0919 - val_accuracy: 0.6806\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1272 - accuracy: 0.6669 - val_loss: 1.0644 - val_accuracy: 0.6934\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0977 - accuracy: 0.6743 - val_loss: 0.9732 - val_accuracy: 0.7294\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0708 - accuracy: 0.6819 - val_loss: 0.9650 - val_accuracy: 0.7259\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0433 - accuracy: 0.6904 - val_loss: 1.0096 - val_accuracy: 0.7035\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0152 - accuracy: 0.6973 - val_loss: 0.8993 - val_accuracy: 0.7433\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9935 - accuracy: 0.7044 - val_loss: 0.7493 - val_accuracy: 0.7920\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9717 - accuracy: 0.7084 - val_loss: 0.7671 - val_accuracy: 0.7814\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9471 - accuracy: 0.7160 - val_loss: 0.8277 - val_accuracy: 0.7621\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9279 - accuracy: 0.7227 - val_loss: 0.8592 - val_accuracy: 0.7429\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9092 - accuracy: 0.7257 - val_loss: 0.8864 - val_accuracy: 0.7329\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8896 - accuracy: 0.7317 - val_loss: 0.7901 - val_accuracy: 0.7699\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8770 - accuracy: 0.7344 - val_loss: 0.7548 - val_accuracy: 0.7808\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8578 - accuracy: 0.7414 - val_loss: 0.7479 - val_accuracy: 0.7773\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8451 - accuracy: 0.7436 - val_loss: 0.7724 - val_accuracy: 0.7674\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8265 - accuracy: 0.7518 - val_loss: 0.6109 - val_accuracy: 0.8215\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8158 - accuracy: 0.7525 - val_loss: 0.7618 - val_accuracy: 0.7646\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8051 - accuracy: 0.7544 - val_loss: 0.6875 - val_accuracy: 0.7945\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7945 - accuracy: 0.7581 - val_loss: 0.8178 - val_accuracy: 0.7496\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7782 - accuracy: 0.7629 - val_loss: 0.6050 - val_accuracy: 0.8204\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7668 - accuracy: 0.7684 - val_loss: 0.6761 - val_accuracy: 0.7959\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7575 - accuracy: 0.7685 - val_loss: 0.8819 - val_accuracy: 0.7316\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7413 - accuracy: 0.7751 - val_loss: 0.6756 - val_accuracy: 0.7937\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7390 - accuracy: 0.7741 - val_loss: 0.6188 - val_accuracy: 0.8193\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7251 - accuracy: 0.7772 - val_loss: 0.6820 - val_accuracy: 0.7946\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7146 - accuracy: 0.7810 - val_loss: 0.6595 - val_accuracy: 0.8035\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7118 - accuracy: 0.7827 - val_loss: 0.6777 - val_accuracy: 0.7849\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6984 - accuracy: 0.7840 - val_loss: 0.6222 - val_accuracy: 0.8139\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6873 - accuracy: 0.7900 - val_loss: 0.6009 - val_accuracy: 0.8198\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6838 - accuracy: 0.7914 - val_loss: 0.6061 - val_accuracy: 0.8169\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6769 - accuracy: 0.7907 - val_loss: 0.6463 - val_accuracy: 0.7970\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6719 - accuracy: 0.7928 - val_loss: 0.5993 - val_accuracy: 0.8187\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6513 - accuracy: 0.8004 - val_loss: 0.5663 - val_accuracy: 0.8329\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6488 - accuracy: 0.8003 - val_loss: 0.5109 - val_accuracy: 0.8486\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6401 - accuracy: 0.8038 - val_loss: 0.5357 - val_accuracy: 0.8425\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6379 - accuracy: 0.8044 - val_loss: 0.5610 - val_accuracy: 0.8354\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6300 - accuracy: 0.8075 - val_loss: 0.5982 - val_accuracy: 0.8196\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6297 - accuracy: 0.8065 - val_loss: 0.5794 - val_accuracy: 0.8244\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6151 - accuracy: 0.8088 - val_loss: 0.5048 - val_accuracy: 0.8517\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6128 - accuracy: 0.8115 - val_loss: 0.5692 - val_accuracy: 0.8299\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.8130 - val_loss: 0.5664 - val_accuracy: 0.8309\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5999 - accuracy: 0.8149 - val_loss: 0.4485 - val_accuracy: 0.8666\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6038 - accuracy: 0.8130 - val_loss: 0.6479 - val_accuracy: 0.8069\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5930 - accuracy: 0.8176 - val_loss: 0.5476 - val_accuracy: 0.8302\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5847 - accuracy: 0.8196 - val_loss: 0.5301 - val_accuracy: 0.8441\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5847 - accuracy: 0.8204 - val_loss: 0.7081 - val_accuracy: 0.7893\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5774 - accuracy: 0.8227 - val_loss: 0.5153 - val_accuracy: 0.8477\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5701 - accuracy: 0.8254 - val_loss: 0.4708 - val_accuracy: 0.8643\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5702 - accuracy: 0.8245 - val_loss: 0.5910 - val_accuracy: 0.8177\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5694 - accuracy: 0.8263 - val_loss: 0.5885 - val_accuracy: 0.8187\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5556 - accuracy: 0.8294 - val_loss: 0.5013 - val_accuracy: 0.8514\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5480 - accuracy: 0.8310 - val_loss: 0.4646 - val_accuracy: 0.8609\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5515 - accuracy: 0.8307 - val_loss: 0.5136 - val_accuracy: 0.8484\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5425 - accuracy: 0.8320 - val_loss: 0.4443 - val_accuracy: 0.8704\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5379 - accuracy: 0.8344 - val_loss: 0.5267 - val_accuracy: 0.8396\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5409 - accuracy: 0.8331 - val_loss: 0.4151 - val_accuracy: 0.8816\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5347 - accuracy: 0.8349 - val_loss: 0.5905 - val_accuracy: 0.8209\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5280 - accuracy: 0.8370 - val_loss: 0.4794 - val_accuracy: 0.8544\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5275 - accuracy: 0.8373 - val_loss: 0.5570 - val_accuracy: 0.8320\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5238 - accuracy: 0.8366 - val_loss: 0.4234 - val_accuracy: 0.8776\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5246 - accuracy: 0.8369 - val_loss: 0.5007 - val_accuracy: 0.8496\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5192 - accuracy: 0.8383 - val_loss: 0.5383 - val_accuracy: 0.8374\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5140 - accuracy: 0.8402 - val_loss: 0.5223 - val_accuracy: 0.8383\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5069 - accuracy: 0.8429 - val_loss: 0.4406 - val_accuracy: 0.8698\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5025 - accuracy: 0.8448 - val_loss: 0.4163 - val_accuracy: 0.8756\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5058 - accuracy: 0.8410 - val_loss: 0.4976 - val_accuracy: 0.8461\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5033 - accuracy: 0.8452 - val_loss: 0.5436 - val_accuracy: 0.8271\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4980 - accuracy: 0.8456 - val_loss: 0.4649 - val_accuracy: 0.8622\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4948 - accuracy: 0.8460 - val_loss: 0.4602 - val_accuracy: 0.8596\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4894 - accuracy: 0.8489 - val_loss: 0.5094 - val_accuracy: 0.8439\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4879 - accuracy: 0.8484 - val_loss: 0.3845 - val_accuracy: 0.8844\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4839 - accuracy: 0.8508 - val_loss: 0.4450 - val_accuracy: 0.8676\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4855 - accuracy: 0.8498 - val_loss: 0.4172 - val_accuracy: 0.8781\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4792 - accuracy: 0.8504 - val_loss: 0.4299 - val_accuracy: 0.8718\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4805 - accuracy: 0.8507 - val_loss: 0.4219 - val_accuracy: 0.8714\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4740 - accuracy: 0.8527 - val_loss: 0.4038 - val_accuracy: 0.8832\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4679 - accuracy: 0.8561 - val_loss: 0.4028 - val_accuracy: 0.8841\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4709 - accuracy: 0.8557 - val_loss: 0.4131 - val_accuracy: 0.8759\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4683 - accuracy: 0.8543 - val_loss: 0.3768 - val_accuracy: 0.8899\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4644 - accuracy: 0.8545 - val_loss: 0.3916 - val_accuracy: 0.8869\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4643 - accuracy: 0.8564 - val_loss: 0.4339 - val_accuracy: 0.8691\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4569 - accuracy: 0.8586 - val_loss: 0.4453 - val_accuracy: 0.8661\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4520 - accuracy: 0.8598 - val_loss: 0.3796 - val_accuracy: 0.8914\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4544 - accuracy: 0.8590 - val_loss: 0.4326 - val_accuracy: 0.8744\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4507 - accuracy: 0.8617 - val_loss: 0.4331 - val_accuracy: 0.8734\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4483 - accuracy: 0.8619 - val_loss: 0.3589 - val_accuracy: 0.8976\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4497 - accuracy: 0.8595 - val_loss: 0.4202 - val_accuracy: 0.8753\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4425 - accuracy: 0.8623 - val_loss: 0.3569 - val_accuracy: 0.8940\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4408 - accuracy: 0.8627 - val_loss: 0.4016 - val_accuracy: 0.8816\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4356 - accuracy: 0.8641 - val_loss: 0.4152 - val_accuracy: 0.8774\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4355 - accuracy: 0.8653 - val_loss: 0.3759 - val_accuracy: 0.8902\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4378 - accuracy: 0.8645 - val_loss: 0.4366 - val_accuracy: 0.8723\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4294 - accuracy: 0.8665 - val_loss: 0.4988 - val_accuracy: 0.8502\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4331 - accuracy: 0.8654 - val_loss: 0.5069 - val_accuracy: 0.8511\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4343 - accuracy: 0.8650 - val_loss: 0.5421 - val_accuracy: 0.8389\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4248 - accuracy: 0.8680 - val_loss: 0.3762 - val_accuracy: 0.8889\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4235 - accuracy: 0.8690 - val_loss: 0.4431 - val_accuracy: 0.8657\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4231 - accuracy: 0.8692 - val_loss: 0.4198 - val_accuracy: 0.8774\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4258 - accuracy: 0.8686 - val_loss: 0.4172 - val_accuracy: 0.8743\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4158 - accuracy: 0.8727 - val_loss: 0.4589 - val_accuracy: 0.8622\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8704 - val_loss: 0.3947 - val_accuracy: 0.8816\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4139 - accuracy: 0.8703 - val_loss: 0.4214 - val_accuracy: 0.8757\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4143 - accuracy: 0.8699 - val_loss: 0.4148 - val_accuracy: 0.8762\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4098 - accuracy: 0.8725 - val_loss: 0.4454 - val_accuracy: 0.8680\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4112 - accuracy: 0.8716 - val_loss: 0.5362 - val_accuracy: 0.8356\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4117 - accuracy: 0.8704 - val_loss: 0.3436 - val_accuracy: 0.8994\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4043 - accuracy: 0.8741 - val_loss: 0.4528 - val_accuracy: 0.8632\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4000 - accuracy: 0.8757 - val_loss: 0.3730 - val_accuracy: 0.8899\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4024 - accuracy: 0.8736 - val_loss: 0.3913 - val_accuracy: 0.8845\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4046 - accuracy: 0.8740 - val_loss: 0.4394 - val_accuracy: 0.8697\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3974 - accuracy: 0.8768 - val_loss: 0.3610 - val_accuracy: 0.8939\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3958 - accuracy: 0.8776 - val_loss: 0.5543 - val_accuracy: 0.8336\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8760 - val_loss: 0.3756 - val_accuracy: 0.8949\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3980 - accuracy: 0.8764 - val_loss: 0.3582 - val_accuracy: 0.8936\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3925 - accuracy: 0.8774 - val_loss: 0.5535 - val_accuracy: 0.8367\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8780 - val_loss: 0.4061 - val_accuracy: 0.8801\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3851 - accuracy: 0.8813 - val_loss: 0.3501 - val_accuracy: 0.8979\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3889 - accuracy: 0.8809 - val_loss: 0.3356 - val_accuracy: 0.9011\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3817 - accuracy: 0.8821 - val_loss: 0.3469 - val_accuracy: 0.9009\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3807 - accuracy: 0.8803 - val_loss: 0.4999 - val_accuracy: 0.8496\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3811 - accuracy: 0.8811 - val_loss: 0.3813 - val_accuracy: 0.8906\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3834 - accuracy: 0.8820 - val_loss: 0.5462 - val_accuracy: 0.8341\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3810 - accuracy: 0.8814 - val_loss: 0.3412 - val_accuracy: 0.9021\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3775 - accuracy: 0.8817 - val_loss: 0.3400 - val_accuracy: 0.9029\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3751 - accuracy: 0.8852 - val_loss: 0.4608 - val_accuracy: 0.8582\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3748 - accuracy: 0.8832 - val_loss: 0.3193 - val_accuracy: 0.9099\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3677 - accuracy: 0.8861 - val_loss: 0.4084 - val_accuracy: 0.8809\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3705 - accuracy: 0.8848 - val_loss: 0.4183 - val_accuracy: 0.8766\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3707 - accuracy: 0.8846 - val_loss: 0.6446 - val_accuracy: 0.8097\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3673 - accuracy: 0.8857 - val_loss: 0.3649 - val_accuracy: 0.8940\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3599 - accuracy: 0.8881 - val_loss: 0.3770 - val_accuracy: 0.8883\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3627 - accuracy: 0.8880 - val_loss: 0.4151 - val_accuracy: 0.8797\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3626 - accuracy: 0.8893 - val_loss: 0.3120 - val_accuracy: 0.9104\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3602 - accuracy: 0.8875 - val_loss: 0.4156 - val_accuracy: 0.8761\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3623 - accuracy: 0.8862 - val_loss: 0.3274 - val_accuracy: 0.9047\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3617 - accuracy: 0.8871 - val_loss: 0.4112 - val_accuracy: 0.8807\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3598 - accuracy: 0.8870 - val_loss: 0.4105 - val_accuracy: 0.8773\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3565 - accuracy: 0.8892 - val_loss: 0.3675 - val_accuracy: 0.8932\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3558 - accuracy: 0.8893 - val_loss: 0.3507 - val_accuracy: 0.8966\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3511 - accuracy: 0.8915 - val_loss: 0.3634 - val_accuracy: 0.8932\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3522 - accuracy: 0.8920 - val_loss: 0.3477 - val_accuracy: 0.8976\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3546 - accuracy: 0.8893 - val_loss: 0.3099 - val_accuracy: 0.9129\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3493 - accuracy: 0.8909 - val_loss: 0.3343 - val_accuracy: 0.9049\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3492 - accuracy: 0.8915 - val_loss: 0.4266 - val_accuracy: 0.8764\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3474 - accuracy: 0.8925 - val_loss: 0.3336 - val_accuracy: 0.9021\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3543 - accuracy: 0.8900 - val_loss: 0.3284 - val_accuracy: 0.9065\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3441 - accuracy: 0.8945 - val_loss: 0.4140 - val_accuracy: 0.8791\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3490 - accuracy: 0.8923 - val_loss: 0.3306 - val_accuracy: 0.9067\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3423 - accuracy: 0.8930 - val_loss: 0.3646 - val_accuracy: 0.8974\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3414 - accuracy: 0.8930 - val_loss: 0.3483 - val_accuracy: 0.8981\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3424 - accuracy: 0.8927 - val_loss: 0.3438 - val_accuracy: 0.9016\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3418 - accuracy: 0.8932 - val_loss: 0.4706 - val_accuracy: 0.8636\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3364 - accuracy: 0.8941 - val_loss: 0.3655 - val_accuracy: 0.8925\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3332 - accuracy: 0.8976 - val_loss: 0.4407 - val_accuracy: 0.8761\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3374 - accuracy: 0.8959 - val_loss: 0.3244 - val_accuracy: 0.9061\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3352 - accuracy: 0.8952 - val_loss: 0.3483 - val_accuracy: 0.8971\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3353 - accuracy: 0.8959 - val_loss: 0.4039 - val_accuracy: 0.8811\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3300 - accuracy: 0.8980 - val_loss: 0.3755 - val_accuracy: 0.8887\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3281 - accuracy: 0.8979 - val_loss: 0.3356 - val_accuracy: 0.9021\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3280 - accuracy: 0.8969 - val_loss: 0.2950 - val_accuracy: 0.9169\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3256 - accuracy: 0.8994 - val_loss: 0.3319 - val_accuracy: 0.9033\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3245 - accuracy: 0.8989 - val_loss: 0.3286 - val_accuracy: 0.9044\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3267 - accuracy: 0.8982 - val_loss: 0.2953 - val_accuracy: 0.9161\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3256 - accuracy: 0.8970 - val_loss: 0.3019 - val_accuracy: 0.9127\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3241 - accuracy: 0.8995 - val_loss: 0.3545 - val_accuracy: 0.8947\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3211 - accuracy: 0.9005 - val_loss: 0.3769 - val_accuracy: 0.8919\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3190 - accuracy: 0.8993 - val_loss: 0.3563 - val_accuracy: 0.8960\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3193 - accuracy: 0.8995 - val_loss: 0.4330 - val_accuracy: 0.8717\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3146 - accuracy: 0.9019 - val_loss: 0.3743 - val_accuracy: 0.8900\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3149 - accuracy: 0.9020 - val_loss: 0.3403 - val_accuracy: 0.9002\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3159 - accuracy: 0.9013 - val_loss: 0.3462 - val_accuracy: 0.9007\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3205 - accuracy: 0.8984 - val_loss: 0.4250 - val_accuracy: 0.8808\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3183 - accuracy: 0.9009 - val_loss: 0.3626 - val_accuracy: 0.8951\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3071 - accuracy: 0.9038 - val_loss: 0.2913 - val_accuracy: 0.9185\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3115 - accuracy: 0.9032 - val_loss: 0.4529 - val_accuracy: 0.8690\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3120 - accuracy: 0.9032 - val_loss: 0.3434 - val_accuracy: 0.8979\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3109 - accuracy: 0.9026 - val_loss: 0.2900 - val_accuracy: 0.9137\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3078 - accuracy: 0.9034 - val_loss: 0.3738 - val_accuracy: 0.8934\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3124 - accuracy: 0.9040 - val_loss: 0.3360 - val_accuracy: 0.9003\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3111 - accuracy: 0.9024 - val_loss: 0.4089 - val_accuracy: 0.8830\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3053 - accuracy: 0.9047 - val_loss: 0.3222 - val_accuracy: 0.9077\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3021 - accuracy: 0.9056 - val_loss: 0.4225 - val_accuracy: 0.8799\n",
      "Try 8/100: Best_val_acc: [0.529062807559967, 0.8514444231987], lr: 6.035902455476381e-05, Lambda: 5.8422541652451445e-05\n",
      "\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_414 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_415 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_416 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_417 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_418 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_419 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6219 - accuracy: 0.1128 - val_loss: 2.2370 - val_accuracy: 0.1269\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4487 - accuracy: 0.1395 - val_loss: 2.2051 - val_accuracy: 0.1519\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3131 - accuracy: 0.1727 - val_loss: 2.1382 - val_accuracy: 0.1937\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1872 - accuracy: 0.2159 - val_loss: 2.0374 - val_accuracy: 0.2692\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0610 - accuracy: 0.2666 - val_loss: 1.9003 - val_accuracy: 0.3856\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9430 - accuracy: 0.3218 - val_loss: 1.8004 - val_accuracy: 0.4468\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8392 - accuracy: 0.3738 - val_loss: 1.6666 - val_accuracy: 0.5021\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7479 - accuracy: 0.4175 - val_loss: 1.5396 - val_accuracy: 0.5766\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6660 - accuracy: 0.4589 - val_loss: 1.4250 - val_accuracy: 0.6411\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5923 - accuracy: 0.4917 - val_loss: 1.3817 - val_accuracy: 0.6383\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5223 - accuracy: 0.5209 - val_loss: 1.3597 - val_accuracy: 0.6509\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4619 - accuracy: 0.5460 - val_loss: 1.2417 - val_accuracy: 0.6947\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4089 - accuracy: 0.5687 - val_loss: 1.2370 - val_accuracy: 0.6880\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3583 - accuracy: 0.5871 - val_loss: 1.2679 - val_accuracy: 0.6653\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3132 - accuracy: 0.6035 - val_loss: 1.0889 - val_accuracy: 0.7251\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2638 - accuracy: 0.6203 - val_loss: 1.1577 - val_accuracy: 0.6874\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2321 - accuracy: 0.6300 - val_loss: 1.1331 - val_accuracy: 0.6881\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1894 - accuracy: 0.6467 - val_loss: 0.9928 - val_accuracy: 0.7380\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1566 - accuracy: 0.6531 - val_loss: 1.0085 - val_accuracy: 0.7274\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1256 - accuracy: 0.6640 - val_loss: 0.9099 - val_accuracy: 0.7597\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0969 - accuracy: 0.6742 - val_loss: 0.9417 - val_accuracy: 0.7581\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0677 - accuracy: 0.6816 - val_loss: 0.9550 - val_accuracy: 0.7320\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0413 - accuracy: 0.6904 - val_loss: 0.9023 - val_accuracy: 0.7574\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 1.0181 - accuracy: 0.6945 - val_loss: 0.9971 - val_accuracy: 0.7059\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9920 - accuracy: 0.7043 - val_loss: 0.8692 - val_accuracy: 0.7636\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9709 - accuracy: 0.7107 - val_loss: 0.8067 - val_accuracy: 0.7816\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9521 - accuracy: 0.7146 - val_loss: 0.9121 - val_accuracy: 0.7359\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9330 - accuracy: 0.7194 - val_loss: 0.7922 - val_accuracy: 0.7777\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9085 - accuracy: 0.7259 - val_loss: 0.7841 - val_accuracy: 0.7766\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8924 - accuracy: 0.7301 - val_loss: 0.8462 - val_accuracy: 0.7608\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8728 - accuracy: 0.7372 - val_loss: 0.8215 - val_accuracy: 0.7632\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8587 - accuracy: 0.7394 - val_loss: 0.8279 - val_accuracy: 0.7604\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8467 - accuracy: 0.7437 - val_loss: 0.8066 - val_accuracy: 0.7658\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.8315 - accuracy: 0.7493 - val_loss: 0.7217 - val_accuracy: 0.7942\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8170 - accuracy: 0.7527 - val_loss: 0.6641 - val_accuracy: 0.8089\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8072 - accuracy: 0.7524 - val_loss: 0.7056 - val_accuracy: 0.8001\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7952 - accuracy: 0.7597 - val_loss: 0.6312 - val_accuracy: 0.8219\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7843 - accuracy: 0.7621 - val_loss: 0.7179 - val_accuracy: 0.7907\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7668 - accuracy: 0.7646 - val_loss: 0.7955 - val_accuracy: 0.7564\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7580 - accuracy: 0.7684 - val_loss: 0.6209 - val_accuracy: 0.8239\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7487 - accuracy: 0.7713 - val_loss: 0.6715 - val_accuracy: 0.8036\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7423 - accuracy: 0.7727 - val_loss: 0.7081 - val_accuracy: 0.7916\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7268 - accuracy: 0.7764 - val_loss: 0.5357 - val_accuracy: 0.8436\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7138 - accuracy: 0.7807 - val_loss: 0.5878 - val_accuracy: 0.8296\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7068 - accuracy: 0.7840 - val_loss: 0.6403 - val_accuracy: 0.8103\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6980 - accuracy: 0.7860 - val_loss: 0.6312 - val_accuracy: 0.8122\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6943 - accuracy: 0.7856 - val_loss: 0.6262 - val_accuracy: 0.8144\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6807 - accuracy: 0.7923 - val_loss: 0.6225 - val_accuracy: 0.8196\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6770 - accuracy: 0.7902 - val_loss: 0.5558 - val_accuracy: 0.8336\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6646 - accuracy: 0.7960 - val_loss: 0.6096 - val_accuracy: 0.8220\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6571 - accuracy: 0.7966 - val_loss: 0.5633 - val_accuracy: 0.8303\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6566 - accuracy: 0.7976 - val_loss: 0.5975 - val_accuracy: 0.8236\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6449 - accuracy: 0.7995 - val_loss: 0.5141 - val_accuracy: 0.8483\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6419 - accuracy: 0.8035 - val_loss: 0.6102 - val_accuracy: 0.8166\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6309 - accuracy: 0.8057 - val_loss: 0.5666 - val_accuracy: 0.8351\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6296 - accuracy: 0.8053 - val_loss: 0.5676 - val_accuracy: 0.8289\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6204 - accuracy: 0.8090 - val_loss: 0.6356 - val_accuracy: 0.8091\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6143 - accuracy: 0.8120 - val_loss: 0.5268 - val_accuracy: 0.8458\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.6124 - accuracy: 0.8127 - val_loss: 0.4879 - val_accuracy: 0.8582\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5992 - accuracy: 0.8152 - val_loss: 0.5886 - val_accuracy: 0.8235\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6044 - accuracy: 0.8113 - val_loss: 0.5569 - val_accuracy: 0.8351\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5901 - accuracy: 0.8189 - val_loss: 0.7627 - val_accuracy: 0.7654\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5974 - accuracy: 0.8152 - val_loss: 0.6205 - val_accuracy: 0.8084\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5850 - accuracy: 0.8190 - val_loss: 0.5258 - val_accuracy: 0.8442\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5788 - accuracy: 0.8207 - val_loss: 0.5778 - val_accuracy: 0.8264\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5760 - accuracy: 0.8222 - val_loss: 0.5928 - val_accuracy: 0.8204\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5681 - accuracy: 0.8240 - val_loss: 0.8660 - val_accuracy: 0.7346\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5697 - accuracy: 0.8241 - val_loss: 0.6597 - val_accuracy: 0.7829\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5622 - accuracy: 0.8275 - val_loss: 0.4831 - val_accuracy: 0.8499\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5566 - accuracy: 0.8257 - val_loss: 0.6721 - val_accuracy: 0.7954\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5512 - accuracy: 0.8284 - val_loss: 0.5133 - val_accuracy: 0.8417\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5511 - accuracy: 0.8283 - val_loss: 0.5853 - val_accuracy: 0.8266\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5466 - accuracy: 0.8317 - val_loss: 0.5387 - val_accuracy: 0.8336\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5438 - accuracy: 0.8327 - val_loss: 0.4931 - val_accuracy: 0.8569\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5392 - accuracy: 0.8346 - val_loss: 0.5157 - val_accuracy: 0.8474\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5283 - accuracy: 0.8370 - val_loss: 0.5538 - val_accuracy: 0.8344\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5293 - accuracy: 0.8371 - val_loss: 0.4539 - val_accuracy: 0.8694\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5232 - accuracy: 0.8374 - val_loss: 0.5500 - val_accuracy: 0.8361\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5229 - accuracy: 0.8399 - val_loss: 0.5578 - val_accuracy: 0.8353\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5170 - accuracy: 0.8396 - val_loss: 0.4082 - val_accuracy: 0.8832\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5148 - accuracy: 0.8404 - val_loss: 0.5218 - val_accuracy: 0.8419\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5126 - accuracy: 0.8399 - val_loss: 0.4525 - val_accuracy: 0.8651\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5114 - accuracy: 0.8423 - val_loss: 0.4457 - val_accuracy: 0.8686\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5052 - accuracy: 0.8442 - val_loss: 0.4500 - val_accuracy: 0.8685\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4991 - accuracy: 0.8456 - val_loss: 0.4446 - val_accuracy: 0.8676\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4944 - accuracy: 0.8477 - val_loss: 0.4365 - val_accuracy: 0.8714\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4949 - accuracy: 0.8489 - val_loss: 0.4043 - val_accuracy: 0.8819\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4981 - accuracy: 0.8480 - val_loss: 0.5056 - val_accuracy: 0.8484\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4915 - accuracy: 0.8469 - val_loss: 0.3732 - val_accuracy: 0.8909\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4885 - accuracy: 0.8473 - val_loss: 0.5001 - val_accuracy: 0.8507\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4835 - accuracy: 0.8515 - val_loss: 0.4349 - val_accuracy: 0.8674\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4807 - accuracy: 0.8515 - val_loss: 0.5154 - val_accuracy: 0.8368\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4763 - accuracy: 0.8511 - val_loss: 0.4959 - val_accuracy: 0.8514\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4727 - accuracy: 0.8533 - val_loss: 0.4786 - val_accuracy: 0.8514\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4755 - accuracy: 0.8518 - val_loss: 0.4632 - val_accuracy: 0.8618\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4684 - accuracy: 0.8556 - val_loss: 0.4642 - val_accuracy: 0.8629\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4683 - accuracy: 0.8546 - val_loss: 0.4349 - val_accuracy: 0.8744\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4702 - accuracy: 0.8550 - val_loss: 0.4597 - val_accuracy: 0.8605\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4653 - accuracy: 0.8555 - val_loss: 0.4277 - val_accuracy: 0.8744\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4583 - accuracy: 0.8579 - val_loss: 0.4639 - val_accuracy: 0.8602\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4597 - accuracy: 0.8574 - val_loss: 0.3892 - val_accuracy: 0.8879\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4548 - accuracy: 0.8580 - val_loss: 0.4217 - val_accuracy: 0.8752\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4536 - accuracy: 0.8606 - val_loss: 0.4522 - val_accuracy: 0.8601\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4451 - accuracy: 0.8612 - val_loss: 0.4376 - val_accuracy: 0.8706\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4500 - accuracy: 0.8602 - val_loss: 0.5015 - val_accuracy: 0.8498\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4498 - accuracy: 0.8588 - val_loss: 0.3661 - val_accuracy: 0.8916\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4436 - accuracy: 0.8607 - val_loss: 0.4759 - val_accuracy: 0.8602\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4402 - accuracy: 0.8635 - val_loss: 0.3910 - val_accuracy: 0.8859\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4376 - accuracy: 0.8638 - val_loss: 0.5373 - val_accuracy: 0.8405\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4391 - accuracy: 0.8636 - val_loss: 0.4366 - val_accuracy: 0.8747\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4334 - accuracy: 0.8649 - val_loss: 0.4791 - val_accuracy: 0.8581\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4332 - accuracy: 0.8655 - val_loss: 0.5139 - val_accuracy: 0.8342\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4313 - accuracy: 0.8665 - val_loss: 0.4554 - val_accuracy: 0.8625\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4315 - accuracy: 0.8640 - val_loss: 0.5134 - val_accuracy: 0.8386\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4273 - accuracy: 0.8676 - val_loss: 0.3953 - val_accuracy: 0.8849\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4243 - accuracy: 0.8672 - val_loss: 0.4036 - val_accuracy: 0.8792\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4216 - accuracy: 0.8685 - val_loss: 0.3607 - val_accuracy: 0.8956\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4186 - accuracy: 0.8688 - val_loss: 0.4604 - val_accuracy: 0.8673\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4226 - accuracy: 0.8684 - val_loss: 0.4881 - val_accuracy: 0.8562\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8702 - val_loss: 0.4325 - val_accuracy: 0.8731\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4144 - accuracy: 0.8719 - val_loss: 0.3726 - val_accuracy: 0.8905\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4181 - accuracy: 0.8683 - val_loss: 0.3862 - val_accuracy: 0.8844\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4119 - accuracy: 0.8713 - val_loss: 0.3030 - val_accuracy: 0.9167\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4133 - accuracy: 0.8710 - val_loss: 0.5944 - val_accuracy: 0.8206\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4043 - accuracy: 0.8750 - val_loss: 0.4575 - val_accuracy: 0.8662\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4085 - accuracy: 0.8725 - val_loss: 0.3513 - val_accuracy: 0.8984\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4041 - accuracy: 0.8748 - val_loss: 0.3432 - val_accuracy: 0.9021\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3971 - accuracy: 0.8767 - val_loss: 0.5395 - val_accuracy: 0.8348\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4001 - accuracy: 0.8759 - val_loss: 0.4020 - val_accuracy: 0.8835\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3994 - accuracy: 0.8763 - val_loss: 0.4563 - val_accuracy: 0.8650\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8770 - val_loss: 0.4479 - val_accuracy: 0.8693\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8748 - val_loss: 0.3935 - val_accuracy: 0.8847\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3941 - accuracy: 0.8770 - val_loss: 0.4185 - val_accuracy: 0.8762\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3908 - accuracy: 0.8793 - val_loss: 0.2769 - val_accuracy: 0.9227\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3885 - accuracy: 0.8789 - val_loss: 0.5319 - val_accuracy: 0.8379\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3856 - accuracy: 0.8786 - val_loss: 0.3510 - val_accuracy: 0.8989\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3836 - accuracy: 0.8801 - val_loss: 0.3390 - val_accuracy: 0.9025\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3833 - accuracy: 0.8825 - val_loss: 0.4178 - val_accuracy: 0.8796\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3872 - accuracy: 0.8793 - val_loss: 0.3554 - val_accuracy: 0.8995\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3774 - accuracy: 0.8807 - val_loss: 0.4029 - val_accuracy: 0.8823\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3778 - accuracy: 0.8822 - val_loss: 0.4735 - val_accuracy: 0.8586\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3739 - accuracy: 0.8842 - val_loss: 0.3328 - val_accuracy: 0.9036\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3732 - accuracy: 0.8834 - val_loss: 0.3783 - val_accuracy: 0.8878\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3757 - accuracy: 0.8824 - val_loss: 0.5257 - val_accuracy: 0.8463\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3771 - accuracy: 0.8809 - val_loss: 0.4575 - val_accuracy: 0.8606\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3742 - accuracy: 0.8821 - val_loss: 0.3590 - val_accuracy: 0.8899\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3663 - accuracy: 0.8870 - val_loss: 0.4362 - val_accuracy: 0.8771\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3664 - accuracy: 0.8850 - val_loss: 0.3473 - val_accuracy: 0.8958\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3705 - accuracy: 0.8855 - val_loss: 0.5700 - val_accuracy: 0.8259\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3655 - accuracy: 0.8861 - val_loss: 0.3918 - val_accuracy: 0.8827\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3626 - accuracy: 0.8868 - val_loss: 0.4448 - val_accuracy: 0.8664\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3561 - accuracy: 0.8884 - val_loss: 0.3929 - val_accuracy: 0.8882\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3640 - accuracy: 0.8869 - val_loss: 0.4157 - val_accuracy: 0.8736\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3601 - accuracy: 0.8874 - val_loss: 0.3556 - val_accuracy: 0.8968\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3657 - accuracy: 0.8861 - val_loss: 0.4858 - val_accuracy: 0.8649\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3594 - accuracy: 0.8880 - val_loss: 0.3284 - val_accuracy: 0.9036\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3525 - accuracy: 0.8908 - val_loss: 0.3163 - val_accuracy: 0.9109\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3525 - accuracy: 0.8902 - val_loss: 0.3616 - val_accuracy: 0.8932\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3536 - accuracy: 0.8888 - val_loss: 0.3503 - val_accuracy: 0.8974\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3511 - accuracy: 0.8915 - val_loss: 0.3201 - val_accuracy: 0.9056\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3504 - accuracy: 0.8904 - val_loss: 0.3550 - val_accuracy: 0.8989\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3543 - accuracy: 0.8887 - val_loss: 0.3401 - val_accuracy: 0.8904\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3470 - accuracy: 0.8917 - val_loss: 0.3755 - val_accuracy: 0.8926\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3466 - accuracy: 0.8924 - val_loss: 0.3332 - val_accuracy: 0.9029\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3450 - accuracy: 0.8926 - val_loss: 0.3493 - val_accuracy: 0.9002\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3421 - accuracy: 0.8924 - val_loss: 0.4637 - val_accuracy: 0.8636\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3429 - accuracy: 0.8925 - val_loss: 0.4929 - val_accuracy: 0.8548\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3441 - accuracy: 0.8920 - val_loss: 0.4891 - val_accuracy: 0.8592\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3413 - accuracy: 0.8930 - val_loss: 0.3798 - val_accuracy: 0.8908\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3409 - accuracy: 0.8940 - val_loss: 0.3895 - val_accuracy: 0.8824\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3375 - accuracy: 0.8953 - val_loss: 0.2476 - val_accuracy: 0.9326\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3351 - accuracy: 0.8951 - val_loss: 0.3705 - val_accuracy: 0.8907\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3352 - accuracy: 0.8957 - val_loss: 0.4107 - val_accuracy: 0.8782\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3356 - accuracy: 0.8942 - val_loss: 0.4011 - val_accuracy: 0.8785\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3306 - accuracy: 0.8967 - val_loss: 0.3027 - val_accuracy: 0.9156\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3314 - accuracy: 0.8946 - val_loss: 0.3685 - val_accuracy: 0.8888\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3295 - accuracy: 0.8974 - val_loss: 0.3637 - val_accuracy: 0.8924\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3296 - accuracy: 0.8961 - val_loss: 0.3828 - val_accuracy: 0.8912\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3323 - accuracy: 0.8962 - val_loss: 0.3533 - val_accuracy: 0.8936\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3292 - accuracy: 0.8977 - val_loss: 0.3624 - val_accuracy: 0.8894\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3247 - accuracy: 0.8988 - val_loss: 0.3222 - val_accuracy: 0.9067\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3274 - accuracy: 0.8979 - val_loss: 0.4013 - val_accuracy: 0.8821\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3213 - accuracy: 0.8992 - val_loss: 0.3349 - val_accuracy: 0.9028\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3207 - accuracy: 0.8985 - val_loss: 0.3457 - val_accuracy: 0.8992\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3232 - accuracy: 0.8999 - val_loss: 0.3727 - val_accuracy: 0.8897\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3247 - accuracy: 0.8965 - val_loss: 0.2467 - val_accuracy: 0.9313\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3215 - accuracy: 0.8976 - val_loss: 0.4176 - val_accuracy: 0.8756\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3140 - accuracy: 0.9025 - val_loss: 0.3240 - val_accuracy: 0.9071\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3167 - accuracy: 0.9007 - val_loss: 0.3601 - val_accuracy: 0.8974\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3113 - accuracy: 0.9038 - val_loss: 0.3510 - val_accuracy: 0.8994\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3121 - accuracy: 0.9020 - val_loss: 0.3429 - val_accuracy: 0.8999\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3127 - accuracy: 0.9023 - val_loss: 0.3646 - val_accuracy: 0.8940\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3091 - accuracy: 0.9025 - val_loss: 0.4081 - val_accuracy: 0.8801\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3117 - accuracy: 0.9024 - val_loss: 0.4250 - val_accuracy: 0.8760\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3082 - accuracy: 0.9036 - val_loss: 0.3143 - val_accuracy: 0.9113\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3079 - accuracy: 0.9033 - val_loss: 0.2752 - val_accuracy: 0.9202\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3067 - accuracy: 0.9040 - val_loss: 0.3765 - val_accuracy: 0.8871\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3074 - accuracy: 0.9054 - val_loss: 0.3139 - val_accuracy: 0.9090\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3043 - accuracy: 0.9053 - val_loss: 0.3623 - val_accuracy: 0.8953\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3087 - accuracy: 0.9040 - val_loss: 0.4989 - val_accuracy: 0.8485\n",
      "Try 9/100: Best_val_acc: [0.5771573781967163, 0.831333339214325], lr: 6.130568494488568e-05, Lambda: 5.7609014931736495e-05\n",
      "\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_420 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_421 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_422 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_423 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_424 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_425 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6009 - accuracy: 0.1086 - val_loss: 2.3226 - val_accuracy: 0.1139\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4355 - accuracy: 0.1387 - val_loss: 2.2463 - val_accuracy: 0.1152\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3120 - accuracy: 0.1741 - val_loss: 2.1767 - val_accuracy: 0.1705\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1835 - accuracy: 0.2231 - val_loss: 2.1160 - val_accuracy: 0.2258\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0720 - accuracy: 0.2718 - val_loss: 2.0168 - val_accuracy: 0.2984\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9729 - accuracy: 0.3155 - val_loss: 1.8655 - val_accuracy: 0.3719\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8732 - accuracy: 0.3613 - val_loss: 1.7729 - val_accuracy: 0.4226\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7850 - accuracy: 0.4034 - val_loss: 1.5828 - val_accuracy: 0.5541\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7084 - accuracy: 0.4399 - val_loss: 1.5295 - val_accuracy: 0.5729\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6284 - accuracy: 0.4734 - val_loss: 1.4163 - val_accuracy: 0.6357\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5605 - accuracy: 0.5031 - val_loss: 1.3535 - val_accuracy: 0.6275\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4907 - accuracy: 0.5325 - val_loss: 1.3059 - val_accuracy: 0.6664\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4304 - accuracy: 0.5610 - val_loss: 1.1941 - val_accuracy: 0.7038\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3741 - accuracy: 0.5815 - val_loss: 1.1907 - val_accuracy: 0.6794\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3187 - accuracy: 0.6044 - val_loss: 1.1122 - val_accuracy: 0.7075\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2726 - accuracy: 0.6196 - val_loss: 1.0062 - val_accuracy: 0.7376\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2300 - accuracy: 0.6317 - val_loss: 1.0224 - val_accuracy: 0.7289\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1867 - accuracy: 0.6458 - val_loss: 1.0287 - val_accuracy: 0.7227\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1490 - accuracy: 0.6582 - val_loss: 0.9069 - val_accuracy: 0.7623\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1133 - accuracy: 0.6707 - val_loss: 1.0115 - val_accuracy: 0.7208\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0858 - accuracy: 0.6763 - val_loss: 1.0268 - val_accuracy: 0.6984\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0523 - accuracy: 0.6873 - val_loss: 0.8612 - val_accuracy: 0.7696\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0325 - accuracy: 0.6905 - val_loss: 0.9188 - val_accuracy: 0.7445\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0001 - accuracy: 0.7023 - val_loss: 0.9147 - val_accuracy: 0.7381\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.9757 - accuracy: 0.7075 - val_loss: 0.7624 - val_accuracy: 0.7897\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9565 - accuracy: 0.7122 - val_loss: 0.8882 - val_accuracy: 0.7497\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9338 - accuracy: 0.7203 - val_loss: 0.8252 - val_accuracy: 0.7673\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9113 - accuracy: 0.7274 - val_loss: 0.6885 - val_accuracy: 0.8103\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8963 - accuracy: 0.7298 - val_loss: 0.9029 - val_accuracy: 0.7313\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8798 - accuracy: 0.7333 - val_loss: 0.8238 - val_accuracy: 0.7603\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8673 - accuracy: 0.7356 - val_loss: 0.7799 - val_accuracy: 0.7750\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8499 - accuracy: 0.7415 - val_loss: 0.6841 - val_accuracy: 0.7945\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8358 - accuracy: 0.7482 - val_loss: 0.6780 - val_accuracy: 0.8060\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8214 - accuracy: 0.7498 - val_loss: 0.7507 - val_accuracy: 0.7776\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8111 - accuracy: 0.7554 - val_loss: 0.7553 - val_accuracy: 0.7689\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7935 - accuracy: 0.7598 - val_loss: 0.6447 - val_accuracy: 0.8134\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7846 - accuracy: 0.7597 - val_loss: 0.6818 - val_accuracy: 0.8050\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7734 - accuracy: 0.7661 - val_loss: 0.6535 - val_accuracy: 0.8022\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7561 - accuracy: 0.7705 - val_loss: 0.5814 - val_accuracy: 0.8299\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7473 - accuracy: 0.7715 - val_loss: 0.6909 - val_accuracy: 0.7894\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7372 - accuracy: 0.7748 - val_loss: 0.6393 - val_accuracy: 0.8146\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7295 - accuracy: 0.7767 - val_loss: 0.7289 - val_accuracy: 0.7839\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7171 - accuracy: 0.7803 - val_loss: 0.6056 - val_accuracy: 0.8292\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7110 - accuracy: 0.7811 - val_loss: 0.6861 - val_accuracy: 0.7961\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.7016 - accuracy: 0.7833 - val_loss: 0.6025 - val_accuracy: 0.8195\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6959 - accuracy: 0.7866 - val_loss: 0.7651 - val_accuracy: 0.7715\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6859 - accuracy: 0.7892 - val_loss: 0.6754 - val_accuracy: 0.7991\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6792 - accuracy: 0.7897 - val_loss: 0.5939 - val_accuracy: 0.8247\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6707 - accuracy: 0.7922 - val_loss: 0.6000 - val_accuracy: 0.8183\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6598 - accuracy: 0.7972 - val_loss: 0.5921 - val_accuracy: 0.8277\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6587 - accuracy: 0.7960 - val_loss: 0.6989 - val_accuracy: 0.7904\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6511 - accuracy: 0.7988 - val_loss: 0.5656 - val_accuracy: 0.8349\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6417 - accuracy: 0.8040 - val_loss: 0.6207 - val_accuracy: 0.8178\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6352 - accuracy: 0.8024 - val_loss: 0.5429 - val_accuracy: 0.8424\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6259 - accuracy: 0.8061 - val_loss: 0.5275 - val_accuracy: 0.8504\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6275 - accuracy: 0.8058 - val_loss: 0.5860 - val_accuracy: 0.8237\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6176 - accuracy: 0.8097 - val_loss: 0.4869 - val_accuracy: 0.8559\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6115 - accuracy: 0.8111 - val_loss: 0.5384 - val_accuracy: 0.8381\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5996 - accuracy: 0.8148 - val_loss: 0.5457 - val_accuracy: 0.8416\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5996 - accuracy: 0.8154 - val_loss: 0.5313 - val_accuracy: 0.8367\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5897 - accuracy: 0.8195 - val_loss: 0.5580 - val_accuracy: 0.8319\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5915 - accuracy: 0.8177 - val_loss: 0.5945 - val_accuracy: 0.8176\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5856 - accuracy: 0.8190 - val_loss: 0.6582 - val_accuracy: 0.8047\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5784 - accuracy: 0.8210 - val_loss: 0.5433 - val_accuracy: 0.8404\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5748 - accuracy: 0.8229 - val_loss: 0.6444 - val_accuracy: 0.8110\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5687 - accuracy: 0.8237 - val_loss: 0.5420 - val_accuracy: 0.8391\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5647 - accuracy: 0.8243 - val_loss: 0.6138 - val_accuracy: 0.8183\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5583 - accuracy: 0.8267 - val_loss: 0.5443 - val_accuracy: 0.8372\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5560 - accuracy: 0.8290 - val_loss: 0.5411 - val_accuracy: 0.8331\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5539 - accuracy: 0.8295 - val_loss: 0.5795 - val_accuracy: 0.8242\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5492 - accuracy: 0.8289 - val_loss: 0.4948 - val_accuracy: 0.8532\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5441 - accuracy: 0.8311 - val_loss: 0.4365 - val_accuracy: 0.8709\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5428 - accuracy: 0.8312 - val_loss: 0.4137 - val_accuracy: 0.8789\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5381 - accuracy: 0.8340 - val_loss: 0.4918 - val_accuracy: 0.8491\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5370 - accuracy: 0.8346 - val_loss: 0.4796 - val_accuracy: 0.8581\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5296 - accuracy: 0.8370 - val_loss: 0.4742 - val_accuracy: 0.8581\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5279 - accuracy: 0.8353 - val_loss: 0.4499 - val_accuracy: 0.8601\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5253 - accuracy: 0.8372 - val_loss: 0.6048 - val_accuracy: 0.8173\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5180 - accuracy: 0.8402 - val_loss: 0.5812 - val_accuracy: 0.8289\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5154 - accuracy: 0.8395 - val_loss: 0.4343 - val_accuracy: 0.8704\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5127 - accuracy: 0.8410 - val_loss: 0.3958 - val_accuracy: 0.8864\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5028 - accuracy: 0.8430 - val_loss: 0.5365 - val_accuracy: 0.8344\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5101 - accuracy: 0.8418 - val_loss: 0.4290 - val_accuracy: 0.8724\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5020 - accuracy: 0.8444 - val_loss: 0.4150 - val_accuracy: 0.8787\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5035 - accuracy: 0.8427 - val_loss: 0.4911 - val_accuracy: 0.8514\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4961 - accuracy: 0.8457 - val_loss: 0.4208 - val_accuracy: 0.8721\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4948 - accuracy: 0.8480 - val_loss: 0.4737 - val_accuracy: 0.8588\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4895 - accuracy: 0.8482 - val_loss: 0.4161 - val_accuracy: 0.8764\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8488 - val_loss: 0.3932 - val_accuracy: 0.8823\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4869 - accuracy: 0.8492 - val_loss: 0.3660 - val_accuracy: 0.8942\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4856 - accuracy: 0.8496 - val_loss: 0.4583 - val_accuracy: 0.8644\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4872 - accuracy: 0.8486 - val_loss: 0.3849 - val_accuracy: 0.8902\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4726 - accuracy: 0.8528 - val_loss: 0.4161 - val_accuracy: 0.8798\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4720 - accuracy: 0.8531 - val_loss: 0.3837 - val_accuracy: 0.8888\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4772 - accuracy: 0.8531 - val_loss: 0.4811 - val_accuracy: 0.8536\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4650 - accuracy: 0.8566 - val_loss: 0.4092 - val_accuracy: 0.8735\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4649 - accuracy: 0.8565 - val_loss: 0.5303 - val_accuracy: 0.8420\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4614 - accuracy: 0.8564 - val_loss: 0.4487 - val_accuracy: 0.8686\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4574 - accuracy: 0.8582 - val_loss: 0.4348 - val_accuracy: 0.8726\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4568 - accuracy: 0.8575 - val_loss: 0.4692 - val_accuracy: 0.8586\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4580 - accuracy: 0.8581 - val_loss: 0.4229 - val_accuracy: 0.8740\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4537 - accuracy: 0.8593 - val_loss: 0.4277 - val_accuracy: 0.8738\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4541 - accuracy: 0.8595 - val_loss: 0.4398 - val_accuracy: 0.8691\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4492 - accuracy: 0.8603 - val_loss: 0.5213 - val_accuracy: 0.8409\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4444 - accuracy: 0.8629 - val_loss: 0.4339 - val_accuracy: 0.8655\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4462 - accuracy: 0.8621 - val_loss: 0.4552 - val_accuracy: 0.8636\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4422 - accuracy: 0.8624 - val_loss: 0.3676 - val_accuracy: 0.8938\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4338 - accuracy: 0.8664 - val_loss: 0.4020 - val_accuracy: 0.8807\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.4353 - accuracy: 0.8650 - val_loss: 0.3958 - val_accuracy: 0.8847\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4345 - accuracy: 0.8654 - val_loss: 0.5061 - val_accuracy: 0.8469\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4303 - accuracy: 0.8675 - val_loss: 0.3684 - val_accuracy: 0.8928\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4306 - accuracy: 0.8654 - val_loss: 0.4733 - val_accuracy: 0.8484\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4294 - accuracy: 0.8672 - val_loss: 0.3951 - val_accuracy: 0.8842\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4260 - accuracy: 0.8680 - val_loss: 0.4160 - val_accuracy: 0.8759\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4239 - accuracy: 0.8680 - val_loss: 0.4504 - val_accuracy: 0.8632\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4254 - accuracy: 0.8686 - val_loss: 0.3625 - val_accuracy: 0.8958\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4216 - accuracy: 0.8676 - val_loss: 0.4123 - val_accuracy: 0.8775\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4212 - accuracy: 0.8673 - val_loss: 0.3595 - val_accuracy: 0.8960\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4138 - accuracy: 0.8726 - val_loss: 0.5043 - val_accuracy: 0.8436\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8700 - val_loss: 0.4688 - val_accuracy: 0.8589\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4127 - accuracy: 0.8715 - val_loss: 0.4262 - val_accuracy: 0.8732\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4115 - accuracy: 0.8718 - val_loss: 0.3780 - val_accuracy: 0.8871\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8740 - val_loss: 0.3540 - val_accuracy: 0.8976\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4115 - accuracy: 0.8715 - val_loss: 0.3529 - val_accuracy: 0.8966\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4029 - accuracy: 0.8758 - val_loss: 0.3580 - val_accuracy: 0.8934\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8743 - val_loss: 0.4536 - val_accuracy: 0.8626\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3995 - accuracy: 0.8767 - val_loss: 0.4249 - val_accuracy: 0.8708\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3929 - accuracy: 0.8781 - val_loss: 0.3360 - val_accuracy: 0.9037\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3974 - accuracy: 0.8778 - val_loss: 0.3927 - val_accuracy: 0.8873\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3967 - accuracy: 0.8773 - val_loss: 0.3116 - val_accuracy: 0.9104\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3971 - accuracy: 0.8766 - val_loss: 0.4303 - val_accuracy: 0.8712\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3918 - accuracy: 0.8801 - val_loss: 0.3779 - val_accuracy: 0.8887\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3919 - accuracy: 0.8783 - val_loss: 0.4585 - val_accuracy: 0.8619\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3841 - accuracy: 0.8806 - val_loss: 0.4648 - val_accuracy: 0.8639\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3912 - accuracy: 0.8770 - val_loss: 0.3550 - val_accuracy: 0.8980\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3808 - accuracy: 0.8795 - val_loss: 0.4517 - val_accuracy: 0.8610\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3854 - accuracy: 0.8797 - val_loss: 0.3967 - val_accuracy: 0.8874\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3803 - accuracy: 0.8812 - val_loss: 0.4318 - val_accuracy: 0.8716\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3800 - accuracy: 0.8810 - val_loss: 0.3262 - val_accuracy: 0.9071\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3798 - accuracy: 0.8828 - val_loss: 0.4594 - val_accuracy: 0.8639\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3819 - accuracy: 0.8820 - val_loss: 0.4721 - val_accuracy: 0.8598\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3766 - accuracy: 0.8837 - val_loss: 0.3821 - val_accuracy: 0.8887\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3799 - accuracy: 0.8825 - val_loss: 0.4692 - val_accuracy: 0.8603\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3708 - accuracy: 0.8839 - val_loss: 0.3332 - val_accuracy: 0.9022\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3740 - accuracy: 0.8828 - val_loss: 0.3691 - val_accuracy: 0.8921\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3704 - accuracy: 0.8846 - val_loss: 0.3280 - val_accuracy: 0.9055\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3721 - accuracy: 0.8840 - val_loss: 0.3574 - val_accuracy: 0.8945\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3696 - accuracy: 0.8846 - val_loss: 0.3546 - val_accuracy: 0.8981\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3649 - accuracy: 0.8866 - val_loss: 0.5402 - val_accuracy: 0.8385\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3677 - accuracy: 0.8844 - val_loss: 0.3960 - val_accuracy: 0.8831\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3684 - accuracy: 0.8840 - val_loss: 0.4462 - val_accuracy: 0.8666\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3597 - accuracy: 0.8888 - val_loss: 0.3559 - val_accuracy: 0.8954\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3565 - accuracy: 0.8886 - val_loss: 0.3528 - val_accuracy: 0.8990\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3555 - accuracy: 0.8887 - val_loss: 0.3721 - val_accuracy: 0.8882\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3582 - accuracy: 0.8903 - val_loss: 0.3384 - val_accuracy: 0.9006\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3574 - accuracy: 0.8890 - val_loss: 0.2841 - val_accuracy: 0.9161\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3555 - accuracy: 0.8898 - val_loss: 0.3471 - val_accuracy: 0.8998\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3559 - accuracy: 0.8890 - val_loss: 0.3530 - val_accuracy: 0.8918\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3527 - accuracy: 0.8898 - val_loss: 0.3736 - val_accuracy: 0.8919\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3491 - accuracy: 0.8909 - val_loss: 0.4468 - val_accuracy: 0.8698\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3491 - accuracy: 0.8900 - val_loss: 0.4244 - val_accuracy: 0.8757\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3457 - accuracy: 0.8922 - val_loss: 0.4644 - val_accuracy: 0.8607\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3478 - accuracy: 0.8930 - val_loss: 0.3871 - val_accuracy: 0.8871\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3465 - accuracy: 0.8916 - val_loss: 0.2891 - val_accuracy: 0.9148\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3455 - accuracy: 0.8915 - val_loss: 0.3849 - val_accuracy: 0.8921\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3365 - accuracy: 0.8957 - val_loss: 0.5008 - val_accuracy: 0.8520\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3378 - accuracy: 0.8935 - val_loss: 0.2734 - val_accuracy: 0.9220\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3364 - accuracy: 0.8937 - val_loss: 0.3393 - val_accuracy: 0.9002\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3361 - accuracy: 0.8948 - val_loss: 0.3953 - val_accuracy: 0.8807\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3331 - accuracy: 0.8945 - val_loss: 0.2956 - val_accuracy: 0.9166\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3386 - accuracy: 0.8947 - val_loss: 0.4010 - val_accuracy: 0.8816\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3369 - accuracy: 0.8943 - val_loss: 0.3654 - val_accuracy: 0.8926\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3342 - accuracy: 0.8969 - val_loss: 0.3376 - val_accuracy: 0.9007\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3331 - accuracy: 0.8967 - val_loss: 0.3938 - val_accuracy: 0.8907\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3313 - accuracy: 0.8970 - val_loss: 0.3232 - val_accuracy: 0.9086\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3333 - accuracy: 0.8961 - val_loss: 0.2940 - val_accuracy: 0.9155\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3308 - accuracy: 0.8970 - val_loss: 0.5392 - val_accuracy: 0.8407\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.3308 - accuracy: 0.8966 - val_loss: 0.3438 - val_accuracy: 0.8977\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3281 - accuracy: 0.8979 - val_loss: 0.3822 - val_accuracy: 0.8884\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3228 - accuracy: 0.8996 - val_loss: 0.4289 - val_accuracy: 0.8738\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3239 - accuracy: 0.8995 - val_loss: 0.4185 - val_accuracy: 0.8760\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3243 - accuracy: 0.8993 - val_loss: 0.3315 - val_accuracy: 0.9040\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3214 - accuracy: 0.8987 - val_loss: 0.3086 - val_accuracy: 0.9092\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3254 - accuracy: 0.8990 - val_loss: 0.3929 - val_accuracy: 0.8841\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3228 - accuracy: 0.8997 - val_loss: 0.4521 - val_accuracy: 0.8658\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3198 - accuracy: 0.8994 - val_loss: 0.3730 - val_accuracy: 0.8901\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3166 - accuracy: 0.9024 - val_loss: 0.2630 - val_accuracy: 0.9237\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3154 - accuracy: 0.9029 - val_loss: 0.3311 - val_accuracy: 0.9046\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3141 - accuracy: 0.9022 - val_loss: 0.4333 - val_accuracy: 0.8676\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3125 - accuracy: 0.9031 - val_loss: 0.3030 - val_accuracy: 0.9124\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3142 - accuracy: 0.9024 - val_loss: 0.3464 - val_accuracy: 0.8999\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3127 - accuracy: 0.9021 - val_loss: 0.4413 - val_accuracy: 0.8702\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3121 - accuracy: 0.9028 - val_loss: 0.3066 - val_accuracy: 0.9125\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3131 - accuracy: 0.9013 - val_loss: 0.3825 - val_accuracy: 0.8818\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3116 - accuracy: 0.9022 - val_loss: 0.2853 - val_accuracy: 0.9200\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3095 - accuracy: 0.9041 - val_loss: 0.3158 - val_accuracy: 0.9058\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3054 - accuracy: 0.9047 - val_loss: 0.3347 - val_accuracy: 0.9025\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3019 - accuracy: 0.9059 - val_loss: 0.2796 - val_accuracy: 0.9220\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3039 - accuracy: 0.9057 - val_loss: 0.4413 - val_accuracy: 0.8709\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3079 - accuracy: 0.9056 - val_loss: 0.4237 - val_accuracy: 0.8768\n",
      "Try 10/100: Best_val_acc: [0.5172194242477417, 0.8508333563804626], lr: 6.077465025554081e-05, Lambda: 5.801727403352634e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,11):\n",
    "    lr = math.pow(10, np.random.uniform(-4.22, -4.21))\n",
    "    Lambda = math.pow(10, np.random.uniform(-4.24, -4.23))\n",
    "    best_acc = basicDeepNN2(200, lr, Lambda,'relu', 'he_normal', False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3ViEwofTX7J"
   },
   "source": [
    "### Not much difference with dropout left out in last two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWAiDOTAJn-O"
   },
   "source": [
    "## Adding Leaky ReLU as activation inplace of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1639,
     "status": "ok",
     "timestamp": 1594544783823,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "56gD2eyyHftA"
   },
   "outputs": [],
   "source": [
    "def basicDeepNN3(iterations, lr, Lambda, activation, k_initial, al, verb=True):\n",
    "    ## hyperparameters\n",
    "    epochs = iterations\n",
    "    learning_rate = lr\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape = (X_train.shape[1], ), kernel_initializer=k_initial, name='Input'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=al))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, kernel_initializer=k_initial, name='Hidden_Layer_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=al))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, kernel_initializer=k_initial, name='Hidden_Layer_2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=al))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, kernel_initializer=k_initial, name='Hidden_Layer_3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=al))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, kernel_initializer=k_initial, name='Hidden_Layer_4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=al))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer=k_initial ,kernel_regularizer=regularizers.l2(Lambda), name='Output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #opt = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9)\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Fit the model\n",
    "    train_model = model.fit(X_train, y_train, validation_data=(X_val[:14000], y_val[:14000]),\n",
    "              epochs=iterations, batch_size=500, verbose= 1)\n",
    "    test = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return (train_model , test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1880730,
     "status": "ok",
     "timestamp": 1594547212316,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "LryGr8LrJKrm",
    "outputId": "55351f15-7e17-4de9-9c05-a76e13785027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_426 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6244 - accuracy: 0.1076 - val_loss: 2.3076 - val_accuracy: 0.1129\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5014 - accuracy: 0.1241 - val_loss: 2.2619 - val_accuracy: 0.1607\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3990 - accuracy: 0.1454 - val_loss: 2.2299 - val_accuracy: 0.1946\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3030 - accuracy: 0.1772 - val_loss: 2.1718 - val_accuracy: 0.2374\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2135 - accuracy: 0.2036 - val_loss: 2.1262 - val_accuracy: 0.2553\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1376 - accuracy: 0.2366 - val_loss: 1.9846 - val_accuracy: 0.3292\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0585 - accuracy: 0.2701 - val_loss: 1.9056 - val_accuracy: 0.3538\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9869 - accuracy: 0.2993 - val_loss: 1.8232 - val_accuracy: 0.4030\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9183 - accuracy: 0.3323 - val_loss: 1.6933 - val_accuracy: 0.5019\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8620 - accuracy: 0.3549 - val_loss: 1.6166 - val_accuracy: 0.5296\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8078 - accuracy: 0.3780 - val_loss: 1.5946 - val_accuracy: 0.5289\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7549 - accuracy: 0.4056 - val_loss: 1.5438 - val_accuracy: 0.5595\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7021 - accuracy: 0.4277 - val_loss: 1.3719 - val_accuracy: 0.6406\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6599 - accuracy: 0.4441 - val_loss: 1.4099 - val_accuracy: 0.6053\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6097 - accuracy: 0.4665 - val_loss: 1.3191 - val_accuracy: 0.6346\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5650 - accuracy: 0.4851 - val_loss: 1.3111 - val_accuracy: 0.6371\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5235 - accuracy: 0.5055 - val_loss: 1.2545 - val_accuracy: 0.6543\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4876 - accuracy: 0.5203 - val_loss: 1.2335 - val_accuracy: 0.6617\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4488 - accuracy: 0.5358 - val_loss: 1.2574 - val_accuracy: 0.6285\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4092 - accuracy: 0.5515 - val_loss: 1.1219 - val_accuracy: 0.6899\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3808 - accuracy: 0.5648 - val_loss: 1.1415 - val_accuracy: 0.6792\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3480 - accuracy: 0.5753 - val_loss: 1.1380 - val_accuracy: 0.6856\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3113 - accuracy: 0.5914 - val_loss: 0.9912 - val_accuracy: 0.7321\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2770 - accuracy: 0.6053 - val_loss: 1.0422 - val_accuracy: 0.7068\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2513 - accuracy: 0.6117 - val_loss: 1.0629 - val_accuracy: 0.7043\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2196 - accuracy: 0.6247 - val_loss: 0.9723 - val_accuracy: 0.7286\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1905 - accuracy: 0.6350 - val_loss: 1.0134 - val_accuracy: 0.7136\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1634 - accuracy: 0.6441 - val_loss: 0.9222 - val_accuracy: 0.7444\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1478 - accuracy: 0.6493 - val_loss: 0.9368 - val_accuracy: 0.7331\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1230 - accuracy: 0.6594 - val_loss: 0.9446 - val_accuracy: 0.7313\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1033 - accuracy: 0.6643 - val_loss: 0.8867 - val_accuracy: 0.7425\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0772 - accuracy: 0.6719 - val_loss: 0.8857 - val_accuracy: 0.7550\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0562 - accuracy: 0.6792 - val_loss: 0.8893 - val_accuracy: 0.7409\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0443 - accuracy: 0.6819 - val_loss: 0.8537 - val_accuracy: 0.7519\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0216 - accuracy: 0.6909 - val_loss: 0.8572 - val_accuracy: 0.7520\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0000 - accuracy: 0.6975 - val_loss: 0.7457 - val_accuracy: 0.7870\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9849 - accuracy: 0.7021 - val_loss: 0.7552 - val_accuracy: 0.7853\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9774 - accuracy: 0.7056 - val_loss: 0.8347 - val_accuracy: 0.7550\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9583 - accuracy: 0.7116 - val_loss: 0.8275 - val_accuracy: 0.7532\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9410 - accuracy: 0.7183 - val_loss: 0.8260 - val_accuracy: 0.7565\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9315 - accuracy: 0.7190 - val_loss: 0.6998 - val_accuracy: 0.7939\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9096 - accuracy: 0.7267 - val_loss: 0.6509 - val_accuracy: 0.8147\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9043 - accuracy: 0.7286 - val_loss: 0.8149 - val_accuracy: 0.7594\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8937 - accuracy: 0.7302 - val_loss: 0.6518 - val_accuracy: 0.8153\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8793 - accuracy: 0.7389 - val_loss: 0.6980 - val_accuracy: 0.7957\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8718 - accuracy: 0.7385 - val_loss: 0.7343 - val_accuracy: 0.7799\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8534 - accuracy: 0.7455 - val_loss: 0.7789 - val_accuracy: 0.7699\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8481 - accuracy: 0.7469 - val_loss: 0.7701 - val_accuracy: 0.7659\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8444 - accuracy: 0.7481 - val_loss: 0.6282 - val_accuracy: 0.8171\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8318 - accuracy: 0.7519 - val_loss: 0.6604 - val_accuracy: 0.8039\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8251 - accuracy: 0.7539 - val_loss: 0.6551 - val_accuracy: 0.8066\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8079 - accuracy: 0.7575 - val_loss: 0.6656 - val_accuracy: 0.8044\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8037 - accuracy: 0.7622 - val_loss: 0.6569 - val_accuracy: 0.8047\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7922 - accuracy: 0.7650 - val_loss: 0.6604 - val_accuracy: 0.8044\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7836 - accuracy: 0.7645 - val_loss: 0.6349 - val_accuracy: 0.8081\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7795 - accuracy: 0.7676 - val_loss: 0.6664 - val_accuracy: 0.8006\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7670 - accuracy: 0.7725 - val_loss: 0.7905 - val_accuracy: 0.7600\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7601 - accuracy: 0.7735 - val_loss: 0.6024 - val_accuracy: 0.8235\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7582 - accuracy: 0.7747 - val_loss: 0.5678 - val_accuracy: 0.8274\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7461 - accuracy: 0.7778 - val_loss: 0.6414 - val_accuracy: 0.8099\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7410 - accuracy: 0.7804 - val_loss: 0.5290 - val_accuracy: 0.8460\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7304 - accuracy: 0.7806 - val_loss: 0.6132 - val_accuracy: 0.8129\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7237 - accuracy: 0.7870 - val_loss: 0.5572 - val_accuracy: 0.8306\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7182 - accuracy: 0.7863 - val_loss: 0.6117 - val_accuracy: 0.8151\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7162 - accuracy: 0.7887 - val_loss: 0.6494 - val_accuracy: 0.8018\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7062 - accuracy: 0.7895 - val_loss: 0.7312 - val_accuracy: 0.7866\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7050 - accuracy: 0.7909 - val_loss: 0.6096 - val_accuracy: 0.8166\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6980 - accuracy: 0.7932 - val_loss: 0.5525 - val_accuracy: 0.8336\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6911 - accuracy: 0.7958 - val_loss: 0.5899 - val_accuracy: 0.8237\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6866 - accuracy: 0.7944 - val_loss: 0.5455 - val_accuracy: 0.8393\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6836 - accuracy: 0.7976 - val_loss: 0.5900 - val_accuracy: 0.8264\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6662 - accuracy: 0.8044 - val_loss: 0.6569 - val_accuracy: 0.7987\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6689 - accuracy: 0.8029 - val_loss: 0.4910 - val_accuracy: 0.8591\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6632 - accuracy: 0.8036 - val_loss: 0.5886 - val_accuracy: 0.8239\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6581 - accuracy: 0.8060 - val_loss: 0.4986 - val_accuracy: 0.8551\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6487 - accuracy: 0.8086 - val_loss: 0.4860 - val_accuracy: 0.8534\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6503 - accuracy: 0.8078 - val_loss: 0.6424 - val_accuracy: 0.8083\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6451 - accuracy: 0.8093 - val_loss: 0.5416 - val_accuracy: 0.8403\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6419 - accuracy: 0.8111 - val_loss: 0.6680 - val_accuracy: 0.8002\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6359 - accuracy: 0.8122 - val_loss: 0.6158 - val_accuracy: 0.8147\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6334 - accuracy: 0.8138 - val_loss: 0.5145 - val_accuracy: 0.8435\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6226 - accuracy: 0.8171 - val_loss: 0.5703 - val_accuracy: 0.8295\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6256 - accuracy: 0.8150 - val_loss: 0.4857 - val_accuracy: 0.8531\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6178 - accuracy: 0.8182 - val_loss: 0.4889 - val_accuracy: 0.8583\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6170 - accuracy: 0.8191 - val_loss: 0.5851 - val_accuracy: 0.8251\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6146 - accuracy: 0.8171 - val_loss: 0.4778 - val_accuracy: 0.8479\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6018 - accuracy: 0.8218 - val_loss: 0.6519 - val_accuracy: 0.8056\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6063 - accuracy: 0.8224 - val_loss: 0.4813 - val_accuracy: 0.8561\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5957 - accuracy: 0.8232 - val_loss: 0.4651 - val_accuracy: 0.8594\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5933 - accuracy: 0.8252 - val_loss: 0.4236 - val_accuracy: 0.8774\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5926 - accuracy: 0.8253 - val_loss: 0.4219 - val_accuracy: 0.8747\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5939 - accuracy: 0.8245 - val_loss: 0.5344 - val_accuracy: 0.8369\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5882 - accuracy: 0.8260 - val_loss: 0.4392 - val_accuracy: 0.8642\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5898 - accuracy: 0.8257 - val_loss: 0.6036 - val_accuracy: 0.8244\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5777 - accuracy: 0.8300 - val_loss: 0.4996 - val_accuracy: 0.8429\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5764 - accuracy: 0.8317 - val_loss: 0.5504 - val_accuracy: 0.8357\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5695 - accuracy: 0.8318 - val_loss: 0.4546 - val_accuracy: 0.8625\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5691 - accuracy: 0.8330 - val_loss: 0.4621 - val_accuracy: 0.8609\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5660 - accuracy: 0.8336 - val_loss: 0.5188 - val_accuracy: 0.8454\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5622 - accuracy: 0.8348 - val_loss: 0.5293 - val_accuracy: 0.8429\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5567 - accuracy: 0.8365 - val_loss: 0.5538 - val_accuracy: 0.8359\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5531 - accuracy: 0.8373 - val_loss: 0.4584 - val_accuracy: 0.8649\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5549 - accuracy: 0.8363 - val_loss: 0.5170 - val_accuracy: 0.8450\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5480 - accuracy: 0.8392 - val_loss: 0.4436 - val_accuracy: 0.8699\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5498 - accuracy: 0.8393 - val_loss: 0.4868 - val_accuracy: 0.8543\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5427 - accuracy: 0.8385 - val_loss: 0.5546 - val_accuracy: 0.8303\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5435 - accuracy: 0.8408 - val_loss: 0.6079 - val_accuracy: 0.8211\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5438 - accuracy: 0.8396 - val_loss: 0.4401 - val_accuracy: 0.8716\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5407 - accuracy: 0.8413 - val_loss: 0.5832 - val_accuracy: 0.8305\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5379 - accuracy: 0.8408 - val_loss: 0.4241 - val_accuracy: 0.8770\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5341 - accuracy: 0.8427 - val_loss: 0.5239 - val_accuracy: 0.8276\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5303 - accuracy: 0.8445 - val_loss: 0.5335 - val_accuracy: 0.8431\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5298 - accuracy: 0.8443 - val_loss: 0.5247 - val_accuracy: 0.8446\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5273 - accuracy: 0.8445 - val_loss: 0.4150 - val_accuracy: 0.8785\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5175 - accuracy: 0.8479 - val_loss: 0.4038 - val_accuracy: 0.8841\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5189 - accuracy: 0.8471 - val_loss: 0.4102 - val_accuracy: 0.8804\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5142 - accuracy: 0.8499 - val_loss: 0.5408 - val_accuracy: 0.8416\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5117 - accuracy: 0.8492 - val_loss: 0.4675 - val_accuracy: 0.8606\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5085 - accuracy: 0.8516 - val_loss: 0.5077 - val_accuracy: 0.8483\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5068 - accuracy: 0.8504 - val_loss: 0.4682 - val_accuracy: 0.8623\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5097 - accuracy: 0.8505 - val_loss: 0.5250 - val_accuracy: 0.8450\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5087 - accuracy: 0.8515 - val_loss: 0.4048 - val_accuracy: 0.8811\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5047 - accuracy: 0.8535 - val_loss: 0.5475 - val_accuracy: 0.8396\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4963 - accuracy: 0.8530 - val_loss: 0.3839 - val_accuracy: 0.8904\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4966 - accuracy: 0.8559 - val_loss: 0.4051 - val_accuracy: 0.8823\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8524 - val_loss: 0.6063 - val_accuracy: 0.8168\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4947 - accuracy: 0.8553 - val_loss: 0.3858 - val_accuracy: 0.8879\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4966 - accuracy: 0.8557 - val_loss: 0.4566 - val_accuracy: 0.8637\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4895 - accuracy: 0.8562 - val_loss: 0.4086 - val_accuracy: 0.8774\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4890 - accuracy: 0.8576 - val_loss: 0.5108 - val_accuracy: 0.8465\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4829 - accuracy: 0.8581 - val_loss: 0.4812 - val_accuracy: 0.8588\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4862 - accuracy: 0.8563 - val_loss: 0.3849 - val_accuracy: 0.8902\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.8574 - val_loss: 0.4653 - val_accuracy: 0.8642\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4804 - accuracy: 0.8587 - val_loss: 0.4084 - val_accuracy: 0.8811\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4765 - accuracy: 0.8600 - val_loss: 0.3743 - val_accuracy: 0.8928\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4755 - accuracy: 0.8614 - val_loss: 0.4182 - val_accuracy: 0.8808\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4739 - accuracy: 0.8612 - val_loss: 0.4283 - val_accuracy: 0.8738\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4735 - accuracy: 0.8607 - val_loss: 0.5359 - val_accuracy: 0.8401\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4685 - accuracy: 0.8622 - val_loss: 0.3958 - val_accuracy: 0.8830\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4696 - accuracy: 0.8606 - val_loss: 0.4408 - val_accuracy: 0.8723\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4670 - accuracy: 0.8614 - val_loss: 0.4406 - val_accuracy: 0.8705\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4655 - accuracy: 0.8640 - val_loss: 0.4218 - val_accuracy: 0.8775\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4628 - accuracy: 0.8641 - val_loss: 0.4424 - val_accuracy: 0.8690\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4609 - accuracy: 0.8641 - val_loss: 0.4082 - val_accuracy: 0.8811\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4523 - accuracy: 0.8678 - val_loss: 0.3750 - val_accuracy: 0.8929\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4548 - accuracy: 0.8672 - val_loss: 0.5273 - val_accuracy: 0.8472\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4519 - accuracy: 0.8676 - val_loss: 0.3741 - val_accuracy: 0.8879\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4593 - accuracy: 0.8661 - val_loss: 0.4699 - val_accuracy: 0.8614\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4520 - accuracy: 0.8682 - val_loss: 0.4498 - val_accuracy: 0.8661\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4538 - accuracy: 0.8681 - val_loss: 0.4523 - val_accuracy: 0.8644\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4465 - accuracy: 0.8703 - val_loss: 0.4365 - val_accuracy: 0.8711\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4518 - accuracy: 0.8663 - val_loss: 0.5221 - val_accuracy: 0.8434\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4452 - accuracy: 0.8684 - val_loss: 0.4185 - val_accuracy: 0.8794\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4444 - accuracy: 0.8684 - val_loss: 0.4375 - val_accuracy: 0.8689\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4421 - accuracy: 0.8700 - val_loss: 0.4341 - val_accuracy: 0.8745\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4383 - accuracy: 0.8727 - val_loss: 0.4295 - val_accuracy: 0.8725\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4414 - accuracy: 0.8697 - val_loss: 0.3975 - val_accuracy: 0.8808\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8706 - val_loss: 0.4072 - val_accuracy: 0.8826\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4350 - accuracy: 0.8707 - val_loss: 0.3975 - val_accuracy: 0.8796\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4346 - accuracy: 0.8722 - val_loss: 0.4131 - val_accuracy: 0.8779\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4306 - accuracy: 0.8746 - val_loss: 0.3098 - val_accuracy: 0.9121\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4309 - accuracy: 0.8741 - val_loss: 0.3606 - val_accuracy: 0.8960\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8754 - val_loss: 0.4476 - val_accuracy: 0.8688\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4301 - accuracy: 0.8723 - val_loss: 0.3542 - val_accuracy: 0.8993\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4287 - accuracy: 0.8753 - val_loss: 0.4328 - val_accuracy: 0.8736\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4268 - accuracy: 0.8756 - val_loss: 0.3589 - val_accuracy: 0.8971\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8755 - val_loss: 0.4714 - val_accuracy: 0.8624\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4252 - accuracy: 0.8759 - val_loss: 0.4189 - val_accuracy: 0.8742\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4203 - accuracy: 0.8773 - val_loss: 0.5295 - val_accuracy: 0.8443\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4244 - accuracy: 0.8756 - val_loss: 0.4227 - val_accuracy: 0.8761\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8778 - val_loss: 0.5953 - val_accuracy: 0.8249\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4194 - accuracy: 0.8786 - val_loss: 0.3756 - val_accuracy: 0.8911\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4162 - accuracy: 0.8776 - val_loss: 0.3452 - val_accuracy: 0.8994\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4135 - accuracy: 0.8799 - val_loss: 0.3325 - val_accuracy: 0.9051\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4114 - accuracy: 0.8780 - val_loss: 0.3495 - val_accuracy: 0.9001\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4150 - accuracy: 0.8776 - val_loss: 0.4075 - val_accuracy: 0.8756\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4116 - accuracy: 0.8799 - val_loss: 0.3682 - val_accuracy: 0.8934\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4094 - accuracy: 0.8815 - val_loss: 0.4397 - val_accuracy: 0.8737\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4033 - accuracy: 0.8829 - val_loss: 0.3269 - val_accuracy: 0.9063\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4083 - accuracy: 0.8813 - val_loss: 0.4703 - val_accuracy: 0.8614\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4111 - accuracy: 0.8795 - val_loss: 0.3637 - val_accuracy: 0.8942\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4006 - accuracy: 0.8822 - val_loss: 0.6022 - val_accuracy: 0.8265\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4047 - accuracy: 0.8817 - val_loss: 0.4231 - val_accuracy: 0.8771\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4047 - accuracy: 0.8824 - val_loss: 0.3921 - val_accuracy: 0.8884\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3983 - accuracy: 0.8838 - val_loss: 0.3918 - val_accuracy: 0.8877\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4026 - accuracy: 0.8821 - val_loss: 0.3035 - val_accuracy: 0.9150\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4003 - accuracy: 0.8834 - val_loss: 0.3921 - val_accuracy: 0.8844\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3999 - accuracy: 0.8834 - val_loss: 0.4408 - val_accuracy: 0.8721\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3966 - accuracy: 0.8844 - val_loss: 0.3017 - val_accuracy: 0.9151\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3952 - accuracy: 0.8844 - val_loss: 0.3641 - val_accuracy: 0.8916\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3921 - accuracy: 0.8878 - val_loss: 0.3626 - val_accuracy: 0.8947\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3936 - accuracy: 0.8856 - val_loss: 0.4697 - val_accuracy: 0.8629\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3921 - accuracy: 0.8858 - val_loss: 0.5268 - val_accuracy: 0.8431\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8852 - val_loss: 0.3997 - val_accuracy: 0.8819\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3909 - accuracy: 0.8856 - val_loss: 0.5945 - val_accuracy: 0.8261\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3916 - accuracy: 0.8853 - val_loss: 0.3937 - val_accuracy: 0.8889\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3861 - accuracy: 0.8893 - val_loss: 0.3955 - val_accuracy: 0.8854\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3844 - accuracy: 0.8879 - val_loss: 0.4263 - val_accuracy: 0.8780\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3780 - accuracy: 0.8908 - val_loss: 0.3044 - val_accuracy: 0.9136\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3843 - accuracy: 0.8882 - val_loss: 0.4834 - val_accuracy: 0.8586\n",
      "Try 1/100: Best_val_acc: [0.515749454498291, 0.8478333353996277], lr: 6.046494355983916e-05, Lambda: 5.825822518421015e-05\n",
      "\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_427 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6703 - accuracy: 0.1083 - val_loss: 2.3788 - val_accuracy: 0.0357\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 2.5427 - accuracy: 0.1269 - val_loss: 2.2857 - val_accuracy: 0.1381\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 2.4384 - accuracy: 0.1482 - val_loss: 2.2008 - val_accuracy: 0.2100\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3416 - accuracy: 0.1707 - val_loss: 2.0867 - val_accuracy: 0.2657\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 2.2591 - accuracy: 0.1940 - val_loss: 1.9945 - val_accuracy: 0.3238\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1810 - accuracy: 0.2208 - val_loss: 1.9727 - val_accuracy: 0.3315\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1032 - accuracy: 0.2527 - val_loss: 1.8328 - val_accuracy: 0.4389\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0315 - accuracy: 0.2805 - val_loss: 1.8125 - val_accuracy: 0.4361\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9637 - accuracy: 0.3033 - val_loss: 1.6895 - val_accuracy: 0.5008\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9004 - accuracy: 0.3316 - val_loss: 1.6032 - val_accuracy: 0.5399\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8441 - accuracy: 0.3571 - val_loss: 1.5466 - val_accuracy: 0.5766\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7836 - accuracy: 0.3851 - val_loss: 1.5088 - val_accuracy: 0.5820\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7376 - accuracy: 0.4046 - val_loss: 1.4397 - val_accuracy: 0.6213\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6869 - accuracy: 0.4267 - val_loss: 1.3832 - val_accuracy: 0.6354\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6376 - accuracy: 0.4467 - val_loss: 1.3507 - val_accuracy: 0.6409\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5968 - accuracy: 0.4636 - val_loss: 1.2753 - val_accuracy: 0.6751\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5562 - accuracy: 0.4833 - val_loss: 1.2607 - val_accuracy: 0.6759\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5149 - accuracy: 0.4990 - val_loss: 1.2361 - val_accuracy: 0.6806\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4820 - accuracy: 0.5140 - val_loss: 1.1462 - val_accuracy: 0.7141\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4405 - accuracy: 0.5290 - val_loss: 1.3259 - val_accuracy: 0.6146\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4095 - accuracy: 0.5443 - val_loss: 1.0894 - val_accuracy: 0.7226\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3717 - accuracy: 0.5569 - val_loss: 1.1445 - val_accuracy: 0.6942\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3371 - accuracy: 0.5759 - val_loss: 1.1148 - val_accuracy: 0.7059\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3077 - accuracy: 0.5847 - val_loss: 1.0198 - val_accuracy: 0.7321\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2832 - accuracy: 0.5932 - val_loss: 1.0633 - val_accuracy: 0.7167\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2495 - accuracy: 0.6040 - val_loss: 1.1153 - val_accuracy: 0.6892\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2236 - accuracy: 0.6142 - val_loss: 0.9515 - val_accuracy: 0.7466\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1878 - accuracy: 0.6278 - val_loss: 0.8847 - val_accuracy: 0.7689\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1768 - accuracy: 0.6322 - val_loss: 0.9792 - val_accuracy: 0.7308\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1513 - accuracy: 0.6448 - val_loss: 0.8768 - val_accuracy: 0.7587\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1279 - accuracy: 0.6506 - val_loss: 0.8795 - val_accuracy: 0.7579\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1011 - accuracy: 0.6613 - val_loss: 1.0546 - val_accuracy: 0.6889\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0857 - accuracy: 0.6655 - val_loss: 0.8950 - val_accuracy: 0.7514\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0615 - accuracy: 0.6745 - val_loss: 0.8423 - val_accuracy: 0.7617\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0396 - accuracy: 0.6814 - val_loss: 0.7932 - val_accuracy: 0.7605\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0287 - accuracy: 0.6846 - val_loss: 0.7944 - val_accuracy: 0.7811\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0092 - accuracy: 0.6913 - val_loss: 0.7650 - val_accuracy: 0.7854\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9887 - accuracy: 0.6988 - val_loss: 0.7556 - val_accuracy: 0.7863\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9691 - accuracy: 0.7050 - val_loss: 0.7616 - val_accuracy: 0.7731\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9592 - accuracy: 0.7069 - val_loss: 0.6920 - val_accuracy: 0.8051\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9458 - accuracy: 0.7125 - val_loss: 0.8492 - val_accuracy: 0.7515\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9314 - accuracy: 0.7157 - val_loss: 0.7279 - val_accuracy: 0.7916\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9206 - accuracy: 0.7191 - val_loss: 0.6684 - val_accuracy: 0.8082\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9079 - accuracy: 0.7224 - val_loss: 0.6756 - val_accuracy: 0.8076\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8934 - accuracy: 0.7290 - val_loss: 0.7331 - val_accuracy: 0.7919\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8761 - accuracy: 0.7357 - val_loss: 0.7197 - val_accuracy: 0.7888\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8713 - accuracy: 0.7355 - val_loss: 0.6700 - val_accuracy: 0.8109\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8589 - accuracy: 0.7400 - val_loss: 1.0147 - val_accuracy: 0.6959\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8467 - accuracy: 0.7440 - val_loss: 0.6409 - val_accuracy: 0.8163\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8355 - accuracy: 0.7489 - val_loss: 0.6982 - val_accuracy: 0.8013\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8275 - accuracy: 0.7493 - val_loss: 0.6243 - val_accuracy: 0.8239\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8217 - accuracy: 0.7526 - val_loss: 0.5809 - val_accuracy: 0.8352\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8138 - accuracy: 0.7556 - val_loss: 0.6192 - val_accuracy: 0.8218\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8048 - accuracy: 0.7573 - val_loss: 0.6197 - val_accuracy: 0.8231\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7973 - accuracy: 0.7593 - val_loss: 0.7897 - val_accuracy: 0.7584\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7857 - accuracy: 0.7629 - val_loss: 0.5630 - val_accuracy: 0.8331\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7735 - accuracy: 0.7680 - val_loss: 0.6470 - val_accuracy: 0.8039\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7742 - accuracy: 0.7652 - val_loss: 0.6162 - val_accuracy: 0.8200\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7617 - accuracy: 0.7716 - val_loss: 0.5993 - val_accuracy: 0.8273\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7557 - accuracy: 0.7744 - val_loss: 0.5512 - val_accuracy: 0.8330\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7478 - accuracy: 0.7752 - val_loss: 0.5533 - val_accuracy: 0.8446\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7356 - accuracy: 0.7790 - val_loss: 0.6011 - val_accuracy: 0.8274\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7321 - accuracy: 0.7809 - val_loss: 0.6355 - val_accuracy: 0.8111\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7276 - accuracy: 0.7812 - val_loss: 0.5272 - val_accuracy: 0.8472\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7178 - accuracy: 0.7870 - val_loss: 0.6391 - val_accuracy: 0.8040\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7111 - accuracy: 0.7890 - val_loss: 0.5506 - val_accuracy: 0.8392\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7097 - accuracy: 0.7875 - val_loss: 0.5516 - val_accuracy: 0.8361\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7061 - accuracy: 0.7900 - val_loss: 0.5805 - val_accuracy: 0.8234\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6974 - accuracy: 0.7921 - val_loss: 0.5720 - val_accuracy: 0.8335\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6895 - accuracy: 0.7938 - val_loss: 0.7330 - val_accuracy: 0.7833\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6878 - accuracy: 0.7947 - val_loss: 0.5162 - val_accuracy: 0.8511\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6758 - accuracy: 0.8008 - val_loss: 0.5548 - val_accuracy: 0.8404\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6723 - accuracy: 0.8002 - val_loss: 0.6159 - val_accuracy: 0.8052\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6708 - accuracy: 0.7995 - val_loss: 0.5845 - val_accuracy: 0.8250\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6694 - accuracy: 0.8015 - val_loss: 0.5055 - val_accuracy: 0.8535\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6574 - accuracy: 0.8040 - val_loss: 0.5539 - val_accuracy: 0.8383\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6540 - accuracy: 0.8069 - val_loss: 0.5027 - val_accuracy: 0.8524\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6468 - accuracy: 0.8088 - val_loss: 0.5627 - val_accuracy: 0.8256\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6364 - accuracy: 0.8126 - val_loss: 0.5116 - val_accuracy: 0.8497\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6388 - accuracy: 0.8076 - val_loss: 0.4796 - val_accuracy: 0.8617\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6313 - accuracy: 0.8128 - val_loss: 0.4500 - val_accuracy: 0.8690\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6349 - accuracy: 0.8092 - val_loss: 0.5786 - val_accuracy: 0.8208\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6299 - accuracy: 0.8128 - val_loss: 0.5624 - val_accuracy: 0.8281\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6163 - accuracy: 0.8158 - val_loss: 0.5152 - val_accuracy: 0.8488\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6194 - accuracy: 0.8183 - val_loss: 0.6001 - val_accuracy: 0.8261\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6117 - accuracy: 0.8178 - val_loss: 0.6144 - val_accuracy: 0.8107\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6053 - accuracy: 0.8209 - val_loss: 0.6101 - val_accuracy: 0.8171\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6132 - accuracy: 0.8185 - val_loss: 0.4654 - val_accuracy: 0.8643\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6030 - accuracy: 0.8206 - val_loss: 0.5306 - val_accuracy: 0.8424\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5927 - accuracy: 0.8246 - val_loss: 0.4664 - val_accuracy: 0.8623\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5971 - accuracy: 0.8237 - val_loss: 0.4870 - val_accuracy: 0.8492\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5876 - accuracy: 0.8278 - val_loss: 0.4692 - val_accuracy: 0.8639\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5923 - accuracy: 0.8232 - val_loss: 0.5576 - val_accuracy: 0.8386\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5838 - accuracy: 0.8286 - val_loss: 0.5129 - val_accuracy: 0.8506\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5838 - accuracy: 0.8261 - val_loss: 0.4980 - val_accuracy: 0.8544\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5763 - accuracy: 0.8301 - val_loss: 0.5832 - val_accuracy: 0.8230\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5705 - accuracy: 0.8311 - val_loss: 0.6155 - val_accuracy: 0.8195\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5682 - accuracy: 0.8316 - val_loss: 0.5716 - val_accuracy: 0.8328\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5673 - accuracy: 0.8327 - val_loss: 0.4756 - val_accuracy: 0.8611\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5652 - accuracy: 0.8335 - val_loss: 0.4522 - val_accuracy: 0.8664\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5586 - accuracy: 0.8348 - val_loss: 0.4565 - val_accuracy: 0.8669\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5584 - accuracy: 0.8339 - val_loss: 0.4801 - val_accuracy: 0.8602\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5521 - accuracy: 0.8381 - val_loss: 0.4358 - val_accuracy: 0.8730\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5570 - accuracy: 0.8365 - val_loss: 0.4278 - val_accuracy: 0.8741\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5468 - accuracy: 0.8398 - val_loss: 0.5235 - val_accuracy: 0.8399\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5426 - accuracy: 0.8409 - val_loss: 0.4822 - val_accuracy: 0.8444\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5482 - accuracy: 0.8384 - val_loss: 0.5153 - val_accuracy: 0.8406\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5398 - accuracy: 0.8406 - val_loss: 0.5305 - val_accuracy: 0.8446\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5461 - accuracy: 0.8406 - val_loss: 0.5476 - val_accuracy: 0.8350\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5317 - accuracy: 0.8443 - val_loss: 0.5069 - val_accuracy: 0.8479\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5324 - accuracy: 0.8438 - val_loss: 0.4620 - val_accuracy: 0.8643\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5323 - accuracy: 0.8441 - val_loss: 0.5897 - val_accuracy: 0.8221\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5322 - accuracy: 0.8430 - val_loss: 0.4805 - val_accuracy: 0.8576\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5231 - accuracy: 0.8460 - val_loss: 0.4505 - val_accuracy: 0.8675\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5219 - accuracy: 0.8448 - val_loss: 0.3967 - val_accuracy: 0.8851\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5183 - accuracy: 0.8478 - val_loss: 0.6116 - val_accuracy: 0.8109\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5150 - accuracy: 0.8490 - val_loss: 0.5609 - val_accuracy: 0.8362\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5154 - accuracy: 0.8494 - val_loss: 0.4467 - val_accuracy: 0.8669\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5173 - accuracy: 0.8477 - val_loss: 0.4175 - val_accuracy: 0.8779\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5102 - accuracy: 0.8504 - val_loss: 0.4248 - val_accuracy: 0.8762\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5094 - accuracy: 0.8500 - val_loss: 0.4360 - val_accuracy: 0.8748\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5074 - accuracy: 0.8511 - val_loss: 0.4480 - val_accuracy: 0.8696\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5030 - accuracy: 0.8521 - val_loss: 0.4628 - val_accuracy: 0.8629\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4972 - accuracy: 0.8537 - val_loss: 0.4472 - val_accuracy: 0.8686\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4962 - accuracy: 0.8538 - val_loss: 0.4398 - val_accuracy: 0.8706\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4908 - accuracy: 0.8561 - val_loss: 0.4193 - val_accuracy: 0.8786\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4870 - accuracy: 0.8570 - val_loss: 0.4127 - val_accuracy: 0.8779\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4873 - accuracy: 0.8582 - val_loss: 0.3595 - val_accuracy: 0.8973\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4938 - accuracy: 0.8562 - val_loss: 0.4154 - val_accuracy: 0.8787\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4938 - accuracy: 0.8540 - val_loss: 0.5375 - val_accuracy: 0.8382\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4876 - accuracy: 0.8587 - val_loss: 0.5379 - val_accuracy: 0.8442\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4878 - accuracy: 0.8559 - val_loss: 0.4109 - val_accuracy: 0.8819\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4810 - accuracy: 0.8595 - val_loss: 0.4731 - val_accuracy: 0.8616\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4787 - accuracy: 0.8608 - val_loss: 0.5252 - val_accuracy: 0.8402\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4724 - accuracy: 0.8624 - val_loss: 0.4460 - val_accuracy: 0.8697\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4774 - accuracy: 0.8592 - val_loss: 0.4391 - val_accuracy: 0.8733\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4709 - accuracy: 0.8616 - val_loss: 0.3942 - val_accuracy: 0.8854\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4727 - accuracy: 0.8615 - val_loss: 0.4735 - val_accuracy: 0.8636\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4718 - accuracy: 0.8617 - val_loss: 0.4909 - val_accuracy: 0.8516\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4713 - accuracy: 0.8606 - val_loss: 0.5418 - val_accuracy: 0.8381\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4659 - accuracy: 0.8621 - val_loss: 0.4644 - val_accuracy: 0.8591\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4707 - accuracy: 0.8612 - val_loss: 0.4664 - val_accuracy: 0.8656\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4643 - accuracy: 0.8655 - val_loss: 0.4319 - val_accuracy: 0.8682\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4613 - accuracy: 0.8644 - val_loss: 0.3975 - val_accuracy: 0.8842\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4621 - accuracy: 0.8664 - val_loss: 0.4090 - val_accuracy: 0.8841\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4512 - accuracy: 0.8660 - val_loss: 0.5766 - val_accuracy: 0.8275\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4560 - accuracy: 0.8654 - val_loss: 0.3818 - val_accuracy: 0.8926\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4534 - accuracy: 0.8659 - val_loss: 0.3274 - val_accuracy: 0.9091\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4508 - accuracy: 0.8662 - val_loss: 0.3652 - val_accuracy: 0.8981\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4511 - accuracy: 0.8687 - val_loss: 0.5510 - val_accuracy: 0.8334\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4393 - accuracy: 0.8714 - val_loss: 0.4010 - val_accuracy: 0.8796\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4506 - accuracy: 0.8685 - val_loss: 0.3654 - val_accuracy: 0.8960\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4504 - accuracy: 0.8688 - val_loss: 0.3579 - val_accuracy: 0.8990\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4461 - accuracy: 0.8689 - val_loss: 0.5674 - val_accuracy: 0.8282\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4468 - accuracy: 0.8692 - val_loss: 0.3861 - val_accuracy: 0.8881\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4445 - accuracy: 0.8700 - val_loss: 0.4919 - val_accuracy: 0.8589\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4370 - accuracy: 0.8720 - val_loss: 0.3969 - val_accuracy: 0.8846\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4361 - accuracy: 0.8720 - val_loss: 0.5975 - val_accuracy: 0.8306\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4360 - accuracy: 0.8726 - val_loss: 0.3936 - val_accuracy: 0.8824\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4371 - accuracy: 0.8715 - val_loss: 0.4414 - val_accuracy: 0.8710\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4383 - accuracy: 0.8719 - val_loss: 0.4059 - val_accuracy: 0.8788\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4327 - accuracy: 0.8727 - val_loss: 0.5147 - val_accuracy: 0.8550\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4258 - accuracy: 0.8755 - val_loss: 0.4056 - val_accuracy: 0.8787\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4266 - accuracy: 0.8740 - val_loss: 0.3419 - val_accuracy: 0.9042\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4254 - accuracy: 0.8748 - val_loss: 0.4136 - val_accuracy: 0.8786\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4282 - accuracy: 0.8744 - val_loss: 0.3707 - val_accuracy: 0.8914\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4271 - accuracy: 0.8756 - val_loss: 0.4087 - val_accuracy: 0.8810\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4269 - accuracy: 0.8745 - val_loss: 0.4127 - val_accuracy: 0.8721\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4164 - accuracy: 0.8795 - val_loss: 0.3534 - val_accuracy: 0.8963\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4199 - accuracy: 0.8787 - val_loss: 0.3955 - val_accuracy: 0.8865\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4170 - accuracy: 0.8794 - val_loss: 0.4966 - val_accuracy: 0.8539\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4152 - accuracy: 0.8780 - val_loss: 0.4279 - val_accuracy: 0.8739\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4188 - accuracy: 0.8777 - val_loss: 0.5430 - val_accuracy: 0.8333\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4113 - accuracy: 0.8796 - val_loss: 0.4387 - val_accuracy: 0.8677\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4145 - accuracy: 0.8782 - val_loss: 0.3923 - val_accuracy: 0.8892\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4136 - accuracy: 0.8801 - val_loss: 0.4209 - val_accuracy: 0.8748\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4087 - accuracy: 0.8800 - val_loss: 0.3751 - val_accuracy: 0.8933\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4060 - accuracy: 0.8828 - val_loss: 0.3899 - val_accuracy: 0.8899\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4058 - accuracy: 0.8804 - val_loss: 0.4012 - val_accuracy: 0.8814\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4028 - accuracy: 0.8826 - val_loss: 0.3381 - val_accuracy: 0.9021\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4041 - accuracy: 0.8820 - val_loss: 0.3807 - val_accuracy: 0.8885\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4041 - accuracy: 0.8820 - val_loss: 0.3708 - val_accuracy: 0.8919\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4008 - accuracy: 0.8830 - val_loss: 0.4264 - val_accuracy: 0.8715\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3970 - accuracy: 0.8830 - val_loss: 0.3753 - val_accuracy: 0.8922\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3976 - accuracy: 0.8844 - val_loss: 0.3598 - val_accuracy: 0.8953\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3945 - accuracy: 0.8854 - val_loss: 0.3753 - val_accuracy: 0.8939\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3949 - accuracy: 0.8847 - val_loss: 0.4300 - val_accuracy: 0.8761\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8844 - val_loss: 0.4049 - val_accuracy: 0.8827\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3907 - accuracy: 0.8852 - val_loss: 0.3968 - val_accuracy: 0.8874\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3906 - accuracy: 0.8874 - val_loss: 0.5402 - val_accuracy: 0.8506\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3951 - accuracy: 0.8847 - val_loss: 0.5307 - val_accuracy: 0.8479\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3878 - accuracy: 0.8871 - val_loss: 0.3800 - val_accuracy: 0.8926\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3932 - accuracy: 0.8854 - val_loss: 0.3626 - val_accuracy: 0.8983\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3859 - accuracy: 0.8877 - val_loss: 0.4172 - val_accuracy: 0.8781\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3793 - accuracy: 0.8890 - val_loss: 0.3557 - val_accuracy: 0.8969\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3888 - accuracy: 0.8868 - val_loss: 0.3640 - val_accuracy: 0.8959\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3825 - accuracy: 0.8890 - val_loss: 0.4806 - val_accuracy: 0.8580\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3843 - accuracy: 0.8878 - val_loss: 0.4121 - val_accuracy: 0.8730\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3835 - accuracy: 0.8873 - val_loss: 0.4107 - val_accuracy: 0.8795\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3850 - accuracy: 0.8878 - val_loss: 0.3459 - val_accuracy: 0.9011\n",
      "Try 2/100: Best_val_acc: [0.5222172737121582, 0.8489999771118164], lr: 6.060156175434912e-05, Lambda: 5.869585795526029e-05\n",
      "\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_428 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6424 - accuracy: 0.1110 - val_loss: 2.3786 - val_accuracy: 0.0516\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5232 - accuracy: 0.1271 - val_loss: 2.3537 - val_accuracy: 0.1017\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4233 - accuracy: 0.1500 - val_loss: 2.2096 - val_accuracy: 0.2210\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3398 - accuracy: 0.1691 - val_loss: 2.1416 - val_accuracy: 0.2435\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2494 - accuracy: 0.2006 - val_loss: 2.1289 - val_accuracy: 0.1859\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1683 - accuracy: 0.2250 - val_loss: 1.9928 - val_accuracy: 0.3119\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0964 - accuracy: 0.2519 - val_loss: 1.9032 - val_accuracy: 0.3770\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0198 - accuracy: 0.2817 - val_loss: 1.8849 - val_accuracy: 0.3515\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9464 - accuracy: 0.3125 - val_loss: 1.7778 - val_accuracy: 0.4550\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8846 - accuracy: 0.3387 - val_loss: 1.6038 - val_accuracy: 0.5806\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8198 - accuracy: 0.3674 - val_loss: 1.6487 - val_accuracy: 0.5060\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7690 - accuracy: 0.3886 - val_loss: 1.5709 - val_accuracy: 0.5774\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7066 - accuracy: 0.4176 - val_loss: 1.4411 - val_accuracy: 0.6291\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6522 - accuracy: 0.4394 - val_loss: 1.3099 - val_accuracy: 0.6874\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6060 - accuracy: 0.4581 - val_loss: 1.3426 - val_accuracy: 0.6543\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5621 - accuracy: 0.4783 - val_loss: 1.3203 - val_accuracy: 0.6639\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5156 - accuracy: 0.4990 - val_loss: 1.2267 - val_accuracy: 0.6918\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4706 - accuracy: 0.5187 - val_loss: 1.2496 - val_accuracy: 0.6770\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4398 - accuracy: 0.5309 - val_loss: 1.1461 - val_accuracy: 0.7049\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4043 - accuracy: 0.5458 - val_loss: 1.1179 - val_accuracy: 0.7194\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3679 - accuracy: 0.5587 - val_loss: 1.1000 - val_accuracy: 0.7234\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3368 - accuracy: 0.5704 - val_loss: 1.0744 - val_accuracy: 0.7226\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3072 - accuracy: 0.5810 - val_loss: 1.0535 - val_accuracy: 0.7156\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2796 - accuracy: 0.5952 - val_loss: 1.0279 - val_accuracy: 0.7247\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2498 - accuracy: 0.6046 - val_loss: 0.9525 - val_accuracy: 0.7521\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2195 - accuracy: 0.6154 - val_loss: 0.9433 - val_accuracy: 0.7643\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1883 - accuracy: 0.6263 - val_loss: 0.8990 - val_accuracy: 0.7726\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1689 - accuracy: 0.6346 - val_loss: 1.0224 - val_accuracy: 0.7139\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1417 - accuracy: 0.6463 - val_loss: 0.8603 - val_accuracy: 0.7747\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1251 - accuracy: 0.6504 - val_loss: 0.9180 - val_accuracy: 0.7589\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1070 - accuracy: 0.6579 - val_loss: 0.7753 - val_accuracy: 0.8020\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0800 - accuracy: 0.6634 - val_loss: 0.8493 - val_accuracy: 0.7689\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0662 - accuracy: 0.6714 - val_loss: 0.7855 - val_accuracy: 0.7973\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0418 - accuracy: 0.6797 - val_loss: 0.7677 - val_accuracy: 0.7870\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0206 - accuracy: 0.6873 - val_loss: 0.8804 - val_accuracy: 0.7583\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0043 - accuracy: 0.6924 - val_loss: 0.7378 - val_accuracy: 0.8013\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9955 - accuracy: 0.6976 - val_loss: 0.7964 - val_accuracy: 0.7709\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9763 - accuracy: 0.7017 - val_loss: 0.7613 - val_accuracy: 0.7904\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9600 - accuracy: 0.7067 - val_loss: 0.8101 - val_accuracy: 0.7624\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9433 - accuracy: 0.7128 - val_loss: 0.6510 - val_accuracy: 0.8196\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9310 - accuracy: 0.7154 - val_loss: 0.6835 - val_accuracy: 0.8089\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9183 - accuracy: 0.7206 - val_loss: 0.8040 - val_accuracy: 0.7660\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8982 - accuracy: 0.7271 - val_loss: 0.6989 - val_accuracy: 0.8017\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8916 - accuracy: 0.7301 - val_loss: 0.6886 - val_accuracy: 0.8061\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8777 - accuracy: 0.7352 - val_loss: 0.8189 - val_accuracy: 0.7415\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8624 - accuracy: 0.7393 - val_loss: 0.7108 - val_accuracy: 0.7872\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8526 - accuracy: 0.7439 - val_loss: 0.6146 - val_accuracy: 0.8306\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8458 - accuracy: 0.7448 - val_loss: 0.6300 - val_accuracy: 0.8231\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8297 - accuracy: 0.7510 - val_loss: 0.6009 - val_accuracy: 0.8259\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8205 - accuracy: 0.7538 - val_loss: 0.6478 - val_accuracy: 0.8081\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8124 - accuracy: 0.7583 - val_loss: 0.5708 - val_accuracy: 0.8408\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8068 - accuracy: 0.7568 - val_loss: 0.6028 - val_accuracy: 0.8273\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7920 - accuracy: 0.7647 - val_loss: 0.7433 - val_accuracy: 0.7860\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7909 - accuracy: 0.7632 - val_loss: 0.7487 - val_accuracy: 0.7749\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7775 - accuracy: 0.7657 - val_loss: 0.6083 - val_accuracy: 0.8225\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7698 - accuracy: 0.7684 - val_loss: 0.6547 - val_accuracy: 0.8119\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7638 - accuracy: 0.7706 - val_loss: 0.6511 - val_accuracy: 0.8139\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7496 - accuracy: 0.7757 - val_loss: 0.6308 - val_accuracy: 0.8170\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7457 - accuracy: 0.7768 - val_loss: 0.5812 - val_accuracy: 0.8298\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7396 - accuracy: 0.7804 - val_loss: 0.5048 - val_accuracy: 0.8578\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7339 - accuracy: 0.7793 - val_loss: 0.6576 - val_accuracy: 0.8032\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7258 - accuracy: 0.7822 - val_loss: 0.6537 - val_accuracy: 0.8042\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7159 - accuracy: 0.7875 - val_loss: 0.6746 - val_accuracy: 0.7999\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7171 - accuracy: 0.7877 - val_loss: 0.5875 - val_accuracy: 0.8239\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7021 - accuracy: 0.7904 - val_loss: 0.5304 - val_accuracy: 0.8404\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6993 - accuracy: 0.7916 - val_loss: 0.5352 - val_accuracy: 0.8458\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6941 - accuracy: 0.7941 - val_loss: 0.5120 - val_accuracy: 0.8499\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6888 - accuracy: 0.7948 - val_loss: 0.5072 - val_accuracy: 0.8530\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6830 - accuracy: 0.7977 - val_loss: 0.5216 - val_accuracy: 0.8425\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6764 - accuracy: 0.7979 - val_loss: 0.5782 - val_accuracy: 0.8201\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6688 - accuracy: 0.8020 - val_loss: 0.4857 - val_accuracy: 0.8617\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6672 - accuracy: 0.8010 - val_loss: 0.6271 - val_accuracy: 0.8204\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6540 - accuracy: 0.8061 - val_loss: 0.5241 - val_accuracy: 0.8464\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6523 - accuracy: 0.8079 - val_loss: 0.5771 - val_accuracy: 0.8340\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6531 - accuracy: 0.8072 - val_loss: 0.4858 - val_accuracy: 0.8605\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6450 - accuracy: 0.8105 - val_loss: 0.6392 - val_accuracy: 0.8074\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6308 - accuracy: 0.8119 - val_loss: 0.5474 - val_accuracy: 0.8356\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6356 - accuracy: 0.8112 - val_loss: 0.5287 - val_accuracy: 0.8484\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6279 - accuracy: 0.8138 - val_loss: 0.5981 - val_accuracy: 0.8144\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6285 - accuracy: 0.8127 - val_loss: 0.5031 - val_accuracy: 0.8544\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6173 - accuracy: 0.8169 - val_loss: 0.6069 - val_accuracy: 0.8194\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6198 - accuracy: 0.8161 - val_loss: 0.5158 - val_accuracy: 0.8479\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6171 - accuracy: 0.8154 - val_loss: 0.4617 - val_accuracy: 0.8663\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6129 - accuracy: 0.8192 - val_loss: 0.4937 - val_accuracy: 0.8539\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6076 - accuracy: 0.8213 - val_loss: 0.4574 - val_accuracy: 0.8699\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6054 - accuracy: 0.8204 - val_loss: 0.6413 - val_accuracy: 0.8114\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5973 - accuracy: 0.8242 - val_loss: 0.4575 - val_accuracy: 0.8675\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5929 - accuracy: 0.8232 - val_loss: 0.6380 - val_accuracy: 0.8116\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5935 - accuracy: 0.8241 - val_loss: 0.4782 - val_accuracy: 0.8603\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5832 - accuracy: 0.8297 - val_loss: 0.5450 - val_accuracy: 0.8354\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5849 - accuracy: 0.8291 - val_loss: 0.4465 - val_accuracy: 0.8735\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5790 - accuracy: 0.8281 - val_loss: 0.3924 - val_accuracy: 0.8899\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5809 - accuracy: 0.8279 - val_loss: 0.5700 - val_accuracy: 0.8331\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5754 - accuracy: 0.8317 - val_loss: 0.4014 - val_accuracy: 0.8838\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5639 - accuracy: 0.8335 - val_loss: 0.5692 - val_accuracy: 0.8215\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5677 - accuracy: 0.8327 - val_loss: 0.4139 - val_accuracy: 0.8756\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5657 - accuracy: 0.8330 - val_loss: 0.4208 - val_accuracy: 0.8764\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5659 - accuracy: 0.8333 - val_loss: 0.5451 - val_accuracy: 0.8387\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5616 - accuracy: 0.8345 - val_loss: 0.4280 - val_accuracy: 0.8767\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5537 - accuracy: 0.8387 - val_loss: 0.5061 - val_accuracy: 0.8508\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5532 - accuracy: 0.8362 - val_loss: 0.4469 - val_accuracy: 0.8677\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5523 - accuracy: 0.8374 - val_loss: 0.5234 - val_accuracy: 0.8453\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5493 - accuracy: 0.8389 - val_loss: 0.5031 - val_accuracy: 0.8535\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5509 - accuracy: 0.8380 - val_loss: 0.4672 - val_accuracy: 0.8611\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5491 - accuracy: 0.8380 - val_loss: 0.5022 - val_accuracy: 0.8509\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5377 - accuracy: 0.8435 - val_loss: 0.5531 - val_accuracy: 0.8316\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5353 - accuracy: 0.8428 - val_loss: 0.4454 - val_accuracy: 0.8686\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5282 - accuracy: 0.8446 - val_loss: 0.4221 - val_accuracy: 0.8746\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5327 - accuracy: 0.8431 - val_loss: 0.4079 - val_accuracy: 0.8850\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5294 - accuracy: 0.8454 - val_loss: 0.4527 - val_accuracy: 0.8641\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5217 - accuracy: 0.8460 - val_loss: 0.5443 - val_accuracy: 0.8313\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5295 - accuracy: 0.8438 - val_loss: 0.4521 - val_accuracy: 0.8659\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5140 - accuracy: 0.8501 - val_loss: 0.3693 - val_accuracy: 0.8933\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5177 - accuracy: 0.8480 - val_loss: 0.4069 - val_accuracy: 0.8828\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5145 - accuracy: 0.8482 - val_loss: 0.4191 - val_accuracy: 0.8744\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5078 - accuracy: 0.8515 - val_loss: 0.4657 - val_accuracy: 0.8609\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5099 - accuracy: 0.8508 - val_loss: 0.3880 - val_accuracy: 0.8879\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5096 - accuracy: 0.8494 - val_loss: 0.5092 - val_accuracy: 0.8509\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5033 - accuracy: 0.8513 - val_loss: 0.3941 - val_accuracy: 0.8878\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5048 - accuracy: 0.8504 - val_loss: 0.5182 - val_accuracy: 0.8476\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5020 - accuracy: 0.8523 - val_loss: 0.4241 - val_accuracy: 0.8774\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4973 - accuracy: 0.8537 - val_loss: 0.4619 - val_accuracy: 0.8651\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4957 - accuracy: 0.8545 - val_loss: 0.3614 - val_accuracy: 0.8976\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4974 - accuracy: 0.8544 - val_loss: 0.4491 - val_accuracy: 0.8708\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4873 - accuracy: 0.8572 - val_loss: 0.3580 - val_accuracy: 0.8986\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4896 - accuracy: 0.8565 - val_loss: 0.4177 - val_accuracy: 0.8785\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4894 - accuracy: 0.8561 - val_loss: 0.4106 - val_accuracy: 0.8812\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4844 - accuracy: 0.8592 - val_loss: 0.3738 - val_accuracy: 0.8899\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4894 - accuracy: 0.8568 - val_loss: 0.3422 - val_accuracy: 0.9036\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4818 - accuracy: 0.8593 - val_loss: 0.4363 - val_accuracy: 0.8733\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4836 - accuracy: 0.8594 - val_loss: 0.4068 - val_accuracy: 0.8831\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4849 - accuracy: 0.8577 - val_loss: 0.4413 - val_accuracy: 0.8702\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4758 - accuracy: 0.8604 - val_loss: 0.5205 - val_accuracy: 0.8431\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4707 - accuracy: 0.8613 - val_loss: 0.4408 - val_accuracy: 0.8751\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4711 - accuracy: 0.8626 - val_loss: 0.5182 - val_accuracy: 0.8494\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4711 - accuracy: 0.8609 - val_loss: 0.4272 - val_accuracy: 0.8742\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.8585 - val_loss: 0.3926 - val_accuracy: 0.8869\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4685 - accuracy: 0.8628 - val_loss: 0.4133 - val_accuracy: 0.8756\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4631 - accuracy: 0.8644 - val_loss: 0.3567 - val_accuracy: 0.8966\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4619 - accuracy: 0.8624 - val_loss: 0.4255 - val_accuracy: 0.8729\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4599 - accuracy: 0.8649 - val_loss: 0.4116 - val_accuracy: 0.8814\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4541 - accuracy: 0.8678 - val_loss: 0.4052 - val_accuracy: 0.8801\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4636 - accuracy: 0.8636 - val_loss: 0.4608 - val_accuracy: 0.8617\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4543 - accuracy: 0.8669 - val_loss: 0.3744 - val_accuracy: 0.8911\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4560 - accuracy: 0.8677 - val_loss: 0.4642 - val_accuracy: 0.8632\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4534 - accuracy: 0.8672 - val_loss: 0.6145 - val_accuracy: 0.8149\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4506 - accuracy: 0.8684 - val_loss: 0.5317 - val_accuracy: 0.8466\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4527 - accuracy: 0.8680 - val_loss: 0.4627 - val_accuracy: 0.8638\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4524 - accuracy: 0.8673 - val_loss: 0.4701 - val_accuracy: 0.8569\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4429 - accuracy: 0.8721 - val_loss: 0.4840 - val_accuracy: 0.8468\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4413 - accuracy: 0.8703 - val_loss: 0.3884 - val_accuracy: 0.8891\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4409 - accuracy: 0.8727 - val_loss: 0.3957 - val_accuracy: 0.8875\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4461 - accuracy: 0.8691 - val_loss: 0.4321 - val_accuracy: 0.8749\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4403 - accuracy: 0.8707 - val_loss: 0.5079 - val_accuracy: 0.8443\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4391 - accuracy: 0.8726 - val_loss: 0.3896 - val_accuracy: 0.8876\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4317 - accuracy: 0.8731 - val_loss: 0.4906 - val_accuracy: 0.8596\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4359 - accuracy: 0.8713 - val_loss: 0.4224 - val_accuracy: 0.8749\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4328 - accuracy: 0.8735 - val_loss: 0.3815 - val_accuracy: 0.8894\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4327 - accuracy: 0.8735 - val_loss: 0.3613 - val_accuracy: 0.8904\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4274 - accuracy: 0.8738 - val_loss: 0.3065 - val_accuracy: 0.9134\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4293 - accuracy: 0.8755 - val_loss: 0.5411 - val_accuracy: 0.8418\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4295 - accuracy: 0.8744 - val_loss: 0.3123 - val_accuracy: 0.9114\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4240 - accuracy: 0.8757 - val_loss: 0.3562 - val_accuracy: 0.8997\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4246 - accuracy: 0.8753 - val_loss: 0.3847 - val_accuracy: 0.8897\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4223 - accuracy: 0.8768 - val_loss: 0.4005 - val_accuracy: 0.8775\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4180 - accuracy: 0.8774 - val_loss: 0.3796 - val_accuracy: 0.8896\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4177 - accuracy: 0.8779 - val_loss: 0.4463 - val_accuracy: 0.8641\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4136 - accuracy: 0.8793 - val_loss: 0.4062 - val_accuracy: 0.8824\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4139 - accuracy: 0.8775 - val_loss: 0.3387 - val_accuracy: 0.9040\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8782 - val_loss: 0.3841 - val_accuracy: 0.8889\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4125 - accuracy: 0.8797 - val_loss: 0.3600 - val_accuracy: 0.8935\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4109 - accuracy: 0.8802 - val_loss: 0.4365 - val_accuracy: 0.8733\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4052 - accuracy: 0.8819 - val_loss: 0.3411 - val_accuracy: 0.9035\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8765 - val_loss: 0.4034 - val_accuracy: 0.8871\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4086 - accuracy: 0.8797 - val_loss: 0.3779 - val_accuracy: 0.8906\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4123 - accuracy: 0.8805 - val_loss: 0.3587 - val_accuracy: 0.8961\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8809 - val_loss: 0.3703 - val_accuracy: 0.8899\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4033 - accuracy: 0.8833 - val_loss: 0.4161 - val_accuracy: 0.8819\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4006 - accuracy: 0.8832 - val_loss: 0.3817 - val_accuracy: 0.8902\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4042 - accuracy: 0.8825 - val_loss: 0.4491 - val_accuracy: 0.8711\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4075 - accuracy: 0.8827 - val_loss: 0.3183 - val_accuracy: 0.9096\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8842 - val_loss: 0.3804 - val_accuracy: 0.8894\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8856 - val_loss: 0.3742 - val_accuracy: 0.8965\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4015 - accuracy: 0.8829 - val_loss: 0.4576 - val_accuracy: 0.8666\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3970 - accuracy: 0.8850 - val_loss: 0.2908 - val_accuracy: 0.9188\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8858 - val_loss: 0.4140 - val_accuracy: 0.8813\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8858 - val_loss: 0.3938 - val_accuracy: 0.8865\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8845 - val_loss: 0.3153 - val_accuracy: 0.9111\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3928 - accuracy: 0.8847 - val_loss: 0.3933 - val_accuracy: 0.8901\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3932 - accuracy: 0.8854 - val_loss: 0.3468 - val_accuracy: 0.8972\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3896 - accuracy: 0.8867 - val_loss: 0.3467 - val_accuracy: 0.9004\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3875 - accuracy: 0.8876 - val_loss: 0.3677 - val_accuracy: 0.8925\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3890 - accuracy: 0.8862 - val_loss: 0.3890 - val_accuracy: 0.8855\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3908 - accuracy: 0.8847 - val_loss: 0.3720 - val_accuracy: 0.8906\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8881 - val_loss: 0.5192 - val_accuracy: 0.8477\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3857 - accuracy: 0.8888 - val_loss: 0.3804 - val_accuracy: 0.8912\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3813 - accuracy: 0.8875 - val_loss: 0.3193 - val_accuracy: 0.9107\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3785 - accuracy: 0.8885 - val_loss: 0.3877 - val_accuracy: 0.8861\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3802 - accuracy: 0.8892 - val_loss: 0.2977 - val_accuracy: 0.9159\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3797 - accuracy: 0.8897 - val_loss: 0.3984 - val_accuracy: 0.8826\n",
      "Try 3/100: Best_val_acc: [0.49875184893608093, 0.8531110882759094], lr: 6.136348952574759e-05, Lambda: 5.769352285569157e-05\n",
      "\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_429 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6715 - accuracy: 0.1094 - val_loss: 2.3685 - val_accuracy: 0.1149\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5318 - accuracy: 0.1313 - val_loss: 2.3737 - val_accuracy: 0.1347\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4194 - accuracy: 0.1525 - val_loss: 2.2656 - val_accuracy: 0.1762\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3120 - accuracy: 0.1820 - val_loss: 2.1363 - val_accuracy: 0.2663\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2083 - accuracy: 0.2146 - val_loss: 2.0077 - val_accuracy: 0.3356\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1240 - accuracy: 0.2429 - val_loss: 1.9525 - val_accuracy: 0.3453\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0432 - accuracy: 0.2762 - val_loss: 1.8463 - val_accuracy: 0.3997\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9712 - accuracy: 0.3052 - val_loss: 1.7369 - val_accuracy: 0.4547\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9085 - accuracy: 0.3297 - val_loss: 1.6786 - val_accuracy: 0.4911\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8490 - accuracy: 0.3584 - val_loss: 1.6090 - val_accuracy: 0.5273\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7944 - accuracy: 0.3826 - val_loss: 1.5836 - val_accuracy: 0.5393\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7447 - accuracy: 0.4035 - val_loss: 1.4323 - val_accuracy: 0.5984\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6956 - accuracy: 0.4253 - val_loss: 1.3997 - val_accuracy: 0.5986\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6438 - accuracy: 0.4509 - val_loss: 1.3423 - val_accuracy: 0.6332\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6008 - accuracy: 0.4686 - val_loss: 1.3615 - val_accuracy: 0.6232\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5568 - accuracy: 0.4853 - val_loss: 1.2772 - val_accuracy: 0.6596\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5118 - accuracy: 0.5051 - val_loss: 1.2182 - val_accuracy: 0.6711\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4753 - accuracy: 0.5216 - val_loss: 1.1589 - val_accuracy: 0.7033\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4409 - accuracy: 0.5389 - val_loss: 1.1472 - val_accuracy: 0.6954\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4007 - accuracy: 0.5554 - val_loss: 1.1332 - val_accuracy: 0.6867\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3676 - accuracy: 0.5660 - val_loss: 1.1332 - val_accuracy: 0.6841\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3412 - accuracy: 0.5800 - val_loss: 1.0781 - val_accuracy: 0.7056\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3035 - accuracy: 0.5922 - val_loss: 1.0894 - val_accuracy: 0.7070\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2771 - accuracy: 0.6031 - val_loss: 1.0657 - val_accuracy: 0.7018\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2509 - accuracy: 0.6137 - val_loss: 1.0345 - val_accuracy: 0.7166\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2215 - accuracy: 0.6206 - val_loss: 0.9974 - val_accuracy: 0.7265\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1999 - accuracy: 0.6318 - val_loss: 0.9418 - val_accuracy: 0.7412\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1804 - accuracy: 0.6384 - val_loss: 0.9517 - val_accuracy: 0.7330\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1523 - accuracy: 0.6473 - val_loss: 0.8860 - val_accuracy: 0.7607\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1229 - accuracy: 0.6572 - val_loss: 0.9576 - val_accuracy: 0.7252\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1074 - accuracy: 0.6626 - val_loss: 0.9737 - val_accuracy: 0.7142\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0907 - accuracy: 0.6682 - val_loss: 0.8997 - val_accuracy: 0.7364\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0691 - accuracy: 0.6753 - val_loss: 0.8471 - val_accuracy: 0.7686\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0529 - accuracy: 0.6812 - val_loss: 0.8200 - val_accuracy: 0.7758\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0324 - accuracy: 0.6862 - val_loss: 0.8080 - val_accuracy: 0.7779\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0121 - accuracy: 0.6949 - val_loss: 0.8609 - val_accuracy: 0.7524\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9988 - accuracy: 0.7000 - val_loss: 0.8413 - val_accuracy: 0.7532\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9766 - accuracy: 0.7065 - val_loss: 0.7195 - val_accuracy: 0.8012\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9669 - accuracy: 0.7102 - val_loss: 0.7427 - val_accuracy: 0.7892\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9533 - accuracy: 0.7135 - val_loss: 0.7133 - val_accuracy: 0.7995\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9385 - accuracy: 0.7170 - val_loss: 0.6863 - val_accuracy: 0.8138\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9250 - accuracy: 0.7205 - val_loss: 0.7839 - val_accuracy: 0.7720\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9033 - accuracy: 0.7270 - val_loss: 0.7389 - val_accuracy: 0.7884\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9008 - accuracy: 0.7280 - val_loss: 0.7049 - val_accuracy: 0.7997\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8891 - accuracy: 0.7339 - val_loss: 0.6723 - val_accuracy: 0.8119\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8758 - accuracy: 0.7377 - val_loss: 0.7169 - val_accuracy: 0.7881\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8600 - accuracy: 0.7415 - val_loss: 0.6495 - val_accuracy: 0.8086\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8531 - accuracy: 0.7458 - val_loss: 0.5996 - val_accuracy: 0.8231\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8385 - accuracy: 0.7475 - val_loss: 0.6025 - val_accuracy: 0.8226\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8264 - accuracy: 0.7531 - val_loss: 0.6590 - val_accuracy: 0.8034\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8239 - accuracy: 0.7514 - val_loss: 0.6946 - val_accuracy: 0.7902\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8089 - accuracy: 0.7578 - val_loss: 0.6699 - val_accuracy: 0.7959\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8018 - accuracy: 0.7597 - val_loss: 0.6902 - val_accuracy: 0.7954\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8007 - accuracy: 0.7611 - val_loss: 0.6173 - val_accuracy: 0.8166\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7786 - accuracy: 0.7669 - val_loss: 0.5570 - val_accuracy: 0.8391\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7759 - accuracy: 0.7682 - val_loss: 0.6305 - val_accuracy: 0.8163\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7680 - accuracy: 0.7700 - val_loss: 0.6038 - val_accuracy: 0.8216\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7603 - accuracy: 0.7740 - val_loss: 0.6137 - val_accuracy: 0.8180\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7542 - accuracy: 0.7754 - val_loss: 0.6491 - val_accuracy: 0.8034\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7475 - accuracy: 0.7762 - val_loss: 0.5377 - val_accuracy: 0.8447\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7394 - accuracy: 0.7785 - val_loss: 0.6591 - val_accuracy: 0.8034\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7280 - accuracy: 0.7828 - val_loss: 0.5060 - val_accuracy: 0.8562\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7202 - accuracy: 0.7873 - val_loss: 0.5975 - val_accuracy: 0.8257\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7148 - accuracy: 0.7881 - val_loss: 0.6081 - val_accuracy: 0.8188\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7086 - accuracy: 0.7897 - val_loss: 0.5996 - val_accuracy: 0.8239\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7031 - accuracy: 0.7922 - val_loss: 0.5445 - val_accuracy: 0.8444\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6939 - accuracy: 0.7933 - val_loss: 0.4678 - val_accuracy: 0.8648\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6928 - accuracy: 0.7950 - val_loss: 0.4842 - val_accuracy: 0.8567\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6911 - accuracy: 0.7950 - val_loss: 0.5434 - val_accuracy: 0.8423\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6753 - accuracy: 0.7990 - val_loss: 0.5630 - val_accuracy: 0.8319\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6727 - accuracy: 0.7995 - val_loss: 0.5474 - val_accuracy: 0.8370\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6625 - accuracy: 0.8029 - val_loss: 0.5371 - val_accuracy: 0.8361\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6673 - accuracy: 0.8025 - val_loss: 0.5420 - val_accuracy: 0.8414\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6635 - accuracy: 0.8030 - val_loss: 0.5848 - val_accuracy: 0.8248\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6568 - accuracy: 0.8068 - val_loss: 0.4824 - val_accuracy: 0.8555\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6444 - accuracy: 0.8065 - val_loss: 0.6076 - val_accuracy: 0.8141\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6447 - accuracy: 0.8087 - val_loss: 0.4814 - val_accuracy: 0.8621\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6375 - accuracy: 0.8118 - val_loss: 0.5832 - val_accuracy: 0.8242\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6310 - accuracy: 0.8125 - val_loss: 0.5079 - val_accuracy: 0.8476\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6317 - accuracy: 0.8131 - val_loss: 0.5807 - val_accuracy: 0.8232\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6315 - accuracy: 0.8120 - val_loss: 0.4826 - val_accuracy: 0.8541\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6210 - accuracy: 0.8163 - val_loss: 0.5332 - val_accuracy: 0.8381\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6193 - accuracy: 0.8173 - val_loss: 0.6172 - val_accuracy: 0.8115\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6144 - accuracy: 0.8206 - val_loss: 0.4566 - val_accuracy: 0.8692\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.8195 - val_loss: 0.4953 - val_accuracy: 0.8514\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6051 - accuracy: 0.8214 - val_loss: 0.5902 - val_accuracy: 0.8248\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5948 - accuracy: 0.8246 - val_loss: 0.4626 - val_accuracy: 0.8651\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6023 - accuracy: 0.8221 - val_loss: 0.4434 - val_accuracy: 0.8732\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5956 - accuracy: 0.8242 - val_loss: 0.5039 - val_accuracy: 0.8476\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5906 - accuracy: 0.8235 - val_loss: 0.4536 - val_accuracy: 0.8651\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5887 - accuracy: 0.8245 - val_loss: 0.5657 - val_accuracy: 0.8304\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5854 - accuracy: 0.8275 - val_loss: 0.4801 - val_accuracy: 0.8584\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5831 - accuracy: 0.8266 - val_loss: 0.5228 - val_accuracy: 0.8466\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5803 - accuracy: 0.8293 - val_loss: 0.5008 - val_accuracy: 0.8446\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5755 - accuracy: 0.8309 - val_loss: 0.4527 - val_accuracy: 0.8669\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5686 - accuracy: 0.8317 - val_loss: 0.4837 - val_accuracy: 0.8588\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5663 - accuracy: 0.8319 - val_loss: 0.5300 - val_accuracy: 0.8433\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5688 - accuracy: 0.8317 - val_loss: 0.5536 - val_accuracy: 0.8345\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5614 - accuracy: 0.8344 - val_loss: 0.3993 - val_accuracy: 0.8881\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5584 - accuracy: 0.8354 - val_loss: 0.5885 - val_accuracy: 0.8228\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5529 - accuracy: 0.8371 - val_loss: 0.4633 - val_accuracy: 0.8636\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5563 - accuracy: 0.8375 - val_loss: 0.4803 - val_accuracy: 0.8592\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5490 - accuracy: 0.8394 - val_loss: 0.4489 - val_accuracy: 0.8726\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5443 - accuracy: 0.8413 - val_loss: 0.4846 - val_accuracy: 0.8571\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5459 - accuracy: 0.8365 - val_loss: 0.5632 - val_accuracy: 0.8304\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5376 - accuracy: 0.8424 - val_loss: 0.4709 - val_accuracy: 0.8605\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5395 - accuracy: 0.8405 - val_loss: 0.4976 - val_accuracy: 0.8579\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5365 - accuracy: 0.8422 - val_loss: 0.3898 - val_accuracy: 0.8878\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5305 - accuracy: 0.8439 - val_loss: 0.4262 - val_accuracy: 0.8726\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5346 - accuracy: 0.8417 - val_loss: 0.4855 - val_accuracy: 0.8589\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5248 - accuracy: 0.8460 - val_loss: 0.3721 - val_accuracy: 0.8969\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5247 - accuracy: 0.8450 - val_loss: 0.4712 - val_accuracy: 0.8617\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5270 - accuracy: 0.8465 - val_loss: 0.4456 - val_accuracy: 0.8712\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5126 - accuracy: 0.8485 - val_loss: 0.4449 - val_accuracy: 0.8684\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5193 - accuracy: 0.8470 - val_loss: 0.3613 - val_accuracy: 0.8962\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5142 - accuracy: 0.8490 - val_loss: 0.4893 - val_accuracy: 0.8541\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5107 - accuracy: 0.8502 - val_loss: 0.4744 - val_accuracy: 0.8575\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5062 - accuracy: 0.8487 - val_loss: 0.4821 - val_accuracy: 0.8622\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5126 - accuracy: 0.8491 - val_loss: 0.4345 - val_accuracy: 0.8754\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5034 - accuracy: 0.8530 - val_loss: 0.4791 - val_accuracy: 0.8591\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8505 - val_loss: 0.3986 - val_accuracy: 0.8812\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4968 - accuracy: 0.8555 - val_loss: 0.4803 - val_accuracy: 0.8595\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5011 - accuracy: 0.8522 - val_loss: 0.4059 - val_accuracy: 0.8808\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4903 - accuracy: 0.8557 - val_loss: 0.4151 - val_accuracy: 0.8789\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4956 - accuracy: 0.8554 - val_loss: 0.4075 - val_accuracy: 0.8824\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4910 - accuracy: 0.8574 - val_loss: 0.4623 - val_accuracy: 0.8665\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4951 - accuracy: 0.8554 - val_loss: 0.6102 - val_accuracy: 0.8176\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4890 - accuracy: 0.8565 - val_loss: 0.5050 - val_accuracy: 0.8478\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4914 - accuracy: 0.8556 - val_loss: 0.5113 - val_accuracy: 0.8547\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4831 - accuracy: 0.8585 - val_loss: 0.5166 - val_accuracy: 0.8484\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4811 - accuracy: 0.8590 - val_loss: 0.4320 - val_accuracy: 0.8716\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4793 - accuracy: 0.8593 - val_loss: 0.3827 - val_accuracy: 0.8881\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4768 - accuracy: 0.8595 - val_loss: 0.4521 - val_accuracy: 0.8666\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4729 - accuracy: 0.8630 - val_loss: 0.3875 - val_accuracy: 0.8906\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4728 - accuracy: 0.8608 - val_loss: 0.3401 - val_accuracy: 0.9031\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4735 - accuracy: 0.8620 - val_loss: 0.4672 - val_accuracy: 0.8627\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4643 - accuracy: 0.8639 - val_loss: 0.4520 - val_accuracy: 0.8639\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4714 - accuracy: 0.8633 - val_loss: 0.3765 - val_accuracy: 0.8894\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4629 - accuracy: 0.8646 - val_loss: 0.4495 - val_accuracy: 0.8624\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4630 - accuracy: 0.8653 - val_loss: 0.3866 - val_accuracy: 0.8890\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4714 - accuracy: 0.8617 - val_loss: 0.4666 - val_accuracy: 0.8663\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4630 - accuracy: 0.8635 - val_loss: 0.4371 - val_accuracy: 0.8736\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4592 - accuracy: 0.8645 - val_loss: 0.4190 - val_accuracy: 0.8803\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4614 - accuracy: 0.8660 - val_loss: 0.3640 - val_accuracy: 0.8953\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4555 - accuracy: 0.8671 - val_loss: 0.4600 - val_accuracy: 0.8653\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4522 - accuracy: 0.8684 - val_loss: 0.4160 - val_accuracy: 0.8723\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4540 - accuracy: 0.8671 - val_loss: 0.3792 - val_accuracy: 0.8904\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4509 - accuracy: 0.8686 - val_loss: 0.3739 - val_accuracy: 0.8946\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4488 - accuracy: 0.8694 - val_loss: 0.3798 - val_accuracy: 0.8889\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4457 - accuracy: 0.8695 - val_loss: 0.4010 - val_accuracy: 0.8860\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4463 - accuracy: 0.8687 - val_loss: 0.3717 - val_accuracy: 0.8947\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4428 - accuracy: 0.8716 - val_loss: 0.4530 - val_accuracy: 0.8694\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4411 - accuracy: 0.8712 - val_loss: 0.4445 - val_accuracy: 0.8718\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8701 - val_loss: 0.4398 - val_accuracy: 0.8723\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4393 - accuracy: 0.8715 - val_loss: 0.4753 - val_accuracy: 0.8626\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4371 - accuracy: 0.8716 - val_loss: 0.7663 - val_accuracy: 0.7725\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8712 - val_loss: 0.3985 - val_accuracy: 0.8839\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8717 - val_loss: 0.3561 - val_accuracy: 0.9006\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4356 - accuracy: 0.8729 - val_loss: 0.4993 - val_accuracy: 0.8542\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4261 - accuracy: 0.8759 - val_loss: 0.4029 - val_accuracy: 0.8832\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4328 - accuracy: 0.8740 - val_loss: 0.3975 - val_accuracy: 0.8847\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4264 - accuracy: 0.8754 - val_loss: 0.4695 - val_accuracy: 0.8597\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4266 - accuracy: 0.8745 - val_loss: 0.5831 - val_accuracy: 0.8300\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4256 - accuracy: 0.8754 - val_loss: 0.3532 - val_accuracy: 0.9011\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4259 - accuracy: 0.8763 - val_loss: 0.3434 - val_accuracy: 0.9001\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4236 - accuracy: 0.8772 - val_loss: 0.3577 - val_accuracy: 0.8975\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4272 - accuracy: 0.8747 - val_loss: 0.3882 - val_accuracy: 0.8889\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4126 - accuracy: 0.8795 - val_loss: 0.3593 - val_accuracy: 0.9007\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4192 - accuracy: 0.8773 - val_loss: 0.3180 - val_accuracy: 0.9123\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4147 - accuracy: 0.8791 - val_loss: 0.4679 - val_accuracy: 0.8631\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4171 - accuracy: 0.8791 - val_loss: 0.3219 - val_accuracy: 0.9101\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4144 - accuracy: 0.8787 - val_loss: 0.4467 - val_accuracy: 0.8706\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4152 - accuracy: 0.8804 - val_loss: 0.3739 - val_accuracy: 0.8866\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4135 - accuracy: 0.8783 - val_loss: 0.3511 - val_accuracy: 0.9027\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4093 - accuracy: 0.8809 - val_loss: 0.4493 - val_accuracy: 0.8590\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4139 - accuracy: 0.8797 - val_loss: 0.4441 - val_accuracy: 0.8705\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4104 - accuracy: 0.8797 - val_loss: 0.4863 - val_accuracy: 0.8540\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4046 - accuracy: 0.8824 - val_loss: 0.3258 - val_accuracy: 0.9058\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4083 - accuracy: 0.8818 - val_loss: 0.3039 - val_accuracy: 0.9136\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4021 - accuracy: 0.8820 - val_loss: 0.3528 - val_accuracy: 0.8976\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4073 - accuracy: 0.8807 - val_loss: 0.3476 - val_accuracy: 0.9011\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3959 - accuracy: 0.8847 - val_loss: 0.4853 - val_accuracy: 0.8629\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8810 - val_loss: 0.4132 - val_accuracy: 0.8831\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3989 - accuracy: 0.8833 - val_loss: 0.5620 - val_accuracy: 0.8331\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8836 - val_loss: 0.3293 - val_accuracy: 0.9080\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3907 - accuracy: 0.8861 - val_loss: 0.2933 - val_accuracy: 0.9212\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8845 - val_loss: 0.3989 - val_accuracy: 0.8834\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3989 - accuracy: 0.8840 - val_loss: 0.3434 - val_accuracy: 0.9031\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3948 - accuracy: 0.8854 - val_loss: 0.4610 - val_accuracy: 0.8684\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3941 - accuracy: 0.8854 - val_loss: 0.3825 - val_accuracy: 0.8882\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.3977 - accuracy: 0.8844 - val_loss: 0.3891 - val_accuracy: 0.8899\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3876 - accuracy: 0.8870 - val_loss: 0.5218 - val_accuracy: 0.8479\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3906 - accuracy: 0.8868 - val_loss: 0.4748 - val_accuracy: 0.8624\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3867 - accuracy: 0.8884 - val_loss: 0.3517 - val_accuracy: 0.8987\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3806 - accuracy: 0.8888 - val_loss: 0.3991 - val_accuracy: 0.8841\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3790 - accuracy: 0.8881 - val_loss: 0.3843 - val_accuracy: 0.8888\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3815 - accuracy: 0.8889 - val_loss: 0.3348 - val_accuracy: 0.9046\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3865 - accuracy: 0.8868 - val_loss: 0.3531 - val_accuracy: 0.9026\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3851 - accuracy: 0.8871 - val_loss: 0.3933 - val_accuracy: 0.8849\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3822 - accuracy: 0.8898 - val_loss: 0.3334 - val_accuracy: 0.9068\n",
      "Try 4/100: Best_val_acc: [0.4956532120704651, 0.8551111221313477], lr: 6.151852245826218e-05, Lambda: 5.813284986003816e-05\n",
      "\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_430 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7152 - accuracy: 0.1050 - val_loss: 2.4259 - val_accuracy: 0.0621\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5768 - accuracy: 0.1198 - val_loss: 2.4530 - val_accuracy: 0.1059\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4637 - accuracy: 0.1423 - val_loss: 2.3631 - val_accuracy: 0.1590\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3494 - accuracy: 0.1654 - val_loss: 2.3207 - val_accuracy: 0.1579\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2465 - accuracy: 0.1929 - val_loss: 2.1763 - val_accuracy: 0.2246\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1571 - accuracy: 0.2270 - val_loss: 2.0891 - val_accuracy: 0.2852\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0736 - accuracy: 0.2600 - val_loss: 1.9519 - val_accuracy: 0.3834\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0012 - accuracy: 0.2877 - val_loss: 1.8784 - val_accuracy: 0.4444\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9404 - accuracy: 0.3157 - val_loss: 1.8547 - val_accuracy: 0.4405\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8752 - accuracy: 0.3472 - val_loss: 1.6940 - val_accuracy: 0.5271\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8193 - accuracy: 0.3708 - val_loss: 1.6141 - val_accuracy: 0.5724\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7698 - accuracy: 0.3974 - val_loss: 1.5700 - val_accuracy: 0.5991\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7151 - accuracy: 0.4201 - val_loss: 1.4457 - val_accuracy: 0.6427\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6661 - accuracy: 0.4427 - val_loss: 1.3935 - val_accuracy: 0.6582\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6259 - accuracy: 0.4620 - val_loss: 1.3163 - val_accuracy: 0.6861\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5789 - accuracy: 0.4750 - val_loss: 1.3097 - val_accuracy: 0.6781\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5377 - accuracy: 0.4941 - val_loss: 1.2760 - val_accuracy: 0.6803\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4974 - accuracy: 0.5155 - val_loss: 1.1928 - val_accuracy: 0.7051\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4563 - accuracy: 0.5294 - val_loss: 1.2342 - val_accuracy: 0.6833\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4217 - accuracy: 0.5422 - val_loss: 1.1507 - val_accuracy: 0.7136\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3873 - accuracy: 0.5556 - val_loss: 1.0659 - val_accuracy: 0.7359\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3563 - accuracy: 0.5688 - val_loss: 1.0893 - val_accuracy: 0.7250\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3259 - accuracy: 0.5819 - val_loss: 1.0379 - val_accuracy: 0.7401\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2920 - accuracy: 0.5937 - val_loss: 0.9303 - val_accuracy: 0.7620\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2655 - accuracy: 0.6016 - val_loss: 0.9859 - val_accuracy: 0.7381\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2346 - accuracy: 0.6160 - val_loss: 0.9698 - val_accuracy: 0.7389\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2068 - accuracy: 0.6237 - val_loss: 0.9330 - val_accuracy: 0.7447\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1897 - accuracy: 0.6284 - val_loss: 0.9186 - val_accuracy: 0.7630\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1603 - accuracy: 0.6384 - val_loss: 0.8786 - val_accuracy: 0.7745\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1373 - accuracy: 0.6489 - val_loss: 0.8061 - val_accuracy: 0.7943\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1183 - accuracy: 0.6542 - val_loss: 0.9136 - val_accuracy: 0.7551\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1040 - accuracy: 0.6592 - val_loss: 0.8123 - val_accuracy: 0.7875\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0866 - accuracy: 0.6640 - val_loss: 0.7788 - val_accuracy: 0.7918\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0647 - accuracy: 0.6715 - val_loss: 0.8597 - val_accuracy: 0.7620\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0430 - accuracy: 0.6787 - val_loss: 0.8359 - val_accuracy: 0.7734\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0198 - accuracy: 0.6877 - val_loss: 0.7964 - val_accuracy: 0.7844\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0085 - accuracy: 0.6923 - val_loss: 0.9044 - val_accuracy: 0.7466\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9947 - accuracy: 0.6960 - val_loss: 0.7671 - val_accuracy: 0.7966\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9764 - accuracy: 0.7006 - val_loss: 0.7730 - val_accuracy: 0.7824\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9677 - accuracy: 0.7034 - val_loss: 0.7007 - val_accuracy: 0.8115\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9482 - accuracy: 0.7102 - val_loss: 0.6696 - val_accuracy: 0.8155\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9335 - accuracy: 0.7173 - val_loss: 0.7469 - val_accuracy: 0.7776\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9242 - accuracy: 0.7170 - val_loss: 0.7549 - val_accuracy: 0.7881\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9073 - accuracy: 0.7227 - val_loss: 0.7438 - val_accuracy: 0.7962\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9006 - accuracy: 0.7285 - val_loss: 0.6878 - val_accuracy: 0.8060\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8864 - accuracy: 0.7303 - val_loss: 0.6767 - val_accuracy: 0.8060\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8754 - accuracy: 0.7358 - val_loss: 0.6683 - val_accuracy: 0.8009\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8613 - accuracy: 0.7413 - val_loss: 0.7230 - val_accuracy: 0.7884\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8500 - accuracy: 0.7442 - val_loss: 0.7503 - val_accuracy: 0.7843\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8407 - accuracy: 0.7445 - val_loss: 0.6076 - val_accuracy: 0.8281\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8333 - accuracy: 0.7459 - val_loss: 0.7843 - val_accuracy: 0.7734\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8221 - accuracy: 0.7530 - val_loss: 0.6941 - val_accuracy: 0.7994\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8194 - accuracy: 0.7537 - val_loss: 0.6491 - val_accuracy: 0.8156\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8073 - accuracy: 0.7566 - val_loss: 0.6207 - val_accuracy: 0.8165\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8000 - accuracy: 0.7590 - val_loss: 0.6426 - val_accuracy: 0.8091\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7887 - accuracy: 0.7642 - val_loss: 0.6725 - val_accuracy: 0.7999\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7774 - accuracy: 0.7661 - val_loss: 0.5726 - val_accuracy: 0.8321\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7699 - accuracy: 0.7691 - val_loss: 0.6283 - val_accuracy: 0.8124\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7641 - accuracy: 0.7711 - val_loss: 0.5526 - val_accuracy: 0.8429\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7536 - accuracy: 0.7737 - val_loss: 0.5828 - val_accuracy: 0.8219\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7485 - accuracy: 0.7758 - val_loss: 0.6413 - val_accuracy: 0.8065\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7403 - accuracy: 0.7780 - val_loss: 0.5167 - val_accuracy: 0.8501\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7368 - accuracy: 0.7805 - val_loss: 0.5932 - val_accuracy: 0.8300\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7332 - accuracy: 0.7792 - val_loss: 0.5771 - val_accuracy: 0.8279\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7273 - accuracy: 0.7829 - val_loss: 0.5419 - val_accuracy: 0.8422\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7196 - accuracy: 0.7832 - val_loss: 0.5777 - val_accuracy: 0.8301\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7130 - accuracy: 0.7870 - val_loss: 0.7765 - val_accuracy: 0.7766\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7065 - accuracy: 0.7896 - val_loss: 0.5318 - val_accuracy: 0.8461\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7004 - accuracy: 0.7898 - val_loss: 0.5692 - val_accuracy: 0.8284\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6931 - accuracy: 0.7932 - val_loss: 0.5244 - val_accuracy: 0.8468\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6855 - accuracy: 0.7957 - val_loss: 0.7118 - val_accuracy: 0.7898\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6864 - accuracy: 0.7960 - val_loss: 0.6303 - val_accuracy: 0.8101\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6757 - accuracy: 0.7983 - val_loss: 0.5342 - val_accuracy: 0.8429\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6742 - accuracy: 0.7997 - val_loss: 0.5865 - val_accuracy: 0.8158\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6643 - accuracy: 0.8033 - val_loss: 0.5582 - val_accuracy: 0.8346\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6673 - accuracy: 0.8025 - val_loss: 0.6257 - val_accuracy: 0.8044\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6514 - accuracy: 0.8060 - val_loss: 0.4898 - val_accuracy: 0.8541\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6454 - accuracy: 0.8084 - val_loss: 0.4882 - val_accuracy: 0.8579\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6454 - accuracy: 0.8080 - val_loss: 0.5648 - val_accuracy: 0.8353\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6414 - accuracy: 0.8099 - val_loss: 0.6293 - val_accuracy: 0.8022\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6399 - accuracy: 0.8110 - val_loss: 0.7168 - val_accuracy: 0.7696\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6353 - accuracy: 0.8135 - val_loss: 0.5290 - val_accuracy: 0.8399\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6322 - accuracy: 0.8118 - val_loss: 0.6050 - val_accuracy: 0.8196\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6297 - accuracy: 0.8135 - val_loss: 0.6401 - val_accuracy: 0.8116\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6270 - accuracy: 0.8149 - val_loss: 0.4973 - val_accuracy: 0.8544\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6235 - accuracy: 0.8155 - val_loss: 0.7015 - val_accuracy: 0.7856\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6148 - accuracy: 0.8177 - val_loss: 0.6561 - val_accuracy: 0.8057\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6137 - accuracy: 0.8187 - val_loss: 0.6157 - val_accuracy: 0.8120\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6077 - accuracy: 0.8192 - val_loss: 0.4630 - val_accuracy: 0.8659\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6043 - accuracy: 0.8198 - val_loss: 0.5943 - val_accuracy: 0.8143\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5970 - accuracy: 0.8219 - val_loss: 0.4983 - val_accuracy: 0.8471\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5953 - accuracy: 0.8222 - val_loss: 0.4698 - val_accuracy: 0.8645\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5901 - accuracy: 0.8247 - val_loss: 0.5030 - val_accuracy: 0.8514\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5868 - accuracy: 0.8258 - val_loss: 0.4707 - val_accuracy: 0.8595\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5823 - accuracy: 0.8284 - val_loss: 0.4523 - val_accuracy: 0.8681\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5886 - accuracy: 0.8258 - val_loss: 0.4428 - val_accuracy: 0.8682\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5832 - accuracy: 0.8277 - val_loss: 0.4730 - val_accuracy: 0.8553\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5836 - accuracy: 0.8275 - val_loss: 0.6462 - val_accuracy: 0.8001\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5670 - accuracy: 0.8340 - val_loss: 0.4768 - val_accuracy: 0.8569\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5716 - accuracy: 0.8321 - val_loss: 0.5051 - val_accuracy: 0.8434\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5718 - accuracy: 0.8308 - val_loss: 0.4705 - val_accuracy: 0.8454\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5694 - accuracy: 0.8296 - val_loss: 0.5105 - val_accuracy: 0.8475\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5645 - accuracy: 0.8327 - val_loss: 0.5060 - val_accuracy: 0.8505\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5601 - accuracy: 0.8354 - val_loss: 0.4002 - val_accuracy: 0.8849\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5566 - accuracy: 0.8358 - val_loss: 0.5071 - val_accuracy: 0.8484\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5541 - accuracy: 0.8364 - val_loss: 0.4568 - val_accuracy: 0.8696\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5477 - accuracy: 0.8379 - val_loss: 0.4186 - val_accuracy: 0.8789\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5429 - accuracy: 0.8399 - val_loss: 0.4086 - val_accuracy: 0.8811\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5418 - accuracy: 0.8415 - val_loss: 0.4956 - val_accuracy: 0.8493\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5375 - accuracy: 0.8416 - val_loss: 0.5227 - val_accuracy: 0.8462\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5389 - accuracy: 0.8422 - val_loss: 0.3989 - val_accuracy: 0.8796\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5324 - accuracy: 0.8418 - val_loss: 0.4793 - val_accuracy: 0.8536\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5323 - accuracy: 0.8429 - val_loss: 0.3948 - val_accuracy: 0.8844\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5300 - accuracy: 0.8444 - val_loss: 0.4289 - val_accuracy: 0.8764\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5273 - accuracy: 0.8444 - val_loss: 0.4126 - val_accuracy: 0.8826\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5220 - accuracy: 0.8457 - val_loss: 0.3913 - val_accuracy: 0.8860\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5249 - accuracy: 0.8466 - val_loss: 0.4783 - val_accuracy: 0.8579\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5176 - accuracy: 0.8481 - val_loss: 0.4004 - val_accuracy: 0.8854\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5186 - accuracy: 0.8481 - val_loss: 0.4941 - val_accuracy: 0.8444\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5124 - accuracy: 0.8500 - val_loss: 0.4477 - val_accuracy: 0.8674\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5160 - accuracy: 0.8498 - val_loss: 0.4289 - val_accuracy: 0.8717\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5096 - accuracy: 0.8509 - val_loss: 0.3920 - val_accuracy: 0.8864\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5074 - accuracy: 0.8495 - val_loss: 0.4648 - val_accuracy: 0.8629\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5079 - accuracy: 0.8519 - val_loss: 0.4030 - val_accuracy: 0.8836\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5051 - accuracy: 0.8515 - val_loss: 0.4363 - val_accuracy: 0.8726\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5031 - accuracy: 0.8526 - val_loss: 0.3818 - val_accuracy: 0.8909\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4987 - accuracy: 0.8535 - val_loss: 0.4560 - val_accuracy: 0.8699\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4960 - accuracy: 0.8545 - val_loss: 0.4279 - val_accuracy: 0.8756\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4991 - accuracy: 0.8555 - val_loss: 0.3493 - val_accuracy: 0.9013\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4941 - accuracy: 0.8570 - val_loss: 0.3784 - val_accuracy: 0.8840\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4948 - accuracy: 0.8526 - val_loss: 0.4935 - val_accuracy: 0.8433\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8573 - val_loss: 0.4729 - val_accuracy: 0.8589\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4868 - accuracy: 0.8576 - val_loss: 0.4514 - val_accuracy: 0.8656\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4875 - accuracy: 0.8576 - val_loss: 0.3381 - val_accuracy: 0.9021\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4805 - accuracy: 0.8597 - val_loss: 0.4442 - val_accuracy: 0.8706\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4839 - accuracy: 0.8580 - val_loss: 0.4332 - val_accuracy: 0.8699\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4782 - accuracy: 0.8589 - val_loss: 0.5086 - val_accuracy: 0.8499\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4750 - accuracy: 0.8590 - val_loss: 0.3921 - val_accuracy: 0.8876\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4770 - accuracy: 0.8610 - val_loss: 0.4367 - val_accuracy: 0.8703\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4760 - accuracy: 0.8606 - val_loss: 0.4300 - val_accuracy: 0.8751\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4716 - accuracy: 0.8624 - val_loss: 0.5433 - val_accuracy: 0.8356\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4731 - accuracy: 0.8604 - val_loss: 0.3850 - val_accuracy: 0.8886\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4650 - accuracy: 0.8649 - val_loss: 0.4258 - val_accuracy: 0.8768\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4656 - accuracy: 0.8636 - val_loss: 0.4132 - val_accuracy: 0.8766\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4631 - accuracy: 0.8643 - val_loss: 0.4119 - val_accuracy: 0.8750\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4643 - accuracy: 0.8650 - val_loss: 0.5313 - val_accuracy: 0.8326\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4577 - accuracy: 0.8671 - val_loss: 0.6124 - val_accuracy: 0.8155\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4588 - accuracy: 0.8672 - val_loss: 0.4141 - val_accuracy: 0.8774\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4585 - accuracy: 0.8659 - val_loss: 0.4402 - val_accuracy: 0.8702\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4573 - accuracy: 0.8669 - val_loss: 0.4415 - val_accuracy: 0.8726\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4593 - accuracy: 0.8659 - val_loss: 0.4504 - val_accuracy: 0.8639\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4577 - accuracy: 0.8659 - val_loss: 0.4430 - val_accuracy: 0.8686\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4520 - accuracy: 0.8684 - val_loss: 0.3816 - val_accuracy: 0.8899\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4485 - accuracy: 0.8674 - val_loss: 0.6530 - val_accuracy: 0.8073\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4459 - accuracy: 0.8691 - val_loss: 0.5273 - val_accuracy: 0.8434\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4468 - accuracy: 0.8699 - val_loss: 0.3308 - val_accuracy: 0.9069\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4407 - accuracy: 0.8722 - val_loss: 0.4665 - val_accuracy: 0.8602\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4400 - accuracy: 0.8717 - val_loss: 0.3914 - val_accuracy: 0.8841\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4453 - accuracy: 0.8702 - val_loss: 0.3744 - val_accuracy: 0.8939\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4402 - accuracy: 0.8710 - val_loss: 0.4810 - val_accuracy: 0.8590\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4402 - accuracy: 0.8730 - val_loss: 0.4074 - val_accuracy: 0.8852\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4332 - accuracy: 0.8749 - val_loss: 0.4101 - val_accuracy: 0.8771\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4387 - accuracy: 0.8722 - val_loss: 0.3596 - val_accuracy: 0.8924\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4295 - accuracy: 0.8745 - val_loss: 0.3849 - val_accuracy: 0.8861\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4288 - accuracy: 0.8746 - val_loss: 0.3261 - val_accuracy: 0.9101\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4311 - accuracy: 0.8752 - val_loss: 0.3696 - val_accuracy: 0.8937\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8740 - val_loss: 0.3930 - val_accuracy: 0.8827\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4335 - accuracy: 0.8744 - val_loss: 0.4301 - val_accuracy: 0.8715\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4299 - accuracy: 0.8745 - val_loss: 0.5122 - val_accuracy: 0.8532\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4313 - accuracy: 0.8745 - val_loss: 0.5044 - val_accuracy: 0.8538\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4217 - accuracy: 0.8770 - val_loss: 0.3907 - val_accuracy: 0.8841\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4196 - accuracy: 0.8788 - val_loss: 0.3065 - val_accuracy: 0.9141\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4197 - accuracy: 0.8787 - val_loss: 0.3475 - val_accuracy: 0.9016\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4127 - accuracy: 0.8794 - val_loss: 0.5124 - val_accuracy: 0.8506\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8773 - val_loss: 0.4297 - val_accuracy: 0.8709\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4169 - accuracy: 0.8795 - val_loss: 0.3200 - val_accuracy: 0.9106\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4180 - accuracy: 0.8788 - val_loss: 0.4019 - val_accuracy: 0.8834\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4123 - accuracy: 0.8799 - val_loss: 0.4045 - val_accuracy: 0.8815\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4125 - accuracy: 0.8794 - val_loss: 0.4952 - val_accuracy: 0.8557\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8830 - val_loss: 0.3301 - val_accuracy: 0.9059\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4122 - accuracy: 0.8790 - val_loss: 0.4359 - val_accuracy: 0.8761\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4141 - accuracy: 0.8775 - val_loss: 0.3760 - val_accuracy: 0.8913\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3993 - accuracy: 0.8835 - val_loss: 0.3781 - val_accuracy: 0.8906\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3962 - accuracy: 0.8851 - val_loss: 0.3445 - val_accuracy: 0.8999\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4035 - accuracy: 0.8818 - val_loss: 0.3340 - val_accuracy: 0.9039\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4029 - accuracy: 0.8830 - val_loss: 0.4638 - val_accuracy: 0.8640\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4036 - accuracy: 0.8832 - val_loss: 0.3537 - val_accuracy: 0.9018\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8864 - val_loss: 0.4144 - val_accuracy: 0.8768\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4026 - accuracy: 0.8826 - val_loss: 0.3344 - val_accuracy: 0.9073\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3990 - accuracy: 0.8832 - val_loss: 0.6239 - val_accuracy: 0.8261\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8837 - val_loss: 0.3869 - val_accuracy: 0.8889\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3955 - accuracy: 0.8857 - val_loss: 0.3896 - val_accuracy: 0.8867\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3961 - accuracy: 0.8846 - val_loss: 0.4260 - val_accuracy: 0.8803\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8858 - val_loss: 0.3744 - val_accuracy: 0.8934\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3884 - accuracy: 0.8876 - val_loss: 0.3775 - val_accuracy: 0.8937\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3939 - accuracy: 0.8854 - val_loss: 0.3418 - val_accuracy: 0.9031\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3814 - accuracy: 0.8880 - val_loss: 0.3409 - val_accuracy: 0.9046\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3951 - accuracy: 0.8845 - val_loss: 0.4409 - val_accuracy: 0.8708\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8867 - val_loss: 0.3608 - val_accuracy: 0.8963\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3854 - accuracy: 0.8884 - val_loss: 0.3612 - val_accuracy: 0.8995\n",
      "Try 5/100: Best_val_acc: [0.4696427881717682, 0.8624444603919983], lr: 6.0379383617791225e-05, Lambda: 5.814141068427433e-05\n",
      "\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_431 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7344 - accuracy: 0.1081 - val_loss: 2.3452 - val_accuracy: 0.0399\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.6023 - accuracy: 0.1234 - val_loss: 2.2402 - val_accuracy: 0.1365\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4866 - accuracy: 0.1395 - val_loss: 2.2653 - val_accuracy: 0.1365\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3937 - accuracy: 0.1607 - val_loss: 2.1139 - val_accuracy: 0.2066\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2977 - accuracy: 0.1874 - val_loss: 2.0429 - val_accuracy: 0.2815\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2123 - accuracy: 0.2159 - val_loss: 1.9682 - val_accuracy: 0.3390\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1325 - accuracy: 0.2433 - val_loss: 1.9215 - val_accuracy: 0.3568\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0645 - accuracy: 0.2703 - val_loss: 1.8303 - val_accuracy: 0.4116\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9902 - accuracy: 0.3028 - val_loss: 1.7150 - val_accuracy: 0.5090\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9262 - accuracy: 0.3263 - val_loss: 1.6744 - val_accuracy: 0.5070\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8674 - accuracy: 0.3545 - val_loss: 1.6132 - val_accuracy: 0.5601\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8118 - accuracy: 0.3793 - val_loss: 1.5190 - val_accuracy: 0.5855\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7584 - accuracy: 0.4015 - val_loss: 1.4864 - val_accuracy: 0.6066\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7120 - accuracy: 0.4187 - val_loss: 1.4397 - val_accuracy: 0.6375\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6633 - accuracy: 0.4424 - val_loss: 1.3993 - val_accuracy: 0.6416\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6212 - accuracy: 0.4597 - val_loss: 1.3774 - val_accuracy: 0.6314\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5640 - accuracy: 0.4862 - val_loss: 1.2865 - val_accuracy: 0.6706\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5272 - accuracy: 0.4980 - val_loss: 1.2794 - val_accuracy: 0.6576\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4923 - accuracy: 0.5165 - val_loss: 1.2400 - val_accuracy: 0.6846\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4484 - accuracy: 0.5320 - val_loss: 1.1304 - val_accuracy: 0.7161\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4098 - accuracy: 0.5484 - val_loss: 1.0897 - val_accuracy: 0.7326\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3744 - accuracy: 0.5608 - val_loss: 1.0980 - val_accuracy: 0.7173\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3377 - accuracy: 0.5748 - val_loss: 1.0239 - val_accuracy: 0.7496\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3158 - accuracy: 0.5831 - val_loss: 1.0045 - val_accuracy: 0.7506\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2848 - accuracy: 0.5968 - val_loss: 0.9345 - val_accuracy: 0.7622\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2493 - accuracy: 0.6118 - val_loss: 0.9162 - val_accuracy: 0.7750\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2244 - accuracy: 0.6172 - val_loss: 0.9026 - val_accuracy: 0.7660\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1936 - accuracy: 0.6311 - val_loss: 0.8617 - val_accuracy: 0.7776\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1715 - accuracy: 0.6367 - val_loss: 0.9994 - val_accuracy: 0.7169\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1426 - accuracy: 0.6485 - val_loss: 0.8988 - val_accuracy: 0.7506\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1215 - accuracy: 0.6537 - val_loss: 0.8689 - val_accuracy: 0.7644\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0956 - accuracy: 0.6666 - val_loss: 0.8510 - val_accuracy: 0.7656\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0861 - accuracy: 0.6675 - val_loss: 0.8556 - val_accuracy: 0.7574\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0573 - accuracy: 0.6775 - val_loss: 0.8268 - val_accuracy: 0.7624\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0405 - accuracy: 0.6830 - val_loss: 0.8335 - val_accuracy: 0.7632\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0236 - accuracy: 0.6891 - val_loss: 0.8237 - val_accuracy: 0.7612\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0039 - accuracy: 0.6935 - val_loss: 0.7284 - val_accuracy: 0.7909\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9911 - accuracy: 0.6990 - val_loss: 0.8355 - val_accuracy: 0.7531\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9703 - accuracy: 0.7059 - val_loss: 0.6991 - val_accuracy: 0.7989\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9552 - accuracy: 0.7105 - val_loss: 0.6995 - val_accuracy: 0.8022\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9403 - accuracy: 0.7170 - val_loss: 0.7373 - val_accuracy: 0.7882\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9284 - accuracy: 0.7191 - val_loss: 0.7776 - val_accuracy: 0.7725\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9113 - accuracy: 0.7238 - val_loss: 0.7960 - val_accuracy: 0.7634\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9003 - accuracy: 0.7285 - val_loss: 0.6513 - val_accuracy: 0.8181\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8930 - accuracy: 0.7302 - val_loss: 0.6875 - val_accuracy: 0.7980\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8758 - accuracy: 0.7352 - val_loss: 0.7479 - val_accuracy: 0.7778\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8622 - accuracy: 0.7416 - val_loss: 0.6189 - val_accuracy: 0.8206\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8500 - accuracy: 0.7420 - val_loss: 0.6009 - val_accuracy: 0.8260\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8460 - accuracy: 0.7439 - val_loss: 0.7321 - val_accuracy: 0.7796\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8250 - accuracy: 0.7518 - val_loss: 0.6207 - val_accuracy: 0.8212\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8253 - accuracy: 0.7521 - val_loss: 0.6584 - val_accuracy: 0.8044\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8073 - accuracy: 0.7570 - val_loss: 0.5849 - val_accuracy: 0.8338\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8061 - accuracy: 0.7560 - val_loss: 0.6460 - val_accuracy: 0.8078\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7891 - accuracy: 0.7647 - val_loss: 0.6107 - val_accuracy: 0.8203\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7855 - accuracy: 0.7649 - val_loss: 0.6422 - val_accuracy: 0.8074\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7861 - accuracy: 0.7640 - val_loss: 0.7150 - val_accuracy: 0.7836\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7636 - accuracy: 0.7723 - val_loss: 0.6127 - val_accuracy: 0.8164\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7630 - accuracy: 0.7728 - val_loss: 0.7086 - val_accuracy: 0.7864\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7546 - accuracy: 0.7760 - val_loss: 0.6268 - val_accuracy: 0.8139\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7450 - accuracy: 0.7788 - val_loss: 0.5850 - val_accuracy: 0.8281\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7445 - accuracy: 0.7774 - val_loss: 0.5132 - val_accuracy: 0.8521\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7288 - accuracy: 0.7816 - val_loss: 0.5783 - val_accuracy: 0.8264\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7289 - accuracy: 0.7824 - val_loss: 0.6720 - val_accuracy: 0.8009\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7189 - accuracy: 0.7845 - val_loss: 0.5603 - val_accuracy: 0.8341\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7074 - accuracy: 0.7895 - val_loss: 0.6033 - val_accuracy: 0.8205\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7017 - accuracy: 0.7930 - val_loss: 0.5554 - val_accuracy: 0.8356\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7039 - accuracy: 0.7900 - val_loss: 0.6751 - val_accuracy: 0.7961\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6959 - accuracy: 0.7941 - val_loss: 0.4776 - val_accuracy: 0.8655\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6923 - accuracy: 0.7936 - val_loss: 0.5597 - val_accuracy: 0.8335\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6864 - accuracy: 0.7954 - val_loss: 0.6108 - val_accuracy: 0.8128\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6802 - accuracy: 0.7969 - val_loss: 0.5774 - val_accuracy: 0.8286\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6756 - accuracy: 0.7994 - val_loss: 0.6439 - val_accuracy: 0.8058\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6720 - accuracy: 0.8002 - val_loss: 0.5493 - val_accuracy: 0.8412\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6611 - accuracy: 0.8038 - val_loss: 0.5220 - val_accuracy: 0.8444\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6625 - accuracy: 0.8034 - val_loss: 0.5884 - val_accuracy: 0.8229\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6531 - accuracy: 0.8057 - val_loss: 0.5957 - val_accuracy: 0.8239\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6581 - accuracy: 0.8050 - val_loss: 0.5379 - val_accuracy: 0.8406\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6434 - accuracy: 0.8087 - val_loss: 0.4933 - val_accuracy: 0.8550\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6360 - accuracy: 0.8103 - val_loss: 0.7533 - val_accuracy: 0.7754\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6382 - accuracy: 0.8112 - val_loss: 0.4909 - val_accuracy: 0.8577\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6325 - accuracy: 0.8145 - val_loss: 0.5773 - val_accuracy: 0.8286\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6249 - accuracy: 0.8155 - val_loss: 0.5494 - val_accuracy: 0.8387\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6237 - accuracy: 0.8164 - val_loss: 0.5383 - val_accuracy: 0.8402\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6216 - accuracy: 0.8169 - val_loss: 0.6135 - val_accuracy: 0.8081\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6136 - accuracy: 0.8183 - val_loss: 0.5076 - val_accuracy: 0.8496\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6136 - accuracy: 0.8193 - val_loss: 0.4551 - val_accuracy: 0.8693\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6096 - accuracy: 0.8207 - val_loss: 0.5733 - val_accuracy: 0.8270\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6068 - accuracy: 0.8215 - val_loss: 0.4223 - val_accuracy: 0.8796\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5967 - accuracy: 0.8232 - val_loss: 0.4710 - val_accuracy: 0.8656\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6038 - accuracy: 0.8210 - val_loss: 0.4770 - val_accuracy: 0.8579\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5975 - accuracy: 0.8235 - val_loss: 0.5843 - val_accuracy: 0.8268\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5890 - accuracy: 0.8262 - val_loss: 0.4639 - val_accuracy: 0.8657\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5831 - accuracy: 0.8288 - val_loss: 0.5629 - val_accuracy: 0.8343\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5925 - accuracy: 0.8251 - val_loss: 0.5051 - val_accuracy: 0.8548\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5777 - accuracy: 0.8295 - val_loss: 0.4972 - val_accuracy: 0.8542\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5780 - accuracy: 0.8324 - val_loss: 0.5883 - val_accuracy: 0.8130\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5694 - accuracy: 0.8317 - val_loss: 0.4375 - val_accuracy: 0.8718\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5756 - accuracy: 0.8288 - val_loss: 0.5402 - val_accuracy: 0.8334\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5639 - accuracy: 0.8335 - val_loss: 0.4180 - val_accuracy: 0.8777\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5607 - accuracy: 0.8357 - val_loss: 0.5869 - val_accuracy: 0.8231\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5605 - accuracy: 0.8362 - val_loss: 0.5140 - val_accuracy: 0.8501\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5572 - accuracy: 0.8350 - val_loss: 0.4498 - val_accuracy: 0.8657\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5604 - accuracy: 0.8348 - val_loss: 0.6841 - val_accuracy: 0.7951\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5546 - accuracy: 0.8381 - val_loss: 0.4359 - val_accuracy: 0.8711\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5511 - accuracy: 0.8369 - val_loss: 0.4376 - val_accuracy: 0.8735\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5455 - accuracy: 0.8391 - val_loss: 0.3838 - val_accuracy: 0.8880\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5404 - accuracy: 0.8407 - val_loss: 0.4205 - val_accuracy: 0.8751\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5441 - accuracy: 0.8406 - val_loss: 0.4377 - val_accuracy: 0.8741\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5373 - accuracy: 0.8409 - val_loss: 0.5979 - val_accuracy: 0.8263\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5351 - accuracy: 0.8423 - val_loss: 0.5516 - val_accuracy: 0.8396\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5344 - accuracy: 0.8428 - val_loss: 0.4157 - val_accuracy: 0.8799\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5263 - accuracy: 0.8465 - val_loss: 0.4641 - val_accuracy: 0.8609\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5243 - accuracy: 0.8455 - val_loss: 0.4059 - val_accuracy: 0.8822\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5269 - accuracy: 0.8452 - val_loss: 0.3896 - val_accuracy: 0.8831\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5258 - accuracy: 0.8460 - val_loss: 0.4149 - val_accuracy: 0.8796\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5166 - accuracy: 0.8475 - val_loss: 0.4675 - val_accuracy: 0.8640\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5187 - accuracy: 0.8488 - val_loss: 0.3881 - val_accuracy: 0.8911\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5215 - accuracy: 0.8485 - val_loss: 0.4183 - val_accuracy: 0.8781\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5118 - accuracy: 0.8501 - val_loss: 0.4760 - val_accuracy: 0.8612\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5101 - accuracy: 0.8502 - val_loss: 0.7657 - val_accuracy: 0.7729\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5063 - accuracy: 0.8517 - val_loss: 0.4943 - val_accuracy: 0.8584\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5073 - accuracy: 0.8500 - val_loss: 0.3743 - val_accuracy: 0.8915\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4974 - accuracy: 0.8537 - val_loss: 0.4708 - val_accuracy: 0.8629\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5007 - accuracy: 0.8523 - val_loss: 0.3648 - val_accuracy: 0.8974\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4990 - accuracy: 0.8538 - val_loss: 0.4271 - val_accuracy: 0.8738\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4932 - accuracy: 0.8554 - val_loss: 0.3771 - val_accuracy: 0.8899\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4992 - accuracy: 0.8536 - val_loss: 0.5054 - val_accuracy: 0.8506\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4965 - accuracy: 0.8535 - val_loss: 0.5647 - val_accuracy: 0.8276\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4919 - accuracy: 0.8558 - val_loss: 0.4122 - val_accuracy: 0.8779\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4915 - accuracy: 0.8560 - val_loss: 0.5821 - val_accuracy: 0.8292\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4867 - accuracy: 0.8575 - val_loss: 0.3869 - val_accuracy: 0.8882\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4846 - accuracy: 0.8591 - val_loss: 0.4513 - val_accuracy: 0.8697\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8587 - val_loss: 0.4115 - val_accuracy: 0.8779\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.8606 - val_loss: 0.5371 - val_accuracy: 0.8329\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4791 - accuracy: 0.8598 - val_loss: 0.4640 - val_accuracy: 0.8616\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.8638 - val_loss: 0.4368 - val_accuracy: 0.8709\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4794 - accuracy: 0.8601 - val_loss: 0.4010 - val_accuracy: 0.8845\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4702 - accuracy: 0.8626 - val_loss: 0.3634 - val_accuracy: 0.8942\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4679 - accuracy: 0.8654 - val_loss: 0.4236 - val_accuracy: 0.8796\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4733 - accuracy: 0.8607 - val_loss: 0.4099 - val_accuracy: 0.8759\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4703 - accuracy: 0.8627 - val_loss: 0.4445 - val_accuracy: 0.8658\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4602 - accuracy: 0.8651 - val_loss: 0.3879 - val_accuracy: 0.8875\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4639 - accuracy: 0.8641 - val_loss: 0.4831 - val_accuracy: 0.8586\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4657 - accuracy: 0.8637 - val_loss: 0.4094 - val_accuracy: 0.8821\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4648 - accuracy: 0.8638 - val_loss: 0.4136 - val_accuracy: 0.8789\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4558 - accuracy: 0.8657 - val_loss: 0.4296 - val_accuracy: 0.8736\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4561 - accuracy: 0.8665 - val_loss: 0.4449 - val_accuracy: 0.8671\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4567 - accuracy: 0.8665 - val_loss: 0.3771 - val_accuracy: 0.8928\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4527 - accuracy: 0.8689 - val_loss: 0.4340 - val_accuracy: 0.8695\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4526 - accuracy: 0.8688 - val_loss: 0.3897 - val_accuracy: 0.8862\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4534 - accuracy: 0.8673 - val_loss: 0.3958 - val_accuracy: 0.8849\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4482 - accuracy: 0.8692 - val_loss: 0.3657 - val_accuracy: 0.8956\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4451 - accuracy: 0.8705 - val_loss: 0.3792 - val_accuracy: 0.8911\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4432 - accuracy: 0.8705 - val_loss: 0.3704 - val_accuracy: 0.8939\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4385 - accuracy: 0.8728 - val_loss: 0.3923 - val_accuracy: 0.8874\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4427 - accuracy: 0.8696 - val_loss: 0.4564 - val_accuracy: 0.8621\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4459 - accuracy: 0.8702 - val_loss: 0.4117 - val_accuracy: 0.8784\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4442 - accuracy: 0.8715 - val_loss: 0.3707 - val_accuracy: 0.8931\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4312 - accuracy: 0.8737 - val_loss: 0.3880 - val_accuracy: 0.8904\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4322 - accuracy: 0.8728 - val_loss: 0.3189 - val_accuracy: 0.9098\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4362 - accuracy: 0.8721 - val_loss: 0.3206 - val_accuracy: 0.9118\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4284 - accuracy: 0.8741 - val_loss: 0.5359 - val_accuracy: 0.8398\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4341 - accuracy: 0.8729 - val_loss: 0.4079 - val_accuracy: 0.8823\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4340 - accuracy: 0.8721 - val_loss: 0.3576 - val_accuracy: 0.8996\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4380 - accuracy: 0.8716 - val_loss: 0.4628 - val_accuracy: 0.8649\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4237 - accuracy: 0.8769 - val_loss: 0.4864 - val_accuracy: 0.8541\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4244 - accuracy: 0.8753 - val_loss: 0.4564 - val_accuracy: 0.8654\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4267 - accuracy: 0.8752 - val_loss: 0.4273 - val_accuracy: 0.8759\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4272 - accuracy: 0.8752 - val_loss: 0.4837 - val_accuracy: 0.8588\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4228 - accuracy: 0.8770 - val_loss: 0.5229 - val_accuracy: 0.8494\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4229 - accuracy: 0.8764 - val_loss: 0.5244 - val_accuracy: 0.8437\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4163 - accuracy: 0.8790 - val_loss: 0.4697 - val_accuracy: 0.8647\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4185 - accuracy: 0.8782 - val_loss: 0.4579 - val_accuracy: 0.8685\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4121 - accuracy: 0.8791 - val_loss: 0.3757 - val_accuracy: 0.8901\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4110 - accuracy: 0.8795 - val_loss: 0.3758 - val_accuracy: 0.8887\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4158 - accuracy: 0.8798 - val_loss: 0.3744 - val_accuracy: 0.8924\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4088 - accuracy: 0.8803 - val_loss: 0.3347 - val_accuracy: 0.9059\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4126 - accuracy: 0.8815 - val_loss: 0.4581 - val_accuracy: 0.8630\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4129 - accuracy: 0.8796 - val_loss: 0.3112 - val_accuracy: 0.9138\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4090 - accuracy: 0.8792 - val_loss: 0.3883 - val_accuracy: 0.8866\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4055 - accuracy: 0.8818 - val_loss: 0.4624 - val_accuracy: 0.8629\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4022 - accuracy: 0.8826 - val_loss: 0.4748 - val_accuracy: 0.8581\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4066 - accuracy: 0.8812 - val_loss: 0.4786 - val_accuracy: 0.8474\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3975 - accuracy: 0.8820 - val_loss: 0.3698 - val_accuracy: 0.8976\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4007 - accuracy: 0.8838 - val_loss: 0.3737 - val_accuracy: 0.8942\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8857 - val_loss: 0.5210 - val_accuracy: 0.8486\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3952 - accuracy: 0.8843 - val_loss: 0.6465 - val_accuracy: 0.8145\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3996 - accuracy: 0.8835 - val_loss: 0.4321 - val_accuracy: 0.8724\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8851 - val_loss: 0.3136 - val_accuracy: 0.9120\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3985 - accuracy: 0.8842 - val_loss: 0.3596 - val_accuracy: 0.8973\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8851 - val_loss: 0.3359 - val_accuracy: 0.9037\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8868 - val_loss: 0.3618 - val_accuracy: 0.8939\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3909 - accuracy: 0.8855 - val_loss: 0.3317 - val_accuracy: 0.9072\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3918 - accuracy: 0.8858 - val_loss: 0.3744 - val_accuracy: 0.8876\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3953 - accuracy: 0.8861 - val_loss: 0.3944 - val_accuracy: 0.8847\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3900 - accuracy: 0.8866 - val_loss: 0.4812 - val_accuracy: 0.8546\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3857 - accuracy: 0.8858 - val_loss: 0.3790 - val_accuracy: 0.8899\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3857 - accuracy: 0.8875 - val_loss: 0.3318 - val_accuracy: 0.9062\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3824 - accuracy: 0.8897 - val_loss: 0.3498 - val_accuracy: 0.9004\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3781 - accuracy: 0.8896 - val_loss: 0.4271 - val_accuracy: 0.8739\n",
      "Try 6/100: Best_val_acc: [0.5146023035049438, 0.8464999794960022], lr: 6.111878504492244e-05, Lambda: 5.876863980569314e-05\n",
      "\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_432 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6777 - accuracy: 0.1057 - val_loss: 2.4127 - val_accuracy: 0.2035\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5400 - accuracy: 0.1271 - val_loss: 2.3226 - val_accuracy: 0.1837\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4265 - accuracy: 0.1481 - val_loss: 2.2350 - val_accuracy: 0.2039\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3412 - accuracy: 0.1703 - val_loss: 2.1558 - val_accuracy: 0.2549\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2458 - accuracy: 0.1989 - val_loss: 2.1238 - val_accuracy: 0.2592\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1573 - accuracy: 0.2308 - val_loss: 1.9180 - val_accuracy: 0.4146\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0811 - accuracy: 0.2604 - val_loss: 1.8284 - val_accuracy: 0.4650\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0101 - accuracy: 0.2895 - val_loss: 1.6658 - val_accuracy: 0.5379\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9367 - accuracy: 0.3200 - val_loss: 1.5547 - val_accuracy: 0.6221\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8653 - accuracy: 0.3485 - val_loss: 1.5842 - val_accuracy: 0.5778\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8051 - accuracy: 0.3734 - val_loss: 1.4634 - val_accuracy: 0.6491\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7516 - accuracy: 0.3978 - val_loss: 1.3251 - val_accuracy: 0.7013\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6950 - accuracy: 0.4246 - val_loss: 1.3044 - val_accuracy: 0.7007\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6414 - accuracy: 0.4476 - val_loss: 1.2838 - val_accuracy: 0.6784\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5905 - accuracy: 0.4710 - val_loss: 1.3047 - val_accuracy: 0.6818\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5476 - accuracy: 0.4906 - val_loss: 1.1990 - val_accuracy: 0.7154\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4993 - accuracy: 0.5146 - val_loss: 1.1552 - val_accuracy: 0.7172\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4577 - accuracy: 0.5283 - val_loss: 1.1597 - val_accuracy: 0.7047\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4247 - accuracy: 0.5451 - val_loss: 1.1020 - val_accuracy: 0.6936\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3843 - accuracy: 0.5579 - val_loss: 1.1242 - val_accuracy: 0.7095\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3504 - accuracy: 0.5706 - val_loss: 0.9856 - val_accuracy: 0.7541\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3165 - accuracy: 0.5834 - val_loss: 1.0027 - val_accuracy: 0.7360\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2845 - accuracy: 0.5977 - val_loss: 0.9420 - val_accuracy: 0.7508\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2546 - accuracy: 0.6068 - val_loss: 0.9266 - val_accuracy: 0.7591\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2256 - accuracy: 0.6169 - val_loss: 0.8397 - val_accuracy: 0.7789\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1970 - accuracy: 0.6299 - val_loss: 0.9184 - val_accuracy: 0.7604\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1751 - accuracy: 0.6385 - val_loss: 0.9001 - val_accuracy: 0.7543\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1516 - accuracy: 0.6471 - val_loss: 0.8295 - val_accuracy: 0.7816\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1268 - accuracy: 0.6532 - val_loss: 0.9050 - val_accuracy: 0.7511\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1071 - accuracy: 0.6608 - val_loss: 0.8786 - val_accuracy: 0.7480\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0876 - accuracy: 0.6692 - val_loss: 0.8068 - val_accuracy: 0.7786\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0624 - accuracy: 0.6750 - val_loss: 0.9208 - val_accuracy: 0.7301\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0425 - accuracy: 0.6816 - val_loss: 0.7682 - val_accuracy: 0.7874\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0212 - accuracy: 0.6882 - val_loss: 0.7747 - val_accuracy: 0.7771\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0055 - accuracy: 0.6932 - val_loss: 0.6672 - val_accuracy: 0.8210\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9911 - accuracy: 0.6981 - val_loss: 0.7498 - val_accuracy: 0.7896\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9788 - accuracy: 0.7017 - val_loss: 0.7920 - val_accuracy: 0.7659\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9532 - accuracy: 0.7113 - val_loss: 0.6695 - val_accuracy: 0.8069\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9439 - accuracy: 0.7139 - val_loss: 0.7421 - val_accuracy: 0.7891\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9270 - accuracy: 0.7187 - val_loss: 0.7885 - val_accuracy: 0.7671\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.9178 - accuracy: 0.7225 - val_loss: 0.6800 - val_accuracy: 0.8047\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8957 - accuracy: 0.7298 - val_loss: 0.8096 - val_accuracy: 0.7524\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8842 - accuracy: 0.7336 - val_loss: 0.7965 - val_accuracy: 0.7524\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8768 - accuracy: 0.7334 - val_loss: 0.6409 - val_accuracy: 0.8185\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8656 - accuracy: 0.7375 - val_loss: 0.6930 - val_accuracy: 0.7991\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.8455 - accuracy: 0.7442 - val_loss: 0.6887 - val_accuracy: 0.7944\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8400 - accuracy: 0.7469 - val_loss: 0.6595 - val_accuracy: 0.7938\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8336 - accuracy: 0.7503 - val_loss: 0.5688 - val_accuracy: 0.8329\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8186 - accuracy: 0.7531 - val_loss: 0.6717 - val_accuracy: 0.7930\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8095 - accuracy: 0.7546 - val_loss: 0.6991 - val_accuracy: 0.7796\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8024 - accuracy: 0.7598 - val_loss: 0.7553 - val_accuracy: 0.7774\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7908 - accuracy: 0.7629 - val_loss: 0.6010 - val_accuracy: 0.8217\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7872 - accuracy: 0.7631 - val_loss: 0.7035 - val_accuracy: 0.7761\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7765 - accuracy: 0.7666 - val_loss: 0.5985 - val_accuracy: 0.8235\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7635 - accuracy: 0.7728 - val_loss: 0.6184 - val_accuracy: 0.8168\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7596 - accuracy: 0.7729 - val_loss: 0.8644 - val_accuracy: 0.7372\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7537 - accuracy: 0.7727 - val_loss: 0.5911 - val_accuracy: 0.8174\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7423 - accuracy: 0.7781 - val_loss: 0.6674 - val_accuracy: 0.7927\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7347 - accuracy: 0.7803 - val_loss: 0.6084 - val_accuracy: 0.8202\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7221 - accuracy: 0.7858 - val_loss: 0.4830 - val_accuracy: 0.8591\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7241 - accuracy: 0.7838 - val_loss: 0.5481 - val_accuracy: 0.8383\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7160 - accuracy: 0.7862 - val_loss: 0.5787 - val_accuracy: 0.8221\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7071 - accuracy: 0.7886 - val_loss: 0.6851 - val_accuracy: 0.7951\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7012 - accuracy: 0.7905 - val_loss: 0.8745 - val_accuracy: 0.7312\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6902 - accuracy: 0.7933 - val_loss: 0.6367 - val_accuracy: 0.8039\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6888 - accuracy: 0.7934 - val_loss: 0.5289 - val_accuracy: 0.8396\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6893 - accuracy: 0.7923 - val_loss: 0.5171 - val_accuracy: 0.8489\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6862 - accuracy: 0.7952 - val_loss: 0.6025 - val_accuracy: 0.8191\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6752 - accuracy: 0.8007 - val_loss: 0.4643 - val_accuracy: 0.8659\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6743 - accuracy: 0.7976 - val_loss: 0.4967 - val_accuracy: 0.8535\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6679 - accuracy: 0.8007 - val_loss: 0.5061 - val_accuracy: 0.8486\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6577 - accuracy: 0.8025 - val_loss: 0.5285 - val_accuracy: 0.8395\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6518 - accuracy: 0.8044 - val_loss: 0.4580 - val_accuracy: 0.8686\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6508 - accuracy: 0.8077 - val_loss: 0.5489 - val_accuracy: 0.8342\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6411 - accuracy: 0.8117 - val_loss: 0.7502 - val_accuracy: 0.7687\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6467 - accuracy: 0.8052 - val_loss: 0.5885 - val_accuracy: 0.8202\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6384 - accuracy: 0.8106 - val_loss: 0.5288 - val_accuracy: 0.8418\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6307 - accuracy: 0.8132 - val_loss: 0.5696 - val_accuracy: 0.8269\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6259 - accuracy: 0.8139 - val_loss: 0.5432 - val_accuracy: 0.8389\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6269 - accuracy: 0.8153 - val_loss: 0.5158 - val_accuracy: 0.8472\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6240 - accuracy: 0.8158 - val_loss: 0.6854 - val_accuracy: 0.7762\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6129 - accuracy: 0.8185 - val_loss: 0.4184 - val_accuracy: 0.8759\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6167 - accuracy: 0.8180 - val_loss: 0.5361 - val_accuracy: 0.8390\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6083 - accuracy: 0.8193 - val_loss: 0.5297 - val_accuracy: 0.8431\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6061 - accuracy: 0.8211 - val_loss: 0.5176 - val_accuracy: 0.8490\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6009 - accuracy: 0.8205 - val_loss: 0.6144 - val_accuracy: 0.8139\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5981 - accuracy: 0.8219 - val_loss: 0.5767 - val_accuracy: 0.8243\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5890 - accuracy: 0.8257 - val_loss: 0.4480 - val_accuracy: 0.8638\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5906 - accuracy: 0.8258 - val_loss: 0.4196 - val_accuracy: 0.8757\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5889 - accuracy: 0.8263 - val_loss: 0.4594 - val_accuracy: 0.8650\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5787 - accuracy: 0.8285 - val_loss: 0.5046 - val_accuracy: 0.8531\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5764 - accuracy: 0.8300 - val_loss: 0.4772 - val_accuracy: 0.8566\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5746 - accuracy: 0.8299 - val_loss: 0.4202 - val_accuracy: 0.8749\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5675 - accuracy: 0.8316 - val_loss: 0.3981 - val_accuracy: 0.8844\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5650 - accuracy: 0.8319 - val_loss: 0.4984 - val_accuracy: 0.8505\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5648 - accuracy: 0.8330 - val_loss: 0.4577 - val_accuracy: 0.8617\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5661 - accuracy: 0.8333 - val_loss: 0.3916 - val_accuracy: 0.8874\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5619 - accuracy: 0.8338 - val_loss: 0.5841 - val_accuracy: 0.8266\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5572 - accuracy: 0.8369 - val_loss: 0.4677 - val_accuracy: 0.8628\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5531 - accuracy: 0.8374 - val_loss: 0.4296 - val_accuracy: 0.8734\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5486 - accuracy: 0.8377 - val_loss: 0.5372 - val_accuracy: 0.8394\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5498 - accuracy: 0.8385 - val_loss: 0.5033 - val_accuracy: 0.8463\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5396 - accuracy: 0.8414 - val_loss: 0.5005 - val_accuracy: 0.8484\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5403 - accuracy: 0.8429 - val_loss: 0.4870 - val_accuracy: 0.8556\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5398 - accuracy: 0.8406 - val_loss: 0.4772 - val_accuracy: 0.8609\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5361 - accuracy: 0.8440 - val_loss: 0.3800 - val_accuracy: 0.8901\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5345 - accuracy: 0.8419 - val_loss: 0.4441 - val_accuracy: 0.8689\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5324 - accuracy: 0.8417 - val_loss: 0.4267 - val_accuracy: 0.8766\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5295 - accuracy: 0.8430 - val_loss: 0.3937 - val_accuracy: 0.8841\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5285 - accuracy: 0.8452 - val_loss: 0.4957 - val_accuracy: 0.8519\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5234 - accuracy: 0.8438 - val_loss: 0.4268 - val_accuracy: 0.8755\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5220 - accuracy: 0.8468 - val_loss: 0.4710 - val_accuracy: 0.8593\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5153 - accuracy: 0.8491 - val_loss: 0.4506 - val_accuracy: 0.8689\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5181 - accuracy: 0.8466 - val_loss: 0.4027 - val_accuracy: 0.8860\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5133 - accuracy: 0.8485 - val_loss: 0.4501 - val_accuracy: 0.8688\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5176 - accuracy: 0.8485 - val_loss: 0.4391 - val_accuracy: 0.8724\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5066 - accuracy: 0.8512 - val_loss: 0.5533 - val_accuracy: 0.8340\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5080 - accuracy: 0.8496 - val_loss: 0.4279 - val_accuracy: 0.8720\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5094 - accuracy: 0.8500 - val_loss: 0.3288 - val_accuracy: 0.9065\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5011 - accuracy: 0.8512 - val_loss: 0.4624 - val_accuracy: 0.8581\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5045 - accuracy: 0.8499 - val_loss: 0.4006 - val_accuracy: 0.8803\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4988 - accuracy: 0.8546 - val_loss: 0.3844 - val_accuracy: 0.8898\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4992 - accuracy: 0.8551 - val_loss: 0.4218 - val_accuracy: 0.8769\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4966 - accuracy: 0.8542 - val_loss: 0.4320 - val_accuracy: 0.8709\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4916 - accuracy: 0.8545 - val_loss: 0.3993 - val_accuracy: 0.8850\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4908 - accuracy: 0.8570 - val_loss: 0.3445 - val_accuracy: 0.9002\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4872 - accuracy: 0.8560 - val_loss: 0.3505 - val_accuracy: 0.8987\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4848 - accuracy: 0.8584 - val_loss: 0.4767 - val_accuracy: 0.8594\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4894 - accuracy: 0.8569 - val_loss: 0.3780 - val_accuracy: 0.8914\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4816 - accuracy: 0.8593 - val_loss: 0.3831 - val_accuracy: 0.8865\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4819 - accuracy: 0.8592 - val_loss: 0.4007 - val_accuracy: 0.8826\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4772 - accuracy: 0.8597 - val_loss: 0.3836 - val_accuracy: 0.8930\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4754 - accuracy: 0.8619 - val_loss: 0.4003 - val_accuracy: 0.8821\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4699 - accuracy: 0.8629 - val_loss: 0.4792 - val_accuracy: 0.8570\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4744 - accuracy: 0.8593 - val_loss: 0.4806 - val_accuracy: 0.8617\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4742 - accuracy: 0.8608 - val_loss: 0.3887 - val_accuracy: 0.8876\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4737 - accuracy: 0.8614 - val_loss: 0.3911 - val_accuracy: 0.8837\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4662 - accuracy: 0.8633 - val_loss: 0.4348 - val_accuracy: 0.8732\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4674 - accuracy: 0.8624 - val_loss: 0.4806 - val_accuracy: 0.8593\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4657 - accuracy: 0.8619 - val_loss: 0.4626 - val_accuracy: 0.8624\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4582 - accuracy: 0.8651 - val_loss: 0.4275 - val_accuracy: 0.8729\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4587 - accuracy: 0.8666 - val_loss: 0.4757 - val_accuracy: 0.8599\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4560 - accuracy: 0.8662 - val_loss: 0.4888 - val_accuracy: 0.8551\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4551 - accuracy: 0.8673 - val_loss: 0.4625 - val_accuracy: 0.8645\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4572 - accuracy: 0.8661 - val_loss: 0.4972 - val_accuracy: 0.8511\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4530 - accuracy: 0.8679 - val_loss: 0.3569 - val_accuracy: 0.8916\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4516 - accuracy: 0.8685 - val_loss: 0.3708 - val_accuracy: 0.8885\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4476 - accuracy: 0.8680 - val_loss: 0.4400 - val_accuracy: 0.8709\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4485 - accuracy: 0.8687 - val_loss: 0.2995 - val_accuracy: 0.9156\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4438 - accuracy: 0.8724 - val_loss: 0.4027 - val_accuracy: 0.8839\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4505 - accuracy: 0.8675 - val_loss: 0.4966 - val_accuracy: 0.8561\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4416 - accuracy: 0.8713 - val_loss: 0.3737 - val_accuracy: 0.8927\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4372 - accuracy: 0.8730 - val_loss: 0.3417 - val_accuracy: 0.9025\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4389 - accuracy: 0.8713 - val_loss: 0.3651 - val_accuracy: 0.8966\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4443 - accuracy: 0.8705 - val_loss: 0.3714 - val_accuracy: 0.8937\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4348 - accuracy: 0.8740 - val_loss: 0.4593 - val_accuracy: 0.8630\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4312 - accuracy: 0.8753 - val_loss: 0.4154 - val_accuracy: 0.8781\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4356 - accuracy: 0.8731 - val_loss: 0.3970 - val_accuracy: 0.8821\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4329 - accuracy: 0.8722 - val_loss: 0.3645 - val_accuracy: 0.8953\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4355 - accuracy: 0.8737 - val_loss: 0.4605 - val_accuracy: 0.8622\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4276 - accuracy: 0.8758 - val_loss: 0.3519 - val_accuracy: 0.9011\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4269 - accuracy: 0.8761 - val_loss: 0.3604 - val_accuracy: 0.8968\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4250 - accuracy: 0.8757 - val_loss: 0.4164 - val_accuracy: 0.8774\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4226 - accuracy: 0.8766 - val_loss: 0.3546 - val_accuracy: 0.8987\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4282 - accuracy: 0.8741 - val_loss: 0.3863 - val_accuracy: 0.8879\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4238 - accuracy: 0.8755 - val_loss: 0.4266 - val_accuracy: 0.8785\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8776 - val_loss: 0.3914 - val_accuracy: 0.8870\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4210 - accuracy: 0.8771 - val_loss: 0.4685 - val_accuracy: 0.8595\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4179 - accuracy: 0.8779 - val_loss: 0.3373 - val_accuracy: 0.9049\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4134 - accuracy: 0.8785 - val_loss: 0.3451 - val_accuracy: 0.8989\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4117 - accuracy: 0.8804 - val_loss: 0.4283 - val_accuracy: 0.8729\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4168 - accuracy: 0.8797 - val_loss: 0.3629 - val_accuracy: 0.8934\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4120 - accuracy: 0.8810 - val_loss: 0.4022 - val_accuracy: 0.8854\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4083 - accuracy: 0.8806 - val_loss: 0.4405 - val_accuracy: 0.8680\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4088 - accuracy: 0.8800 - val_loss: 0.3756 - val_accuracy: 0.8899\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4060 - accuracy: 0.8822 - val_loss: 0.2979 - val_accuracy: 0.9184\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4084 - accuracy: 0.8813 - val_loss: 0.3228 - val_accuracy: 0.9074\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4047 - accuracy: 0.8835 - val_loss: 0.3209 - val_accuracy: 0.9094\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4061 - accuracy: 0.8815 - val_loss: 0.4299 - val_accuracy: 0.8736\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3994 - accuracy: 0.8840 - val_loss: 0.6235 - val_accuracy: 0.8089\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4056 - accuracy: 0.8813 - val_loss: 0.3503 - val_accuracy: 0.8951\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3986 - accuracy: 0.8850 - val_loss: 0.3201 - val_accuracy: 0.9101\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4035 - accuracy: 0.8817 - val_loss: 0.4310 - val_accuracy: 0.8728\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3924 - accuracy: 0.8845 - val_loss: 0.3161 - val_accuracy: 0.9124\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3990 - accuracy: 0.8843 - val_loss: 0.3438 - val_accuracy: 0.9018\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8848 - val_loss: 0.4515 - val_accuracy: 0.8659\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8845 - val_loss: 0.4404 - val_accuracy: 0.8717\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3975 - accuracy: 0.8833 - val_loss: 0.6323 - val_accuracy: 0.8161\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3931 - accuracy: 0.8871 - val_loss: 0.3879 - val_accuracy: 0.8863\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8862 - val_loss: 0.4736 - val_accuracy: 0.8628\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3940 - accuracy: 0.8841 - val_loss: 0.3462 - val_accuracy: 0.9018\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3880 - accuracy: 0.8870 - val_loss: 0.6189 - val_accuracy: 0.8179\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3820 - accuracy: 0.8887 - val_loss: 0.3689 - val_accuracy: 0.8949\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8871 - val_loss: 0.3743 - val_accuracy: 0.8881\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8882 - val_loss: 0.4801 - val_accuracy: 0.8613\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3828 - accuracy: 0.8882 - val_loss: 0.4085 - val_accuracy: 0.8834\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3871 - accuracy: 0.8880 - val_loss: 0.3704 - val_accuracy: 0.8944\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3823 - accuracy: 0.8885 - val_loss: 0.4050 - val_accuracy: 0.8814\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3806 - accuracy: 0.8890 - val_loss: 0.4273 - val_accuracy: 0.8769\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3743 - accuracy: 0.8901 - val_loss: 0.5791 - val_accuracy: 0.8189\n",
      "Try 7/100: Best_val_acc: [0.5321826338768005, 0.8389999866485596], lr: 6.1047519555968e-05, Lambda: 5.8161539462174355e-05\n",
      "\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_433 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7187 - accuracy: 0.1111 - val_loss: 2.2445 - val_accuracy: 0.1808\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5603 - accuracy: 0.1304 - val_loss: 2.1265 - val_accuracy: 0.2829\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4380 - accuracy: 0.1579 - val_loss: 2.0180 - val_accuracy: 0.3140\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3375 - accuracy: 0.1819 - val_loss: 1.8898 - val_accuracy: 0.4241\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2410 - accuracy: 0.2113 - val_loss: 1.7714 - val_accuracy: 0.4884\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1561 - accuracy: 0.2350 - val_loss: 1.6909 - val_accuracy: 0.5536\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0743 - accuracy: 0.2638 - val_loss: 1.6411 - val_accuracy: 0.5755\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0020 - accuracy: 0.2923 - val_loss: 1.6382 - val_accuracy: 0.5795\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9321 - accuracy: 0.3200 - val_loss: 1.6067 - val_accuracy: 0.5961\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8759 - accuracy: 0.3426 - val_loss: 1.5804 - val_accuracy: 0.5993\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8151 - accuracy: 0.3680 - val_loss: 1.5148 - val_accuracy: 0.5964\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7675 - accuracy: 0.3859 - val_loss: 1.5023 - val_accuracy: 0.5933\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7061 - accuracy: 0.4164 - val_loss: 1.4115 - val_accuracy: 0.6519\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6669 - accuracy: 0.4345 - val_loss: 1.3050 - val_accuracy: 0.6784\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6089 - accuracy: 0.4571 - val_loss: 1.2918 - val_accuracy: 0.6914\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5658 - accuracy: 0.4778 - val_loss: 1.3528 - val_accuracy: 0.6316\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5289 - accuracy: 0.4943 - val_loss: 1.2153 - val_accuracy: 0.7091\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4858 - accuracy: 0.5123 - val_loss: 1.1796 - val_accuracy: 0.6894\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4468 - accuracy: 0.5276 - val_loss: 1.1327 - val_accuracy: 0.7134\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4018 - accuracy: 0.5472 - val_loss: 1.1167 - val_accuracy: 0.7213\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3668 - accuracy: 0.5589 - val_loss: 1.0924 - val_accuracy: 0.7189\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3353 - accuracy: 0.5750 - val_loss: 1.1559 - val_accuracy: 0.6656\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3111 - accuracy: 0.5805 - val_loss: 1.0230 - val_accuracy: 0.7501\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2759 - accuracy: 0.5950 - val_loss: 1.0058 - val_accuracy: 0.7263\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2554 - accuracy: 0.6011 - val_loss: 0.9766 - val_accuracy: 0.7414\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2216 - accuracy: 0.6112 - val_loss: 0.9393 - val_accuracy: 0.7516\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2008 - accuracy: 0.6203 - val_loss: 0.9336 - val_accuracy: 0.7390\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1714 - accuracy: 0.6312 - val_loss: 0.9186 - val_accuracy: 0.7558\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1529 - accuracy: 0.6380 - val_loss: 0.9133 - val_accuracy: 0.7384\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1269 - accuracy: 0.6490 - val_loss: 0.8965 - val_accuracy: 0.7600\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1068 - accuracy: 0.6566 - val_loss: 0.9258 - val_accuracy: 0.7307\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0869 - accuracy: 0.6624 - val_loss: 0.8209 - val_accuracy: 0.7748\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0660 - accuracy: 0.6709 - val_loss: 0.9130 - val_accuracy: 0.7399\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0514 - accuracy: 0.6739 - val_loss: 0.8093 - val_accuracy: 0.7709\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0284 - accuracy: 0.6821 - val_loss: 0.9031 - val_accuracy: 0.7311\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0142 - accuracy: 0.6879 - val_loss: 0.8091 - val_accuracy: 0.7736\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0007 - accuracy: 0.6915 - val_loss: 0.7940 - val_accuracy: 0.7692\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9804 - accuracy: 0.6996 - val_loss: 0.8172 - val_accuracy: 0.7621\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9703 - accuracy: 0.7031 - val_loss: 0.7347 - val_accuracy: 0.7931\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9506 - accuracy: 0.7099 - val_loss: 0.7540 - val_accuracy: 0.7760\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9362 - accuracy: 0.7145 - val_loss: 0.7393 - val_accuracy: 0.7873\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9237 - accuracy: 0.7165 - val_loss: 0.6540 - val_accuracy: 0.8169\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9119 - accuracy: 0.7234 - val_loss: 0.7051 - val_accuracy: 0.7889\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9010 - accuracy: 0.7268 - val_loss: 0.6351 - val_accuracy: 0.8207\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8921 - accuracy: 0.7292 - val_loss: 0.7175 - val_accuracy: 0.7896\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8769 - accuracy: 0.7331 - val_loss: 0.7035 - val_accuracy: 0.7941\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8664 - accuracy: 0.7353 - val_loss: 0.6471 - val_accuracy: 0.8129\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8553 - accuracy: 0.7392 - val_loss: 0.6919 - val_accuracy: 0.7974\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8359 - accuracy: 0.7485 - val_loss: 0.6914 - val_accuracy: 0.7974\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8339 - accuracy: 0.7465 - val_loss: 0.7888 - val_accuracy: 0.7494\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8274 - accuracy: 0.7485 - val_loss: 0.6615 - val_accuracy: 0.7984\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8134 - accuracy: 0.7538 - val_loss: 0.6662 - val_accuracy: 0.8028\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8033 - accuracy: 0.7588 - val_loss: 0.6074 - val_accuracy: 0.8226\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7955 - accuracy: 0.7615 - val_loss: 0.6275 - val_accuracy: 0.8156\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7889 - accuracy: 0.7635 - val_loss: 0.7435 - val_accuracy: 0.7680\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7789 - accuracy: 0.7670 - val_loss: 0.5807 - val_accuracy: 0.8271\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7776 - accuracy: 0.7667 - val_loss: 0.6028 - val_accuracy: 0.8269\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7694 - accuracy: 0.7696 - val_loss: 0.6727 - val_accuracy: 0.7943\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7616 - accuracy: 0.7724 - val_loss: 0.5187 - val_accuracy: 0.8503\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7519 - accuracy: 0.7722 - val_loss: 0.5891 - val_accuracy: 0.8231\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7424 - accuracy: 0.7758 - val_loss: 0.6218 - val_accuracy: 0.8128\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7408 - accuracy: 0.7785 - val_loss: 0.5493 - val_accuracy: 0.8429\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7283 - accuracy: 0.7810 - val_loss: 0.5328 - val_accuracy: 0.8439\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7243 - accuracy: 0.7816 - val_loss: 0.6246 - val_accuracy: 0.8115\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7202 - accuracy: 0.7819 - val_loss: 0.5398 - val_accuracy: 0.8405\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7135 - accuracy: 0.7870 - val_loss: 0.5735 - val_accuracy: 0.8329\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7022 - accuracy: 0.7890 - val_loss: 0.5738 - val_accuracy: 0.8309\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7011 - accuracy: 0.7914 - val_loss: 0.5624 - val_accuracy: 0.8321\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6938 - accuracy: 0.7931 - val_loss: 0.5758 - val_accuracy: 0.8203\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6885 - accuracy: 0.7952 - val_loss: 0.6268 - val_accuracy: 0.8119\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6845 - accuracy: 0.7944 - val_loss: 0.5387 - val_accuracy: 0.8351\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6763 - accuracy: 0.7995 - val_loss: 0.5569 - val_accuracy: 0.8313\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6761 - accuracy: 0.7988 - val_loss: 0.6577 - val_accuracy: 0.7979\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6657 - accuracy: 0.8017 - val_loss: 0.4806 - val_accuracy: 0.8610\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6617 - accuracy: 0.8017 - val_loss: 0.6755 - val_accuracy: 0.7854\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6579 - accuracy: 0.8040 - val_loss: 0.6146 - val_accuracy: 0.8096\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6560 - accuracy: 0.8038 - val_loss: 0.5756 - val_accuracy: 0.8269\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6435 - accuracy: 0.8093 - val_loss: 0.5534 - val_accuracy: 0.8336\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6454 - accuracy: 0.8079 - val_loss: 0.5533 - val_accuracy: 0.8326\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6327 - accuracy: 0.8109 - val_loss: 0.5112 - val_accuracy: 0.8435\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6327 - accuracy: 0.8102 - val_loss: 0.5428 - val_accuracy: 0.8374\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6246 - accuracy: 0.8141 - val_loss: 0.5302 - val_accuracy: 0.8411\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6286 - accuracy: 0.8123 - val_loss: 0.5054 - val_accuracy: 0.8499\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6219 - accuracy: 0.8143 - val_loss: 0.4958 - val_accuracy: 0.8525\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6187 - accuracy: 0.8150 - val_loss: 0.5218 - val_accuracy: 0.8476\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6130 - accuracy: 0.8182 - val_loss: 0.5099 - val_accuracy: 0.8469\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6112 - accuracy: 0.8167 - val_loss: 0.4409 - val_accuracy: 0.8726\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6103 - accuracy: 0.8193 - val_loss: 0.5410 - val_accuracy: 0.8338\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6120 - accuracy: 0.8173 - val_loss: 0.6014 - val_accuracy: 0.8086\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5980 - accuracy: 0.8215 - val_loss: 0.5157 - val_accuracy: 0.8436\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5924 - accuracy: 0.8240 - val_loss: 0.5719 - val_accuracy: 0.8274\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5888 - accuracy: 0.8260 - val_loss: 0.4399 - val_accuracy: 0.8727\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5890 - accuracy: 0.8241 - val_loss: 0.4411 - val_accuracy: 0.8714\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5928 - accuracy: 0.8246 - val_loss: 0.5499 - val_accuracy: 0.8347\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5815 - accuracy: 0.8272 - val_loss: 0.5760 - val_accuracy: 0.8228\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5804 - accuracy: 0.8264 - val_loss: 0.5162 - val_accuracy: 0.8483\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5795 - accuracy: 0.8273 - val_loss: 0.6157 - val_accuracy: 0.8093\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5671 - accuracy: 0.8313 - val_loss: 0.4940 - val_accuracy: 0.8487\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5655 - accuracy: 0.8342 - val_loss: 0.5359 - val_accuracy: 0.8390\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5691 - accuracy: 0.8331 - val_loss: 0.4644 - val_accuracy: 0.8621\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5599 - accuracy: 0.8346 - val_loss: 0.5972 - val_accuracy: 0.8207\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5675 - accuracy: 0.8301 - val_loss: 0.5872 - val_accuracy: 0.8235\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5578 - accuracy: 0.8331 - val_loss: 0.5810 - val_accuracy: 0.8273\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5523 - accuracy: 0.8354 - val_loss: 0.4826 - val_accuracy: 0.8547\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5549 - accuracy: 0.8350 - val_loss: 0.4343 - val_accuracy: 0.8682\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5519 - accuracy: 0.8379 - val_loss: 0.4905 - val_accuracy: 0.8529\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5453 - accuracy: 0.8403 - val_loss: 0.4891 - val_accuracy: 0.8573\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5465 - accuracy: 0.8391 - val_loss: 0.4410 - val_accuracy: 0.8738\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5381 - accuracy: 0.8396 - val_loss: 0.4647 - val_accuracy: 0.8616\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5396 - accuracy: 0.8403 - val_loss: 0.4595 - val_accuracy: 0.8641\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5328 - accuracy: 0.8414 - val_loss: 0.4132 - val_accuracy: 0.8794\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5284 - accuracy: 0.8422 - val_loss: 0.3931 - val_accuracy: 0.8860\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5314 - accuracy: 0.8435 - val_loss: 0.4506 - val_accuracy: 0.8681\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5278 - accuracy: 0.8424 - val_loss: 0.4381 - val_accuracy: 0.8666\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5265 - accuracy: 0.8436 - val_loss: 0.4538 - val_accuracy: 0.8678\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5254 - accuracy: 0.8445 - val_loss: 0.4653 - val_accuracy: 0.8616\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5153 - accuracy: 0.8461 - val_loss: 0.4650 - val_accuracy: 0.8562\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5177 - accuracy: 0.8465 - val_loss: 0.4276 - val_accuracy: 0.8746\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5199 - accuracy: 0.8456 - val_loss: 0.5311 - val_accuracy: 0.8337\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5174 - accuracy: 0.8473 - val_loss: 0.4510 - val_accuracy: 0.8638\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5096 - accuracy: 0.8504 - val_loss: 0.4338 - val_accuracy: 0.8674\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5092 - accuracy: 0.8486 - val_loss: 0.4307 - val_accuracy: 0.8744\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5074 - accuracy: 0.8497 - val_loss: 0.5013 - val_accuracy: 0.8516\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5027 - accuracy: 0.8512 - val_loss: 0.4969 - val_accuracy: 0.8510\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5019 - accuracy: 0.8530 - val_loss: 0.4944 - val_accuracy: 0.8525\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5014 - accuracy: 0.8521 - val_loss: 0.4799 - val_accuracy: 0.8541\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4945 - accuracy: 0.8516 - val_loss: 0.5198 - val_accuracy: 0.8441\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4962 - accuracy: 0.8535 - val_loss: 0.4441 - val_accuracy: 0.8684\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4903 - accuracy: 0.8557 - val_loss: 0.4847 - val_accuracy: 0.8559\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4885 - accuracy: 0.8563 - val_loss: 0.4057 - val_accuracy: 0.8814\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4892 - accuracy: 0.8568 - val_loss: 0.4147 - val_accuracy: 0.8751\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4895 - accuracy: 0.8551 - val_loss: 0.4971 - val_accuracy: 0.8479\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4849 - accuracy: 0.8568 - val_loss: 0.4707 - val_accuracy: 0.8579\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4839 - accuracy: 0.8591 - val_loss: 0.4574 - val_accuracy: 0.8637\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4789 - accuracy: 0.8594 - val_loss: 0.5462 - val_accuracy: 0.8356\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4788 - accuracy: 0.8579 - val_loss: 0.4262 - val_accuracy: 0.8743\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4784 - accuracy: 0.8595 - val_loss: 0.5054 - val_accuracy: 0.8419\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4775 - accuracy: 0.8597 - val_loss: 0.4080 - val_accuracy: 0.8824\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4761 - accuracy: 0.8591 - val_loss: 0.5190 - val_accuracy: 0.8449\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4822 - accuracy: 0.8583 - val_loss: 0.5130 - val_accuracy: 0.8441\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4761 - accuracy: 0.8608 - val_loss: 0.3945 - val_accuracy: 0.8858\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4689 - accuracy: 0.8642 - val_loss: 0.3919 - val_accuracy: 0.8862\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4592 - accuracy: 0.8639 - val_loss: 0.3802 - val_accuracy: 0.8890\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4658 - accuracy: 0.8626 - val_loss: 0.3842 - val_accuracy: 0.8888\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4621 - accuracy: 0.8625 - val_loss: 0.4770 - val_accuracy: 0.8592\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4661 - accuracy: 0.8630 - val_loss: 0.4612 - val_accuracy: 0.8679\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4571 - accuracy: 0.8649 - val_loss: 0.6335 - val_accuracy: 0.8143\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4621 - accuracy: 0.8649 - val_loss: 0.4649 - val_accuracy: 0.8603\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4519 - accuracy: 0.8683 - val_loss: 0.4060 - val_accuracy: 0.8819\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4565 - accuracy: 0.8657 - val_loss: 0.3797 - val_accuracy: 0.8904\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4522 - accuracy: 0.8664 - val_loss: 0.4291 - val_accuracy: 0.8711\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4598 - accuracy: 0.8649 - val_loss: 0.4740 - val_accuracy: 0.8555\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4486 - accuracy: 0.8670 - val_loss: 0.5178 - val_accuracy: 0.8406\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4441 - accuracy: 0.8698 - val_loss: 0.3580 - val_accuracy: 0.8941\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4510 - accuracy: 0.8670 - val_loss: 0.3316 - val_accuracy: 0.9076\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4474 - accuracy: 0.8688 - val_loss: 0.4289 - val_accuracy: 0.8764\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4455 - accuracy: 0.8693 - val_loss: 0.3597 - val_accuracy: 0.8961\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4438 - accuracy: 0.8686 - val_loss: 0.3591 - val_accuracy: 0.8946\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4371 - accuracy: 0.8717 - val_loss: 0.5248 - val_accuracy: 0.8436\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4384 - accuracy: 0.8713 - val_loss: 0.4238 - val_accuracy: 0.8748\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4389 - accuracy: 0.8694 - val_loss: 0.3632 - val_accuracy: 0.8944\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4358 - accuracy: 0.8720 - val_loss: 0.5218 - val_accuracy: 0.8462\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4351 - accuracy: 0.8713 - val_loss: 0.4903 - val_accuracy: 0.8490\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4404 - accuracy: 0.8714 - val_loss: 0.4706 - val_accuracy: 0.8554\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4305 - accuracy: 0.8727 - val_loss: 0.4107 - val_accuracy: 0.8745\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4319 - accuracy: 0.8730 - val_loss: 0.4461 - val_accuracy: 0.8681\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4307 - accuracy: 0.8745 - val_loss: 0.3651 - val_accuracy: 0.8934\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4247 - accuracy: 0.8745 - val_loss: 0.3733 - val_accuracy: 0.8911\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4203 - accuracy: 0.8752 - val_loss: 0.4106 - val_accuracy: 0.8791\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4237 - accuracy: 0.8762 - val_loss: 0.4033 - val_accuracy: 0.8829\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4253 - accuracy: 0.8744 - val_loss: 0.3393 - val_accuracy: 0.9019\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4172 - accuracy: 0.8782 - val_loss: 0.3370 - val_accuracy: 0.9036\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4196 - accuracy: 0.8779 - val_loss: 0.5024 - val_accuracy: 0.8473\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4130 - accuracy: 0.8794 - val_loss: 0.4045 - val_accuracy: 0.8797\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4168 - accuracy: 0.8779 - val_loss: 0.4527 - val_accuracy: 0.8637\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4156 - accuracy: 0.8791 - val_loss: 0.5003 - val_accuracy: 0.8456\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8770 - val_loss: 0.3440 - val_accuracy: 0.8995\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4122 - accuracy: 0.8783 - val_loss: 0.3757 - val_accuracy: 0.8902\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4045 - accuracy: 0.8813 - val_loss: 0.3866 - val_accuracy: 0.8856\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4076 - accuracy: 0.8803 - val_loss: 0.4857 - val_accuracy: 0.8464\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4111 - accuracy: 0.8783 - val_loss: 0.5118 - val_accuracy: 0.8426\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4079 - accuracy: 0.8795 - val_loss: 0.4201 - val_accuracy: 0.8744\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4056 - accuracy: 0.8802 - val_loss: 0.3307 - val_accuracy: 0.9071\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4058 - accuracy: 0.8806 - val_loss: 0.3566 - val_accuracy: 0.8953\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4010 - accuracy: 0.8825 - val_loss: 0.3407 - val_accuracy: 0.9014\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3940 - accuracy: 0.8841 - val_loss: 0.4058 - val_accuracy: 0.8791\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4039 - accuracy: 0.8820 - val_loss: 0.4128 - val_accuracy: 0.8767\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3992 - accuracy: 0.8825 - val_loss: 0.3696 - val_accuracy: 0.8939\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8830 - val_loss: 0.4107 - val_accuracy: 0.8779\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3974 - accuracy: 0.8834 - val_loss: 0.3350 - val_accuracy: 0.9057\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3965 - accuracy: 0.8835 - val_loss: 0.3739 - val_accuracy: 0.8928\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8827 - val_loss: 0.3468 - val_accuracy: 0.9001\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3932 - accuracy: 0.8836 - val_loss: 0.4127 - val_accuracy: 0.8731\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8850 - val_loss: 0.4032 - val_accuracy: 0.8799\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3964 - accuracy: 0.8849 - val_loss: 0.3948 - val_accuracy: 0.8873\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3928 - accuracy: 0.8861 - val_loss: 0.3809 - val_accuracy: 0.8877\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3889 - accuracy: 0.8842 - val_loss: 0.3157 - val_accuracy: 0.9097\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8867 - val_loss: 0.3523 - val_accuracy: 0.8940\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3894 - accuracy: 0.8852 - val_loss: 0.3796 - val_accuracy: 0.8884\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3851 - accuracy: 0.8872 - val_loss: 0.3840 - val_accuracy: 0.8879\n",
      "Try 8/100: Best_val_acc: [0.5132859349250793, 0.8512222170829773], lr: 6.100758481900149e-05, Lambda: 5.826691858438725e-05\n",
      "\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_434 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7713 - accuracy: 0.1054 - val_loss: 2.1216 - val_accuracy: 0.1882\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.6206 - accuracy: 0.1163 - val_loss: 2.2210 - val_accuracy: 0.2236\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5138 - accuracy: 0.1309 - val_loss: 2.1797 - val_accuracy: 0.2689\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4185 - accuracy: 0.1492 - val_loss: 2.1169 - val_accuracy: 0.2913\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3239 - accuracy: 0.1685 - val_loss: 1.9880 - val_accuracy: 0.3532\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2354 - accuracy: 0.2011 - val_loss: 1.8495 - val_accuracy: 0.4298\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1509 - accuracy: 0.2305 - val_loss: 1.7786 - val_accuracy: 0.4678\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0689 - accuracy: 0.2607 - val_loss: 1.7536 - val_accuracy: 0.5154\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9964 - accuracy: 0.2896 - val_loss: 1.6546 - val_accuracy: 0.5350\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9232 - accuracy: 0.3166 - val_loss: 1.5374 - val_accuracy: 0.5941\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8657 - accuracy: 0.3448 - val_loss: 1.5387 - val_accuracy: 0.5649\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7999 - accuracy: 0.3726 - val_loss: 1.3926 - val_accuracy: 0.6568\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7489 - accuracy: 0.3944 - val_loss: 1.3823 - val_accuracy: 0.6638\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7031 - accuracy: 0.4146 - val_loss: 1.3221 - val_accuracy: 0.6686\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6496 - accuracy: 0.4398 - val_loss: 1.2788 - val_accuracy: 0.6734\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6079 - accuracy: 0.4619 - val_loss: 1.2547 - val_accuracy: 0.6633\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5586 - accuracy: 0.4830 - val_loss: 1.1827 - val_accuracy: 0.7056\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5140 - accuracy: 0.5007 - val_loss: 1.0838 - val_accuracy: 0.7273\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4812 - accuracy: 0.5163 - val_loss: 1.1523 - val_accuracy: 0.7021\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4393 - accuracy: 0.5349 - val_loss: 1.1433 - val_accuracy: 0.6965\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4028 - accuracy: 0.5494 - val_loss: 1.1536 - val_accuracy: 0.6933\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3700 - accuracy: 0.5649 - val_loss: 1.0171 - val_accuracy: 0.7360\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3398 - accuracy: 0.5765 - val_loss: 1.0046 - val_accuracy: 0.7420\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3091 - accuracy: 0.5899 - val_loss: 1.0220 - val_accuracy: 0.7301\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2804 - accuracy: 0.6018 - val_loss: 0.8961 - val_accuracy: 0.7627\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2497 - accuracy: 0.6103 - val_loss: 1.0302 - val_accuracy: 0.7169\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2219 - accuracy: 0.6217 - val_loss: 0.9359 - val_accuracy: 0.7501\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1952 - accuracy: 0.6298 - val_loss: 0.9275 - val_accuracy: 0.7544\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1710 - accuracy: 0.6418 - val_loss: 1.0197 - val_accuracy: 0.7038\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1480 - accuracy: 0.6478 - val_loss: 0.8043 - val_accuracy: 0.7809\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1299 - accuracy: 0.6545 - val_loss: 0.9163 - val_accuracy: 0.7435\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1083 - accuracy: 0.6608 - val_loss: 0.9448 - val_accuracy: 0.7301\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0810 - accuracy: 0.6712 - val_loss: 0.9386 - val_accuracy: 0.7221\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0609 - accuracy: 0.6792 - val_loss: 0.9211 - val_accuracy: 0.7394\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0465 - accuracy: 0.6843 - val_loss: 0.8077 - val_accuracy: 0.7755\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0300 - accuracy: 0.6893 - val_loss: 0.8547 - val_accuracy: 0.7511\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0127 - accuracy: 0.6936 - val_loss: 0.7902 - val_accuracy: 0.7726\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9938 - accuracy: 0.7015 - val_loss: 0.7476 - val_accuracy: 0.7899\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9738 - accuracy: 0.7052 - val_loss: 0.7613 - val_accuracy: 0.7838\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9634 - accuracy: 0.7108 - val_loss: 0.7021 - val_accuracy: 0.7989\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9464 - accuracy: 0.7155 - val_loss: 0.7424 - val_accuracy: 0.7877\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9296 - accuracy: 0.7213 - val_loss: 0.7773 - val_accuracy: 0.7674\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9191 - accuracy: 0.7249 - val_loss: 0.7436 - val_accuracy: 0.7854\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9062 - accuracy: 0.7273 - val_loss: 0.6691 - val_accuracy: 0.8096\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8922 - accuracy: 0.7328 - val_loss: 0.6859 - val_accuracy: 0.8001\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8847 - accuracy: 0.7348 - val_loss: 0.6752 - val_accuracy: 0.8047\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8701 - accuracy: 0.7414 - val_loss: 0.9346 - val_accuracy: 0.7216\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8682 - accuracy: 0.7377 - val_loss: 0.7553 - val_accuracy: 0.7711\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8482 - accuracy: 0.7467 - val_loss: 0.7959 - val_accuracy: 0.7648\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8378 - accuracy: 0.7479 - val_loss: 0.6922 - val_accuracy: 0.7909\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8281 - accuracy: 0.7546 - val_loss: 0.6653 - val_accuracy: 0.7987\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8172 - accuracy: 0.7538 - val_loss: 0.6682 - val_accuracy: 0.8005\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8071 - accuracy: 0.7590 - val_loss: 0.6215 - val_accuracy: 0.8191\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7973 - accuracy: 0.7619 - val_loss: 0.5916 - val_accuracy: 0.8266\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7905 - accuracy: 0.7662 - val_loss: 0.7113 - val_accuracy: 0.7899\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7852 - accuracy: 0.7656 - val_loss: 0.5865 - val_accuracy: 0.8281\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7718 - accuracy: 0.7697 - val_loss: 0.5692 - val_accuracy: 0.8346\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7606 - accuracy: 0.7736 - val_loss: 0.6846 - val_accuracy: 0.7917\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7627 - accuracy: 0.7747 - val_loss: 0.7087 - val_accuracy: 0.7871\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7533 - accuracy: 0.7768 - val_loss: 0.6087 - val_accuracy: 0.8229\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7479 - accuracy: 0.7794 - val_loss: 0.5938 - val_accuracy: 0.8256\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7436 - accuracy: 0.7779 - val_loss: 0.6031 - val_accuracy: 0.8195\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7319 - accuracy: 0.7811 - val_loss: 0.6260 - val_accuracy: 0.8064\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7230 - accuracy: 0.7857 - val_loss: 0.6385 - val_accuracy: 0.8105\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7146 - accuracy: 0.7879 - val_loss: 0.5408 - val_accuracy: 0.8367\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7101 - accuracy: 0.7906 - val_loss: 0.5043 - val_accuracy: 0.8522\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7083 - accuracy: 0.7909 - val_loss: 0.5197 - val_accuracy: 0.8506\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6969 - accuracy: 0.7933 - val_loss: 0.5988 - val_accuracy: 0.8216\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6929 - accuracy: 0.7950 - val_loss: 0.6494 - val_accuracy: 0.8052\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.7968 - val_loss: 0.5058 - val_accuracy: 0.8552\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6796 - accuracy: 0.7966 - val_loss: 0.5958 - val_accuracy: 0.8264\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6808 - accuracy: 0.7992 - val_loss: 0.6502 - val_accuracy: 0.8078\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6772 - accuracy: 0.7986 - val_loss: 0.6429 - val_accuracy: 0.8029\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6678 - accuracy: 0.8008 - val_loss: 0.4999 - val_accuracy: 0.8490\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6613 - accuracy: 0.8046 - val_loss: 0.4654 - val_accuracy: 0.8644\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6564 - accuracy: 0.8050 - val_loss: 0.5142 - val_accuracy: 0.8513\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6523 - accuracy: 0.8077 - val_loss: 0.5170 - val_accuracy: 0.8440\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6463 - accuracy: 0.8089 - val_loss: 0.5788 - val_accuracy: 0.8237\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6400 - accuracy: 0.8097 - val_loss: 0.5351 - val_accuracy: 0.8393\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6350 - accuracy: 0.8140 - val_loss: 0.6205 - val_accuracy: 0.8131\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6312 - accuracy: 0.8138 - val_loss: 0.5505 - val_accuracy: 0.8233\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6307 - accuracy: 0.8144 - val_loss: 0.4749 - val_accuracy: 0.8581\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6295 - accuracy: 0.8124 - val_loss: 0.5504 - val_accuracy: 0.8358\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6162 - accuracy: 0.8182 - val_loss: 0.4811 - val_accuracy: 0.8584\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6186 - accuracy: 0.8172 - val_loss: 0.5342 - val_accuracy: 0.8435\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6164 - accuracy: 0.8182 - val_loss: 0.5138 - val_accuracy: 0.8499\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6072 - accuracy: 0.8202 - val_loss: 0.4910 - val_accuracy: 0.8532\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6049 - accuracy: 0.8230 - val_loss: 0.4404 - val_accuracy: 0.8739\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5992 - accuracy: 0.8232 - val_loss: 0.5241 - val_accuracy: 0.8463\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6001 - accuracy: 0.8241 - val_loss: 0.5210 - val_accuracy: 0.8468\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5981 - accuracy: 0.8235 - val_loss: 0.5378 - val_accuracy: 0.8404\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5914 - accuracy: 0.8261 - val_loss: 0.4422 - val_accuracy: 0.8679\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5889 - accuracy: 0.8260 - val_loss: 0.4741 - val_accuracy: 0.8616\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5891 - accuracy: 0.8255 - val_loss: 0.4568 - val_accuracy: 0.8652\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5812 - accuracy: 0.8280 - val_loss: 0.6283 - val_accuracy: 0.8049\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5835 - accuracy: 0.8266 - val_loss: 0.5246 - val_accuracy: 0.8486\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5763 - accuracy: 0.8300 - val_loss: 0.4469 - val_accuracy: 0.8698\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5774 - accuracy: 0.8315 - val_loss: 0.4913 - val_accuracy: 0.8560\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5674 - accuracy: 0.8336 - val_loss: 0.5132 - val_accuracy: 0.8404\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5645 - accuracy: 0.8331 - val_loss: 0.4776 - val_accuracy: 0.8606\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5627 - accuracy: 0.8368 - val_loss: 0.4025 - val_accuracy: 0.8827\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5527 - accuracy: 0.8372 - val_loss: 0.4230 - val_accuracy: 0.8739\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5532 - accuracy: 0.8363 - val_loss: 0.4658 - val_accuracy: 0.8623\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5539 - accuracy: 0.8355 - val_loss: 0.5595 - val_accuracy: 0.8333\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5432 - accuracy: 0.8388 - val_loss: 0.5521 - val_accuracy: 0.8404\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5487 - accuracy: 0.8394 - val_loss: 0.4408 - val_accuracy: 0.8732\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5435 - accuracy: 0.8411 - val_loss: 0.5513 - val_accuracy: 0.8230\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5384 - accuracy: 0.8436 - val_loss: 0.4215 - val_accuracy: 0.8815\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5445 - accuracy: 0.8413 - val_loss: 0.3776 - val_accuracy: 0.8927\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5330 - accuracy: 0.8442 - val_loss: 0.4417 - val_accuracy: 0.8692\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5389 - accuracy: 0.8406 - val_loss: 0.4094 - val_accuracy: 0.8722\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5317 - accuracy: 0.8453 - val_loss: 0.5147 - val_accuracy: 0.8441\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5310 - accuracy: 0.8438 - val_loss: 0.4413 - val_accuracy: 0.8706\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5297 - accuracy: 0.8462 - val_loss: 0.4588 - val_accuracy: 0.8670\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5185 - accuracy: 0.8490 - val_loss: 0.9784 - val_accuracy: 0.7189\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5228 - accuracy: 0.8465 - val_loss: 0.4330 - val_accuracy: 0.8748\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5222 - accuracy: 0.8481 - val_loss: 0.5000 - val_accuracy: 0.8532\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5164 - accuracy: 0.8483 - val_loss: 0.3560 - val_accuracy: 0.8964\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5166 - accuracy: 0.8504 - val_loss: 0.4191 - val_accuracy: 0.8771\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5082 - accuracy: 0.8512 - val_loss: 0.4666 - val_accuracy: 0.8670\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5142 - accuracy: 0.8489 - val_loss: 0.4764 - val_accuracy: 0.8574\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5050 - accuracy: 0.8529 - val_loss: 0.3427 - val_accuracy: 0.9025\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5107 - accuracy: 0.8504 - val_loss: 0.5833 - val_accuracy: 0.8249\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5009 - accuracy: 0.8538 - val_loss: 0.3753 - val_accuracy: 0.8912\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5046 - accuracy: 0.8532 - val_loss: 0.3924 - val_accuracy: 0.8904\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5015 - accuracy: 0.8536 - val_loss: 0.4855 - val_accuracy: 0.8549\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4948 - accuracy: 0.8544 - val_loss: 0.4089 - val_accuracy: 0.8834\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4919 - accuracy: 0.8560 - val_loss: 0.4979 - val_accuracy: 0.8523\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4958 - accuracy: 0.8560 - val_loss: 0.3833 - val_accuracy: 0.8892\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4954 - accuracy: 0.8560 - val_loss: 0.3933 - val_accuracy: 0.8837\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4925 - accuracy: 0.8543 - val_loss: 0.3633 - val_accuracy: 0.8981\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4838 - accuracy: 0.8585 - val_loss: 0.4020 - val_accuracy: 0.8809\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4867 - accuracy: 0.8581 - val_loss: 0.4022 - val_accuracy: 0.8821\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8594 - val_loss: 0.5045 - val_accuracy: 0.8486\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4768 - accuracy: 0.8602 - val_loss: 0.4664 - val_accuracy: 0.8631\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4799 - accuracy: 0.8598 - val_loss: 0.6672 - val_accuracy: 0.8095\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4749 - accuracy: 0.8626 - val_loss: 0.3813 - val_accuracy: 0.8905\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4740 - accuracy: 0.8618 - val_loss: 0.4224 - val_accuracy: 0.8759\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4745 - accuracy: 0.8628 - val_loss: 0.5385 - val_accuracy: 0.8444\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4801 - accuracy: 0.8600 - val_loss: 0.3530 - val_accuracy: 0.9004\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.8623 - val_loss: 0.4252 - val_accuracy: 0.8790\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4655 - accuracy: 0.8652 - val_loss: 0.4481 - val_accuracy: 0.8659\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4679 - accuracy: 0.8634 - val_loss: 0.4219 - val_accuracy: 0.8766\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4601 - accuracy: 0.8664 - val_loss: 0.4198 - val_accuracy: 0.8779\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4628 - accuracy: 0.8654 - val_loss: 0.4297 - val_accuracy: 0.8701\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4629 - accuracy: 0.8660 - val_loss: 0.3933 - val_accuracy: 0.8861\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4529 - accuracy: 0.8678 - val_loss: 0.3878 - val_accuracy: 0.8879\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4582 - accuracy: 0.8659 - val_loss: 0.4420 - val_accuracy: 0.8710\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4556 - accuracy: 0.8673 - val_loss: 0.4682 - val_accuracy: 0.8627\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4530 - accuracy: 0.8686 - val_loss: 0.3802 - val_accuracy: 0.8888\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4527 - accuracy: 0.8676 - val_loss: 0.3918 - val_accuracy: 0.8835\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4504 - accuracy: 0.8707 - val_loss: 0.3821 - val_accuracy: 0.8904\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4455 - accuracy: 0.8699 - val_loss: 0.4094 - val_accuracy: 0.8836\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4454 - accuracy: 0.8710 - val_loss: 0.4441 - val_accuracy: 0.8676\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4453 - accuracy: 0.8690 - val_loss: 0.3524 - val_accuracy: 0.8966\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4381 - accuracy: 0.8718 - val_loss: 0.3468 - val_accuracy: 0.8999\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4401 - accuracy: 0.8728 - val_loss: 0.3491 - val_accuracy: 0.8992\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4423 - accuracy: 0.8714 - val_loss: 0.4350 - val_accuracy: 0.8731\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4390 - accuracy: 0.8721 - val_loss: 0.3829 - val_accuracy: 0.8909\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4322 - accuracy: 0.8743 - val_loss: 0.4266 - val_accuracy: 0.8704\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4358 - accuracy: 0.8724 - val_loss: 0.3100 - val_accuracy: 0.9119\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4342 - accuracy: 0.8738 - val_loss: 0.4359 - val_accuracy: 0.8731\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4321 - accuracy: 0.8745 - val_loss: 0.4297 - val_accuracy: 0.8709\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4337 - accuracy: 0.8743 - val_loss: 0.4070 - val_accuracy: 0.8787\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4248 - accuracy: 0.8781 - val_loss: 0.3536 - val_accuracy: 0.9021\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4313 - accuracy: 0.8760 - val_loss: 0.4589 - val_accuracy: 0.8635\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4243 - accuracy: 0.8762 - val_loss: 0.3800 - val_accuracy: 0.8904\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4224 - accuracy: 0.8773 - val_loss: 0.4323 - val_accuracy: 0.8757\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4224 - accuracy: 0.8780 - val_loss: 0.3722 - val_accuracy: 0.8943\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4211 - accuracy: 0.8774 - val_loss: 0.3989 - val_accuracy: 0.8832\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4245 - accuracy: 0.8772 - val_loss: 0.4174 - val_accuracy: 0.8810\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4195 - accuracy: 0.8785 - val_loss: 0.3382 - val_accuracy: 0.9056\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4164 - accuracy: 0.8780 - val_loss: 0.4258 - val_accuracy: 0.8783\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4157 - accuracy: 0.8795 - val_loss: 0.3827 - val_accuracy: 0.8870\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4139 - accuracy: 0.8789 - val_loss: 0.3489 - val_accuracy: 0.9010\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4164 - accuracy: 0.8787 - val_loss: 0.3539 - val_accuracy: 0.8984\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4091 - accuracy: 0.8810 - val_loss: 0.3958 - val_accuracy: 0.8835\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4151 - accuracy: 0.8798 - val_loss: 0.4066 - val_accuracy: 0.8794\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4120 - accuracy: 0.8809 - val_loss: 0.3026 - val_accuracy: 0.9152\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4067 - accuracy: 0.8828 - val_loss: 0.4207 - val_accuracy: 0.8795\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4079 - accuracy: 0.8816 - val_loss: 0.3936 - val_accuracy: 0.8857\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4010 - accuracy: 0.8838 - val_loss: 0.3587 - val_accuracy: 0.8945\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4060 - accuracy: 0.8835 - val_loss: 0.3483 - val_accuracy: 0.8986\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4025 - accuracy: 0.8836 - val_loss: 0.4078 - val_accuracy: 0.8794\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4021 - accuracy: 0.8856 - val_loss: 0.5294 - val_accuracy: 0.8429\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3993 - accuracy: 0.8841 - val_loss: 0.3898 - val_accuracy: 0.8882\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4001 - accuracy: 0.8843 - val_loss: 0.3275 - val_accuracy: 0.9073\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3963 - accuracy: 0.8864 - val_loss: 0.4034 - val_accuracy: 0.8861\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4038 - accuracy: 0.8850 - val_loss: 0.4119 - val_accuracy: 0.8824\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8835 - val_loss: 0.3486 - val_accuracy: 0.9014\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.8851 - val_loss: 0.4328 - val_accuracy: 0.8744\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4027 - accuracy: 0.8840 - val_loss: 0.3424 - val_accuracy: 0.9026\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3964 - accuracy: 0.8860 - val_loss: 0.3529 - val_accuracy: 0.8989\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3865 - accuracy: 0.8889 - val_loss: 0.3200 - val_accuracy: 0.9119\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3893 - accuracy: 0.8875 - val_loss: 0.4087 - val_accuracy: 0.8800\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8878 - val_loss: 0.3513 - val_accuracy: 0.9009\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3917 - accuracy: 0.8872 - val_loss: 0.3209 - val_accuracy: 0.9111\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3889 - accuracy: 0.8865 - val_loss: 0.4309 - val_accuracy: 0.8745\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8894 - val_loss: 0.3711 - val_accuracy: 0.8949\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3802 - accuracy: 0.8903 - val_loss: 0.5925 - val_accuracy: 0.8303\n",
      "Try 9/100: Best_val_acc: [0.5947983860969543, 0.8317777514457703], lr: 6.060807806310279e-05, Lambda: 5.7764904388276445e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-4.22, -4.21))\n",
    "    Lambda = math.pow(10, np.random.uniform(-4.24, -4.23))\n",
    "    train , score = basicDeepNN3(200, lr, Lambda,'relu', 'he_normal', 0.01, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, score, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEgBqBZYgpFf"
   },
   "source": [
    "## It looks like we have found our best parameters till now with the learning rate as 6.0379383617791225e-05 and lambda as 5.814141068427433e-05 with alpha as 0.01 in LeakyReLU activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 211275,
     "status": "ok",
     "timestamp": 1594549428235,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "09lW2cXZtiP_",
    "outputId": "55349e02-1500-4b07-c1e4-b1ea9dfa11b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_436 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.7689 - accuracy: 0.1032 - val_loss: 2.2923 - val_accuracy: 0.1005\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.6194 - accuracy: 0.1186 - val_loss: 2.2233 - val_accuracy: 0.1584\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4995 - accuracy: 0.1378 - val_loss: 2.1882 - val_accuracy: 0.1859\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3993 - accuracy: 0.1607 - val_loss: 2.1655 - val_accuracy: 0.1910\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2975 - accuracy: 0.1833 - val_loss: 2.0216 - val_accuracy: 0.3016\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2071 - accuracy: 0.2090 - val_loss: 1.9365 - val_accuracy: 0.3529\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1152 - accuracy: 0.2442 - val_loss: 1.8439 - val_accuracy: 0.4085\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0375 - accuracy: 0.2733 - val_loss: 1.7831 - val_accuracy: 0.4624\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9599 - accuracy: 0.3065 - val_loss: 1.7407 - val_accuracy: 0.4725\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.9004 - accuracy: 0.3333 - val_loss: 1.6582 - val_accuracy: 0.5130\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8303 - accuracy: 0.3632 - val_loss: 1.5539 - val_accuracy: 0.5922\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7688 - accuracy: 0.3918 - val_loss: 1.4509 - val_accuracy: 0.6301\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7154 - accuracy: 0.4177 - val_loss: 1.4138 - val_accuracy: 0.6304\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6675 - accuracy: 0.4353 - val_loss: 1.4210 - val_accuracy: 0.6238\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6202 - accuracy: 0.4585 - val_loss: 1.3712 - val_accuracy: 0.6356\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5724 - accuracy: 0.4782 - val_loss: 1.2575 - val_accuracy: 0.6798\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5315 - accuracy: 0.4993 - val_loss: 1.2260 - val_accuracy: 0.6833\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4997 - accuracy: 0.5085 - val_loss: 1.2709 - val_accuracy: 0.6691\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4572 - accuracy: 0.5260 - val_loss: 1.1680 - val_accuracy: 0.6950\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4220 - accuracy: 0.5405 - val_loss: 1.1386 - val_accuracy: 0.7128\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3899 - accuracy: 0.5535 - val_loss: 1.0995 - val_accuracy: 0.7171\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3602 - accuracy: 0.5659 - val_loss: 1.0975 - val_accuracy: 0.7092\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3348 - accuracy: 0.5775 - val_loss: 1.0910 - val_accuracy: 0.7141\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3018 - accuracy: 0.5896 - val_loss: 1.0068 - val_accuracy: 0.7411\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2756 - accuracy: 0.5963 - val_loss: 0.9904 - val_accuracy: 0.7330\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2455 - accuracy: 0.6113 - val_loss: 1.0792 - val_accuracy: 0.6969\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2255 - accuracy: 0.6160 - val_loss: 0.9238 - val_accuracy: 0.7555\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2016 - accuracy: 0.6293 - val_loss: 1.0248 - val_accuracy: 0.7112\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1740 - accuracy: 0.6374 - val_loss: 0.9314 - val_accuracy: 0.7444\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1542 - accuracy: 0.6450 - val_loss: 0.8689 - val_accuracy: 0.7674\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1337 - accuracy: 0.6541 - val_loss: 0.8364 - val_accuracy: 0.7758\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1090 - accuracy: 0.6620 - val_loss: 0.8994 - val_accuracy: 0.7417\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0876 - accuracy: 0.6674 - val_loss: 0.8485 - val_accuracy: 0.7596\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0751 - accuracy: 0.6740 - val_loss: 0.8551 - val_accuracy: 0.7595\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0513 - accuracy: 0.6850 - val_loss: 0.8496 - val_accuracy: 0.7630\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0277 - accuracy: 0.6903 - val_loss: 0.7909 - val_accuracy: 0.7811\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0146 - accuracy: 0.6948 - val_loss: 0.8469 - val_accuracy: 0.7504\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0018 - accuracy: 0.6985 - val_loss: 0.8391 - val_accuracy: 0.7604\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9823 - accuracy: 0.7070 - val_loss: 0.7609 - val_accuracy: 0.7841\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9687 - accuracy: 0.7095 - val_loss: 0.7433 - val_accuracy: 0.7889\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9538 - accuracy: 0.7138 - val_loss: 0.7200 - val_accuracy: 0.8006\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9377 - accuracy: 0.7221 - val_loss: 0.7392 - val_accuracy: 0.7860\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9255 - accuracy: 0.7233 - val_loss: 0.7615 - val_accuracy: 0.7825\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9101 - accuracy: 0.7298 - val_loss: 0.7461 - val_accuracy: 0.7842\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8968 - accuracy: 0.7314 - val_loss: 0.6947 - val_accuracy: 0.7991\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8877 - accuracy: 0.7354 - val_loss: 0.7217 - val_accuracy: 0.7912\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8759 - accuracy: 0.7359 - val_loss: 0.6991 - val_accuracy: 0.7963\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8572 - accuracy: 0.7427 - val_loss: 0.6456 - val_accuracy: 0.8131\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8428 - accuracy: 0.7492 - val_loss: 0.6402 - val_accuracy: 0.8184\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8387 - accuracy: 0.7509 - val_loss: 0.7127 - val_accuracy: 0.7879\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8264 - accuracy: 0.7548 - val_loss: 0.6363 - val_accuracy: 0.8211\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8228 - accuracy: 0.7544 - val_loss: 0.6570 - val_accuracy: 0.8055\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8129 - accuracy: 0.7597 - val_loss: 0.6531 - val_accuracy: 0.8069\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7997 - accuracy: 0.7647 - val_loss: 0.6405 - val_accuracy: 0.8150\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7910 - accuracy: 0.7645 - val_loss: 0.6537 - val_accuracy: 0.8077\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7823 - accuracy: 0.7671 - val_loss: 0.6988 - val_accuracy: 0.7916\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7711 - accuracy: 0.7722 - val_loss: 0.6318 - val_accuracy: 0.8094\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7652 - accuracy: 0.7730 - val_loss: 0.6384 - val_accuracy: 0.8166\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7625 - accuracy: 0.7744 - val_loss: 0.6051 - val_accuracy: 0.8224\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7486 - accuracy: 0.7783 - val_loss: 0.5534 - val_accuracy: 0.8401\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7421 - accuracy: 0.7802 - val_loss: 0.6517 - val_accuracy: 0.8107\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7346 - accuracy: 0.7820 - val_loss: 0.7121 - val_accuracy: 0.7869\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7326 - accuracy: 0.7835 - val_loss: 0.6091 - val_accuracy: 0.8125\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7228 - accuracy: 0.7858 - val_loss: 0.5514 - val_accuracy: 0.8356\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7168 - accuracy: 0.7895 - val_loss: 0.6033 - val_accuracy: 0.8201\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7083 - accuracy: 0.7894 - val_loss: 0.7066 - val_accuracy: 0.7888\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6998 - accuracy: 0.7931 - val_loss: 0.5817 - val_accuracy: 0.8215\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6977 - accuracy: 0.7950 - val_loss: 0.5399 - val_accuracy: 0.8394\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6917 - accuracy: 0.7958 - val_loss: 0.5968 - val_accuracy: 0.8261\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6925 - accuracy: 0.7948 - val_loss: 0.7029 - val_accuracy: 0.7963\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6788 - accuracy: 0.8005 - val_loss: 0.5625 - val_accuracy: 0.8335\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6733 - accuracy: 0.8005 - val_loss: 0.5425 - val_accuracy: 0.8392\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6707 - accuracy: 0.8003 - val_loss: 0.5911 - val_accuracy: 0.8256\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6638 - accuracy: 0.8045 - val_loss: 0.6046 - val_accuracy: 0.8153\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6637 - accuracy: 0.8019 - val_loss: 0.5243 - val_accuracy: 0.8445\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6569 - accuracy: 0.8052 - val_loss: 0.5208 - val_accuracy: 0.8487\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6467 - accuracy: 0.8097 - val_loss: 0.6094 - val_accuracy: 0.8164\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6479 - accuracy: 0.8096 - val_loss: 0.5831 - val_accuracy: 0.8294\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6410 - accuracy: 0.8117 - val_loss: 0.6077 - val_accuracy: 0.8127\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6346 - accuracy: 0.8130 - val_loss: 0.5147 - val_accuracy: 0.8504\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6278 - accuracy: 0.8154 - val_loss: 0.5448 - val_accuracy: 0.8389\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6232 - accuracy: 0.8165 - val_loss: 0.5387 - val_accuracy: 0.8384\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6167 - accuracy: 0.8183 - val_loss: 0.5461 - val_accuracy: 0.8387\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6252 - accuracy: 0.8153 - val_loss: 0.5088 - val_accuracy: 0.8496\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6217 - accuracy: 0.8149 - val_loss: 0.4905 - val_accuracy: 0.8571\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6150 - accuracy: 0.8178 - val_loss: 0.4733 - val_accuracy: 0.8657\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5998 - accuracy: 0.8248 - val_loss: 0.4677 - val_accuracy: 0.8625\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6064 - accuracy: 0.8205 - val_loss: 0.6470 - val_accuracy: 0.8035\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5952 - accuracy: 0.8242 - val_loss: 0.4870 - val_accuracy: 0.8516\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5962 - accuracy: 0.8248 - val_loss: 0.5244 - val_accuracy: 0.8436\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5918 - accuracy: 0.8240 - val_loss: 0.5109 - val_accuracy: 0.8477\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5884 - accuracy: 0.8272 - val_loss: 0.4676 - val_accuracy: 0.8622\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5867 - accuracy: 0.8291 - val_loss: 0.5674 - val_accuracy: 0.8311\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5838 - accuracy: 0.8294 - val_loss: 0.4663 - val_accuracy: 0.8648\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5778 - accuracy: 0.8296 - val_loss: 0.4514 - val_accuracy: 0.8651\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5744 - accuracy: 0.8305 - val_loss: 0.5698 - val_accuracy: 0.8315\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5706 - accuracy: 0.8338 - val_loss: 0.5414 - val_accuracy: 0.8366\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5713 - accuracy: 0.8320 - val_loss: 0.4641 - val_accuracy: 0.8646\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5683 - accuracy: 0.8322 - val_loss: 0.5435 - val_accuracy: 0.8342\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5531 - accuracy: 0.8376 - val_loss: 0.4153 - val_accuracy: 0.8804\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5628 - accuracy: 0.8345 - val_loss: 0.4881 - val_accuracy: 0.8544\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5584 - accuracy: 0.8372 - val_loss: 0.5792 - val_accuracy: 0.8234\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5560 - accuracy: 0.8372 - val_loss: 0.4534 - val_accuracy: 0.8696\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5495 - accuracy: 0.8401 - val_loss: 0.4470 - val_accuracy: 0.8692\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5486 - accuracy: 0.8397 - val_loss: 0.5641 - val_accuracy: 0.8306\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5454 - accuracy: 0.8390 - val_loss: 0.5041 - val_accuracy: 0.8464\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5431 - accuracy: 0.8416 - val_loss: 0.5744 - val_accuracy: 0.8323\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5344 - accuracy: 0.8422 - val_loss: 0.5051 - val_accuracy: 0.8531\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5351 - accuracy: 0.8438 - val_loss: 0.4487 - val_accuracy: 0.8704\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5404 - accuracy: 0.8411 - val_loss: 0.5945 - val_accuracy: 0.8205\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5309 - accuracy: 0.8453 - val_loss: 0.4054 - val_accuracy: 0.8841\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5271 - accuracy: 0.8470 - val_loss: 0.4846 - val_accuracy: 0.8556\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5155 - accuracy: 0.8493 - val_loss: 0.5849 - val_accuracy: 0.8210\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5262 - accuracy: 0.8453 - val_loss: 0.4912 - val_accuracy: 0.8504\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5213 - accuracy: 0.8469 - val_loss: 0.5948 - val_accuracy: 0.8238\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5129 - accuracy: 0.8502 - val_loss: 0.4203 - val_accuracy: 0.8787\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5156 - accuracy: 0.8499 - val_loss: 0.4982 - val_accuracy: 0.8499\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5143 - accuracy: 0.8506 - val_loss: 0.4825 - val_accuracy: 0.8642\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5142 - accuracy: 0.8505 - val_loss: 0.4423 - val_accuracy: 0.8706\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5152 - accuracy: 0.8490 - val_loss: 0.5045 - val_accuracy: 0.8516\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5108 - accuracy: 0.8502 - val_loss: 0.4316 - val_accuracy: 0.8746\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5062 - accuracy: 0.8522 - val_loss: 0.4169 - val_accuracy: 0.8747\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5051 - accuracy: 0.8523 - val_loss: 0.4652 - val_accuracy: 0.8550\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5004 - accuracy: 0.8562 - val_loss: 0.4471 - val_accuracy: 0.8690\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4975 - accuracy: 0.8550 - val_loss: 0.4036 - val_accuracy: 0.8850\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4946 - accuracy: 0.8554 - val_loss: 0.4621 - val_accuracy: 0.8601\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4957 - accuracy: 0.8568 - val_loss: 0.4467 - val_accuracy: 0.8698\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4881 - accuracy: 0.8581 - val_loss: 0.4678 - val_accuracy: 0.8666\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.8585 - val_loss: 0.4404 - val_accuracy: 0.8669\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4912 - accuracy: 0.8576 - val_loss: 0.4501 - val_accuracy: 0.8670\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4819 - accuracy: 0.8599 - val_loss: 0.3732 - val_accuracy: 0.8907\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4854 - accuracy: 0.8561 - val_loss: 0.5104 - val_accuracy: 0.8481\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4823 - accuracy: 0.8587 - val_loss: 0.5693 - val_accuracy: 0.8343\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4747 - accuracy: 0.8608 - val_loss: 0.5896 - val_accuracy: 0.8227\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4729 - accuracy: 0.8622 - val_loss: 0.3789 - val_accuracy: 0.8913\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4703 - accuracy: 0.8620 - val_loss: 0.5055 - val_accuracy: 0.8505\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4774 - accuracy: 0.8608 - val_loss: 0.4413 - val_accuracy: 0.8668\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4663 - accuracy: 0.8631 - val_loss: 0.4467 - val_accuracy: 0.8699\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4691 - accuracy: 0.8625 - val_loss: 0.4207 - val_accuracy: 0.8814\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4684 - accuracy: 0.8646 - val_loss: 0.4790 - val_accuracy: 0.8561\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4697 - accuracy: 0.8639 - val_loss: 0.4203 - val_accuracy: 0.8765\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4596 - accuracy: 0.8652 - val_loss: 0.5005 - val_accuracy: 0.8548\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4565 - accuracy: 0.8666 - val_loss: 0.4289 - val_accuracy: 0.8735\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4598 - accuracy: 0.8664 - val_loss: 0.4602 - val_accuracy: 0.8611\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4532 - accuracy: 0.8673 - val_loss: 0.5767 - val_accuracy: 0.8281\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4594 - accuracy: 0.8653 - val_loss: 0.4057 - val_accuracy: 0.8842\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4595 - accuracy: 0.8650 - val_loss: 0.4754 - val_accuracy: 0.8614\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4534 - accuracy: 0.8676 - val_loss: 0.4769 - val_accuracy: 0.8575\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4462 - accuracy: 0.8696 - val_loss: 0.3827 - val_accuracy: 0.8907\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4454 - accuracy: 0.8697 - val_loss: 0.3581 - val_accuracy: 0.9000\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4478 - accuracy: 0.8703 - val_loss: 0.4302 - val_accuracy: 0.8745\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4502 - accuracy: 0.8700 - val_loss: 0.4628 - val_accuracy: 0.8619\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4424 - accuracy: 0.8718 - val_loss: 0.3504 - val_accuracy: 0.8994\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4489 - accuracy: 0.8705 - val_loss: 0.4531 - val_accuracy: 0.8715\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4414 - accuracy: 0.8708 - val_loss: 0.4422 - val_accuracy: 0.8703\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4380 - accuracy: 0.8719 - val_loss: 0.3942 - val_accuracy: 0.8841\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8730 - val_loss: 0.3536 - val_accuracy: 0.8978\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4389 - accuracy: 0.8728 - val_loss: 0.3374 - val_accuracy: 0.9059\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4376 - accuracy: 0.8722 - val_loss: 0.4640 - val_accuracy: 0.8632\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4298 - accuracy: 0.8760 - val_loss: 0.5140 - val_accuracy: 0.8473\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4310 - accuracy: 0.8742 - val_loss: 0.4478 - val_accuracy: 0.8674\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4276 - accuracy: 0.8763 - val_loss: 0.3981 - val_accuracy: 0.8829\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4279 - accuracy: 0.8735 - val_loss: 0.3779 - val_accuracy: 0.8897\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4302 - accuracy: 0.8763 - val_loss: 0.4044 - val_accuracy: 0.8834\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4304 - accuracy: 0.8731 - val_loss: 0.4485 - val_accuracy: 0.8688\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4264 - accuracy: 0.8765 - val_loss: 0.5788 - val_accuracy: 0.8318\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4273 - accuracy: 0.8752 - val_loss: 0.3434 - val_accuracy: 0.9011\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4229 - accuracy: 0.8763 - val_loss: 0.3650 - val_accuracy: 0.8959\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4171 - accuracy: 0.8789 - val_loss: 0.3822 - val_accuracy: 0.8888\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4166 - accuracy: 0.8802 - val_loss: 0.3838 - val_accuracy: 0.8890\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4167 - accuracy: 0.8786 - val_loss: 0.3806 - val_accuracy: 0.8881\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4176 - accuracy: 0.8767 - val_loss: 0.4339 - val_accuracy: 0.8744\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8785 - val_loss: 0.4756 - val_accuracy: 0.8614\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4143 - accuracy: 0.8784 - val_loss: 0.4618 - val_accuracy: 0.8623\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4146 - accuracy: 0.8801 - val_loss: 0.3884 - val_accuracy: 0.8849\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4071 - accuracy: 0.8813 - val_loss: 0.3719 - val_accuracy: 0.8937\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4132 - accuracy: 0.8805 - val_loss: 0.4705 - val_accuracy: 0.8579\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4031 - accuracy: 0.8830 - val_loss: 0.3538 - val_accuracy: 0.8978\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8817 - val_loss: 0.3358 - val_accuracy: 0.9046\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4069 - accuracy: 0.8799 - val_loss: 0.3446 - val_accuracy: 0.8989\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3980 - accuracy: 0.8860 - val_loss: 0.4912 - val_accuracy: 0.8554\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4106 - accuracy: 0.8797 - val_loss: 0.4003 - val_accuracy: 0.8803\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3998 - accuracy: 0.8838 - val_loss: 0.3629 - val_accuracy: 0.8947\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3996 - accuracy: 0.8829 - val_loss: 0.4991 - val_accuracy: 0.8582\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4020 - accuracy: 0.8850 - val_loss: 0.4185 - val_accuracy: 0.8765\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8853 - val_loss: 0.3096 - val_accuracy: 0.9133\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3979 - accuracy: 0.8840 - val_loss: 0.3683 - val_accuracy: 0.8925\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3947 - accuracy: 0.8853 - val_loss: 0.4441 - val_accuracy: 0.8717\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3939 - accuracy: 0.8861 - val_loss: 0.4250 - val_accuracy: 0.8809\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3906 - accuracy: 0.8859 - val_loss: 0.5168 - val_accuracy: 0.8473\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3961 - accuracy: 0.8860 - val_loss: 0.3333 - val_accuracy: 0.9014\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3900 - accuracy: 0.8857 - val_loss: 0.3855 - val_accuracy: 0.8855\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3893 - accuracy: 0.8880 - val_loss: 0.3776 - val_accuracy: 0.8919\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3847 - accuracy: 0.8881 - val_loss: 0.4037 - val_accuracy: 0.8822\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3845 - accuracy: 0.8872 - val_loss: 0.3854 - val_accuracy: 0.8880\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3866 - accuracy: 0.8886 - val_loss: 0.3849 - val_accuracy: 0.8849\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3790 - accuracy: 0.8907 - val_loss: 0.3918 - val_accuracy: 0.8842\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3777 - accuracy: 0.8902 - val_loss: 0.3767 - val_accuracy: 0.8864\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8877 - val_loss: 0.3661 - val_accuracy: 0.8923\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3824 - accuracy: 0.8889 - val_loss: 0.3972 - val_accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "train, test = basicDeepNN3(200, 6.0379383617791225e-05, 5.814141068427433e-05,'relu', 'he_normal',0.01, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1594549444903,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "bXEKstxjLnh3",
    "outputId": "2b76c0bf-7573-483e-f1ec-afb6bb97feab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: [0.5196089744567871, 0.8460000157356262], lr: 6.0379383617791225e-05, Lambda: 5.814141068427433e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(1, 100, test, 6.0379383617791225e-05, 5.814141068427433e-05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210487,
     "status": "ok",
     "timestamp": 1594550474132,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "1opxo2yHj7ii",
    "outputId": "2a651fe1-97e6-4f24-9eaf-53dabac1b541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_439 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 2.6805 - accuracy: 0.1078 - val_loss: 2.5015 - val_accuracy: 0.0054\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5498 - accuracy: 0.1255 - val_loss: 2.4659 - val_accuracy: 0.0241\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4442 - accuracy: 0.1480 - val_loss: 2.3399 - val_accuracy: 0.1593\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3450 - accuracy: 0.1699 - val_loss: 2.2283 - val_accuracy: 0.1618\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2550 - accuracy: 0.2013 - val_loss: 2.1693 - val_accuracy: 0.2066\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1838 - accuracy: 0.2255 - val_loss: 1.9966 - val_accuracy: 0.3481\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1000 - accuracy: 0.2574 - val_loss: 1.9931 - val_accuracy: 0.3115\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0261 - accuracy: 0.2898 - val_loss: 1.8471 - val_accuracy: 0.4092\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9602 - accuracy: 0.3157 - val_loss: 1.8130 - val_accuracy: 0.4344\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8966 - accuracy: 0.3439 - val_loss: 1.6797 - val_accuracy: 0.4839\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8325 - accuracy: 0.3692 - val_loss: 1.5610 - val_accuracy: 0.5540\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7687 - accuracy: 0.4007 - val_loss: 1.5350 - val_accuracy: 0.5555\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7157 - accuracy: 0.4244 - val_loss: 1.4249 - val_accuracy: 0.6156\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6643 - accuracy: 0.4456 - val_loss: 1.3922 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.6171 - accuracy: 0.4658 - val_loss: 1.3009 - val_accuracy: 0.6602\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5693 - accuracy: 0.4848 - val_loss: 1.2987 - val_accuracy: 0.6610\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.5231 - accuracy: 0.5050 - val_loss: 1.2046 - val_accuracy: 0.6973\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4844 - accuracy: 0.5220 - val_loss: 1.1916 - val_accuracy: 0.6829\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4464 - accuracy: 0.5374 - val_loss: 1.1180 - val_accuracy: 0.7181\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.4111 - accuracy: 0.5471 - val_loss: 1.1225 - val_accuracy: 0.7041\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3750 - accuracy: 0.5613 - val_loss: 1.1170 - val_accuracy: 0.7030\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3370 - accuracy: 0.5784 - val_loss: 1.1075 - val_accuracy: 0.6956\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3174 - accuracy: 0.5826 - val_loss: 1.0325 - val_accuracy: 0.7299\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2841 - accuracy: 0.5965 - val_loss: 1.0304 - val_accuracy: 0.7169\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2539 - accuracy: 0.6089 - val_loss: 1.0002 - val_accuracy: 0.7269\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2209 - accuracy: 0.6218 - val_loss: 0.9899 - val_accuracy: 0.7389\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2017 - accuracy: 0.6265 - val_loss: 0.9473 - val_accuracy: 0.7325\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1798 - accuracy: 0.6364 - val_loss: 0.8855 - val_accuracy: 0.7624\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1504 - accuracy: 0.6434 - val_loss: 0.9086 - val_accuracy: 0.7556\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1374 - accuracy: 0.6482 - val_loss: 0.9742 - val_accuracy: 0.7264\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1092 - accuracy: 0.6590 - val_loss: 0.8809 - val_accuracy: 0.7505\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0877 - accuracy: 0.6665 - val_loss: 0.8178 - val_accuracy: 0.7719\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0725 - accuracy: 0.6718 - val_loss: 0.9279 - val_accuracy: 0.7302\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0456 - accuracy: 0.6785 - val_loss: 0.7933 - val_accuracy: 0.7774\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0408 - accuracy: 0.6826 - val_loss: 0.8129 - val_accuracy: 0.7784\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0206 - accuracy: 0.6877 - val_loss: 0.9226 - val_accuracy: 0.7286\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9999 - accuracy: 0.6941 - val_loss: 0.6788 - val_accuracy: 0.8103\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9905 - accuracy: 0.6978 - val_loss: 0.8055 - val_accuracy: 0.7649\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9734 - accuracy: 0.7030 - val_loss: 0.6832 - val_accuracy: 0.8082\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9615 - accuracy: 0.7083 - val_loss: 0.7216 - val_accuracy: 0.7965\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9401 - accuracy: 0.7136 - val_loss: 0.8050 - val_accuracy: 0.7604\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9329 - accuracy: 0.7174 - val_loss: 0.8661 - val_accuracy: 0.7379\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9176 - accuracy: 0.7219 - val_loss: 0.6958 - val_accuracy: 0.7907\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9045 - accuracy: 0.7255 - val_loss: 0.6987 - val_accuracy: 0.7928\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9008 - accuracy: 0.7271 - val_loss: 0.8091 - val_accuracy: 0.7546\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8844 - accuracy: 0.7325 - val_loss: 0.6667 - val_accuracy: 0.7962\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8706 - accuracy: 0.7375 - val_loss: 0.6141 - val_accuracy: 0.8246\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8538 - accuracy: 0.7435 - val_loss: 0.7418 - val_accuracy: 0.7831\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8507 - accuracy: 0.7448 - val_loss: 0.6282 - val_accuracy: 0.8199\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8345 - accuracy: 0.7492 - val_loss: 0.6443 - val_accuracy: 0.8158\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8286 - accuracy: 0.7484 - val_loss: 0.6769 - val_accuracy: 0.8001\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8185 - accuracy: 0.7539 - val_loss: 0.7522 - val_accuracy: 0.7797\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8116 - accuracy: 0.7535 - val_loss: 0.6267 - val_accuracy: 0.8081\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8040 - accuracy: 0.7586 - val_loss: 0.6756 - val_accuracy: 0.7952\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7921 - accuracy: 0.7636 - val_loss: 0.6466 - val_accuracy: 0.8081\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7896 - accuracy: 0.7636 - val_loss: 0.7321 - val_accuracy: 0.7809\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7748 - accuracy: 0.7672 - val_loss: 0.5679 - val_accuracy: 0.8338\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7672 - accuracy: 0.7713 - val_loss: 0.5672 - val_accuracy: 0.8389\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7625 - accuracy: 0.7720 - val_loss: 0.5691 - val_accuracy: 0.8331\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7527 - accuracy: 0.7730 - val_loss: 0.5743 - val_accuracy: 0.8319\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7455 - accuracy: 0.7762 - val_loss: 0.5832 - val_accuracy: 0.8275\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7435 - accuracy: 0.7779 - val_loss: 0.7908 - val_accuracy: 0.7602\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7308 - accuracy: 0.7821 - val_loss: 0.6935 - val_accuracy: 0.7747\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7273 - accuracy: 0.7815 - val_loss: 0.5503 - val_accuracy: 0.8394\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7206 - accuracy: 0.7842 - val_loss: 0.5828 - val_accuracy: 0.8229\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7100 - accuracy: 0.7885 - val_loss: 0.6481 - val_accuracy: 0.8009\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7098 - accuracy: 0.7883 - val_loss: 0.5441 - val_accuracy: 0.8426\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6968 - accuracy: 0.7942 - val_loss: 0.5941 - val_accuracy: 0.8171\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7004 - accuracy: 0.7932 - val_loss: 0.5260 - val_accuracy: 0.8450\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6843 - accuracy: 0.7963 - val_loss: 0.7265 - val_accuracy: 0.7869\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6860 - accuracy: 0.7953 - val_loss: 0.5593 - val_accuracy: 0.8383\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6840 - accuracy: 0.7971 - val_loss: 0.5157 - val_accuracy: 0.8496\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6800 - accuracy: 0.7978 - val_loss: 0.6263 - val_accuracy: 0.8116\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6674 - accuracy: 0.8017 - val_loss: 0.5470 - val_accuracy: 0.8364\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6671 - accuracy: 0.8008 - val_loss: 0.4478 - val_accuracy: 0.8689\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6573 - accuracy: 0.8040 - val_loss: 0.5432 - val_accuracy: 0.8403\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6546 - accuracy: 0.8046 - val_loss: 0.4905 - val_accuracy: 0.8559\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6567 - accuracy: 0.8053 - val_loss: 0.5085 - val_accuracy: 0.8494\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6430 - accuracy: 0.8089 - val_loss: 0.5122 - val_accuracy: 0.8458\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6373 - accuracy: 0.8112 - val_loss: 0.5191 - val_accuracy: 0.8451\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6315 - accuracy: 0.8128 - val_loss: 0.5262 - val_accuracy: 0.8418\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6271 - accuracy: 0.8139 - val_loss: 0.6600 - val_accuracy: 0.8055\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6355 - accuracy: 0.8131 - val_loss: 0.6034 - val_accuracy: 0.8187\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6242 - accuracy: 0.8161 - val_loss: 0.4831 - val_accuracy: 0.8562\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6157 - accuracy: 0.8174 - val_loss: 0.5357 - val_accuracy: 0.8361\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6099 - accuracy: 0.8204 - val_loss: 0.4424 - val_accuracy: 0.8714\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6128 - accuracy: 0.8178 - val_loss: 0.5024 - val_accuracy: 0.8529\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5995 - accuracy: 0.8219 - val_loss: 0.4789 - val_accuracy: 0.8611\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6044 - accuracy: 0.8212 - val_loss: 0.4502 - val_accuracy: 0.8656\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6020 - accuracy: 0.8220 - val_loss: 0.4926 - val_accuracy: 0.8521\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5916 - accuracy: 0.8245 - val_loss: 0.4617 - val_accuracy: 0.8652\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5927 - accuracy: 0.8245 - val_loss: 0.4847 - val_accuracy: 0.8572\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5881 - accuracy: 0.8265 - val_loss: 0.4706 - val_accuracy: 0.8544\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5813 - accuracy: 0.8301 - val_loss: 0.4614 - val_accuracy: 0.8657\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5839 - accuracy: 0.8276 - val_loss: 0.4435 - val_accuracy: 0.8706\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5795 - accuracy: 0.8298 - val_loss: 0.4902 - val_accuracy: 0.8552\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5752 - accuracy: 0.8310 - val_loss: 0.5405 - val_accuracy: 0.8396\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5733 - accuracy: 0.8322 - val_loss: 0.5316 - val_accuracy: 0.8410\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5685 - accuracy: 0.8312 - val_loss: 0.6474 - val_accuracy: 0.8117\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5652 - accuracy: 0.8331 - val_loss: 0.5323 - val_accuracy: 0.8405\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5624 - accuracy: 0.8341 - val_loss: 0.4413 - val_accuracy: 0.8676\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5616 - accuracy: 0.8345 - val_loss: 0.4590 - val_accuracy: 0.8623\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5597 - accuracy: 0.8367 - val_loss: 0.4501 - val_accuracy: 0.8701\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5537 - accuracy: 0.8359 - val_loss: 0.4408 - val_accuracy: 0.8689\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5480 - accuracy: 0.8401 - val_loss: 0.4993 - val_accuracy: 0.8469\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5405 - accuracy: 0.8410 - val_loss: 0.4586 - val_accuracy: 0.8674\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5418 - accuracy: 0.8411 - val_loss: 0.4064 - val_accuracy: 0.8839\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5418 - accuracy: 0.8396 - val_loss: 0.4140 - val_accuracy: 0.8796\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5384 - accuracy: 0.8417 - val_loss: 0.4032 - val_accuracy: 0.8830\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5392 - accuracy: 0.8422 - val_loss: 0.4791 - val_accuracy: 0.8561\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5366 - accuracy: 0.8424 - val_loss: 0.4390 - val_accuracy: 0.8693\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5307 - accuracy: 0.8440 - val_loss: 0.4538 - val_accuracy: 0.8704\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5219 - accuracy: 0.8471 - val_loss: 0.4271 - val_accuracy: 0.8752\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5201 - accuracy: 0.8476 - val_loss: 0.3806 - val_accuracy: 0.8928\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5190 - accuracy: 0.8479 - val_loss: 0.4402 - val_accuracy: 0.8731\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5230 - accuracy: 0.8459 - val_loss: 0.5727 - val_accuracy: 0.8321\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5260 - accuracy: 0.8455 - val_loss: 0.3907 - val_accuracy: 0.8897\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5085 - accuracy: 0.8518 - val_loss: 0.5267 - val_accuracy: 0.8426\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5107 - accuracy: 0.8514 - val_loss: 0.4139 - val_accuracy: 0.8824\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5117 - accuracy: 0.8500 - val_loss: 0.4591 - val_accuracy: 0.8632\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5064 - accuracy: 0.8504 - val_loss: 0.4880 - val_accuracy: 0.8555\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5059 - accuracy: 0.8530 - val_loss: 0.4438 - val_accuracy: 0.8745\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5069 - accuracy: 0.8529 - val_loss: 0.4403 - val_accuracy: 0.8736\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5016 - accuracy: 0.8523 - val_loss: 0.5056 - val_accuracy: 0.8474\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4993 - accuracy: 0.8557 - val_loss: 0.5346 - val_accuracy: 0.8441\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4995 - accuracy: 0.8529 - val_loss: 0.4606 - val_accuracy: 0.8658\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4914 - accuracy: 0.8567 - val_loss: 0.5227 - val_accuracy: 0.8391\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4949 - accuracy: 0.8529 - val_loss: 0.5127 - val_accuracy: 0.8365\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4931 - accuracy: 0.8548 - val_loss: 0.4407 - val_accuracy: 0.8689\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4815 - accuracy: 0.8580 - val_loss: 0.3623 - val_accuracy: 0.8970\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4843 - accuracy: 0.8582 - val_loss: 0.6476 - val_accuracy: 0.8014\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8596 - val_loss: 0.4052 - val_accuracy: 0.8775\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4823 - accuracy: 0.8593 - val_loss: 0.4772 - val_accuracy: 0.8588\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4864 - accuracy: 0.8569 - val_loss: 0.4713 - val_accuracy: 0.8611\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4825 - accuracy: 0.8596 - val_loss: 0.4386 - val_accuracy: 0.8740\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4784 - accuracy: 0.8587 - val_loss: 0.4700 - val_accuracy: 0.8535\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4765 - accuracy: 0.8619 - val_loss: 0.3975 - val_accuracy: 0.8851\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4750 - accuracy: 0.8618 - val_loss: 0.3639 - val_accuracy: 0.8961\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4710 - accuracy: 0.8620 - val_loss: 0.5508 - val_accuracy: 0.8313\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4692 - accuracy: 0.8627 - val_loss: 0.4132 - val_accuracy: 0.8816\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4678 - accuracy: 0.8613 - val_loss: 0.4437 - val_accuracy: 0.8714\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4766 - accuracy: 0.8628 - val_loss: 0.3460 - val_accuracy: 0.9014\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4690 - accuracy: 0.8622 - val_loss: 0.4452 - val_accuracy: 0.8705\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4625 - accuracy: 0.8645 - val_loss: 0.3517 - val_accuracy: 0.8994\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4605 - accuracy: 0.8650 - val_loss: 0.3188 - val_accuracy: 0.9101\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4632 - accuracy: 0.8640 - val_loss: 0.4812 - val_accuracy: 0.8521\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4576 - accuracy: 0.8661 - val_loss: 0.5686 - val_accuracy: 0.8294\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4570 - accuracy: 0.8658 - val_loss: 0.4043 - val_accuracy: 0.8808\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4509 - accuracy: 0.8683 - val_loss: 0.3592 - val_accuracy: 0.8978\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4543 - accuracy: 0.8660 - val_loss: 0.3311 - val_accuracy: 0.9067\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4519 - accuracy: 0.8668 - val_loss: 0.4224 - val_accuracy: 0.8756\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4461 - accuracy: 0.8707 - val_loss: 0.3980 - val_accuracy: 0.8836\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4473 - accuracy: 0.8680 - val_loss: 0.5046 - val_accuracy: 0.8502\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4492 - accuracy: 0.8685 - val_loss: 0.4695 - val_accuracy: 0.8563\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4429 - accuracy: 0.8711 - val_loss: 0.3471 - val_accuracy: 0.9031\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4438 - accuracy: 0.8713 - val_loss: 0.3916 - val_accuracy: 0.8863\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4397 - accuracy: 0.8712 - val_loss: 0.4601 - val_accuracy: 0.8614\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4410 - accuracy: 0.8716 - val_loss: 0.5011 - val_accuracy: 0.8601\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4365 - accuracy: 0.8715 - val_loss: 0.4403 - val_accuracy: 0.8739\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4361 - accuracy: 0.8727 - val_loss: 0.3121 - val_accuracy: 0.9119\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4393 - accuracy: 0.8719 - val_loss: 0.3887 - val_accuracy: 0.8914\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4385 - accuracy: 0.8700 - val_loss: 0.4760 - val_accuracy: 0.8620\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4359 - accuracy: 0.8737 - val_loss: 0.4762 - val_accuracy: 0.8641\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4315 - accuracy: 0.8742 - val_loss: 0.3798 - val_accuracy: 0.8862\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4260 - accuracy: 0.8763 - val_loss: 0.5423 - val_accuracy: 0.8408\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4328 - accuracy: 0.8744 - val_loss: 0.3336 - val_accuracy: 0.9066\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4232 - accuracy: 0.8764 - val_loss: 0.4037 - val_accuracy: 0.8827\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4236 - accuracy: 0.8770 - val_loss: 0.3713 - val_accuracy: 0.8914\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.4214 - accuracy: 0.8772 - val_loss: 0.3348 - val_accuracy: 0.9076\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4214 - accuracy: 0.8786 - val_loss: 0.4449 - val_accuracy: 0.8699\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4257 - accuracy: 0.8757 - val_loss: 0.4189 - val_accuracy: 0.8764\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4197 - accuracy: 0.8788 - val_loss: 0.5079 - val_accuracy: 0.8398\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4208 - accuracy: 0.8778 - val_loss: 0.3645 - val_accuracy: 0.8956\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4145 - accuracy: 0.8789 - val_loss: 0.3882 - val_accuracy: 0.8900\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4140 - accuracy: 0.8800 - val_loss: 0.3732 - val_accuracy: 0.8894\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4181 - accuracy: 0.8778 - val_loss: 0.4747 - val_accuracy: 0.8611\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4116 - accuracy: 0.8800 - val_loss: 0.3996 - val_accuracy: 0.8807\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4067 - accuracy: 0.8814 - val_loss: 0.5030 - val_accuracy: 0.8588\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4081 - accuracy: 0.8802 - val_loss: 0.4446 - val_accuracy: 0.8695\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4024 - accuracy: 0.8827 - val_loss: 0.3795 - val_accuracy: 0.8909\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4100 - accuracy: 0.8806 - val_loss: 0.4233 - val_accuracy: 0.8741\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4069 - accuracy: 0.8827 - val_loss: 0.2997 - val_accuracy: 0.9169\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4015 - accuracy: 0.8860 - val_loss: 0.3793 - val_accuracy: 0.8935\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4038 - accuracy: 0.8820 - val_loss: 0.4307 - val_accuracy: 0.8716\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4022 - accuracy: 0.8826 - val_loss: 0.3706 - val_accuracy: 0.8925\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3953 - accuracy: 0.8865 - val_loss: 0.3738 - val_accuracy: 0.8934\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4053 - accuracy: 0.8837 - val_loss: 0.3710 - val_accuracy: 0.8973\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4038 - accuracy: 0.8824 - val_loss: 0.3262 - val_accuracy: 0.9067\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3936 - accuracy: 0.8858 - val_loss: 0.3214 - val_accuracy: 0.9090\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4004 - accuracy: 0.8839 - val_loss: 0.3529 - val_accuracy: 0.8927\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3940 - accuracy: 0.8848 - val_loss: 0.4132 - val_accuracy: 0.8813\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3899 - accuracy: 0.8865 - val_loss: 0.4187 - val_accuracy: 0.8766\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3892 - accuracy: 0.8881 - val_loss: 0.4174 - val_accuracy: 0.8793\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3957 - accuracy: 0.8852 - val_loss: 0.3381 - val_accuracy: 0.9037\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3904 - accuracy: 0.8880 - val_loss: 0.3717 - val_accuracy: 0.8919\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3912 - accuracy: 0.8863 - val_loss: 0.3402 - val_accuracy: 0.9020\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3905 - accuracy: 0.8869 - val_loss: 0.3173 - val_accuracy: 0.9084\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3862 - accuracy: 0.8886 - val_loss: 0.4446 - val_accuracy: 0.8753\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3819 - accuracy: 0.8896 - val_loss: 0.3747 - val_accuracy: 0.8924\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3841 - accuracy: 0.8873 - val_loss: 0.2781 - val_accuracy: 0.9229\n"
     ]
    }
   ],
   "source": [
    "train, score = basicDeepNN3(200, 6.027611403498033e-05, 5.842026004053601e-05,'relu', 'he_normal',0.01, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1594550556461,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "fxtWSwHEj7n_",
    "outputId": "200ea6ee-d85c-4c5d-c820-5b076514cae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_test_acc: [0.4821937382221222, 0.8612222075462341], lr: 6.027611403498033e-05, Lambda: 5.842026004053601e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best_test_acc: {0}, lr: {1}, Lambda: {2}\\n\".format(score, 6.027611403498033e-05, 5.842026004053601e-05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210951,
     "status": "ok",
     "timestamp": 1594550245837,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "I03ijj2-j7vx",
    "outputId": "e691075e-0ef5-4664-e5ee-0ff21de67a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_438 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 2.6378 - accuracy: 0.1052 - val_loss: 2.3020 - val_accuracy: 0.0882\n",
      "Epoch 2/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.5241 - accuracy: 0.1229 - val_loss: 2.2234 - val_accuracy: 0.1513\n",
      "Epoch 3/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.4294 - accuracy: 0.1414 - val_loss: 2.2038 - val_accuracy: 0.1464\n",
      "Epoch 4/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.3445 - accuracy: 0.1616 - val_loss: 2.1665 - val_accuracy: 0.1812\n",
      "Epoch 5/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.2581 - accuracy: 0.1919 - val_loss: 2.1374 - val_accuracy: 0.1979\n",
      "Epoch 6/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1867 - accuracy: 0.2196 - val_loss: 2.0602 - val_accuracy: 0.2554\n",
      "Epoch 7/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.1073 - accuracy: 0.2497 - val_loss: 1.9021 - val_accuracy: 0.3574\n",
      "Epoch 8/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 2.0352 - accuracy: 0.2765 - val_loss: 1.7931 - val_accuracy: 0.4012\n",
      "Epoch 9/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.9601 - accuracy: 0.3098 - val_loss: 1.6402 - val_accuracy: 0.5111\n",
      "Epoch 10/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8880 - accuracy: 0.3426 - val_loss: 1.6825 - val_accuracy: 0.4377\n",
      "Epoch 11/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.8285 - accuracy: 0.3659 - val_loss: 1.6000 - val_accuracy: 0.4622\n",
      "Epoch 12/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.7681 - accuracy: 0.3929 - val_loss: 1.4787 - val_accuracy: 0.5485\n",
      "Epoch 13/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.7132 - accuracy: 0.4140 - val_loss: 1.4547 - val_accuracy: 0.5369\n",
      "Epoch 14/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.6607 - accuracy: 0.4390 - val_loss: 1.4469 - val_accuracy: 0.5430\n",
      "Epoch 15/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.6152 - accuracy: 0.4580 - val_loss: 1.3540 - val_accuracy: 0.5749\n",
      "Epoch 16/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.5645 - accuracy: 0.4777 - val_loss: 1.2519 - val_accuracy: 0.6402\n",
      "Epoch 17/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.5188 - accuracy: 0.4972 - val_loss: 1.3202 - val_accuracy: 0.5939\n",
      "Epoch 18/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.4805 - accuracy: 0.5116 - val_loss: 1.1757 - val_accuracy: 0.6744\n",
      "Epoch 19/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.4447 - accuracy: 0.5306 - val_loss: 1.1705 - val_accuracy: 0.6594\n",
      "Epoch 20/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 1.4054 - accuracy: 0.5464 - val_loss: 1.1509 - val_accuracy: 0.6683\n",
      "Epoch 21/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3681 - accuracy: 0.5589 - val_loss: 1.1565 - val_accuracy: 0.6628\n",
      "Epoch 22/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3295 - accuracy: 0.5741 - val_loss: 1.1197 - val_accuracy: 0.6744\n",
      "Epoch 23/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.3070 - accuracy: 0.5827 - val_loss: 1.0319 - val_accuracy: 0.7166\n",
      "Epoch 24/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2681 - accuracy: 0.5957 - val_loss: 0.9768 - val_accuracy: 0.7229\n",
      "Epoch 25/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2431 - accuracy: 0.6075 - val_loss: 0.9793 - val_accuracy: 0.7198\n",
      "Epoch 26/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.2075 - accuracy: 0.6210 - val_loss: 0.9545 - val_accuracy: 0.7299\n",
      "Epoch 27/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1836 - accuracy: 0.6308 - val_loss: 1.0406 - val_accuracy: 0.6864\n",
      "Epoch 28/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1596 - accuracy: 0.6389 - val_loss: 0.9200 - val_accuracy: 0.7395\n",
      "Epoch 29/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1407 - accuracy: 0.6442 - val_loss: 0.9059 - val_accuracy: 0.7399\n",
      "Epoch 30/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.1170 - accuracy: 0.6508 - val_loss: 0.8661 - val_accuracy: 0.7445\n",
      "Epoch 31/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0897 - accuracy: 0.6618 - val_loss: 0.9506 - val_accuracy: 0.7039\n",
      "Epoch 32/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0733 - accuracy: 0.6679 - val_loss: 0.9982 - val_accuracy: 0.7006\n",
      "Epoch 33/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0513 - accuracy: 0.6774 - val_loss: 0.8944 - val_accuracy: 0.7450\n",
      "Epoch 34/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0382 - accuracy: 0.6814 - val_loss: 0.7956 - val_accuracy: 0.7723\n",
      "Epoch 35/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 1.0186 - accuracy: 0.6869 - val_loss: 0.8250 - val_accuracy: 0.7574\n",
      "Epoch 36/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9991 - accuracy: 0.6920 - val_loss: 0.7464 - val_accuracy: 0.7848\n",
      "Epoch 37/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9775 - accuracy: 0.6994 - val_loss: 0.8048 - val_accuracy: 0.7667\n",
      "Epoch 38/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9706 - accuracy: 0.7022 - val_loss: 0.7416 - val_accuracy: 0.7834\n",
      "Epoch 39/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9514 - accuracy: 0.7076 - val_loss: 0.7258 - val_accuracy: 0.7904\n",
      "Epoch 40/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9366 - accuracy: 0.7132 - val_loss: 0.8581 - val_accuracy: 0.7428\n",
      "Epoch 41/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9287 - accuracy: 0.7170 - val_loss: 0.6867 - val_accuracy: 0.7880\n",
      "Epoch 42/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9191 - accuracy: 0.7200 - val_loss: 0.6224 - val_accuracy: 0.8212\n",
      "Epoch 43/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.9024 - accuracy: 0.7253 - val_loss: 0.7878 - val_accuracy: 0.7617\n",
      "Epoch 44/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8868 - accuracy: 0.7314 - val_loss: 0.7315 - val_accuracy: 0.7834\n",
      "Epoch 45/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8710 - accuracy: 0.7360 - val_loss: 0.6684 - val_accuracy: 0.8039\n",
      "Epoch 46/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8655 - accuracy: 0.7367 - val_loss: 0.6881 - val_accuracy: 0.7929\n",
      "Epoch 47/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8538 - accuracy: 0.7412 - val_loss: 0.7838 - val_accuracy: 0.7579\n",
      "Epoch 48/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8403 - accuracy: 0.7440 - val_loss: 0.6668 - val_accuracy: 0.7986\n",
      "Epoch 49/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8361 - accuracy: 0.7481 - val_loss: 0.6775 - val_accuracy: 0.7968\n",
      "Epoch 50/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8249 - accuracy: 0.7527 - val_loss: 0.6612 - val_accuracy: 0.8076\n",
      "Epoch 51/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8104 - accuracy: 0.7560 - val_loss: 0.6687 - val_accuracy: 0.7996\n",
      "Epoch 52/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8045 - accuracy: 0.7574 - val_loss: 0.6581 - val_accuracy: 0.8073\n",
      "Epoch 53/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7944 - accuracy: 0.7607 - val_loss: 0.6530 - val_accuracy: 0.7979\n",
      "Epoch 54/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7873 - accuracy: 0.7636 - val_loss: 0.6882 - val_accuracy: 0.7779\n",
      "Epoch 55/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7806 - accuracy: 0.7659 - val_loss: 0.6386 - val_accuracy: 0.8057\n",
      "Epoch 56/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7737 - accuracy: 0.7676 - val_loss: 0.5617 - val_accuracy: 0.8284\n",
      "Epoch 57/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7631 - accuracy: 0.7710 - val_loss: 0.6080 - val_accuracy: 0.8225\n",
      "Epoch 58/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7589 - accuracy: 0.7711 - val_loss: 0.6059 - val_accuracy: 0.8203\n",
      "Epoch 59/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7538 - accuracy: 0.7726 - val_loss: 0.5906 - val_accuracy: 0.8245\n",
      "Epoch 60/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7394 - accuracy: 0.7787 - val_loss: 0.5654 - val_accuracy: 0.8323\n",
      "Epoch 61/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7336 - accuracy: 0.7794 - val_loss: 0.7115 - val_accuracy: 0.7916\n",
      "Epoch 62/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7384 - accuracy: 0.7801 - val_loss: 0.6208 - val_accuracy: 0.7976\n",
      "Epoch 63/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7167 - accuracy: 0.7855 - val_loss: 0.4603 - val_accuracy: 0.8646\n",
      "Epoch 64/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7143 - accuracy: 0.7873 - val_loss: 0.5543 - val_accuracy: 0.8365\n",
      "Epoch 65/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7097 - accuracy: 0.7891 - val_loss: 0.5906 - val_accuracy: 0.8241\n",
      "Epoch 66/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7013 - accuracy: 0.7926 - val_loss: 0.6099 - val_accuracy: 0.8134\n",
      "Epoch 67/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6975 - accuracy: 0.7920 - val_loss: 0.5197 - val_accuracy: 0.8441\n",
      "Epoch 68/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6904 - accuracy: 0.7939 - val_loss: 0.5471 - val_accuracy: 0.8444\n",
      "Epoch 69/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6840 - accuracy: 0.7974 - val_loss: 0.4916 - val_accuracy: 0.8576\n",
      "Epoch 70/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6742 - accuracy: 0.8004 - val_loss: 0.5494 - val_accuracy: 0.8330\n",
      "Epoch 71/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6747 - accuracy: 0.8000 - val_loss: 0.5587 - val_accuracy: 0.8352\n",
      "Epoch 72/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6684 - accuracy: 0.7994 - val_loss: 0.5790 - val_accuracy: 0.8251\n",
      "Epoch 73/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6659 - accuracy: 0.8015 - val_loss: 0.5489 - val_accuracy: 0.8387\n",
      "Epoch 74/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6576 - accuracy: 0.8057 - val_loss: 0.5819 - val_accuracy: 0.8224\n",
      "Epoch 75/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6513 - accuracy: 0.8070 - val_loss: 0.5583 - val_accuracy: 0.8336\n",
      "Epoch 76/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6397 - accuracy: 0.8088 - val_loss: 0.5280 - val_accuracy: 0.8444\n",
      "Epoch 77/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6464 - accuracy: 0.8091 - val_loss: 0.5738 - val_accuracy: 0.8316\n",
      "Epoch 78/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6328 - accuracy: 0.8124 - val_loss: 0.5879 - val_accuracy: 0.8260\n",
      "Epoch 79/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6327 - accuracy: 0.8125 - val_loss: 0.5751 - val_accuracy: 0.8274\n",
      "Epoch 80/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6311 - accuracy: 0.8142 - val_loss: 0.5474 - val_accuracy: 0.8380\n",
      "Epoch 81/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6307 - accuracy: 0.8116 - val_loss: 0.4960 - val_accuracy: 0.8571\n",
      "Epoch 82/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6230 - accuracy: 0.8148 - val_loss: 0.5234 - val_accuracy: 0.8444\n",
      "Epoch 83/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6215 - accuracy: 0.8164 - val_loss: 0.7748 - val_accuracy: 0.7744\n",
      "Epoch 84/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6111 - accuracy: 0.8195 - val_loss: 0.4265 - val_accuracy: 0.8761\n",
      "Epoch 85/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6097 - accuracy: 0.8193 - val_loss: 0.5317 - val_accuracy: 0.8439\n",
      "Epoch 86/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6096 - accuracy: 0.8206 - val_loss: 0.5942 - val_accuracy: 0.8162\n",
      "Epoch 87/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6053 - accuracy: 0.8199 - val_loss: 0.4639 - val_accuracy: 0.8616\n",
      "Epoch 88/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6002 - accuracy: 0.8221 - val_loss: 0.4176 - val_accuracy: 0.8790\n",
      "Epoch 89/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5971 - accuracy: 0.8237 - val_loss: 0.5699 - val_accuracy: 0.8347\n",
      "Epoch 90/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5949 - accuracy: 0.8244 - val_loss: 0.4539 - val_accuracy: 0.8686\n",
      "Epoch 91/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5819 - accuracy: 0.8273 - val_loss: 0.4816 - val_accuracy: 0.8556\n",
      "Epoch 92/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5850 - accuracy: 0.8289 - val_loss: 0.6348 - val_accuracy: 0.8059\n",
      "Epoch 93/200\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5795 - accuracy: 0.8273 - val_loss: 0.4816 - val_accuracy: 0.8609\n",
      "Epoch 94/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5834 - accuracy: 0.8278 - val_loss: 0.3951 - val_accuracy: 0.8891\n",
      "Epoch 95/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5753 - accuracy: 0.8314 - val_loss: 0.4373 - val_accuracy: 0.8691\n",
      "Epoch 96/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5760 - accuracy: 0.8304 - val_loss: 0.4730 - val_accuracy: 0.8604\n",
      "Epoch 97/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5711 - accuracy: 0.8328 - val_loss: 0.4266 - val_accuracy: 0.8774\n",
      "Epoch 98/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5667 - accuracy: 0.8321 - val_loss: 0.6443 - val_accuracy: 0.8079\n",
      "Epoch 99/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5621 - accuracy: 0.8361 - val_loss: 0.4650 - val_accuracy: 0.8632\n",
      "Epoch 100/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5600 - accuracy: 0.8332 - val_loss: 0.5272 - val_accuracy: 0.8481\n",
      "Epoch 101/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5563 - accuracy: 0.8365 - val_loss: 0.4471 - val_accuracy: 0.8724\n",
      "Epoch 102/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5542 - accuracy: 0.8369 - val_loss: 0.5026 - val_accuracy: 0.8494\n",
      "Epoch 103/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5474 - accuracy: 0.8403 - val_loss: 0.4558 - val_accuracy: 0.8650\n",
      "Epoch 104/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5466 - accuracy: 0.8398 - val_loss: 0.5539 - val_accuracy: 0.8289\n",
      "Epoch 105/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5411 - accuracy: 0.8409 - val_loss: 0.5440 - val_accuracy: 0.8382\n",
      "Epoch 106/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5372 - accuracy: 0.8421 - val_loss: 0.4817 - val_accuracy: 0.8611\n",
      "Epoch 107/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5433 - accuracy: 0.8419 - val_loss: 0.4952 - val_accuracy: 0.8544\n",
      "Epoch 108/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5359 - accuracy: 0.8424 - val_loss: 0.4495 - val_accuracy: 0.8673\n",
      "Epoch 109/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5400 - accuracy: 0.8436 - val_loss: 0.4406 - val_accuracy: 0.8701\n",
      "Epoch 110/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5277 - accuracy: 0.8434 - val_loss: 0.4337 - val_accuracy: 0.8707\n",
      "Epoch 111/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5305 - accuracy: 0.8437 - val_loss: 0.4065 - val_accuracy: 0.8833\n",
      "Epoch 112/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5320 - accuracy: 0.8425 - val_loss: 0.4396 - val_accuracy: 0.8748\n",
      "Epoch 113/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5224 - accuracy: 0.8473 - val_loss: 0.4108 - val_accuracy: 0.8797\n",
      "Epoch 114/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5209 - accuracy: 0.8470 - val_loss: 0.5567 - val_accuracy: 0.8314\n",
      "Epoch 115/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5200 - accuracy: 0.8480 - val_loss: 0.4980 - val_accuracy: 0.8569\n",
      "Epoch 116/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5178 - accuracy: 0.8488 - val_loss: 0.3677 - val_accuracy: 0.8951\n",
      "Epoch 117/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5198 - accuracy: 0.8485 - val_loss: 0.4728 - val_accuracy: 0.8610\n",
      "Epoch 118/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5144 - accuracy: 0.8486 - val_loss: 0.3898 - val_accuracy: 0.8846\n",
      "Epoch 119/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5093 - accuracy: 0.8523 - val_loss: 0.4170 - val_accuracy: 0.8800\n",
      "Epoch 120/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5067 - accuracy: 0.8514 - val_loss: 0.4773 - val_accuracy: 0.8575\n",
      "Epoch 121/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5054 - accuracy: 0.8505 - val_loss: 0.4625 - val_accuracy: 0.8664\n",
      "Epoch 122/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4985 - accuracy: 0.8547 - val_loss: 0.5953 - val_accuracy: 0.8284\n",
      "Epoch 123/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.5041 - accuracy: 0.8538 - val_loss: 0.4124 - val_accuracy: 0.8797\n",
      "Epoch 124/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4968 - accuracy: 0.8542 - val_loss: 0.4997 - val_accuracy: 0.8511\n",
      "Epoch 125/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4953 - accuracy: 0.8553 - val_loss: 0.4034 - val_accuracy: 0.8832\n",
      "Epoch 126/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4941 - accuracy: 0.8547 - val_loss: 0.4145 - val_accuracy: 0.8815\n",
      "Epoch 127/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4906 - accuracy: 0.8558 - val_loss: 0.3683 - val_accuracy: 0.8967\n",
      "Epoch 128/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4946 - accuracy: 0.8564 - val_loss: 0.4891 - val_accuracy: 0.8582\n",
      "Epoch 129/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4841 - accuracy: 0.8585 - val_loss: 0.4348 - val_accuracy: 0.8706\n",
      "Epoch 130/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4802 - accuracy: 0.8597 - val_loss: 0.5330 - val_accuracy: 0.8407\n",
      "Epoch 131/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4782 - accuracy: 0.8595 - val_loss: 0.5041 - val_accuracy: 0.8512\n",
      "Epoch 132/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4817 - accuracy: 0.8591 - val_loss: 0.3403 - val_accuracy: 0.9012\n",
      "Epoch 133/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4786 - accuracy: 0.8595 - val_loss: 0.3815 - val_accuracy: 0.8889\n",
      "Epoch 134/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4768 - accuracy: 0.8615 - val_loss: 0.5387 - val_accuracy: 0.8429\n",
      "Epoch 135/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4755 - accuracy: 0.8614 - val_loss: 0.4898 - val_accuracy: 0.8516\n",
      "Epoch 136/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4699 - accuracy: 0.8624 - val_loss: 0.4253 - val_accuracy: 0.8745\n",
      "Epoch 137/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4755 - accuracy: 0.8613 - val_loss: 0.3866 - val_accuracy: 0.8853\n",
      "Epoch 138/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4691 - accuracy: 0.8635 - val_loss: 0.4050 - val_accuracy: 0.8817\n",
      "Epoch 139/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4714 - accuracy: 0.8619 - val_loss: 0.4296 - val_accuracy: 0.8743\n",
      "Epoch 140/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4680 - accuracy: 0.8632 - val_loss: 0.4579 - val_accuracy: 0.8704\n",
      "Epoch 141/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4663 - accuracy: 0.8646 - val_loss: 0.4680 - val_accuracy: 0.8651\n",
      "Epoch 142/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4589 - accuracy: 0.8666 - val_loss: 0.3445 - val_accuracy: 0.9019\n",
      "Epoch 143/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4581 - accuracy: 0.8659 - val_loss: 0.4651 - val_accuracy: 0.8669\n",
      "Epoch 144/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4605 - accuracy: 0.8657 - val_loss: 0.4449 - val_accuracy: 0.8685\n",
      "Epoch 145/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4595 - accuracy: 0.8647 - val_loss: 0.4038 - val_accuracy: 0.8801\n",
      "Epoch 146/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4535 - accuracy: 0.8659 - val_loss: 0.4711 - val_accuracy: 0.8674\n",
      "Epoch 147/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4527 - accuracy: 0.8662 - val_loss: 0.5193 - val_accuracy: 0.8426\n",
      "Epoch 148/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4518 - accuracy: 0.8686 - val_loss: 0.5440 - val_accuracy: 0.8406\n",
      "Epoch 149/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4490 - accuracy: 0.8693 - val_loss: 0.4137 - val_accuracy: 0.8796\n",
      "Epoch 150/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4511 - accuracy: 0.8692 - val_loss: 0.3925 - val_accuracy: 0.8891\n",
      "Epoch 151/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4405 - accuracy: 0.8712 - val_loss: 0.3883 - val_accuracy: 0.8867\n",
      "Epoch 152/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4435 - accuracy: 0.8711 - val_loss: 0.4879 - val_accuracy: 0.8500\n",
      "Epoch 153/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4447 - accuracy: 0.8698 - val_loss: 0.3726 - val_accuracy: 0.8908\n",
      "Epoch 154/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4380 - accuracy: 0.8727 - val_loss: 0.3719 - val_accuracy: 0.8904\n",
      "Epoch 155/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4475 - accuracy: 0.8706 - val_loss: 0.4153 - val_accuracy: 0.8816\n",
      "Epoch 156/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4297 - accuracy: 0.8748 - val_loss: 0.5187 - val_accuracy: 0.8436\n",
      "Epoch 157/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4338 - accuracy: 0.8735 - val_loss: 0.4075 - val_accuracy: 0.8831\n",
      "Epoch 158/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4386 - accuracy: 0.8729 - val_loss: 0.3550 - val_accuracy: 0.8973\n",
      "Epoch 159/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4345 - accuracy: 0.8733 - val_loss: 0.3289 - val_accuracy: 0.9046\n",
      "Epoch 160/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8745 - val_loss: 0.3496 - val_accuracy: 0.9008\n",
      "Epoch 161/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4335 - accuracy: 0.8726 - val_loss: 0.3834 - val_accuracy: 0.8869\n",
      "Epoch 162/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4294 - accuracy: 0.8740 - val_loss: 0.5143 - val_accuracy: 0.8427\n",
      "Epoch 163/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4290 - accuracy: 0.8743 - val_loss: 0.2967 - val_accuracy: 0.9190\n",
      "Epoch 164/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4311 - accuracy: 0.8755 - val_loss: 0.4782 - val_accuracy: 0.8577\n",
      "Epoch 165/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4198 - accuracy: 0.8776 - val_loss: 0.4803 - val_accuracy: 0.8631\n",
      "Epoch 166/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4201 - accuracy: 0.8775 - val_loss: 0.3978 - val_accuracy: 0.8838\n",
      "Epoch 167/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4239 - accuracy: 0.8749 - val_loss: 0.3726 - val_accuracy: 0.8915\n",
      "Epoch 168/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4222 - accuracy: 0.8758 - val_loss: 0.4266 - val_accuracy: 0.8787\n",
      "Epoch 169/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4215 - accuracy: 0.8769 - val_loss: 0.3362 - val_accuracy: 0.9056\n",
      "Epoch 170/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4136 - accuracy: 0.8786 - val_loss: 0.4616 - val_accuracy: 0.8666\n",
      "Epoch 171/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4129 - accuracy: 0.8790 - val_loss: 0.3615 - val_accuracy: 0.8977\n",
      "Epoch 172/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4144 - accuracy: 0.8790 - val_loss: 0.3162 - val_accuracy: 0.9116\n",
      "Epoch 173/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4153 - accuracy: 0.8783 - val_loss: 0.4247 - val_accuracy: 0.8759\n",
      "Epoch 174/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4161 - accuracy: 0.8789 - val_loss: 0.3439 - val_accuracy: 0.9011\n",
      "Epoch 175/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4121 - accuracy: 0.8816 - val_loss: 0.3359 - val_accuracy: 0.9047\n",
      "Epoch 176/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4108 - accuracy: 0.8794 - val_loss: 0.4071 - val_accuracy: 0.8786\n",
      "Epoch 177/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4107 - accuracy: 0.8823 - val_loss: 0.4103 - val_accuracy: 0.8813\n",
      "Epoch 178/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4036 - accuracy: 0.8836 - val_loss: 0.3733 - val_accuracy: 0.8906\n",
      "Epoch 179/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4071 - accuracy: 0.8806 - val_loss: 0.4258 - val_accuracy: 0.8716\n",
      "Epoch 180/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4055 - accuracy: 0.8815 - val_loss: 0.3719 - val_accuracy: 0.8906\n",
      "Epoch 181/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4034 - accuracy: 0.8833 - val_loss: 0.3672 - val_accuracy: 0.8965\n",
      "Epoch 182/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4039 - accuracy: 0.8832 - val_loss: 0.3589 - val_accuracy: 0.8983\n",
      "Epoch 183/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3977 - accuracy: 0.8852 - val_loss: 0.4532 - val_accuracy: 0.8684\n",
      "Epoch 184/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4029 - accuracy: 0.8832 - val_loss: 0.3598 - val_accuracy: 0.8980\n",
      "Epoch 185/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.4014 - accuracy: 0.8844 - val_loss: 0.3369 - val_accuracy: 0.9007\n",
      "Epoch 186/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3968 - accuracy: 0.8844 - val_loss: 0.4182 - val_accuracy: 0.8766\n",
      "Epoch 187/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3909 - accuracy: 0.8853 - val_loss: 0.3707 - val_accuracy: 0.8916\n",
      "Epoch 188/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3961 - accuracy: 0.8843 - val_loss: 0.3378 - val_accuracy: 0.9048\n",
      "Epoch 189/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3933 - accuracy: 0.8850 - val_loss: 0.4483 - val_accuracy: 0.8714\n",
      "Epoch 190/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3970 - accuracy: 0.8856 - val_loss: 0.3703 - val_accuracy: 0.8956\n",
      "Epoch 191/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3917 - accuracy: 0.8856 - val_loss: 0.4708 - val_accuracy: 0.8565\n",
      "Epoch 192/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3927 - accuracy: 0.8868 - val_loss: 0.3334 - val_accuracy: 0.9058\n",
      "Epoch 193/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3827 - accuracy: 0.8899 - val_loss: 0.3962 - val_accuracy: 0.8851\n",
      "Epoch 194/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3879 - accuracy: 0.8872 - val_loss: 0.4755 - val_accuracy: 0.8669\n",
      "Epoch 195/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3846 - accuracy: 0.8880 - val_loss: 0.5220 - val_accuracy: 0.8420\n",
      "Epoch 196/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3896 - accuracy: 0.8857 - val_loss: 0.3828 - val_accuracy: 0.8845\n",
      "Epoch 197/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3872 - accuracy: 0.8869 - val_loss: 0.3827 - val_accuracy: 0.8884\n",
      "Epoch 198/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3763 - accuracy: 0.8907 - val_loss: 0.3658 - val_accuracy: 0.8945\n",
      "Epoch 199/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3816 - accuracy: 0.8883 - val_loss: 0.6202 - val_accuracy: 0.8211\n",
      "Epoch 200/200\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.3831 - accuracy: 0.8887 - val_loss: 0.5308 - val_accuracy: 0.8449\n"
     ]
    }
   ],
   "source": [
    "train, score = basicDeepNN3(200, 6.137809361954345e-05, 5.8364246649734e-05,'relu', 'he_normal',0.01, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1594550245840,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "TrPFBgC_j71h",
    "outputId": "3089458e-3e15-4986-a67f-3ef8c37776b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_test_acc: [0.5280612111091614, 0.8437222242355347], lr: 6.137809361954345e-05, Lambda: 5.8364246649734e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best_test_acc: {0}, lr: {1}, Lambda: {2}\\n\".format( score, 6.027611403498033e-05, 5.8364246649734e-05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnetsljlnCSJ"
   },
   "source": [
    "### It looks like we had to take a detour to find our best parameters till now with the learning rate as 6.027611403498033e-05 and lambda as 5.842026004053601e-05 with alpha as 0.01 in LeakyReLU activation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPoArtcOnaS6"
   },
   "source": [
    "### It is highly likely that we have fallen into a local minima because of which we cannot get into the prestigious 90%+ accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1594550572916,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "FFkZsi-CJjK5"
   },
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "train.model.save('DL3_model.h5')\n",
    "train.model.save_weights('DL3_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1182,
     "status": "ok",
     "timestamp": 1594550575535,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "ZbryvKk2MH2G",
    "outputId": "dab5c7ee-eb99-4798-da9f-fdeb585147d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1685,
     "status": "ok",
     "timestamp": 1594550637127,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "CHT-CBI-JsZ6",
    "outputId": "a36b3c5f-f79b-4986-99e7-9b011053f337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7a3db49a58>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting Accuracy and Loss plots\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].set_title('Accuracy')\n",
    "ax[0].plot(train.history['accuracy'],label='train')\n",
    "ax[0].plot(train.history['val_accuracy'],label='test')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Loss')\n",
    "ax[1].plot(train.history['loss'],label='train')\n",
    "ax[1].plot(train.history['val_loss'],label='test')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzfjXaMkoOjr"
   },
   "source": [
    "## Predicting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3819,
     "status": "ok",
     "timestamp": 1594550668433,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "qVEnUQTXKl2p",
    "outputId": "669b0d8d-4615-416e-8826-d91f8b9f5169"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(Xtest, ytest, 2, 8, train.model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As seen above we are predicting 13 out of 16 images correctly. The ones where we made wrong prediction, we have three whole digits in the image. This might be interferring with the prediction. The correctly predicted images have the true label at the center of the image and the other interferring digits partially present in image but not fully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtmqzevxoS4-"
   },
   "source": [
    "## Generating the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1594550743161,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "Jtj5S-FsJ5su"
   },
   "outputs": [],
   "source": [
    "def classificationReport(y_true,y_pred):\n",
    "    target_names = [f\"Class {i}\" for i in range(10) ]\n",
    "    print(classification_report(y_true,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2494,
     "status": "ok",
     "timestamp": 1594550749989,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "ku7QfLWGJ9Ko"
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(train.model.predict(X_test), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1594550751973,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "jAwKXtrxPDzS",
    "outputId": "de1130df-2d65-44db-b594-fefef0134322"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 7, 2, ..., 7, 9, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1594550767871,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "Umi3HPLMKG5y",
    "outputId": "142118d3-4c8a-49e9-c0ad-ddbe582155f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.84      0.91      0.88      1814\n",
      "     Class 1       0.79      0.91      0.85      1828\n",
      "     Class 2       0.91      0.87      0.89      1803\n",
      "     Class 3       0.76      0.86      0.81      1719\n",
      "     Class 4       0.94      0.86      0.90      1812\n",
      "     Class 5       0.84      0.86      0.85      1768\n",
      "     Class 6       0.93      0.80      0.86      1832\n",
      "     Class 7       0.88      0.91      0.89      1808\n",
      "     Class 8       0.87      0.79      0.83      1812\n",
      "     Class 9       0.88      0.84      0.86      1804\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     18000\n",
      "   macro avg       0.87      0.86      0.86     18000\n",
      "weighted avg       0.87      0.86      0.86     18000\n",
      " samples avg       0.86      0.86      0.86     18000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[15879,   307],\n",
       "        [  156,  1658]],\n",
       "\n",
       "       [[15737,   435],\n",
       "        [  165,  1663]],\n",
       "\n",
       "       [[16046,   151],\n",
       "        [  239,  1564]],\n",
       "\n",
       "       [[15825,   456],\n",
       "        [  248,  1471]],\n",
       "\n",
       "       [[16095,    93],\n",
       "        [  246,  1566]],\n",
       "\n",
       "       [[15935,   297],\n",
       "        [  245,  1523]],\n",
       "\n",
       "       [[16055,   113],\n",
       "        [  366,  1466]],\n",
       "\n",
       "       [[15959,   233],\n",
       "        [  165,  1643]],\n",
       "\n",
       "       [[15982,   206],\n",
       "        [  372,  1440]],\n",
       "\n",
       "       [[15989,   207],\n",
       "        [  296,  1508]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = to_categorical(preds,num_classes=10)\n",
    "classificationReport(y_test,pred)\n",
    "multilabel_confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3181,
     "status": "ok",
     "timestamp": 1594550895929,
     "user": {
      "displayName": "Aishik Sengupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyyVuE-ktBLgY_l6gAXGuzflZipz-yzOwYwZsKBw=s64",
      "userId": "15717739389583515153"
     },
     "user_tz": -330
    },
    "id": "lWmzd5GlKLKl",
    "outputId": "7305e4f8-123a-47ef-b173-23e49c082f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7a377d0908>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test.argmax(axis=1),pred.argmax(axis=1))\n",
    "cm_df = pd.DataFrame(cm,index=[i for i in '0123456789'], columns= [i for i in '0123456789'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylabel('True Value')\n",
    "plt.xlabel('Predicted Value')\n",
    "sns.heatmap(cm_df,annot=True,cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCPQ8Q8NON7D"
   },
   "source": [
    "## So the F1 score lies at 0.86 for the test data. The worst F1 score lies at 0.81 for the digit 3 . The best F1 score lies at 0.90 for the digit 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOCLxAZRFdKWSrB+kBEQ0z2",
   "collapsed_sections": [],
   "name": "NN Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
